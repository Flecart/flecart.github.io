<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>✏Information-Theory on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/information-theory/</link>
    <description>Recent content in ✏Information-Theory on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.143.1</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/information-theory/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Asymptotic Equipartition Property</title>
      <link>https://flecart.github.io/notes/asymptotic-equipartition-property/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/asymptotic-equipartition-property/</guid>
      <description>&lt;p&gt;Sembra essere molto simile a &lt;a href=&#34;https://flecart.github.io/notes/central-limit-theorem-and-law-of-large-numbers&#34;&gt;Central Limit Theorem and Law of Large Numbers&lt;/a&gt; però per &lt;a href=&#34;https://flecart.github.io/notes/entropy&#34;&gt;Entropy&lt;/a&gt;.
This is also called &lt;strong&gt;Shannon&amp;rsquo;s source coding theorem&lt;/strong&gt; see &lt;a href=&#34;https://www.youtube.com/watch?v=0SxJl5G2bp0&amp;list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6&amp;index=3&amp;ab_channel=JakobFoerster&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;enunciato-aep&#34;&gt;Enunciato AEP&lt;/h3&gt;
$$
-\frac{1}{n} \log p(X_{1}, X_{2}, \dots, X_{n}) \to H(X)
$$&lt;p&gt;
in probability (la definizione data in &lt;a href=&#34;https://flecart.github.io/notes/central-limit-theorem-and-law-of-large-numbers#convergence-in-probability&#34;&gt;Central Limit Theorem and Law of Large Numbers#Convergence in probability&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Un modo alternativo per enunciarla è così, segue il metodo in (MacKay 2003).&lt;/p&gt;
$$
\left\lvert  \frac{1}{N} H_{\delta}(X^{N}) - H(x)  \right\rvert \leq \varepsilon
$$&lt;p&gt;Ossia a grandi linee: dato una variabile aleatoria $X$ e $N$ estrazioni della stessa, possiamo comprimere questa sequenza in $NH(X)$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Entropy</title>
      <link>https://flecart.github.io/notes/entropy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/entropy/</guid>
      <description>&lt;p&gt;Questo è stato creato da 1948 Shannon in &lt;a href=&#34;https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf&#34;&gt;(Shannon 1948)&lt;/a&gt;. Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso.&lt;/p&gt;
&lt;h3 id=&#34;introduction-to-entropy&#34;&gt;Introduction to Entropy&lt;/h3&gt;
&lt;h4 id=&#34;the-shannon-information-content&#34;&gt;The Shannon Information Content&lt;/h4&gt;
$$
h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})}
$$&lt;p&gt;
We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://flecart.github.io/notes/kolmogorov-complexity&#34;&gt;Kolmogorov complexity&lt;/a&gt; è un modo diverso per definire la complessità.
Legato è &lt;a href=&#34;https://flecart.github.io/notes/neural-networks#kullback-leibler-divergence&#34;&gt;Neural Networks#Kullback-Leibler Divergence&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Information Theory</title>
      <link>https://flecart.github.io/notes/introduction-to-information-theory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/introduction-to-information-theory/</guid>
      <description>&lt;p&gt;The course will be more about the the quantization, talking about lossless and lossy compression (how many bits will be needed to describe something? This is not a CS course so it will not be so much algorithmically focused course), then we will talk about channel and capacity and DMC things.
Most of the things explained in the Lapidoth course will be theoretical there will be some heavy maths.&lt;/p&gt;
&lt;p&gt;The professor starts with some mathy definitions (not very important, just that the $\mathbb{E}[ \cdot]$ needs a domain to be defined, so notations like $\mathbb{E}[x]$ do not make sense, while $\mathbb{E}[g(x)]$ do make sense because $g(x) : \mathcal{X} \to \mathbb{R}$).&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
