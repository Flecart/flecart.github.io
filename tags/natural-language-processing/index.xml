<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ðŸ’¬Natural-Language-Processing on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/natural-language-processing/</link>
    <description>Recent content in ðŸ’¬Natural-Language-Processing on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Exponential Family</title>
      <link>https://flecart.github.io/notes/the-exponential-family/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/the-exponential-family/</guid>
      <description>This is the generalization of the family of function where Softmax Function belongs. Many many functions are part of this family, most of the distributions that are used in science are part of the exponential family, e.g. beta, Gaussian, Bernoulli, Categorical distribution, Gamma, Beta, Poisson, are all part of the exponential family. The useful thing is the generalization power of this set of functions: if you prove something about this family, you prove it for every distribution that is part of this family.</description>
    </item>
    <item>
      <title>Word Embeddings</title>
      <link>https://flecart.github.io/notes/word-embeddings/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/word-embeddings/</guid>
      <description>One very insightful idea is unsupervised word representation. That is just say take a lot of text and try to model the word representations statistically.
Salton (1975) was one of the first researchers that tried to use the techniques that later will be common in Google searches.
Theory Johnson-Lindenstrauss Lemma This lemma basically says that semantic embedding is possible, without giving a real algorithm to do so. This seems to be a nice resource about this lemma.</description>
    </item>
    <item>
      <title>Introduction to Natural Language Processing</title>
      <link>https://flecart.github.io/notes/introduction-to-natural-language-processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/introduction-to-natural-language-processing/</guid>
      <description>The landscape of NLP was very different in the beginning of the field.
&amp;ldquo;But it must be recognized that the notion &amp;lsquo;probability of a sentence&amp;rsquo; is an entirely useless one, under any known interpretation of this term 1968 p 53. Noam Chomsky.
Probability was not seen very well (Chomsky has said many wrong things indeed), and linguists were considered useless. Recently deep learning and computational papers are ubiquitous in major conferences in linguistics, e.</description>
    </item>
    <item>
      <title>Sentiment Analysis</title>
      <link>https://flecart.github.io/notes/sentiment-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/sentiment-analysis/</guid>
      <description>Sentiment analysis is one of the oldest tasks in natural language processing. In this note we will introduce some examples and terminology, some key problems in the field and a simple model that we can understand by just knowing Backpropagation Log Linear Models and the Softmax Function.
We say:
Polarity: the orientation of the sentiment. Subjectivity: if it expresses personal feelings. See demo
Some applications: Businesses use sentiment analysis to understand if users are happy or not with their product.</description>
    </item>
    <item>
      <title>Bag of words</title>
      <link>https://flecart.github.io/notes/bag-of-words/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/bag-of-words/</guid>
      <description>Bag of words only takes into account the count of the words inside a document, ignoring all the syntax and boundaries. This method is very common for email classifications techniques. We can say bag of words can be some sort of pooling, it&amp;rsquo;s similar to the computer vision analogue. It&amp;rsquo;s difficult to say what is the best method (also a reason why people say NLP is difficult to teach).
Introduction to bag of words Faremo una introduzione di applicazione di NaÃ¯ve Bayes applicato alla classificazione di documenti.</description>
    </item>
    <item>
      <title>Backpropagation</title>
      <link>https://flecart.github.io/notes/backpropagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/backpropagation/</guid>
      <description>Backpropagation is perhaps the most important algorithm of the 21th century. It is used everywhere in machine learning. It has also connected to computing marginal distributions. This is the motivation why all machine learning scientists, all data scientists should understand this algorithm very well. An important observation is that this algorithm is linear: the time complexity is the same as the forward pass. Karpathy has a nice resource for this topic.</description>
    </item>
    <item>
      <title>Log Linear Models</title>
      <link>https://flecart.github.io/notes/log-linear-models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/log-linear-models/</guid>
      <description>Log Linear Models can be considered the most basic model used in natural languages. The main idea is to try to model the correlations of our data, or how the posterior $p(y \mid x)$ varies, where $x$ is our single data point features and $y$ are the labels of interest. This is a form of generalization because contextualized events (x, y) with similar descriptions tend to have similar probabilities.
These kinds of models are so common that it has been discovered in many fields (and thus assuming different names): some of the most famous are Gibbs distributions, undirected graphical models, Markov Random Fields or Conditional Random Fields, exponential models, and (regularized) maximum entropy models.</description>
    </item>
    <item>
      <title>Part of Speech Tagging</title>
      <link>https://flecart.github.io/notes/part-of-speech-tagging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/part-of-speech-tagging/</guid>
      <description>What is a part of Speech? A part of speech (POS) is a category of words that display similar syntactic behavior, i.e., they play similar roles within the grammatical structure of sentences. It has been known since the Latin era that some categories of words behave similarly (verbs for declination for example).
The intuitive take is that knowing a specific part of speech can help understand the meaning of the sentence.</description>
    </item>
    <item>
      <title>Recurrent Neural Networks</title>
      <link>https://flecart.github.io/notes/recurrent-neural-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/recurrent-neural-networks/</guid>
      <description>Recurrent Neural Networks allows us to model arbitrarily long sequence dependencies, at least in theory. This is very handy, and has many interesting theoretical implication. But here we are also interested in the practical applicability, so we may need to analyze common architectures used to implement these models, the main limitation and drawbacks, the nice properties and some applications.
A simple theoretical motivation Recall the content presented in Language Models (this is a hard prerequisite): we want an efficient way to model $p(y \mid \boldsymbol{y}_{</description>
    </item>
    <item>
      <title>Language Models</title>
      <link>https://flecart.github.io/notes/language-models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/language-models/</guid>
      <description>In order to understand language models we need to understand structured prediction. If you are familiar with Sentiment Analysis, where given an input text we need to classify it in a binary manner, in this case the output space usually scales in an exponential manner. The output has some structure, for example it could be a tree, it could be a set of words etc&amp;hellip; This usually needs an intersection between statistics and computer science.</description>
    </item>
  </channel>
</rss>
