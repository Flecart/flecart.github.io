<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>üí¨Natural-Language-Processing on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/natural-language-processing/</link>
    <description>Recent content in üí¨Natural-Language-Processing on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.143.1</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Backpropagation</title>
      <link>https://flecart.github.io/notes/backpropagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/backpropagation/</guid>
      <description>&lt;p&gt;Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well.
An important observation is that this algorithm is &lt;strong&gt;linear&lt;/strong&gt;: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover. See &lt;a href=&#34;https://colah.github.io/posts/2015-08-Backprop/&#34;&gt;colah&amp;rsquo;s blog&lt;/a&gt;.
&lt;a href=&#34;https://youtu.be/VMj-3S1tku0?si=wRCObFw7woZTwU56&#34;&gt;Karpathy&lt;/a&gt; has a nice resource for this topic too!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dependency Parsing</title>
      <link>https://flecart.github.io/notes/dependency-parsing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/dependency-parsing/</guid>
      <description>&lt;p&gt;This set of note is still in TODO&lt;/p&gt;
&lt;p&gt;Dependency Grammar has been much bigger in Europe compared to USA, where Chomsky&amp;rsquo;s grammars ruled. One of the main developers of this theory is Lucien Tesni√®re (1959):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ÄúThe sentence is an organized whole, the constituent elements of which are words. Every word that
belongs to a sentence ceases by itself to be isolated as in the dictionary. Between the word and its neighbors, the mind perceives connections, the totality of which forms the structure of the sentence.
The structural connections establish dependency relations between the words. Each connection in principle unites a superior term and an inferior term. The superior term receives the name governor (head). The inferior term receives the name subordinate (dependent).‚Äù ~&lt;em&gt;Lucien Tesni√®re&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Natural Language Processing</title>
      <link>https://flecart.github.io/notes/introduction-to-natural-language-processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/introduction-to-natural-language-processing/</guid>
      <description>&lt;p&gt;The landscape of NLP was very different in the beginning of the field.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;But it must be recognized that the notion &amp;lsquo;probability of a sentence&amp;rsquo; is an entirely useless one, under any known interpretation of this term 1968 p 53. &lt;em&gt;Noam Chomsky&lt;/em&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Probability was not seen very well (Chomsky has said many wrong things indeed), and linguists were considered useless. Recently deep learning and computational papers are ubiquitous in major conferences in linguistics, e.g. ACL.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Language Models</title>
      <link>https://flecart.github.io/notes/language-models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/language-models/</guid>
      <description>&lt;p&gt;In order to understand language models we need to understand &lt;strong&gt;structured prediction&lt;/strong&gt;. If you are familiar with &lt;a href=&#34;https://flecart.github.io/notes/sentiment-analysis&#34;&gt;Sentiment Analysis&lt;/a&gt;, where given an input text we need to classify it in a binary manner, in this case the output space usually scales in an &lt;em&gt;exponential&lt;/em&gt; manner. The output has some structure, for example it could be a tree, it could be a set of words etc&amp;hellip; This usually needs an intersection between statistics and computer science.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Log Linear Models</title>
      <link>https://flecart.github.io/notes/log-linear-models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/log-linear-models/</guid>
      <description>&lt;p&gt;Log Linear Models can be considered the most basic model used in natural languages. The main idea is to try to model the correlations of our data, or how the posterior $p(y \mid x)$ varies, where $x$ is our single data point features and $y$ are the labels of interest. This is a &lt;em&gt;form of generalization&lt;/em&gt; because contextualized events (x, y) with similar descriptions tend to have similar probabilities.&lt;/p&gt;
&lt;p&gt;These kinds of models are so common that it has been discovered in many fields (and thus assuming different names): some of the most famous are Gibbs distributions, undirected graphical models, Markov Random Fields or Conditional Random Fields, exponential models, and (regularized) maximum entropy models. Special cases include logistic regression and Boltzmann machines.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Part of Speech Tagging</title>
      <link>https://flecart.github.io/notes/part-of-speech-tagging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/part-of-speech-tagging/</guid>
      <description>&lt;h4 id=&#34;what-is-a-part-of-speech&#34;&gt;What is a part of Speech?&lt;/h4&gt;
&lt;p&gt;A part of speech (POS) is a &lt;strong&gt;category of words that display similar syntactic behavior&lt;/strong&gt;, i.e.,
they play similar roles within the grammatical structure of sentences. It has been known since the Latin era that some categories of words behave similarly (verbs for declination for example).&lt;/p&gt;
&lt;p&gt;The intuitive take is that knowing a specific part of speech can help understand the &lt;strong&gt;meaning&lt;/strong&gt; of the sentence.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Probabilistic Parsing</title>
      <link>https://flecart.github.io/notes/probabilistic-parsing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/probabilistic-parsing/</guid>
      <description>&lt;h2 id=&#34;language-constituents&#34;&gt;Language Constituents&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;A constituent is a word or a group of words that function as a single unit within a
hierarchical structure&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is because there is a lot of evidence pointing towards an hierarchical organization of human language.&lt;/p&gt;
&lt;h4 id=&#34;example-of-constituents&#34;&gt;Example of constituents&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s have some examples:
John speaks [Spanish] fluently
John speaks [Spanish and French] fluently&lt;/p&gt;
&lt;p&gt;Mary programs the homework [in the ETH computer laboratory]
Mary programs the homework [in the laboratory]&lt;/p&gt;</description>
    </item>
    <item>
      <title>Semirings</title>
      <link>https://flecart.github.io/notes/semirings/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/semirings/</guid>
      <description>&lt;p&gt;Semirings allow us to generalize many many common operations. One of the most powerful usages is the algebraic view of dynamic programming.&lt;/p&gt;
&lt;h3 id=&#34;definition-of-a-semiring&#34;&gt;Definition of a semiring&lt;/h3&gt;
&lt;p&gt;A semiring is a 5-tuple $R = (A, \oplus, \otimes, \bar{0}, \bar{1})$ such that.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$(A, \oplus, \bar{0})$ is a commutative monoid&lt;/li&gt;
&lt;li&gt;$(A, \otimes, \bar{1})$ is a monoid&lt;/li&gt;
&lt;li&gt;$\otimes$ distributes over $\oplus$.&lt;/li&gt;
&lt;li&gt;$\bar{0}$ is annihilator for $\otimes$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;monoid&#34;&gt;Monoid&lt;/h4&gt;
&lt;p&gt;Let $K, \oplus$ be a set and a operation, then:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sentiment Analysis</title>
      <link>https://flecart.github.io/notes/sentiment-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/sentiment-analysis/</guid>
      <description>&lt;p&gt;Sentiment analysis is one of the oldest tasks in natural language processing. In this note we will introduce some examples and terminology, some key problems in the field and a simple model that we can understand by just knowing &lt;a href=&#34;https://flecart.github.io/notes/backpropagation&#34;&gt;Backpropagation&lt;/a&gt; &lt;a href=&#34;https://flecart.github.io/notes/log-linear-models&#34;&gt;Log Linear Models&lt;/a&gt; and the &lt;a href=&#34;https://flecart.github.io/notes/softmax-function&#34;&gt;Softmax Function&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We say:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Polarity:&lt;/strong&gt; the orientation of the sentiment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subjectivity:&lt;/strong&gt; if it expresses personal feelings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See &lt;a href=&#34;http://text-processing.com/demo/sentiment&#34;&gt;demo&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;some-applications&#34;&gt;Some applications:&lt;/h4&gt;
&lt;p&gt;Businesses use sentiment analysis to understand if users are happy or not with their product. It&amp;rsquo;s linked to revenue: if the reviews are good, usually you make more money. But companies can&amp;rsquo;t read every review, so they want automatic methods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Softmax Function</title>
      <link>https://flecart.github.io/notes/softmax-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/softmax-function/</guid>
      <description>&lt;p&gt;Softmax is one of the most important functions for neural networks. It also has some interesting properties that we list here. This function is part of &lt;a href=&#34;https://flecart.github.io/notes/the-exponential-family&#34;&gt;The Exponential Family&lt;/a&gt;, one can also see that the sigmoid function is a particular case of this softmax, just two variables.
Sometimes this could be seen as a relaxation of the action potential inspired by neuroscience (See &lt;a href=&#34;https://flecart.github.io/notes/the-neuron&#34;&gt;The Neuron&lt;/a&gt; for a little bit more about neurons). This is because we need &lt;strong&gt;differentiable&lt;/strong&gt;, for gradient descent. The action potential is an all or nothing thing.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Exponential Family</title>
      <link>https://flecart.github.io/notes/the-exponential-family/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/the-exponential-family/</guid>
      <description>&lt;p&gt;This is the generalization of the family of function where &lt;a href=&#34;https://flecart.github.io/notes/softmax-function&#34;&gt;Softmax Function&lt;/a&gt; belongs. Many many functions are part of this family, most of the distributions that are used in science are part of the exponential family, e.g. beta, Gaussian, Bernoulli, Categorical distribution, Gamma, Beta, Poisson, are all part of the exponential family.
The useful thing is the generalization power of this set of functions: if you prove something about this family, you prove it for every distribution that is part of this family.
This family of functions is also closely linked too Generalized Linear Models (GLMs).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transliteration systems</title>
      <link>https://flecart.github.io/notes/transliteration-systems/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/transliteration-systems/</guid>
      <description>&lt;p&gt;This note is still a TODO.&lt;/p&gt;
&lt;p&gt;Transliteration is learning learning a function to map strings in one character set to strings in another character set.
The basic example is in &lt;strong&gt;multilingual&lt;/strong&gt; applications, where it is needed to have the same string written in different languages.&lt;/p&gt;
&lt;p&gt;The goal is to develop a probabilistic model that can map strings from
input vocabulary $\Sigma$ to an output vocabulary $\Omega$.&lt;/p&gt;
&lt;p&gt;We will extend the concepts presented in &lt;a href=&#34;https://flecart.github.io/notes/automi-e-regexp&#34;&gt;Automi e Regexp&lt;/a&gt; for Finite state automata  to a weighted version. You will also need knowledge from &lt;a href=&#34;https://flecart.github.io/notes/descrizione-linguaggio&#34;&gt;Descrizione linguaggio&lt;/a&gt; for definitions of alphabets and strings, Kleene Star operations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bag of words</title>
      <link>https://flecart.github.io/notes/bag-of-words/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/bag-of-words/</guid>
      <description>&lt;p&gt;Bag of words only takes into account the &lt;strong&gt;count&lt;/strong&gt; of the words inside a document, ignoring all the syntax and boundaries. This method is very common for email classifications techniques.
We can say bag of words can be some sort of &lt;strong&gt;pooling&lt;/strong&gt;, it&amp;rsquo;s similar to the computer vision analogue.
It&amp;rsquo;s difficult to say what is the best method (also a reason why people say NLP is difficult to teach).&lt;/p&gt;
&lt;h3 id=&#34;introduction-to-bag-of-words&#34;&gt;Introduction to bag of words&lt;/h3&gt;
&lt;p&gt;Faremo una introduzione di applicazione di &lt;a href=&#34;https://flecart.github.io/notes/na√Øve-bayes&#34;&gt;Na√Øve Bayes&lt;/a&gt; applicato alla classificazione di documenti.
&lt;img src=&#34;https://flecart.github.io/images/notes/Bag of words-20240907182135341.webp&#34; style=&#34;width: 100%&#34; class=&#34;center&#34; alt=&#34;Bag of words-20240907182135341&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
