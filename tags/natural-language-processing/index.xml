<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ðŸ’¬Natural-Language-Processing on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/natural-language-processing/</link>
    <description>Recent content in ðŸ’¬Natural-Language-Processing on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Backpropagation</title>
      <link>https://flecart.github.io/notes/backpropagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/backpropagation/</guid>
      <description>Backpropagation is perhaps the most important algorithm of the 21th century. It is used everywhere in machine learning. It has also connected to computing marginal distributions. This is the motivation why all machine learning scientists, all data scientists should understand this algorithm very well. An important observation is that this algorithm is linear: the time complexity is the same as the forward pass. Karpathy has a nice resource for this topic.</description>
    </item>
    <item>
      <title>Bag of words</title>
      <link>https://flecart.github.io/notes/bag-of-words/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/bag-of-words/</guid>
      <description>Bag of words only takes into account the count of the words inside a document, ignoring all the syntax and boundaries. This method is very common for email classifications techniques. We can say bag of words can be some sort of pooling, it&amp;rsquo;s similar to the computer vision analogue. It&amp;rsquo;s difficult to say what is the best method (also a reason why people say NLP is difficult to teach).
Introduzione a bag of words Faremo una introduzione di applicazione di NaÃ¯ve Bayes applicato alla classificazione di documenti.</description>
    </item>
    <item>
      <title>Word Embeddings</title>
      <link>https://flecart.github.io/notes/word-embeddings/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/word-embeddings/</guid>
      <description>One very insightful idea is unsupervised word representation. That is just say take a lot of text and try to model the word representations statistically.
Salton (1975) was one of the first researchers that tried to use the techniques that later will be common in Google searches.
Theory Johnson-Lindenstrauss Lemma This lemma basically says that semantic embedding is possible, without giving a real algorithm to do so. This seems to be a nice resource about this lemma.</description>
    </item>
  </channel>
</rss>
