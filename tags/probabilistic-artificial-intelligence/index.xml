<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>➕Probabilistic-Artificial-Intelligence on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/probabilistic-artificial-intelligence/</link>
    <description>Recent content in ➕Probabilistic-Artificial-Intelligence on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gaussians</title>
      <link>https://flecart.github.io/notes/gaussians/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/gaussians/</guid>
      <description>aussians are one of the most important probability distributions. They arise naturally in the law of large numbers and have some nice properties that we will briefly present and prove here in this note. They are also quite common for Gaussian Processes and the Expectation Maximization algorithm. The best thing if you want to learn this part actually well is section 2.3 of (Bishop 2006), so go there my friend :)</description>
    </item>
    <item>
      <title>Markov Chains</title>
      <link>https://flecart.github.io/notes/markov-chains/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/markov-chains/</guid>
      <description>Introduzione alle catene di Markov La proprietà di Markov Una sequenza di variabili aleatorie $X_{1}, X_{2}, X_{3}, \dots$ gode della proprietà di Markov se vale:
$$ P(X_{n}| X_{n - 1}, X_{n - 2}, \dots, X_{1}) = P(X_{n}|X_{n-1}) $$ Ossia posso scordarmi tutta la storia precedente, mi interessa solamente lo stato precedente per sapere la probabilità attuale.
Da un punto di vista filosofico/fisico, ha senso perché mi sta dicendo che posso predire lo stato successivo se ho una conoscenza (completa, (lo dico io completo, originariamente non esiste)) del presente.</description>
    </item>
    <item>
      <title>Reinforcement Learning, a introduction</title>
      <link>https://flecart.github.io/notes/reinforcement-learning-a-introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/reinforcement-learning-a-introduction/</guid>
      <description>The main difference between reinforcement learning and other machine learning, pattern inference methods is that reinforcement learning takes the concept of actions into its core: models developed in this field can be actively developed to have an effect in its environment, while other methods are mainly used to summarize interesting data or generating sort of reports.
Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward.</description>
    </item>
    <item>
      <title>N-Bandit Problem</title>
      <link>https://flecart.github.io/notes/n-bandit-problem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/n-bandit-problem/</guid>
      <description>Impostazione del problema Supponiamo di stare giocando a n slot machine contemporaneamente. Queste macchine hanno internamente un valore di reward che non conosciamo. Ad ogni step possiamo scegliere una singola macchina e andare a tirare la sua leva. Riceviamo il valore del reward nascosto con un pò di rumore. Vogliamo capire nel lungo quale sia la strategia che possa dare migliore reward medio possibile.
Questo è un semplice problema, ma lo possiamo considerare un fulcro molto importante per poter comprendere meglio reinforcement learning.</description>
    </item>
    <item>
      <title>Bayesian Linear Regression</title>
      <link>https://flecart.github.io/notes/bayesian-linear-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/bayesian-linear-regression/</guid>
      <description>We have a prior $p(\text{model})$, we have a posterior $p(\text{model} \mid \text{data})$, a likelihood $p(\text{data} \mid \text{model})$ and $p(\text{data})$ is called the evidence.
Classical Linear regression Let&amp;rsquo;s start with a classical regression. In this setting we need to estimate a model that is generated from this kind of data: $$ y = w^{T}x + \varepsilon $$ Where $\varepsilon \sim \mathcal{N}(0, \sigma_{n}^{2}I)$ and it&amp;rsquo;s the irreducible noise, an error that cannot be eliminated by any model in the model class, this is also called aleatoric uncertainty.</description>
    </item>
    <item>
      <title>Maximum Entropy Principle</title>
      <link>https://flecart.github.io/notes/maximum-entropy-principle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/maximum-entropy-principle/</guid>
      <description>The maximum entropy principle is one of the most important guiding motives in artificial artificial intelligence. Its roots emerge from a long tradition of probabilistic inference that goes back to Laplace and Occam&amp;rsquo;s Razor, i.e. the principle of parsimony.
Let&amp;rsquo;s start with a simple example taken from Andreas Kraus&amp;rsquo;s Lecture notes in the ETH course of Probabilistic Artificial Intelligence:
Consider a criminal trial with three suspects, A, B, and C. The collected evidence shows that suspect C can not have committed the crime, however it does not yield any information about sus- pects A and B.</description>
    </item>
    <item>
      <title>Kalman Filters</title>
      <link>https://flecart.github.io/notes/kalman-filters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/kalman-filters/</guid>
      <description>Kalman Filters are defined as follows:
We start with a variable $X_{0} \sim \mathcal{N}(\mu, \Sigma)$, then we have a motion model and a sensor model:
$$ \begin{cases} X_{t + 1} = FX_{t} + \varepsilon_{t} &amp; F \in \mathbb{R}^{d\times d}, \varepsilon_{t} \sim \mathcal{N}(0, \Sigma_{x})\\ Y_{t} = HX_{t} + \eta_{t} &amp; H \in \mathbb{R}^{m \times d}, \eta_{t} \sim \mathcal{N}(0, \Sigma_{y}) \end{cases} $$ Inference is just doing things with the Gaussians. One can interpret the $Y$ to be the observations and $X$ to be the underlying beliefs about a certain state.</description>
    </item>
    <item>
      <title>Variational Inference</title>
      <link>https://flecart.github.io/notes/variational-inference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/variational-inference/</guid>
      <description>With variational inference we want to find a good approximation of the posterior distribution from which it is easy to sample. The objective is to approximate the posterior with a simpler one, because sometimes the prior or likelihood are difficult to compute. $$ p(\theta \mid x_{1:n}, y_{1:n}) = \frac{1}{z} p(y_{1:n} \mid \theta, x_{1:n}) p(\theta \mid x_{1:n}) \approx q(\theta \mid \lambda) $$ For Bayesian Linear Regression we had high dimensional Gaussians which made the inference closed form, in general this is not true, so we need some kinds of approximation.</description>
    </item>
    <item>
      <title>Monte Carlo Methods</title>
      <link>https://flecart.github.io/notes/monte-carlo-methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/monte-carlo-methods/</guid>
      <description>DI Law of Large Numbers e Central limit theorem ne parliamo in Central Limit Theorem and Law of Large Numbers. Usually these methods are useful when you need to calculate following something similar to Bayes rule, but don&amp;rsquo;t know how to calculate the denominator, often infeasible integral. We estimate this value without explicitly calculating that.
Interested in $\mathbb{P}(x) = \frac{1}{z} \mathbb{P}^{*}(x) = \frac{1}{Z} e^{-E(x)}$ Can evaluate E(x) at any x.</description>
    </item>
    <item>
      <title>Active Learning</title>
      <link>https://flecart.github.io/notes/active-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/active-learning/</guid>
      <description>Active Learning concerns methods to decide how to sample the most useful information in a specific domain; how can you select the best sample for an unknown model? In this setting, we are interested in the concept of usefulness of information. One of our main goals is to reduce uncertainty, thus, Entropy-based (mutual information) methods are often used. For example, we can use active learning to choose what samples needs to be labelled in order to have highest accuracy on the trained model, when labelling is costly.</description>
    </item>
    <item>
      <title>Gaussian Processes</title>
      <link>https://flecart.github.io/notes/gaussian-processes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/gaussian-processes/</guid>
      <description>Gaussian processes can be viewed through a Bayesian lens of the function space: rather than sampling over individual data points, we are now sampling over entire functions. They extend the idea of bayesian linear regression by introducing an infinite number of feature functions for the input XXX.
In geostatistics, Gaussian processes are referred to as kriging regressions, and many other models, such as Kalman Filters or radial basis function networks, can be understood as special cases of Gaussian processes.</description>
    </item>
    <item>
      <title>On intuitive notions of probability</title>
      <link>https://flecart.github.io/notes/on-intuitive-notions-of-probability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/on-intuitive-notions-of-probability/</guid>
      <description>This note will mainly attempt to summarize the introduction of some intuitive notions of probability used in common sense human reasoning. Most of what is said here is available here (Jaynes 2003).
Three intuitive notions of probability Jaynes presents some forms of inference that are not possible in classical first order or propositional logic, yet they are frequent in human common sense reasoning. Let&amp;rsquo;s present some rules and some examples along them:</description>
    </item>
    <item>
      <title>Kernel Methods</title>
      <link>https://flecart.github.io/notes/kernel-methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/kernel-methods/</guid>
      <description>As we will briefly see, Kernels will have an important role in many machine learning applications. In this note we will get to know what are Kernels and why are they useful. Intuitively they measure the similarity between two input points. So if they are close the kernel should be big, else it should be small.
We briefly state the requirements of a Kernel, then we will argue with a simple example why they are useful.</description>
    </item>
  </channel>
</rss>
