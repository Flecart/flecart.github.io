<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>➕Probabilistic-Artificial-Intelligence on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/probabilistic-artificial-intelligence/</link>
    <description>Recent content in ➕Probabilistic-Artificial-Intelligence on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>On intuitive notions of probability</title>
      <link>https://flecart.github.io/notes/on-intuitive-notions-of-probability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/on-intuitive-notions-of-probability/</guid>
      <description>This note will mainly attempt to summarize the introduction of some intuitive notions of probability used in common sense human reasoning. Most of what is said here is available here (Jaynes 2003).
Three intuitive notions of probability Jaynes presents some forms of inference that are not possible in classical first order or propositional logic, yet they are frequent in human common sense reasoning. Let&amp;rsquo;s present some rules and some examples along them:</description>
    </item>
    <item>
      <title>Gaussians</title>
      <link>https://flecart.github.io/notes/gaussians/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/gaussians/</guid>
      <description>Gaussians are one of the most important probability distributions. They arise naturally in the law of large numbers and have some nice properties that we will briefly present and prove here in this note. They are also quite common for Gaussian Processes and the Expectation Maximization algorithm. The best thing if you want to learn this part actually well is section 2.3 of (Bishop 2006), so go there my friend :)</description>
    </item>
    <item>
      <title>Maximum Entropy Principle</title>
      <link>https://flecart.github.io/notes/maximum-entropy-principle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/maximum-entropy-principle/</guid>
      <description>The maximum entropy principle is one of the most important guiding motives in artificial artificial intelligence. Its roots emerge from a long tradition of probabilistic inference that goes back to Laplace and Occam&amp;rsquo;s Razor, i.e. the principle of parsimony.
Let&amp;rsquo;s start with a simple example taken from Andreas Kraus&amp;rsquo;s Lecture notes in the ETH course of Probabilistic Artificial Intelligence:
Consider a criminal trial with three suspects, A, B, and C. The collected evidence shows that suspect C can not have committed the crime, however it does not yield any information about sus- pects A and B.</description>
    </item>
    <item>
      <title>Bayesian learning</title>
      <link>https://flecart.github.io/notes/bayesian-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/bayesian-learning/</guid>
      <description>We have a prior $p(\text{model})$, we have a posterior $p(\text{model} \mid \text{data})$, a likelihood $p(\text{data} \mid \text{model})$ and $p(\text{data})$ is called the evidence.
Recall: ridge regression With linear regression we assume that $y \approx w^{T}x = f(x;w) = f_{w}(x)$. And usually we have some regularization thing that prevents the overfitting.
For example a loss function could be: $$ \hat{w} = \arg \min_{w\in \mathbb{R}^{d}} \sum_{i = 1}^{n} (y_{i} - w^{T}x_{i})^{2} + \lambda \lVert w \rVert ^{2}_{2} $$ This is called ridge regression.</description>
    </item>
  </channel>
</rss>
