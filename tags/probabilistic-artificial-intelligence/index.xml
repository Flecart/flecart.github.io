<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>➕Probabilistic-Artificial-Intelligence on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/probabilistic-artificial-intelligence/</link>
    <description>Recent content in ➕Probabilistic-Artificial-Intelligence on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Maximum Entropy Principle</title>
      <link>https://flecart.github.io/notes/maximum-entropy-principle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/maximum-entropy-principle/</guid>
      <description>The maximum entropy principle is one of the most important guiding motives in artificial artificial intelligence. Its roots emerge from a long tradition of probabilistic inference that goes back to Laplace and Occam&amp;rsquo;s Razor, i.e. the principle of parsimony.
Let&amp;rsquo;s start with a simple example taken from Andreas Kraus&amp;rsquo;s Lecture notes in the ETH course of Probabilistic Artificial Intelligence:
Consider a criminal trial with three suspects, A, B, and C. The collected evidence shows that suspect C can not have committed the crime, however it does not yield any information about sus- pects A and B.</description>
    </item>
    <item>
      <title>Kalman Filters</title>
      <link>https://flecart.github.io/notes/kalman-filters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/kalman-filters/</guid>
      <description>Kalman Filters are defined as follows:
We start with a variable $X_{0} \sim \mathcal{N}(\mu, \Sigma)$, then we have a motion model and a sensor model:
$$ \begin{cases} X_{t + 1} = FX_{t} + \varepsilon_{t} &amp; F \in \mathbb{R}^{d\times d}, \varepsilon_{t} \sim \mathcal{N}(0, \Sigma_{x})\\ Y_{t} = HX_{t} + \eta_{t} &amp; H \in \mathbb{R}^{m \times d}, \eta_{t} \sim \mathcal{N}(0, \Sigma_{y}) \end{cases} $$ Inference is just doing things with the Gaussians. One can interpret the $Y$ to be the observations and $X$ to be the underlying beliefs about a certain state.</description>
    </item>
    <item>
      <title>Gaussians</title>
      <link>https://flecart.github.io/notes/gaussians/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/gaussians/</guid>
      <description>Gaussians are one of the most important probability distributions. They arise naturally in the law of large numbers and have some nice properties that we will briefly present and prove here in this note. They are also quite common for Gaussian Processes and the Expectation Maximization algorithm. The best thing if you want to learn this part actually well is section 2.3 of (Bishop 2006), so go there my friend :)</description>
    </item>
    <item>
      <title>The Kernel Trick</title>
      <link>https://flecart.github.io/notes/the-kernel-trick/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/the-kernel-trick/</guid>
      <description>As we will briefly see, Kernels will have an important role in many machine learning applications. In this note we will get to know what are Kernels and why are they useful.
We briefly state the requirements of a Kernel, then we will argue with a simple example why they are useful.
Kernel Function requirements Every function $k$ must be
Symmetric: $\forall x, x&#39; \in \mathbb{X}$ we have $k(x, x&#39;) = k(x&#39;, x)$ Positive definiteness For all $A$ we have $K_{A A}$ is positive definite.</description>
    </item>
    <item>
      <title>Gaussian Processes</title>
      <link>https://flecart.github.io/notes/gaussian-processes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/gaussian-processes/</guid>
      <description>Gaussian processes are similar to bayesian linear regression with the extension that we are using now infinite feature functions of $X$. Gaussian processes are known as kriging regressions in geostatistics, and many different models can be seen as a special case of Gaussian Processes, for instance Kalman Filters, or radial basis function networks. In this case, some functions are more probable than others. We want to model this fact. We still have epistemic uncertainty with this model, and it is still a tractable model.</description>
    </item>
    <item>
      <title>On intuitive notions of probability</title>
      <link>https://flecart.github.io/notes/on-intuitive-notions-of-probability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/on-intuitive-notions-of-probability/</guid>
      <description>This note will mainly attempt to summarize the introduction of some intuitive notions of probability used in common sense human reasoning. Most of what is said here is available here (Jaynes 2003).
Three intuitive notions of probability Jaynes presents some forms of inference that are not possible in classical first order or propositional logic, yet they are frequent in human common sense reasoning. Let&amp;rsquo;s present some rules and some examples along them:</description>
    </item>
    <item>
      <title>Bayesian Linear Regression</title>
      <link>https://flecart.github.io/notes/bayesian-linear-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/bayesian-linear-regression/</guid>
      <description>We have a prior $p(\text{model})$, we have a posterior $p(\text{model} \mid \text{data})$, a likelihood $p(\text{data} \mid \text{model})$ and $p(\text{data})$ is called the evidence.
Classical Linear regression Let&amp;rsquo;s start with a classical regression. In this setting we need to estimate a model that is generated from this kind of data: $$ y = w^{T}x + \varepsilon $$ Where $\varepsilon \sim \mathcal{N}(0, \sigma_{n}^{2}I)$ and it&amp;rsquo;s the irreducible noise, an error that cannot be eliminated by any model in the model class, this is also called aleatoric uncertainty.</description>
    </item>
  </channel>
</rss>
