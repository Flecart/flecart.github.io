<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine-Perception on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/machine-perception/</link>
    <description>Recent content in Machine-Perception on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.143.1</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/machine-perception/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Autoregressive Modelling</title>
      <link>https://flecart.github.io/notes/autoregressive-modelling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/autoregressive-modelling/</guid>
      <description>&lt;h3 id=&#34;on-autoregressivity&#34;&gt;On Autoregressivity&lt;/h3&gt;
&lt;p&gt;The main idea of autoregressivity is to use previous prediction to predict the next state.&lt;/p&gt;
&lt;h4 id=&#34;the-autoregressive-property&#34;&gt;The Autoregressive property&lt;/h4&gt;
&lt;p&gt;Autoregressive models model a joint distribution of aleatoric variables by assuming a chain rule like decomposition:&lt;/p&gt;
$$
p(x) = \prod_{i=1}^{n} p(x_i | x_{1:i-1})
$$&lt;p&gt;
If we assume independence between the variables, we don&amp;rsquo;t need many variables to model it $2T$, but this assumption is too strong.
If we just use a tabular approach, we&amp;rsquo;ll have a combinatorial explosion: we will have about $2^{T - 1}$ possible states (if we assume the aleatoric variables are binary, and we are creating a table for each intermediate variable).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Convolutional NN</title>
      <link>https://flecart.github.io/notes/convolutional-nn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/convolutional-nn/</guid>
      <description>&lt;h2 id=&#34;introduction-to-convolutional-nn&#34;&gt;Introduction to convolutional NN&lt;/h2&gt;
&lt;h4 id=&#34;design-goals&#34;&gt;Design Goals&lt;/h4&gt;
&lt;p&gt;We want to be invariant to some transformations but also at the same time to be specific to some thing&lt;/p&gt;
&lt;h3 id=&#34;the-convolution-operator--&#34;&gt;The convolution operator ðŸŸ©-&lt;/h3&gt;
$$
\sum_{i} \sum_{j} h(x - i, y - j) f(i, j)
$$&lt;p&gt;Il prodotto di convoluzione Ã¨ matematicamente molto contorto, anche se nella pratica Ã¨ una cosa molto molto semplice. In pratica voglio calcolare il valore di un pixel in funzione di certi suoi vicini, moltiplicati per un &lt;strong&gt;filter&lt;/strong&gt; che in pratica Ã¨ una matrice di pesi, che definisce un pattern lineare a cui sarei interessato di cercare nellâ€™immagine.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Normalizing Flows</title>
      <link>https://flecart.github.io/notes/normalizing-flows/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/normalizing-flows/</guid>
      <description>&lt;p&gt;Normalizing flows have both  latent space and can produce tractable &lt;strong&gt;explicit&lt;/strong&gt; probability distributions (closer to &lt;a href=&#34;https://flecart.github.io/notes/autoregressive-modelling&#34;&gt;Autoregressive Modelling&lt;/a&gt;)
The intuition here is that we can both have a latent space and some sort of autoregressive modelling.
We want&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An analytical model that is easy to sample from.&lt;/li&gt;
&lt;li&gt;We want this distribution to be able to represent complex data.&lt;/li&gt;
&lt;li&gt;The idea is to use many change of variables.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Recurrent Neural Networks</title>
      <link>https://flecart.github.io/notes/recurrent-neural-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/recurrent-neural-networks/</guid>
      <description>&lt;p&gt;Recurrent Neural Networks allows us to model &lt;em&gt;arbitrarily long&lt;/em&gt; sequence dependencies, at least in theory. This is very handy, and has many interesting theoretical implication. But here we are also interested in the practical applicability, so we may need to analyze common architectures used to implement these models, the main limitation and drawbacks, the nice properties and some applications.&lt;/p&gt;
&lt;p&gt;These network can bee seen as &lt;strong&gt;chaotic&lt;/strong&gt; systems (non-linear dynamical systems), see Introduction to Chaos Theory.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Backpropagation</title>
      <link>https://flecart.github.io/notes/backpropagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/backpropagation/</guid>
      <description>&lt;p&gt;Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well.
An important observation is that this algorithm is &lt;strong&gt;linear&lt;/strong&gt;: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover. See &lt;a href=&#34;https://colah.github.io/posts/2015-08-Backprop/&#34;&gt;colah&amp;rsquo;s blog&lt;/a&gt;.
&lt;a href=&#34;https://youtu.be/VMj-3S1tku0?si=wRCObFw7woZTwU56&#34;&gt;Karpathy&lt;/a&gt; has a nice resource for this topic too!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generative Adversarial Networks</title>
      <link>https://flecart.github.io/notes/generative-adversarial-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/generative-adversarial-networks/</guid>
      <description></description>
    </item>
    <item>
      <title>The Perceptron Model</title>
      <link>https://flecart.github.io/notes/the-perceptron-model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/the-perceptron-model/</guid>
      <description>&lt;p&gt;The &lt;strong&gt;perceptron&lt;/strong&gt; is a fundamental binary linear classifier introduced by &lt;a href=&#34;https://psycnet.apa.org/record/1959-09865-001&#34;&gt;(Rosenblatt 1958)&lt;/a&gt;. It maps an input vector $\mathbf{x} \in \mathbb{R}^n$ to an output $y \in \{0,1\}$ using a weighted sum followed by a threshold function.&lt;/p&gt;
&lt;h3 id=&#34;the-mathematical-definition&#34;&gt;The Mathematical Definition&lt;/h3&gt;
&lt;p&gt;Given an input vector $\mathbf{x} = (x_1, x_2, \dots, x_n)$ and a weight vector $\mathbf{w} = (w_1, w_2, \dots, w_n)$, the perceptron computes:&lt;/p&gt;
$$
z = \mathbf{w}^\top \mathbf{x} + b = \sum_{i=1}^{n} w_i x_i + b
$$&lt;p&gt;where $b$ is the &lt;strong&gt;bias&lt;/strong&gt; term. The output is determined by the Heaviside step function:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
