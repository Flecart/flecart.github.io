<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine-Perception on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/machine-perception/</link>
    <description>Recent content in Machine-Perception on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.143.1</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/machine-perception/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Convolutional NN</title>
      <link>https://flecart.github.io/notes/convolutional-nn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/convolutional-nn/</guid>
      <description>&lt;h2 id=&#34;introduction-to-convolutional-nn&#34;&gt;Introduction to convolutional NN&lt;/h2&gt;
&lt;h3 id=&#34;the-convolution-operator--&#34;&gt;The convolution operator ðŸŸ©-&lt;/h3&gt;
&lt;p&gt;Il prodotto di convoluzione Ã¨ matematicamente molto contorto, anche se nella pratica Ã¨ una cosa molto molto semplice. In pratica voglio calcolare il valore di un pixel in funzione di certi suoi vicini, moltiplicati per un &lt;strong&gt;filter&lt;/strong&gt; che in pratica Ã¨ una matrice di pesi, che definisce un pattern lineare a cui sarei interessato di cercare nellâ€™immagine.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Slides ed esempi (molto piÃ¹ chiaril)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neural Networks</title>
      <link>https://flecart.github.io/notes/neural-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/neural-networks/</guid>
      <description>&lt;h2 id=&#34;introduction-a-neuron&#34;&gt;Introduction: a neuron&lt;/h2&gt;
&lt;p&gt;I am lazy, so I&amp;rsquo;m skipping the introduction for this set of notes. Look at Andrew Ng&amp;rsquo;s Coursera course for this part. Historical notes are &lt;a href=&#34;https://psycnet.apa.org/record/1959-09865-001&#34;&gt;(Rosenblatt 1958)&lt;/a&gt;.
One can view a perceptron to be a &lt;a href=&#34;https://flecart.github.io/notes/log-linear-models&#34;&gt;Log Linear Models&lt;/a&gt; with the temperature of the softmax that goes to 0 (so that it is an argmax).
Trained with a stochastic gradient descent with a batch of 1 (this is called the perceptron update rule).&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Perceptron Model</title>
      <link>https://flecart.github.io/notes/the-perceptron-model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/the-perceptron-model/</guid>
      <description>&lt;p&gt;The &lt;strong&gt;perceptron&lt;/strong&gt; is a fundamental binary linear classifier introduced by &lt;a href=&#34;https://psycnet.apa.org/record/1959-09865-001&#34;&gt;(Rosenblatt 1958)&lt;/a&gt;. It maps an input vector $\mathbf{x} \in \mathbb{R}^n$ to an output $y \in \{0,1\}$ using a weighted sum followed by a threshold function.&lt;/p&gt;
&lt;h3 id=&#34;the-mathematical-definition&#34;&gt;The Mathematical Definition&lt;/h3&gt;
&lt;p&gt;Given an input vector $\mathbf{x} = (x_1, x_2, \dots, x_n)$ and a weight vector $\mathbf{w} = (w_1, w_2, \dots, w_n)$, the perceptron computes:&lt;/p&gt;
$$
z = \mathbf{w}^\top \mathbf{x} + b = \sum_{i=1}^{n} w_i x_i + b
$$&lt;p&gt;where $b$ is the &lt;strong&gt;bias&lt;/strong&gt; term. The output is determined by the Heaviside step function:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
