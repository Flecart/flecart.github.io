<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machinelearning on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/machinelearning/</link>
    <description>Recent content in Machinelearning on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.143.1</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/machinelearning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Backpropagation</title>
      <link>https://flecart.github.io/notes/backpropagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/backpropagation/</guid>
      <description>&lt;p&gt;Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well.
An important observation is that this algorithm is &lt;strong&gt;linear&lt;/strong&gt;: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover. See &lt;a href=&#34;https://colah.github.io/posts/2015-08-Backprop/&#34;&gt;colah&amp;rsquo;s blog&lt;/a&gt;.
&lt;a href=&#34;https://youtu.be/VMj-3S1tku0?si=wRCObFw7woZTwU56&#34;&gt;Karpathy&lt;/a&gt; has a nice resource for this topic too!
&lt;a href=&#34;https://www.youtube.com/watch?v=zUazLXZZA2U&amp;list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&#34;&gt;Stanford lecture&lt;/a&gt; on backpropagation is another resource.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Perceptron Model</title>
      <link>https://flecart.github.io/notes/the-perceptron-model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/the-perceptron-model/</guid>
      <description>&lt;p&gt;The &lt;strong&gt;perceptron&lt;/strong&gt; is a fundamental binary linear classifier introduced by &lt;a href=&#34;https://psycnet.apa.org/record/1959-09865-001&#34;&gt;(Rosenblatt 1958)&lt;/a&gt;. It maps an input vector $\mathbf{x} \in \mathbb{R}^n$ to an output $y \in \{0,1\}$ using a weighted sum followed by a threshold function.&lt;/p&gt;
&lt;h3 id=&#34;introduction-to-the-perceptron&#34;&gt;Introduction to the Perceptron&lt;/h3&gt;
&lt;h4 id=&#34;a-mathematical-model-&#34;&gt;A mathematical model 🟩&lt;/h4&gt;
&lt;p&gt;Given an input vector $\mathbf{x} = (x_1, x_2, \dots, x_n)$ and a weight vector $\mathbf{w} = (w_1, w_2, \dots, w_n)$, the perceptron computes:&lt;/p&gt;
$$
z = \mathbf{w}^\top \mathbf{x} + b = \sum_{i=1}^{n} w_i x_i + b
$$$$
y = f(z) = 
\begin{cases}
1, &amp; \text{if } z \geq 0 \\
0, &amp; \text{otherwise}
\end{cases}
$$&lt;h4 id=&#34;learning-rule-&#34;&gt;Learning Rule 🟩&lt;/h4&gt;
&lt;p&gt;Given a labeled dataset $\{ (\mathbf{x}^{(i)}, y^{(i)}) \}_{i=1}^{m}$, the perceptron uses the following weight update rule for misclassified samples ($y^{(i)} \neq f(\mathbf{w}^\top \mathbf{x}^{(i)} + b)$):&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parametric Modeling</title>
      <link>https://flecart.github.io/notes/parametric-modeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/parametric-modeling/</guid>
      <description>&lt;p&gt;In this note we will first talk about briefly some of the main differences of the three main approaches regarding statistics: the bayesian, the frequentist and the statistical learning methods and then present the concept of the estimator, compare how the approaches differ from method to method, we will explain maximum likelihood estimator and the Rao-Cramer Bound.&lt;/p&gt;
&lt;h3 id=&#34;short-introduction-to-the-statistical-methods&#34;&gt;Short introduction to the statistical methods&lt;/h3&gt;
&lt;h4 id=&#34;bayesian-&#34;&gt;Bayesian 🟩&lt;/h4&gt;
$$
p(\theta \mid X) = \frac{1}{z}p(X \mid \theta) p(\theta) 
$$&lt;p&gt;The quantity $P(X \mid \theta)$ could be very complicated if our model is complicated.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Diffusion Models</title>
      <link>https://flecart.github.io/notes/diffusion-models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/diffusion-models/</guid>
      <description>&lt;p&gt;Diffusion is a physical process that models random motion, first analyzed by Brown when studying pollen grains in water. In this section, we will first analyze a simplified 1-dimensional version, and then delve into diffusion models for images, the ones closest to &lt;a href=&#34;http://arxiv.org/abs/2006.11239&#34;&gt;(Ho et al. 2020)&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-diffusion-process&#34;&gt;The Diffusion Process&lt;/h3&gt;
&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/pdf/cond-mat/0701242&#34;&gt;note&lt;/a&gt; follows original Einstein&amp;rsquo;s presentation, here we have a simplified version.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s suppose we have a particle at $t = 0$ at some position $i$. We have a probability of jumping to the left of $p$ to right of $q$, the rest is staying at the same position.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reti convoluzionali</title>
      <link>https://flecart.github.io/notes/reti-convoluzionali/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/reti-convoluzionali/</guid>
      <description>&lt;p&gt;Abbiamo trattato i modelli classici in &lt;a href=&#34;https://flecart.github.io/notes/convolutional-neural-network&#34;&gt;Convolutional Neural Network&lt;/a&gt;. Con i vecchi files di notion&lt;/p&gt;
&lt;h3 id=&#34;il-kernel&#34;&gt;Il Kernel&lt;/h3&gt;
&lt;p&gt;I punti interessanti delle immagini sono solamente i &lt;strong&gt;punti di cambio&lt;/strong&gt; solo che attualmente siamo in stato discreto, quindi ci è difficile usare una derivata, si usano kernel del tipo:
$\left[ 1, 0, -1 \right]$, che sarà positivo se cresce verso sinistra, negativo se scende.
&lt;img src=&#34;https://flecart.github.io/images/notes/Reti convoluzionali-1700037160855.jpeg&#34; style=&#34;width: 100%&#34; class=&#34;center&#34; alt=&#34;Reti convoluzionali-1700037160855&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;feature-map&#34;&gt;feature map&lt;/h4&gt;
&lt;p&gt;Sono delle mappe che rappresentano alcune informazioni interessanti della nostra immagine.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Clustering</title>
      <link>https://flecart.github.io/notes/clustering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/clustering/</guid>
      <description>&lt;h3 id=&#34;gaussian-mixture-models&#34;&gt;Gaussian Mixture Models&lt;/h3&gt;
&lt;p&gt;This set takes inspiration from chapter 9.2 of &lt;a href=&#34;https://link.springer.com/book/9780387310732&#34;&gt;(Bishop 2006)&lt;/a&gt;.
We assume that the reader already knows quite well what is a &lt;a href=&#34;https://flecart.github.io/notes/gaussian-mixture-models&#34;&gt;Gaussian mixture model&lt;/a&gt; and we will just restate the models here. We will discuss the problem of estimating the best possible parameters (so, this is a density estimation problem) when the data is generated by a mixture of Gaussians.&lt;/p&gt;
$$
\mathcal{N}(x \mid \mu, \Sigma) = \frac{1}{\sqrt{ 2\pi }} \frac{1}{\lvert \Sigma \rvert^{1/2}  } \exp \left( -\frac{1}{2} (x - \mu)^{T} \Sigma^{-1}(x - \mu) \right) 
$$&lt;h4 id=&#34;problem-statement-&#34;&gt;Problem statement 🟩&lt;/h4&gt;
$$
p(z) = \prod_{i = 1}^{k} \pi_{i}^{z_{i}}
$$&lt;p&gt;
Because we know that $z$ is a $k$ dimensional vector that has a single digit indicating which Gaussian was chosen.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Anomaly Detection</title>
      <link>https://flecart.github.io/notes/anomaly-detection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/anomaly-detection/</guid>
      <description>&lt;p&gt;Anomaly detection is a problem in machine learning that is of a big interest in industry. For example a bank needs to identify problems in transactions, doctors need it to see illness, or suspicious behaviors for law (no Orwell here).
The main difference between this and classification is that here we have no classes.&lt;/p&gt;
&lt;h4 id=&#34;setting-of-the-problem&#34;&gt;Setting of the problem&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s say we have a set $X = \left\{ x_{1}, \dots, x_{n} \right\} \subseteq \mathcal{N} \subseteq \mathcal{X} = \mathbb{R}^{d}$  We say this set is the normal set, and $X$ are our samples but it&amp;rsquo;s quite complex, so we need an approximation to say whether if a set is normal or not.
We need a function $\phi : \mathcal{X} \to \left\{ 0, 1 \right\}$ with $\phi(x) = 1 \iff x \not \in \mathcal{N}$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian neural networks</title>
      <link>https://flecart.github.io/notes/bayesian-neural-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/bayesian-neural-networks/</guid>
      <description>&lt;h3 id=&#34;robbins-moro-algorithm&#34;&gt;Robbins-Moro Algorithm&lt;/h3&gt;
&lt;h4 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h4&gt;
$$
w_{n+1} = w_{n} - \alpha_{n} \Delta w_{n}
$$&lt;p&gt;For example with $\alpha_{0} &gt; \alpha_{1} &gt; \dots &gt; \alpha_{n} \dots$, and $\alpha_{t} = \frac{1}{t}$ they satisfy the condition (in practice we use a constant $\alpha$, but we lose the convergence guarantee by Robbins Moro).
More generally, the Robbins-Moro conditions re:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\sum_{n} \alpha_{n} = \infty$&lt;/li&gt;
&lt;li&gt;$\sum_{n} \alpha_{n}^{2} &lt; \infty$
Then the algorithm is guaranteed to converge to the best answer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One nice thing about this, is that we &lt;strong&gt;don&amp;rsquo;t need gradients&lt;/strong&gt;.
But often we use gradient versions (stochastic gradient descent and similar), using auto-grad, see &lt;a href=&#34;https://flecart.github.io/notes/backpropagation&#34;&gt;Backpropagation&lt;/a&gt;.
But learning with gradients brings some drawbacks:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Active Learning</title>
      <link>https://flecart.github.io/notes/active-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/active-learning/</guid>
      <description>&lt;p&gt;Active Learning concerns methods to decide how to sample the most useful information in a specific domain; how can you select the best sample for an unknown model?
Gathering data is very costly, we would like to create some principled manner to choose the best data point to humanly label in order to have the best model.&lt;/p&gt;
&lt;p&gt;In this setting, we are interested in the concept of &lt;strong&gt;usefulness of information&lt;/strong&gt;. One of our main goals is to &lt;em&gt;reduce uncertainty&lt;/em&gt;, thus, &lt;a href=&#34;https://flecart.github.io/notes/entropy&#34;&gt;Entropy&lt;/a&gt;-based (mutual information) methods are often used.
For example, we can use active learning to choose what samples needs to be labelled in order to have highest accuracy on the trained model, when labelling is costly.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Information Criterion</title>
      <link>https://flecart.github.io/notes/bayesian-information-criterion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/bayesian-information-criterion/</guid>
      <description>&lt;h3 id=&#34;bayesian-information-criterion-bic&#34;&gt;&lt;strong&gt;Bayesian Information Criterion (BIC)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Bayesian Information Criterion (BIC)&lt;/strong&gt; is a model selection criterion that helps compare different statistical models while penalizing model complexity. It is rooted in Bayesian probability theory but is commonly used even in frequentist settings.&lt;/p&gt;
&lt;h3 id=&#34;mathematically-precise-definition&#34;&gt;&lt;strong&gt;Mathematically Precise Definition&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;For a statistical model $M$ with $k$ parameters fitted to a dataset $\mathcal{D} = \{x_1, x_2, \dots, x_n\}$, the BIC is defined as:&lt;/p&gt;
$$
\text{BIC} = -2 \cdot \ln \hat{L} + k \cdot \ln(n)
$$&lt;p&gt;where:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Beta and Dirichlet Distributions</title>
      <link>https://flecart.github.io/notes/beta-and-dirichlet-distributions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/beta-and-dirichlet-distributions/</guid>
      <description>&lt;h1 id=&#34;the-beta-distribution&#34;&gt;The beta distribution&lt;/h1&gt;
&lt;p&gt;The beta distribution is a powerful tool for modeling probabilities and proportions between 0 and 1. Here&amp;rsquo;s a structured intuition to grasp its essence:&lt;/p&gt;
&lt;h3 id=&#34;core-concept&#34;&gt;Core Concept&lt;/h3&gt;
&lt;p&gt;The beta distribution, defined on $[0, 1]$, is parameterized by two shape parameters: &lt;strong&gt;α (alpha)&lt;/strong&gt; and &lt;strong&gt;β (beta)&lt;/strong&gt;. These parameters dictate the distribution’s shape, allowing it to flexibly represent beliefs about probabilities, rates, or proportions.&lt;/p&gt;
&lt;h3 id=&#34;key-intuitions&#34;&gt;Key Intuitions&lt;/h3&gt;
&lt;h4 id=&#34;a-pseudo-counts-interpretation&#34;&gt;&lt;strong&gt;a. &amp;ldquo;Pseudo-Counts&amp;rdquo; Interpretation&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;α&lt;/strong&gt; acts like &amp;ldquo;successes&amp;rdquo; and &lt;strong&gt;β&lt;/strong&gt; like &amp;ldquo;failures&amp;rdquo; in a hypothetical experiment.
&lt;ul&gt;
&lt;li&gt;Example: If you use &lt;strong&gt;Beta(5, 3)&lt;/strong&gt;, it’s as if you’ve observed &lt;strong&gt;5 successes&lt;/strong&gt; and &lt;strong&gt;3 failures&lt;/strong&gt; &lt;em&gt;before seeing actual data&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;After observing &lt;strong&gt;x real successes&lt;/strong&gt; and &lt;strong&gt;y real failures&lt;/strong&gt;, the posterior becomes &lt;strong&gt;Beta(α+x, β+y)&lt;/strong&gt;. This makes beta the &lt;strong&gt;conjugate prior&lt;/strong&gt; for the binomial distribution (bernoulli process).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;b-shape-flexibility&#34;&gt;&lt;strong&gt;b. Shape Flexibility&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Uniform distribution&lt;/strong&gt;: When &lt;strong&gt;α = β = 1&lt;/strong&gt;, all values in [0, 1] are equally likely.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bell-shaped&lt;/strong&gt;: When &lt;strong&gt;α, β &amp;gt; 1&lt;/strong&gt;, the distribution peaks at &lt;strong&gt;mode = (α-1)/(α+β-2)&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;Symmetric if &lt;strong&gt;α = β&lt;/strong&gt; (e.g., &lt;strong&gt;Beta(5, 5)&lt;/strong&gt; is centered at 0.5).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U-shaped&lt;/strong&gt;: When &lt;strong&gt;α, β &amp;lt; 1&lt;/strong&gt;, density spikes at 0 and 1 (useful for modeling polarization, meaning we believe the model to only produce values at 0 or 1, not in the middle.).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Skewed&lt;/strong&gt;: If &lt;strong&gt;α &amp;gt; β&lt;/strong&gt;, skewed toward 1; if &lt;strong&gt;β &amp;gt; α&lt;/strong&gt;, skewed toward 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;c-moments&#34;&gt;&lt;strong&gt;c. Moments&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mean&lt;/strong&gt;: $α/(α+β)$ – your &amp;ldquo;expected&amp;rdquo; probability of success.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variance&lt;/strong&gt;: $αβ / [(α+β)²(α+β+1)]$ – decreases as &lt;strong&gt;α&lt;/strong&gt; and &lt;strong&gt;β&lt;/strong&gt; grow (more confidence).&lt;/li&gt;
&lt;/ul&gt;
$$
\text{Mode} = \frac{\alpha - 1}{\alpha + \beta - 2}
$$&lt;h3 id=&#34;the-mathematical-model&#34;&gt;The mathematical model&lt;/h3&gt;
$$
\text{Beta} (x \mid a, b) = \frac{1}{B(a, b)} \cdot x^{a -1 }(1 - x)^{b - 1}
$$&lt;p&gt;
Where $B(a, b) = \Gamma(a) \Gamma(b) / \Gamma( + b)$
And $\Gamma(t) = \int_{0}^{\infty}e^{-x}x^{t - 1} \, dx$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Counterfactual Invariance</title>
      <link>https://flecart.github.io/notes/counterfactual-invariance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/counterfactual-invariance/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Machine learning cannot distinguish between causal and environment features.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;shortcut-learning&#34;&gt;Shortcut learning&lt;/h4&gt;
&lt;p&gt;Often we observe &lt;strong&gt;shortcut learning&lt;/strong&gt;: the model learns some dataset dependent shortcuts (e.g. the machine that was used to take the X-ray) to make inference, but this is very brittle, and is not usually able to generalize.&lt;/p&gt;
&lt;p&gt;Shortcut learning happens when there are correlations in the test set between causal and non-causal features. Our object of interest should be the main focus, not the environment around, in most of the cases. For example, a camel in a grass land should still be recognized as a camel, not a cow.
One solution could be engineering &lt;strong&gt;invariant representations&lt;/strong&gt; which are independent of the environment. So having a kind of encoder that creates these representations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cross Validation and Model Selection</title>
      <link>https://flecart.github.io/notes/cross-validation-and-model-selection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/cross-validation-and-model-selection/</guid>
      <description>&lt;p&gt;There is a big difference between the empirical score and the expected score; in the beginning, we had said something about this in &lt;a href=&#34;https://flecart.github.io/notes/introduction-to-advanced-machine-learning&#34;&gt;Introduction to Advanced Machine Learning&lt;/a&gt;. We will develop more methods to better comprehend this fundamental principles.&lt;/p&gt;
&lt;p&gt;How can we estimate the expected risk of a particular estimator or algorithm? We can use the &lt;strong&gt;cross-validation&lt;/strong&gt; method. This method is used to estimate the expected risk of a model, and it is a fundamental method in machine learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dirichlet Processes</title>
      <link>https://flecart.github.io/notes/dirichlet-processes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/dirichlet-processes/</guid>
      <description>&lt;p&gt;The DP (Dirichlet Processes) is part of family of models called &lt;strong&gt;non-parametric&lt;/strong&gt; models.
Non parametric models concern learning models with potentially infinite number of parameters.
One of the classical application is unsupervised techniques like clustering.
Intuitively, clustering concerns in finding &lt;em&gt;compact subsets&lt;/em&gt; of data, i.e. finding groups of points in the space that are particularly close by some measure.&lt;/p&gt;
&lt;h3 id=&#34;the-dirichlet-process&#34;&gt;The Dirichlet Process&lt;/h3&gt;
&lt;p&gt;See &lt;a href=&#34;https://flecart.github.io/notes/beta-and-dirichlet-distributions&#34;&gt;Beta and Dirichlet Distributions&lt;/a&gt; for the definition and intuition of these two distributions.
One quite important thing that Dirichlet allows to do is the ability of assigning an ever growing number of clusters to data. This models are thus quite flexible to change and growth.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ensemble Methods</title>
      <link>https://flecart.github.io/notes/ensemble-methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/ensemble-methods/</guid>
      <description>&lt;p&gt;The idea of ensemble methods goes back to Sir Francis Galton. In 787, he noted that although not every single person got the right value, the average estimate of a crowd of people predicted quite well.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The main idea of ensemble methods is to combine relatively weak classifiers into a highly accurate predictor.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The motivation for boosting was a procedure that combines the outputs of many “weak” classifiers to produce a powerful “committee.”&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fisher&#39;s Linear Discriminant</title>
      <link>https://flecart.github.io/notes/fishers-linear-discriminant/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/fishers-linear-discriminant/</guid>
      <description>&lt;h4 id=&#34;a-simple-motivation&#34;&gt;A simple motivation&lt;/h4&gt;
&lt;p&gt;Fisher&amp;rsquo;s Linear Discriminant is a simple idea used to linearly classify our data.
&lt;img src=&#34;https://flecart.github.io/images/notes/Fisher&#39;s Linear Discriminant-20241031125847321.webp&#34; style=&#34;width: 100%&#34; class=&#34;center&#34; alt=&#34;Fisher&#39;s Linear Discriminant-20241031125847321&#34;&gt;&lt;/p&gt;
&lt;p&gt;The image above, taken from &lt;a href=&#34;https://link.springer.com/book/9780387310732&#34;&gt;(Bishop 2006)&lt;/a&gt;, is the summary of the idea.  We clearly see that if we first project using the direction of maximum variance (See Principal Component Analysis) then the data is not linearly separable, but if we take other notions into consideration, then the idea becomes much more cleaner.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gaussian Processes</title>
      <link>https://flecart.github.io/notes/gaussian-processes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/gaussian-processes/</guid>
      <description>&lt;p&gt;Gaussian processes can be viewed through a Bayesian lens of the function space: rather than sampling over individual data points, we are now sampling over entire functions. They extend the idea of &lt;a href=&#34;https://flecart.github.io/notes/bayesian-linear-regression&#34;&gt;bayesian linear regression&lt;/a&gt; by introducing an infinite number of feature functions for the input XXX.&lt;/p&gt;
&lt;p&gt;In geostatistics, Gaussian processes are referred to as &lt;em&gt;kriging&lt;/em&gt; regressions, and many other models, such as &lt;a href=&#34;https://flecart.github.io/notes/kalman-filters&#34;&gt;Kalman Filters&lt;/a&gt; or radial basis function networks, can be understood as special cases of Gaussian processes. In this framework, certain functions are more likely than others, and we aim to model this probability distribution.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Advanced Machine Learning</title>
      <link>https://flecart.github.io/notes/introduction-to-advanced-machine-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/introduction-to-advanced-machine-learning/</guid>
      <description>&lt;h2 id=&#34;introduction-to-the-course&#34;&gt;Introduction to the course&lt;/h2&gt;
&lt;p&gt;Machine learning offers a new way of thinking about reality: rather than attempting to directly capture a fragment of reality, as many traditional sciences have done, we elevate to the meta-level and strive to create an automated method for capturing it.&lt;/p&gt;
&lt;p&gt;This first lesson will be more philosophical in nature. We are witnessing a &lt;strong&gt;paradigm shift&lt;/strong&gt; in the sense described by Thomas Kuhn in his theory of scientific revolutions. But what drives such a shift, and how does it unfold?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kernel Methods</title>
      <link>https://flecart.github.io/notes/kernel-methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/kernel-methods/</guid>
      <description>&lt;p&gt;As we will briefly see, Kernels will have an important role in many machine learning applications. In this note we will get to know what are Kernels and why are they useful. Intuitively they measure the &lt;strong&gt;similarity&lt;/strong&gt; between two input points. So if they are close the kernel should be big, else it should be small.&lt;/p&gt;
&lt;p&gt;We briefly state the requirements of a Kernel, then we will argue with a simple example why they are useful.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Regression methods</title>
      <link>https://flecart.github.io/notes/linear-regression-methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/linear-regression-methods/</guid>
      <description>&lt;p&gt;We will present some methods related to regression methods for data analysis.
Some of the work here is from (Hastie et al. 2009). This note does not treat the bayesian case, you should see &lt;a href=&#34;https://flecart.github.io/notes/bayesian-linear-regression&#34;&gt;Bayesian Linear Regression&lt;/a&gt; for that.&lt;/p&gt;
&lt;h3 id=&#34;problem-setting&#34;&gt;Problem setting&lt;/h3&gt;
$$
Y = \beta_{0} + \sum_{j = 1}^{d} X_{j}\beta_{j}
$$&lt;p&gt;We usually don&amp;rsquo;t know the distribution of $P(X)$ or $P(Y \mid X)$ so we need to assume something about these distributions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Provably Approximately Correct Learning</title>
      <link>https://flecart.github.io/notes/provably-approximately-correct-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/provably-approximately-correct-learning/</guid>
      <description>&lt;p&gt;PAC Learning is one of the most famous theories in learning theory. Learning theory concerns in answering questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is learnable? Somewhat akin to &lt;a href=&#34;https://flecart.github.io/notes/la-macchina-di-turing&#34;&gt;La macchina di Turing&lt;/a&gt; for computability theory.&lt;/li&gt;
&lt;li&gt;How well can you learn something?
PAC is a framework that allows to formally answer these questions.
Now there is also a bayesian version of PAC in which there is a lot of research.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-definitions&#34;&gt;Some definitions&lt;/h3&gt;
&lt;h4 id=&#34;empirical-risk-minimizer-and-errors&#34;&gt;Empirical Risk Minimizer and Errors&lt;/h4&gt;
$$
\arg \min_{\hat{c} \in \mathcal{H}} \hat{R}_{n}(\hat{c})
$$&lt;p&gt;
Where the inside is the empirical error.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rademacher Complexity</title>
      <link>https://flecart.github.io/notes/rademacher-complexity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/rademacher-complexity/</guid>
      <description>&lt;p&gt;This note used the definitions present in &lt;a href=&#34;https://flecart.github.io/notes/provably-approximately-correct-learning&#34;&gt;Provably Approximately Correct Learning&lt;/a&gt;. So, go there when you encounter a word you don&amp;rsquo;t know. Or search online&lt;/p&gt;
&lt;h2 id=&#34;rademacher-complexity&#34;&gt;Rademacher Complexity&lt;/h2&gt;
$$
\mathcal{G}  = \left\{ g : (x, y) \to L(h(x), y) : h \in \mathcal{H} \right\} 
$$&lt;p&gt;
Where $L : \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$ is a generic loss function.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Rademacher complexity captures the richness of a family of functions by measuring the degree to which a hypothesis set can fit random noise. From (Mohri et al. 2012).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Softmax Function</title>
      <link>https://flecart.github.io/notes/softmax-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/softmax-function/</guid>
      <description>&lt;p&gt;Softmax is one of the most important functions for neural networks. It also has some interesting properties that we list here. This function is part of &lt;a href=&#34;https://flecart.github.io/notes/the-exponential-family&#34;&gt;The Exponential Family&lt;/a&gt;, one can also see that the sigmoid function is a particular case of this softmax, just two variables.
Sometimes this could be seen as a relaxation of the action potential inspired by neuroscience (See &lt;a href=&#34;https://flecart.github.io/notes/the-neuron&#34;&gt;The Neuron&lt;/a&gt; for a little bit more about neurons). This is because we need &lt;strong&gt;differentiable&lt;/strong&gt;, for gradient descent. The action potential is an all or nothing thing.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Support Vector Machines</title>
      <link>https://flecart.github.io/notes/support-vector-machines/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/support-vector-machines/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://cs229.stanford.edu/main_notes.pdf&#34;&gt;This&lt;/a&gt; is a quite good resource about this part of Support Vector Machines (step by step derivation). &lt;a href=&#34;https://link.springer.com/book/9780387310732&#34;&gt;(Bishop 2006)&lt;/a&gt; chapter 7 is a good resource. The main idea about this &lt;em&gt;supervised&lt;/em&gt; method is separating with a &lt;strong&gt;large gap&lt;/strong&gt;. The thing is that we have a hyperplane, when this plane is projected to lower dimensional data, it can look like a non-linear separator. After we have found this separator, we can intuitively have an idea of &lt;em&gt;confidence&lt;/em&gt; based on the distance of the separator.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational Inference</title>
      <link>https://flecart.github.io/notes/variational-inference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/variational-inference/</guid>
      <description>$$
p(\theta \mid x_{1:n}, y_{1:n}) = \frac{1}{z} p(y_{1:n} \mid \theta, x_{1:n}) p(\theta \mid x_{1:n}) \approx q(\theta \mid \lambda)
$$&lt;p&gt;For &lt;a href=&#34;https://flecart.github.io/notes/bayesian-linear-regression&#34;&gt;Bayesian Linear Regression&lt;/a&gt; we had high dimensional &lt;a href=&#34;https://flecart.github.io/notes/gaussians&#34;&gt;Gaussians&lt;/a&gt; which made the inference &lt;em&gt;closed form&lt;/em&gt;, in general this is not true, so we need some kinds of approximation.&lt;/p&gt;
&lt;h2 id=&#34;laplace-approximation&#34;&gt;Laplace approximation&lt;/h2&gt;
&lt;h4 id=&#34;introduction-to-the-idea-&#34;&gt;Introduction to the Idea 🟩&lt;/h4&gt;
$$
\psi(\theta) \approx \hat{\psi}(\theta) = \psi(\hat{\theta}) + (\theta-\hat{\theta} ) ^{T} \nabla \psi(\hat{\theta}) + \frac{1}{2} (\theta-\hat{\theta} ) ^{T} H_{\psi}(\hat{\theta})(\theta-\hat{\theta} ) = \psi(\hat{\theta}) + \frac{1}{2} (\theta-\hat{\theta} ) ^{T} H_{\psi}(\hat{\theta})(\theta-\hat{\theta} ) 
$$&lt;p&gt;
We simplified the term on the first order because we are considering the mode, so the gradient should be zero for the stationary point.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lagrange Multipliers</title>
      <link>https://flecart.github.io/notes/lagrange-multipliers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/lagrange-multipliers/</guid>
      <description>&lt;p&gt;This is also known as &lt;em&gt;Lagrange Optimization&lt;/em&gt; or &lt;em&gt;undetermined multipliers&lt;/em&gt;. Some of these notes are based on Appendix E of &lt;a href=&#34;https://link.springer.com/book/9780387310732&#34;&gt;(Bishop 2006)&lt;/a&gt;, others were found when studying bits of rational mechanics.
Also &lt;a href=&#34;https://web.stanford.edu/~boyd/cvxbook/&#34;&gt;(Boyd &amp;amp; Vandenberghe 2004)&lt;/a&gt; chapter 5 should be a good resource on this topic.&lt;/p&gt;
$$
\begin{array} \\
\min f_{0}(x)  \\
\text{subject to } f_{i}(x) \leq 0 \\
h_{j}(x) = 0
\end{array}
$$&lt;h3 id=&#34;lagrangian-function&#34;&gt;Lagrangian function&lt;/h3&gt;
$$
\mathcal{L}(x, \lambda, \nu) = f_{0}(x) + \sum \lambda_{i}f_{i}(x) + \sum\nu_{j}h_{j}(x)
$$&lt;p&gt;
We want to say something about this function, because it is able to simplify the optimization problem a lot, but first we want to study this mathematically.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Object detection and Segmentation</title>
      <link>https://flecart.github.io/notes/object-detection-and-segmentation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/object-detection-and-segmentation/</guid>
      <description>&lt;h3 id=&#34;definition-of-problems&#34;&gt;Definition of problems&lt;/h3&gt;
&lt;h4 id=&#34;object-detection&#34;&gt;Object detection&lt;/h4&gt;
&lt;p&gt;Bisogna trovare all&amp;rsquo;interno dell&amp;rsquo;immagine quali siano gli oggetti presenti, e in più &lt;strong&gt;vogliamo sapere dove siano&lt;/strong&gt; quindi utilizzare una bounding box per caratterizzarli sarebbe buono.&lt;/p&gt;
&lt;h4 id=&#34;object-segmentation&#34;&gt;Object segmentation&lt;/h4&gt;
&lt;p&gt;È riuscire a caratterizzare categoria per categoria per &lt;strong&gt;singoli pixels&lt;/strong&gt;m e per questo motivo potrei riuscire a fare delle image map in cui colorare singoli oggetti in una categoria.&lt;/p&gt;
&lt;h3 id=&#34;datasets&#34;&gt;Datasets&lt;/h3&gt;
&lt;h4 id=&#34;example-datasets&#34;&gt;Example datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Pascal VOC 2012&lt;/li&gt;
&lt;li&gt;Coco datasets&lt;/li&gt;
&lt;li&gt;Cityscapes dataset&lt;/li&gt;
&lt;li&gt;Autogenerated datasets
But I don&amp;rsquo;t know much about these datasets&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;applications&#34;&gt;Applications&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Auto drive&lt;/li&gt;
&lt;li&gt;Campo medico (per segmentazione medica o riconoscimento immagini).&lt;/li&gt;
&lt;li&gt;reidentificazione.&lt;/li&gt;
&lt;li&gt;Key posse extimations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;u-net&#34;&gt;U-net&lt;/h4&gt;
&lt;p&gt;Il primo skip connection ci permette di capire bene quali siano i bordi, perché sappiamo che la convoluzione riesce a prendere bene&lt;/p&gt;</description>
    </item>
    <item>
      <title>Naïve Bayes</title>
      <link>https://flecart.github.io/notes/na%C3%AFve-bayes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/na%C3%AFve-bayes/</guid>
      <description>&lt;h3 id=&#34;introduzione-a-naïve-bayes&#34;&gt;Introduzione a Naïve Bayes&lt;/h3&gt;
&lt;p&gt;NOTE: this note should be reviewed after the course I took in NLP. This is a very old note, not even well written.&lt;/p&gt;
&lt;p&gt;Bisognerebbe in primo momento avere benissimo in mente il significato di &lt;strong&gt;probabilità condizionata&lt;/strong&gt; e la regola di naive Bayes in seguito.&lt;/p&gt;
&lt;h4 id=&#34;bayes-ad-alto-livello-&#34;&gt;Bayes ad alto livello 🟩&lt;/h4&gt;
&lt;p&gt;Da un punto di vista intuitivo non è altro che predire la cosa che abbiamo &lt;strong&gt;visto più spesso in quello spazio&lt;/strong&gt;
&lt;img src=&#34;https://flecart.github.io/images/notes/Naïve Bayes-1696854772448.jpeg&#34; width=&#34;500&#34; class=&#34;center&#34; alt=&#34;Naïve Bayes-1696854772448&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alberi di decisione</title>
      <link>https://flecart.github.io/notes/alberi-di-decisione/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/alberi-di-decisione/</guid>
      <description>&lt;h2 id=&#34;introduzione-agli-alberi-di-decisione&#34;&gt;Introduzione agli alberi di decisione&lt;/h2&gt;
&lt;h3 id=&#34;setting-del-problema--&#34;&gt;Setting del problema 🟩-&lt;/h3&gt;
&lt;img src=&#34;https://flecart.github.io/images/notes/Alberi di decisione-1696950382366.jpeg&#34; width=&#34;576&#34; class=&#34;center&#34; alt=&#34;Alberi di decisione-1696950382366&#34;/&gt;
&lt;h3 id=&#34;spazio-delle-ipotesi&#34;&gt;Spazio delle ipotesi&lt;/h3&gt;
&lt;h4 id=&#34;definizione-spazio-ipotesi----&#34;&gt;Definizione spazio ipotesi 🟩&amp;mdash;&lt;/h4&gt;
&lt;p&gt;Per spazio delle ipotesi andiamo a considerare l&amp;rsquo;insieme delle &lt;em&gt;funzioni rappresentabili dal nostro modello&lt;/em&gt;.
Questo implica che &lt;strong&gt;l&amp;rsquo;allenamento ricerca l&amp;rsquo;ipotesi&lt;/strong&gt; ossia la parametrizzazione &lt;em&gt;ottimale&lt;/em&gt; del nostro modello, ottimale in quanto &lt;em&gt;minimizza&lt;/em&gt; l&amp;rsquo;errore che viene compiuto nel training set.&lt;/p&gt;
&lt;p&gt;L&amp;rsquo;insieme iniziale si può anche considerare come &lt;strong&gt;inductive bias&lt;/strong&gt; ossia il restringimento solamente a certe ipotesi e non tutte. Altrimenti abbiamo no free lunch.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logistic Regression</title>
      <link>https://flecart.github.io/notes/logistic-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/logistic-regression/</guid>
      <description>&lt;p&gt;Queste note sono molto di base. Per cose leggermente più avanzate bisogna guardare &lt;a href=&#34;https://flecart.github.io/notes/bayesian-linear-regression&#34;&gt;Bayesian Linear Regression&lt;/a&gt;, &lt;a href=&#34;https://flecart.github.io/notes/linear-regression-methods&#34;&gt;Linear Regression methods&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;introduzione-alla-logistic-regression&#34;&gt;Introduzione alla logistic regression&lt;/h2&gt;
&lt;h3 id=&#34;giustificazione-del-metodo&#34;&gt;Giustificazione del metodo&lt;/h3&gt;
&lt;p&gt;Questo è uno dei modelli classici, creati da &lt;strong&gt;Minsky&lt;/strong&gt; qualche decennio fa
In questo caso andiamo direttamente a computare il valore di $P(Y|X)$ durante l&amp;rsquo;inferenza, quindi si parla di modello &lt;strong&gt;discriminativo&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;introduzione-al-problema&#34;&gt;Introduzione al problema&lt;/h4&gt;
&lt;p&gt;Supponiamo che&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Y$ siano variabili booleane&lt;/li&gt;
&lt;li&gt;$X_{i}$ siano variabili continue&lt;/li&gt;
&lt;li&gt;$X_{i}$ siano indipendenti uno dall&amp;rsquo;altro.&lt;/li&gt;
&lt;li&gt;$P(X_{i}| Y= k)$ sono modellate tramite distribuzioni gaussiane $\mathbb{N}(\mu_{ik}, \sigma_{i})$
&lt;ul&gt;
&lt;li&gt;NOTA! la varianza non dipende dalle feature!, questo mi permetterebbe di poi togliere la cosa quadratico dopo, rendendo poi l&amp;rsquo;approssimazione lineare&lt;/li&gt;
&lt;li&gt;Per esempio se utilizziamo nelle immagini, avrebbe senso normalizzare pixel by pixel, e non image wide con un unico valore, è una assunzione, che se funziona dovrebbe poi far andare meglio la regressione logistica!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$Y$ è una distribuzione bernoulliana.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ci chiediamo come è fatto $P(Y|X)$?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Proximal Policy Optimization</title>
      <link>https://flecart.github.io/notes/proximal-policy-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/proximal-policy-optimization/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/1707.06347&#34;&gt;(Schulman et al. 2017)&lt;/a&gt; è uno degli articoli principali che praticamente hanno dato via al campo.
Anche questo è buono per Policy gradients:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2018-04-08-policy-gradient/&#34;&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2018-04-08-policy-gradient/&#34;&gt;https://lilianweng.github.io/posts/2018-04-08-policy-gradient/&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;introduzione-a-ppo&#34;&gt;Introduzione a PPO&lt;/h3&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Schulman et al. &lt;a href=&#34;http://arxiv.org/abs/1707.06347&#34;&gt;“Proximal Policy Optimization Algorithms”&lt;/a&gt; arXiv preprint arXiv:1707.06347 2017&lt;/p&gt;</description>
    </item>
    <item>
      <title>The RLHF pipeline</title>
      <link>https://flecart.github.io/notes/the-rlhf-pipeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/the-rlhf-pipeline/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://huyenchip.com/2023/05/02/rlhf.html&#34;&gt;&lt;a href=&#34;https://huyenchip.com/2023/05/02/rlhf.html&#34;&gt;https://huyenchip.com/2023/05/02/rlhf.html&lt;/a&gt;&lt;/a&gt; è un blog post che lo descrive in modo abbastanza dettagliato e buono.&lt;/p&gt;
&lt;h2 id=&#34;introduzione-a-rlhf&#34;&gt;Introduzione a RLHF&lt;/h2&gt;
&lt;p&gt;Questo è il processo che è quasi la migliore per la produzione di LLM moderni (maggior parte si basano su questo per dire).&lt;/p&gt;
&lt;h3 id=&#34;struttura-generale&#34;&gt;Struttura generale&lt;/h3&gt;
&lt;p&gt;Si può dire che RLHF si divida in 3 parti fondamentali&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Completion&lt;/strong&gt; il modello viene allenato a completare parole dal web,solitamente è molto inutile&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine tuning&lt;/strong&gt; per le singole task, per esempio riassumere, rispondere in certo modo etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt; basato su un &lt;strong&gt;reward model&lt;/strong&gt; scoperto.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Partiamo con l&amp;rsquo;approccio di reinforcement learning che è la parte un po&amp;rsquo; più interessante in questo momento&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tokenization</title>
      <link>https://flecart.github.io/notes/tokenization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/tokenization/</guid>
      <description>&lt;h3 id=&#34;introduction-to-tokenization&#34;&gt;Introduction to tokenization&lt;/h3&gt;
&lt;p&gt;Tokenization is the process of converting normal strings into small little pieces that could be fed into one of our models. It usually comes from a tradition in programming languages, as we can see in &lt;a href=&#34;https://flecart.github.io/notes/automi-e-regexp&#34;&gt;Automi e Regexp&lt;/a&gt; where we define a specific token to have a known pattern, usually recognized by regular expressions.&lt;/p&gt;
&lt;p&gt;There have been historically been many approaches to tokenization, let&amp;rsquo;s see a few:&lt;/p&gt;
&lt;h4 id=&#34;un-approccio-semplice-e-non-funzionante&#34;&gt;Un approccio semplice (e non funzionante)&lt;/h4&gt;
&lt;p&gt;Uno dei primi approcci che potrebbe venire in mente per questo problema di divisione delle parole è avere delle componenti fisse (ad esempio lettere di alfabeto, o lettere) e utilizzare queste per fare tokenization.
Cioè stiamo mappando parti delle parole in modo greedy, prima arriva meglio è. Si potrebbe rappresentare in questo modo:
Da &lt;a href=&#34;https://github.com/microsoft/LoRA/blob/main/examples/NLU/notebooks/01-training-tokenizers.ipynb&#34;&gt;questo ipynb&lt;/a&gt;
&lt;img src=&#34;https://flecart.github.io/images/notes/Tokenization-20240121105419785.webp&#34; style=&#34;width: 100%&#34; class=&#34;center&#34; alt=&#34;Tokenization-20240121105419785&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
