<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machinelearning on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/machinelearning/</link>
    <description>Recent content in Machinelearning on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/machinelearning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bag of words</title>
      <link>https://flecart.github.io/notes/bag-of-words/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/bag-of-words/</guid>
      <description>Bag of words only takes into account the count of the words inside a document, ignoring all the syntax and boundaries. This method is very common for email classifications techniques. We can say bag of words can be some sort of pooling, it&amp;rsquo;s similar to the computer vision analogue. It&amp;rsquo;s difficult to say what is the best method (also a reason why people say NLP is difficult to teach).
Introduction to bag of words Faremo una introduzione di applicazione di Na√Øve Bayes applicato alla classificazione di documenti.</description>
    </item>
    <item>
      <title>Reti convoluzionali</title>
      <link>https://flecart.github.io/notes/reti-convoluzionali/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/reti-convoluzionali/</guid>
      <description>Abbiamo trattato i modelli classici in Convolutional NN. Con i vecchi files di notion
Il Kernel I punti interessanti delle immagini sono solamente i punti di cambio solo che attualmente siamo in stato discreto, quindi ci √® difficile usare una derivata, si usano kernel del tipo: $\left[ 1, 0, -1 \right]$, che sar√† positivo se cresce verso sinistra, negativo se scende. feature map Sono delle mappe che rappresentano alcune informazioni interessanti della nostra immagine.</description>
    </item>
    <item>
      <title>Principal Component Analysis</title>
      <link>https://flecart.github.io/notes/principal-component-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/principal-component-analysis/</guid>
      <description>See https://peterbloem.nl/blog/pca series. The main idea is to find the directions with the most variance in a dataset. Those will be principal components.
There is a very easy derivation present in (Bishop &amp;amp; Bishop 2024). It is also known as the Kosambi-Karhunen-Lo√®ve transform (you will probably like this name more if you are from physics). Another good resource is the Wilkinson.
Technique setting Le&amp;rsquo;t say we have $\left\{ \boldsymbol{x}_{n} \right\}$ observations of dimension $D$ and $n \in \left\{ 1, \dots, N \right\}$.</description>
    </item>
    <item>
      <title>The RLHF pipeline</title>
      <link>https://flecart.github.io/notes/the-rlhf-pipeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/the-rlhf-pipeline/</guid>
      <description>https://huyenchip.com/2023/05/02/rlhf.html √® un blog post che lo descrive in modo abbastanza dettagliato e buono.
Introduzione a RLHF Questo √® il processo che √® quasi la migliore per la produzione di LLM moderni (maggior parte si basano su questo per dire).
Struttura generale Si pu√≤ dire che RLHF si divida in 3 parti fondamentali
Completion il modello viene allenato a completare parole dal web,solitamente √® molto inutile Fine tuning per le singole task, per esempio riassumere, rispondere in certo modo etc.</description>
    </item>
    <item>
      <title>Proximal Policy Optimization</title>
      <link>https://flecart.github.io/notes/proximal-policy-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/proximal-policy-optimization/</guid>
      <description>(Schulman et al. 2017) √® uno degli articoli principali che praticamente hanno dato via al campo. Anche questo √® buono per Policy gradients:
https://lilianweng.github.io/posts/2018-04-08-policy-gradient/
Introduzione a PPO References [1] Schulman et al. ‚ÄúProximal Policy Optimization Algorithms‚Äù 2017</description>
    </item>
    <item>
      <title>Expectation Maximization</title>
      <link>https://flecart.github.io/notes/expectation-maximization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/expectation-maximization/</guid>
      <description>This set takes inspiration from chapter 9.2 of (Bishop 2006). We assume that the reader already knows quite well what is a Gaussian mixture model and we will just restate the models here. We will discuss the problem of estimating the best possible parameters (so, this is a density estimation problem) when the data is generated by a mixture of Gaussians.
Remember that the standard multivariate Gaussian has this format: $$ \mathcal{N}(x \mid \mu, \Sigma) = \frac{1}{\sqrt{ 2\pi }} \frac{1}{\lvert \Sigma \rvert^{1/2} } \exp \left( -\frac{1}{2} (x - \mu)^{T} \Sigma^{-1}(x - \mu) \right) $$ Problem statement Given a set of data points $x_{1}, \dots, x_{n}$ in $\mathbb{R}^{d}$ sampled by $k$ Gaussian each with responsibility $\pi_{k}$ the objective of this problem is to estimate the best $\pi_{k}$ for each Gaussian and the relative mean and covariance matrix.</description>
    </item>
    <item>
      <title>Tokenization</title>
      <link>https://flecart.github.io/notes/tokenization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/tokenization/</guid>
      <description>Introduction to tokenization Tokenization is the process of converting normal strings into small little pieces that could be fed into one of our models. It usually comes from a tradition in programming languages, as we can see in Automi e Regexp where we define a specific token to have a known pattern, usually recognized by regular expressions.
There have been historically been many approaches to tokenization, let&amp;rsquo;s see a few:</description>
    </item>
    <item>
      <title>Na√Øve Bayes</title>
      <link>https://flecart.github.io/notes/na%C3%AFve-bayes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/na%C3%AFve-bayes/</guid>
      <description>Introduzione a Na√Øve Bayes Bisognerebbe in primo momento avere benissimo in mente il significato di probabilit√† condizionata e la regola di naive Bayes in seguito.
Bayes ad alto livello üü© Da un punto di vista intuitivo non √® altro che predire la cosa che abbiamo visto pi√π spesso in quello spazio Assunzioni principali per na√Øve Bayes üü© I sample di input sono condizionalmente indipendenti uno con l&amp;rsquo;altro. Questo permette di utilizzare questa ipotesi $$ P(X_{1}\dots X_{n} | Y = y_{i}) = \prod_{i}^{n} P(X_{i} | Y) $$ E permette di rendere la parte di inferenza anche molto semplice perch√© per classificare un caso basta prendere label con la probabilit√† maggiore.</description>
    </item>
    <item>
      <title>Object detection and Segmentation</title>
      <link>https://flecart.github.io/notes/object-detection-and-segmentation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/object-detection-and-segmentation/</guid>
      <description>Definition of problems Object detection Bisogna trovare all&amp;rsquo;interno dell&amp;rsquo;immagine quali siano gli oggetti presenti, e in pi√π vogliamo sapere dove siano quindi utilizzare una bounding box per caratterizzarli sarebbe buono.
Object segmentation √à riuscire a caratterizzare categoria per categoria per singoli pixelsm e per questo motivo potrei riuscire a fare delle image map in cui colorare singoli oggetti in una categoria.
Datasets Example datasets Pascal VOC 2012 Coco datasets Cityscapes dataset Autogenerated datasets But I don&amp;rsquo;t know much about these datasets Applications Auto drive Campo medico (per segmentazione medica o riconoscimento immagini).</description>
    </item>
    <item>
      <title>Autoencoders</title>
      <link>https://flecart.github.io/notes/autoencoders/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/autoencoders/</guid>
      <description>In questa serie di appunti proviamo a descrivere tutto quello che sappiamo al meglio riguardanti gli autoencoders Blog di riferimento Blog secondario che sembra buono
Introduzione agli autoencoders L&amp;rsquo;idea degli autoencoders √® rappresentare la stessa cosa attraverso uno spazio minore, in un certo senso √® la compressione con loss. Per cosa intendiamo qualunque tipologia di dato, che pu√≤ spaziare fra immagini, video, testi, musica e simili. Qualunque cosa che noi possiamo rappresentare in modo digitale possiamo costruirci un autoencoder.</description>
    </item>
    <item>
      <title>Parametric Models</title>
      <link>https://flecart.github.io/notes/parametric-models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/parametric-models/</guid>
      <description>In this note we will first talk about briefly some of the main differences of the three main approaches regarding statistics: the bayesian, the frequentist and the statistical learning methods and then present the concept of the estimator, compare how the approaches differ from method to method, we will explain maximum likelihood estimator and the Rao Cramer Bound.
Short introduction to the statistical methods Bayesian With bayesian methods we often assume a prior on the parameters, often human picked, that allows to give a regularizer term over the possible distribution that we are trying to model.</description>
    </item>
    <item>
      <title>Linear Regression methods</title>
      <link>https://flecart.github.io/notes/linear-regression-methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/linear-regression-methods/</guid>
      <description>We will present some methods related to regression methods for data analysis. Most of the work here is from (Hastie et al. 2009).
Problem setting In usual regression problems we want to reach the $\arg \min \mathbb{E}_{Y \mid X} \left[ (Y - f(X))^{2} \right]$ and the solution is given by the conditional mean: $f^{*} = \mathbb{E}(Y \mid X = x)$. We have done something similar with Logistic Regression, but that is just for classification analysis.</description>
    </item>
    <item>
      <title>Logistic Regression</title>
      <link>https://flecart.github.io/notes/logistic-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/logistic-regression/</guid>
      <description>Introduzione alla logistic regression Giustificazione del metodo Questo √® uno dei modelli classici, creati da Minsky qualche decennio fa In questo caso andiamo direttamente a computare il valore di $P(Y|X)$ durante l&amp;rsquo;inferenza, quindi si parla di modello discriminativo.
Introduzione al problema Supponiamo che
$Y$ siano variabili booleane $X_{i}$ siano variabili continue $X_{i}$ siano indipendenti uno dall&amp;rsquo;altro. $P(X_{i}| Y= k)$ sono modellate tramite distribuzioni gaussiane $\mathbb{N}(\mu_{ik}, \sigma_{i})$ NOTA! la varianza non dipende dalle feature!</description>
    </item>
    <item>
      <title>Alberi di decisione</title>
      <link>https://flecart.github.io/notes/alberi-di-decisione/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/alberi-di-decisione/</guid>
      <description>Introduzione agli alberi di decisione Setting del problema üü©- Spazio delle ipotesi Definizione spazio ipotesi üü©&amp;mdash; Per spazio delle ipotesi andiamo a considerare l&amp;rsquo;insieme delle funzioni rappresentabili dal nostro modello. Questo implica che l&amp;rsquo;allenamento ricerca l&amp;rsquo;ipotesi ossia la parametrizzazione ottimale del nostro modello, ottimale in quanto minimizza l&amp;rsquo;errore che viene compiuto nel training set.
L&amp;rsquo;insieme iniziale si pu√≤ anche considerare come inductive bias ossia il restringimento solamente a certe ipotesi e non tutte.</description>
    </item>
    <item>
      <title>Softmax Function</title>
      <link>https://flecart.github.io/notes/softmax-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/softmax-function/</guid>
      <description>Softmax is one of the most important functions for neural networks. It also has some interesting properties that we list here. This function is part of The Exponential Family, one can also see that the sigmoid function is a particular case of this softmax, just two variables. Sometimes this could be seen as a relaxation of the action potential inspired by neuroscience (See The Neuron for a little bit more about neurons).</description>
    </item>
    <item>
      <title>Backpropagation</title>
      <link>https://flecart.github.io/notes/backpropagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/backpropagation/</guid>
      <description>Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well. An important observation is that this algorithm is linear: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover.</description>
    </item>
    <item>
      <title>Introduction to Advanced Machine Learning</title>
      <link>https://flecart.github.io/notes/introduction-to-advanced-machine-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/introduction-to-advanced-machine-learning/</guid>
      <description>Introduction to the course Machine learning gives a new way of thinking about reality: instead of trying to capture a part of reality, as most of the old sciences, we go to the meta-level and try to produce an automated method to capture reality. This first lesson will be more a lesson of philosophy. There a paradigm shift in the sense of Thomas Kuhn&amp;rsquo;s scientific revolutions. But how can such paradigm shift?</description>
    </item>
    <item>
      <title>Anomaly Detection</title>
      <link>https://flecart.github.io/notes/anomaly-detection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/anomaly-detection/</guid>
      <description>Anomaly detection is a problem in machine learning that is of a big interest in industry. For example a bank needs to identify problems in transactions, doctors need it to see illness, or suspicious behaviours for law (no Orwell here). The main difference between this and classification is that here we have no classes.
Setting of the problem Let&amp;rsquo;s say we have a set $X = \left\{ x_{1}, \dots, x_{n} \right\} \subseteq \mathcal{N} \subseteq \mathcal{X} = \mathbb{R}^{d}$ We say this set is the normal set, and $X$ are our samples but it&amp;rsquo;s quite complex, so we need an approximation to say whether if a set is normal or not.</description>
    </item>
    <item>
      <title>The Kernel Trick</title>
      <link>https://flecart.github.io/notes/the-kernel-trick/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/the-kernel-trick/</guid>
      <description>As we will briefly see, Kernels will have an important role in many machine learning applications. In this note we will get to know what are Kernels and why are they useful.
We briefly state the requirements of a Kernel, then we will argue with a simple example why they are useful.
Kernel Function requirements Every function $k$ must be
Symmetric: $\forall x, x&#39; \in \mathbb{X}$ we have $k(x, x&#39;) = k(x&#39;, x)$ Positive definiteness For all $A$ we have $K_{A A}$ is positive definite.</description>
    </item>
  </channel>
</rss>
