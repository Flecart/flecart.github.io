<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machinelearning on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/machinelearning/</link>
    <description>Recent content in Machinelearning on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/machinelearning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Alberi di decisione</title>
      <link>https://flecart.github.io/notes/alberi-di-decisione/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/alberi-di-decisione/</guid>
      <description>Introduzione agli alberi di decisione Setting del problema üü©- Spazio delle ipotesi Definizione spazio ipotesi üü©&amp;mdash; Per spazio delle ipotesi andiamo a considerare l&amp;rsquo;insieme delle funzioni rappresentabili dal nostro modello. Questo implica che l&amp;rsquo;allenamento ricerca l&amp;rsquo;ipotesi ossia la parametrizzazione ottimale del nostro modello, ottimale in quanto minimizza l&amp;rsquo;errore che viene compiuto nel training set.
L&amp;rsquo;insieme iniziale si pu√≤ anche considerare come inductive bias ossia il restringimento solamente a certe ipotesi e non tutte.</description>
    </item>
    <item>
      <title>Autoencoders</title>
      <link>https://flecart.github.io/notes/autoencoders/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/autoencoders/</guid>
      <description>In questa serie di appunti proviamo a descrivere tutto quello che sappiamo al meglio riguardanti gli autoencoders Blog di riferimento Blog secondario che sembra buono
Introduzione agli autoencoders L&amp;rsquo;idea degli autoencoders √® rappresentare la stessa cosa attraverso uno spazio minore, in un certo senso √® la compressione con loss. Per cosa intendiamo qualunque tipologia di dato, che pu√≤ spaziare fra immagini, video, testi, musica e simili. Qualunque cosa che noi possiamo rappresentare in modo digitale possiamo costruirci un autoencoder.</description>
    </item>
    <item>
      <title>Fisher&#39;s Linear Discriminant</title>
      <link>https://flecart.github.io/notes/fishers-linear-discriminant/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/fishers-linear-discriminant/</guid>
      <description>Fisher&amp;rsquo;s Linear Discriminant is a simple idea used to linearly classify our data. The image above, taken from (Bishop 2006), is the summary of the idea. We want to maximize the distance from the centers, while minimizing the interclass variance. Let&amp;rsquo;s consider these two classes of points and let&amp;rsquo;s set $$ m_{j} = \frac{1}{N_{j}}\sum_{i : y_{i} = j} x_{i} $$ Which is just the mean of our cluster. Then if we consider a linear projection with $w$ the projected mean would be $w^{T}m_{j}$ over every class $j$.</description>
    </item>
    <item>
      <title>Expectation Maximization</title>
      <link>https://flecart.github.io/notes/expectation-maximization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/expectation-maximization/</guid>
      <description>This set takes inspiration from chapter 9.2 of (Bishop 2006). We assume that the reader already knows quite well what is a Gaussian mixture model and we will just restate the models here. We will discuss the problem of estimating the best possible parameters (so, this is a density estimation problem) when the data is generated by a mixture of Gaussians.
Remember that the standard multivariate Gaussian has this format: $$ \mathcal{N}(x \mid \mu, \Sigma) = \frac{1}{\sqrt{ 2\pi }} \frac{1}{\lvert \Sigma \rvert^{1/2} } \exp \left( -\frac{1}{2} (x - \mu)^{T} \Sigma^{-1}(x - \mu) \right) $$ Problem statement Given a set of data points $x_{1}, \dots, x_{n}$ in $\mathbb{R}^{d}$ sampled by $k$ Gaussian each with responsibility $\pi_{k}$ the objective of this problem is to estimate the best $\pi_{k}$ for each Gaussian and the relative mean and covariance matrix.</description>
    </item>
    <item>
      <title>Tokenization</title>
      <link>https://flecart.github.io/notes/tokenization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/tokenization/</guid>
      <description>Introduction to tokenization Tokenization is the process of converting normal strings into small little pieces that could be fed into one of our models. It usually comes from a tradition in programming languages, as we can see in Automi e Regexp where we define a specific token to have a known pattern, usually recognized by regular expressions.
There have been historically been many approaches to tokenization, let&amp;rsquo;s see a few:</description>
    </item>
    <item>
      <title>Backpropagation</title>
      <link>https://flecart.github.io/notes/backpropagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/backpropagation/</guid>
      <description>Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well. An important observation is that this algorithm is linear: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover.</description>
    </item>
    <item>
      <title>Counterfactual Invariance</title>
      <link>https://flecart.github.io/notes/counterfactual-invariance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/counterfactual-invariance/</guid>
      <description>Machine learning cannot distinguish between causal and environment features.
Shortcut learning Often we observe shortcut learning: the model learns some dataset dependent shortcuts (e.g. the machine that was used to take the X-ray) to make inference, but this is very brittle, and is not usually able to generalize.
Shortcut learning happens when there are correlations in the test set between causal and non-causal features. Our object of interest should be the main focus, not the environment around, in most of the cases.</description>
    </item>
    <item>
      <title>Lagrange Multipliers</title>
      <link>https://flecart.github.io/notes/lagrange-multipliers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/lagrange-multipliers/</guid>
      <description>This is also known as Lagrange Optimization or undetermined multipliers. Some of these notes are based on Appendix E of (Bishop 2006), others were found when studying bits of rational mechanics. Also (Boyd &amp;amp; Vandenberghe 2004) chapter 5 should be a good resource on this topic.
Let&amp;rsquo;s consider a standard linear optimization problem $$ \begin{array} \\ \min f_{0}(x) \\ \text{subject to } f_{i}(x) \leq 0 \\ h_{j}(x) = 0 \end{array} $$ Lagrangian function And let&amp;rsquo;s consider the Lagrangian function associated to this problem defined as $$ \mathcal{L}(x, \lambda, \nu) = f_{0}(x) + \sum \lambda_{i}f_{i}(x) + \sum\nu_{j}h_{j}(x) $$ We want to say something about this function, because it is able to simplify the optimization problem a lot, but first we want to study this mathematically.</description>
    </item>
    <item>
      <title>Active Learning</title>
      <link>https://flecart.github.io/notes/active-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/active-learning/</guid>
      <description>Active Learning concerns methods to decide how to sample the most useful information in a specific domain. In this setting, we are interested in the concept of usefulness of information. One of our main goals is to reduce uncertainty, thus, Entropy-based (mutual information) methods are often used. For example, we can use active learning to choose what samples needs to be labelled in order to have highest accuracy on the trained model, when labelling is costly.</description>
    </item>
    <item>
      <title>The RLHF pipeline</title>
      <link>https://flecart.github.io/notes/the-rlhf-pipeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/the-rlhf-pipeline/</guid>
      <description>https://huyenchip.com/2023/05/02/rlhf.html √® un blog post che lo descrive in modo abbastanza dettagliato e buono.
Introduzione a RLHF Questo √® il processo che √® quasi la migliore per la produzione di LLM moderni (maggior parte si basano su questo per dire).
Struttura generale Si pu√≤ dire che RLHF si divida in 3 parti fondamentali
Completion il modello viene allenato a completare parole dal web,solitamente √® molto inutile Fine tuning per le singole task, per esempio riassumere, rispondere in certo modo etc.</description>
    </item>
    <item>
      <title>Principal Component Analysis</title>
      <link>https://flecart.github.io/notes/principal-component-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/principal-component-analysis/</guid>
      <description>See https://peterbloem.nl/blog/pca series. The main idea is to find the directions with the most variance in a dataset. Those will be principal components.
There is a very easy derivation present in (Bishop &amp;amp; Bishop 2024). It is also known as the Kosambi-Karhunen-Lo√®ve transform (you will probably like this name more if you are from physics). Another good resource is the Wilkinson.
Technique setting Le&amp;rsquo;t say we have $\left\{ \boldsymbol{x}_{n} \right\}$ observations of dimension $D$ and $n \in \left\{ 1, \dots, N \right\}$.</description>
    </item>
    <item>
      <title>Softmax Function</title>
      <link>https://flecart.github.io/notes/softmax-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/softmax-function/</guid>
      <description>Softmax is one of the most important functions for neural networks. It also has some interesting properties that we list here. This function is part of The Exponential Family, one can also see that the sigmoid function is a particular case of this softmax, just two variables. Sometimes this could be seen as a relaxation of the action potential inspired by neuroscience (See The Neuron for a little bit more about neurons).</description>
    </item>
    <item>
      <title>Anomaly Detection</title>
      <link>https://flecart.github.io/notes/anomaly-detection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/anomaly-detection/</guid>
      <description>Anomaly detection is a problem in machine learning that is of a big interest in industry. For example a bank needs to identify problems in transactions, doctors need it to see illness, or suspicious behaviors for law (no Orwell here). The main difference between this and classification is that here we have no classes.
Setting of the problem Let&amp;rsquo;s say we have a set $X = \left\{ x_{1}, \dots, x_{n} \right\} \subseteq \mathcal{N} \subseteq \mathcal{X} = \mathbb{R}^{d}$ We say this set is the normal set, and $X$ are our samples but it&amp;rsquo;s quite complex, so we need an approximation to say whether if a set is normal or not.</description>
    </item>
    <item>
      <title>Parametric Models</title>
      <link>https://flecart.github.io/notes/parametric-models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/parametric-models/</guid>
      <description>In this note we will first talk about briefly some of the main differences of the three main approaches regarding statistics: the bayesian, the frequentist and the statistical learning methods and then present the concept of the estimator, compare how the approaches differ from method to method, we will explain maximum likelihood estimator and the Rao Cramer Bound.
Short introduction to the statistical methods Bayesian With bayesian methods we often assume a prior on the parameters, often human picked, that allows to give a regularizer term over the possible distribution that we are trying to model.</description>
    </item>
    <item>
      <title>Na√Øve Bayes</title>
      <link>https://flecart.github.io/notes/na%C3%AFve-bayes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/na%C3%AFve-bayes/</guid>
      <description>Introduzione a Na√Øve Bayes NOTE: this note should be reviewed after the course I took in NLP. This is a very old note, not even well written.
Bisognerebbe in primo momento avere benissimo in mente il significato di probabilit√† condizionata e la regola di naive Bayes in seguito.
Bayes ad alto livello üü© Da un punto di vista intuitivo non √® altro che predire la cosa che abbiamo visto pi√π spesso in quello spazio Assunzioni principali per na√Øve Bayes üü© I sample di input sono condizionalmente indipendenti uno con l&amp;rsquo;altro.</description>
    </item>
    <item>
      <title>Reti convoluzionali</title>
      <link>https://flecart.github.io/notes/reti-convoluzionali/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/reti-convoluzionali/</guid>
      <description>Abbiamo trattato i modelli classici in Convolutional NN. Con i vecchi files di notion
Il Kernel I punti interessanti delle immagini sono solamente i punti di cambio solo che attualmente siamo in stato discreto, quindi ci √® difficile usare una derivata, si usano kernel del tipo: $\left[ 1, 0, -1 \right]$, che sar√† positivo se cresce verso sinistra, negativo se scende. feature map Sono delle mappe che rappresentano alcune informazioni interessanti della nostra immagine.</description>
    </item>
    <item>
      <title>Gaussian Processes</title>
      <link>https://flecart.github.io/notes/gaussian-processes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/gaussian-processes/</guid>
      <description>Gaussian processes can be viewed through a Bayesian lens of the function space: rather than sampling over individual data points, we are now sampling over entire functions. They extend the idea of bayesian linear regression by introducing an infinite number of feature functions for the input XXX.
In geostatistics, Gaussian processes are referred to as kriging regressions, and many other models, such as Kalman Filters or radial basis function networks, can be understood as special cases of Gaussian processes.</description>
    </item>
    <item>
      <title>Bag of words</title>
      <link>https://flecart.github.io/notes/bag-of-words/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/bag-of-words/</guid>
      <description>Bag of words only takes into account the count of the words inside a document, ignoring all the syntax and boundaries. This method is very common for email classifications techniques. We can say bag of words can be some sort of pooling, it&amp;rsquo;s similar to the computer vision analogue. It&amp;rsquo;s difficult to say what is the best method (also a reason why people say NLP is difficult to teach).
Introduction to bag of words Faremo una introduzione di applicazione di Na√Øve Bayes applicato alla classificazione di documenti.</description>
    </item>
    <item>
      <title>Object detection and Segmentation</title>
      <link>https://flecart.github.io/notes/object-detection-and-segmentation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/object-detection-and-segmentation/</guid>
      <description>Definition of problems Object detection Bisogna trovare all&amp;rsquo;interno dell&amp;rsquo;immagine quali siano gli oggetti presenti, e in pi√π vogliamo sapere dove siano quindi utilizzare una bounding box per caratterizzarli sarebbe buono.
Object segmentation √à riuscire a caratterizzare categoria per categoria per singoli pixelsm e per questo motivo potrei riuscire a fare delle image map in cui colorare singoli oggetti in una categoria.
Datasets Example datasets Pascal VOC 2012 Coco datasets Cityscapes dataset Autogenerated datasets But I don&amp;rsquo;t know much about these datasets Applications Auto drive Campo medico (per segmentazione medica o riconoscimento immagini).</description>
    </item>
    <item>
      <title>Proximal Policy Optimization</title>
      <link>https://flecart.github.io/notes/proximal-policy-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/proximal-policy-optimization/</guid>
      <description>(Schulman et al. 2017) √® uno degli articoli principali che praticamente hanno dato via al campo. Anche questo √® buono per Policy gradients:
https://lilianweng.github.io/posts/2018-04-08-policy-gradient/
Introduzione a PPO References [1] Schulman et al. ‚ÄúProximal Policy Optimization Algorithms‚Äù 2017</description>
    </item>
    <item>
      <title>Introduction to Advanced Machine Learning</title>
      <link>https://flecart.github.io/notes/introduction-to-advanced-machine-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/introduction-to-advanced-machine-learning/</guid>
      <description>Introduction to the course Machine learning offers a new way of thinking about reality: rather than attempting to directly capture a fragment of reality, as many traditional sciences have done, we elevate to the meta-level and strive to create an automated method for capturing it.
This first lesson will be more philosophical in nature. We are witnessing a paradigm shift in the sense described by Thomas Kuhn in his theory of scientific revolutions.</description>
    </item>
    <item>
      <title>Logistic Regression</title>
      <link>https://flecart.github.io/notes/logistic-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/logistic-regression/</guid>
      <description>Queste note sono molto di base. Per cose leggermente pi√π avanzate bisogna guardare Bayesian Linear Regression, Linear Regression methods.
Introduzione alla logistic regression Giustificazione del metodo Questo √® uno dei modelli classici, creati da Minsky qualche decennio fa In questo caso andiamo direttamente a computare il valore di $P(Y|X)$ durante l&amp;rsquo;inferenza, quindi si parla di modello discriminativo.
Introduzione al problema Supponiamo che
$Y$ siano variabili booleane $X_{i}$ siano variabili continue $X_{i}$ siano indipendenti uno dall&amp;rsquo;altro.</description>
    </item>
    <item>
      <title>Cross Validation and Model Selection</title>
      <link>https://flecart.github.io/notes/cross-validation-and-model-selection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/cross-validation-and-model-selection/</guid>
      <description>There is a big difference between the empirical score and the expected score; in the beginning, we had said something about this in Introduction to Advanced Machine Learning. We will develop more methods to better comprehend this fundamental principles.
How can we estimate the expected risk of a particular estimator or algorithm? We can use the cross-validation method. This method is used to estimate the expected risk of a model, and it is a fundamental method in machine learning.</description>
    </item>
    <item>
      <title>The Kernel Trick</title>
      <link>https://flecart.github.io/notes/the-kernel-trick/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/the-kernel-trick/</guid>
      <description>As we will briefly see, Kernels will have an important role in many machine learning applications. In this note we will get to know what are Kernels and why are they useful. Intuitively they measure the similarity between two input points. So if they are close the kernel should be big, else it should be small.
We briefly state the requirements of a Kernel, then we will argue with a simple example why they are useful.</description>
    </item>
    <item>
      <title>Linear Regression methods</title>
      <link>https://flecart.github.io/notes/linear-regression-methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/linear-regression-methods/</guid>
      <description>We will present some methods related to regression methods for data analysis. Some of the work here is from (Hastie et al. 2009). This note does not treat the bayesian case, you should see Bayesian Linear Regression for that.
Problem setting In usual regression problems we want to reach the $\arg \min \mathbb{E}_{Y \mid X} \left[ (Y - f(X))^{2} \right]$ and the solution is given by the conditional mean: $f^{*} = \mathbb{E}(Y \mid X = x)$.</description>
    </item>
  </channel>
</rss>
