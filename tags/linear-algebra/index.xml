<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>⚗Linear-Algebra on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/linear-algebra/</link>
    <description>Recent content in ⚗Linear-Algebra on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Inner product spaces</title>
      <link>https://flecart.github.io/notes/inner-product-spaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/inner-product-spaces/</guid>
      <description>This set of notes tries to fix what I haven&amp;rsquo;t learned in 2021 course in algebra. It&amp;rsquo;s about inner product spaces. A good online reference on the topic is wilkinson.
Definitions Inner product space We define the vector space $V$ to be a inner product space, if we define a inner product operator ($\langle \cdot, \cdot \rangle : V \times V \to R$) such that the following are valid:
It is linear on both arguments: $$ \langle \alpha x_{1} + \beta x_{2}, y \rangle = \alpha \langle x_{1}, y \rangle + \beta \langle x_{2}, y \rangle $$ It is a symmetric operator: $\langle x, y \rangle = \langle y, x \rangle$ It is positive definite that is we have $\forall x \in V: \langle x, x \rangle \geq 0$ with equality only if $x = \boldsymbol{0}$ An example of such operator is the classical cosine distance which is just the angle, or euclidean distance.</description>
    </item>
    <item>
      <title>Introduzione algebra</title>
      <link>https://flecart.github.io/notes/introduzione-algebra/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/introduzione-algebra/</guid>
      <description>Tutta sta parte si fa in modo formale in Sistemi Lineari e determinanti, quindi potresti saltarla totalmente
Equazioni lineari L&amp;rsquo;obiettivo dell&amp;rsquo;algebra lineare è risolvere n equazioni con n sconosciuti di primo grado. Cosa che ci riesce con grandissimo successo! Andiamo ora a definire meglio cosa è una equazione lineare
Definizione Una equazione lineare è una equazione a coefficienti appartenenti a un certo campo (che può essere R) e incognite il cui grado è 1 e che siano indipendenti:</description>
    </item>
    <item>
      <title>Multi Variable Derivatives</title>
      <link>https://flecart.github.io/notes/multi-variable-derivatives/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/multi-variable-derivatives/</guid>
      <description>Multi-variable derivative To the people that are not used to matrix derivatives (like me) it could be useful to see how $$ \frac{ \partial u^{T}Su }{ \partial u } = 2Su $$ First, we note that if you derive with respect to some matrix, the output will be of the same dimension of that matrix. That notation is just deriving every single component independently and then joining them together, so it will be better understood as as $$ \frac{ \partial u^{T}Su }{ \partial u } = \begin{bmatrix} \frac{ \partial u^{T}Su }{ \partial u_{1} } \ \dots \ \frac{ \partial u^{T}Su }{ \partial u_{M} } \ \end{bmatrix} \begin{bmatrix} 2(Su){1} \ \dots \ 2(Su){M} \end{bmatrix} = 2Su $$ So we can prove each derivative independently, it&#39;s just a lot of manual work!</description>
    </item>
    <item>
      <title>Spazi vettoriali</title>
      <link>https://flecart.github.io/notes/spazi-vettoriali/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/spazi-vettoriali/</guid>
      <description>Spazi vettoriali 1.1 Piano cartesiano 1.1.1 Definizione Possiamo considerare il piano cartesiano come l&amp;rsquo;insieme $\R^2$ potremmo dire che esiste una corrispondenza fra una coordinata e un punto del piano, una volta che abbiamo definito un punto di origine. Si può vedere anche come corrispondenza biunivoca con vettori del piano per l&amp;rsquo;origine (parte dall&amp;rsquo;origine).
Questa cosa vale anche per uno spazio n-dimensionale, non soltanto due, ma per semplicità di introduzione di questo lo faccio con 2</description>
    </item>
    <item>
      <title>Autovalori e Autovettori</title>
      <link>https://flecart.github.io/notes/autovalori-e-autovettori/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/autovalori-e-autovettori/</guid>
      <description>Ha senso solamente parlare di autovettori quando si ha una applicazione lineare con stesso dominio e stesso codominio.
Vorremmo trovare una buona matrice che sia diagonale.
6.1 Diagonalizzabilità 6.1.1 Definizione per funzione e matrice Questo perché vorrei una base in cui si abbia un matrice diagonale. (quindi probabilmente P è una matrice identità).
Perché ci piacciono le matrici diagonali
Se ho una matrice diagonale, si ha che l&amp;rsquo;applicazione lineare è un semplice scaling dei vettori della base.</description>
    </item>
    <item>
      <title>Sistemi Lineari e determinanti</title>
      <link>https://flecart.github.io/notes/sistemi-lineari-e-determinanti/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/sistemi-lineari-e-determinanti/</guid>
      <description>4.1 Sistemi lineari La cosa buona è che possiamo analizzare il sistema lineare utilizzando tutti i teoremi che abbiamo sviluppato finora, quindi siamo molto più potenti per attaccare questo problema.
Definiamo un sistema lineare così
$Ax = b$ con A la matrice associata.
4.1.1 Preimmagine Data una applicazione lineare $F:V \to W$, allora la controimmagine è l&amp;rsquo;insieme dei vettori di V che fanno a finire in quel punto, in matematichese:</description>
    </item>
    <item>
      <title>Algebra modulare</title>
      <link>https://flecart.github.io/notes/algebra-modulare/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/algebra-modulare/</guid>
      <description>Algebra modulare Assunzioni Andiamo ora ad assumere l&amp;rsquo;esistenza e correttezza di alcune cose di base. (in teoria si possono dimostrare da cose più di base, ma non ho tempo).
Teorema fondamentale dell&amp;rsquo;algebra Ogni numero intero si fattorizza in modo unico.
Algoritmo di Euclide La conseguenza più importante di questo teorema, dovuto ad Euclide è che se ho $a, b \in \mathbb{Z}$ allora esistono resto e dividendo fra i due. Ossia $\exists q, p : a\mid b = qk + p$ per qualche $k$ intero</description>
    </item>
    <item>
      <title>Cauchy-Schwarz Inequality</title>
      <link>https://flecart.github.io/notes/cauchy-schwarz-inequality/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/cauchy-schwarz-inequality/</guid>
      <description>This note briefly states and proves one of the most famous inequalities in geometry/analysis.
Theorem Statement Given $2n$ real numbers (you can see these two also as $n$ dimensional vectors), such as $x_{1}, \dots, x_{n}$ and $y_{1}, \dots, y_{n}$ then we have that $$ \left( \sum_{i = 1}^{n} x_{i}y_{i} \right) ^{2} \leq \left( \sum_{i= 1}^{n} x^{2}_{i} \right) \left( \sum_{i = 1}^{n} y^{2}_{i} \right) $$ In vectorial form we can rewrite this as $$ \lvert \langle u, v \rangle \rvert ^{2} \leq \langle u, u \rangle \cdot \langle v, v \rangle $$ with $u = \left( x_{1}, \dots, x_{n} \right)$ and $v = \left( y_{1}, \dots, y_{n} \right)$ and the $\langle \cdot, \cdot \rangle$ operator is the inner product.</description>
    </item>
    <item>
      <title>Cambio di Base</title>
      <link>https://flecart.github.io/notes/cambio-di-base/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/cambio-di-base/</guid>
      <description>Nozioni da avere prima di Cambio di Base Applicazioni lineari La definizione di applicazione lineare La matrice associata L&amp;rsquo;esistenza e unicità di una applicazione lineare rispetto a una base Le coordinate di un punto rispetto a una base. Matrice del Cambio di Base Se ho due spazi vettoriali
Intuizione in $R$ Le coordinate dei punti in $R$ sono uguali a $V$ per le basi canoniche, ma questo vale solamente per $R$, ora vogliamo andare a dire una cosa più forte, il cambio di base Poi sarà importantissimo questa nozione, applicazione di base in ML è Principal Component Analysis.</description>
    </item>
    <item>
      <title>Base e dimensione</title>
      <link>https://flecart.github.io/notes/base-e-dimensione/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/base-e-dimensione/</guid>
      <description>2.1 Basi 2.1.1 Definizione Un insieme di vettori $v_1,...,v_n$ sono basi di uno spazio vettoriale $V$ se sono soddisfatte queste proprietà
$V = \langle v_1,...,v_n\rangle$ $v_1,...,v_n$ sono linearmente indipendenti Dalla proprietà 2 potremmo anche dire che è il minimo insieme di vettori necessario per avere questa base.
Finitamente generato
Se l&amp;rsquo;insieme dei vettori nella base è finito allora posso dire che è finitamente generato
Ma possiamo trovare anche spazi che non sono finitamente generati come $\R[x]$ che non hanno un numero finito di basi (perché dipende dal grado dei polinomi che può essere infinito).</description>
    </item>
    <item>
      <title>Applicazioni lineari</title>
      <link>https://flecart.github.io/notes/applicazioni-lineari/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/applicazioni-lineari/</guid>
      <description>3.1 Introduzione e definizione Si definisce applicazione lineare una funzione (omomorfica) che preserva la struttura dello spazio vettoriale, ossia vale che
$$ f:V \to W, \text{ tale che } \\ f(u + v) = f(u) +f(v)\\, f(\lambda v) = \lambda f(v) $$ Vengono mantenute alcune caratteristiche principali. In modo simile si possono definire omomorfismi per tutte le altre strutture algebriche, la cosa importante è che lo spazio d&amp;rsquo;arrivo possieda ancora tutte le stesse operazioni.</description>
    </item>
  </channel>
</rss>
