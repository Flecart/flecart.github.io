<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>⚗Linear-Algebra on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/linear-algebra/</link>
    <description>Recent content in ⚗Linear-Algebra on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.143.1</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Spazi vettoriali</title>
      <link>https://flecart.github.io/notes/spazi-vettoriali/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/spazi-vettoriali/</guid>
      <description>&lt;h1 id=&#34;spazi-vettoriali&#34;&gt;Spazi vettoriali&lt;/h1&gt;
&lt;h2 id=&#34;11-piano-cartesiano&#34;&gt;1.1 Piano cartesiano&lt;/h2&gt;
&lt;h3 id=&#34;111-definizione&#34;&gt;1.1.1 Definizione&lt;/h3&gt;
&lt;p&gt;Possiamo considerare il piano cartesiano come l&amp;rsquo;insieme $\R^2$ potremmo dire che esiste una corrispondenza fra una coordinata e un punto del piano, una volta che abbiamo definito un punto di origine. Si può vedere anche come corrispondenza biunivoca con vettori del piano per l&amp;rsquo;origine (parte dall&amp;rsquo;origine).&lt;/p&gt;
&lt;p&gt;Questa cosa vale anche per uno spazio n-dimensionale, non soltanto due, ma per semplicità di introduzione di questo lo faccio con 2&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multi Variable Derivatives</title>
      <link>https://flecart.github.io/notes/multi-variable-derivatives/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/multi-variable-derivatives/</guid>
      <description>&lt;h4 id=&#34;multi-variable-derivative&#34;&gt;Multi-variable derivative&lt;/h4&gt;
&lt;h1 id=&#34;endbmatrix&#34;&gt;To the people that are not used to matrix derivatives (like me) it could be useful to see how
$$
\frac{ \partial u^{T}Su }{ \partial u }  = 2Su
$$
First, we note that if you derive with respect to some matrix, the output will be of the same dimension of that matrix. That notation is just deriving every single component independently and then joining them together, so it will be better understood as as
$$
\frac{ \partial u^{T}Su }{ \partial u }  =
\begin{bmatrix}
\frac{ \partial u^{T}Su }{ \partial u_{1} }  \
\dots \
\frac{ \partial u^{T}Su }{ \partial u_{M} }  \
\end{bmatrix}&lt;/h1&gt;
$$
So we can prove each derivative independently, it&#39;s just a lot of manual work!
We see that $u^{T}Su$ is just a quadratic form, studied in &lt;a href=&#34;https://flecart.github.io/notes/massimi-minimi-multi-variabile#forme-quadratiche&#34;&gt;Massimi minimi multi-variabile#Forme quadratiche&lt;/a&gt; so it is just computing this:
$$&lt;p&gt;
u^{T}Su = \sum_{i, j = 1, 1}^{M} u_{i}u_{j}S_{ij} \implies \frac{ \partial u^{T}Su }{ \partial u_{1} } =2u_{1}S_{11} + \sum_{j \neq 1}^{M}(u_{j}S_{1j}  + u_{j}S_{j1}) = 2\left( u_{1}S_{11} + \sum_{j \neq 1}u_{j}S_{1j} \right) = 2(Su)_{1}
$$
Last equation is true because $S$ is a symmetric matrix, then we easily see that indeed it&amp;rsquo;s true that indeed it&amp;rsquo;s the first row of the $Su$ matrix multiplied by 2.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Autovalori e Autovettori</title>
      <link>https://flecart.github.io/notes/autovalori-e-autovettori/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/autovalori-e-autovettori/</guid>
      <description>&lt;p&gt;Ha senso solamente parlare di autovettori quando si ha una &lt;strong&gt;applicazione lineare con stesso dominio e stesso codominio.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Vorremmo trovare una buona matrice che sia diagonale.&lt;/p&gt;
&lt;h2 id=&#34;61-diagonalizzabilità&#34;&gt;6.1 Diagonalizzabilità&lt;/h2&gt;
&lt;h3 id=&#34;611-definizione-per-funzione-e-matrice&#34;&gt;6.1.1 Definizione per funzione e matrice&lt;/h3&gt;
&lt;img src=&#34;https://flecart.github.io/images/notes/image/universita/ex-notion/Cambio di Base e Autovalori/Untitled 3.png&#34; style=&#34;width: 100%&#34; class=&#34;center&#34; alt=&#34;image/universita/ex-notion/Cambio di Base e Autovalori/Untitled 3&#34;&gt;
&lt;img src=&#34;https://flecart.github.io/images/notes/image/universita/ex-notion/Cambio di Base e Autovalori/Untitled 4.png&#34; style=&#34;width: 100%&#34; class=&#34;center&#34; alt=&#34;image/universita/ex-notion/Cambio di Base e Autovalori/Untitled 4&#34;&gt;
&lt;p&gt;Questo perché vorrei una base in cui si abbia un matrice diagonale. (quindi probabilmente P è una matrice identità).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cauchy-Schwarz Inequality</title>
      <link>https://flecart.github.io/notes/cauchy-schwarz-inequality/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/cauchy-schwarz-inequality/</guid>
      <description>&lt;p&gt;This note briefly states and proves one of the most famous inequalities in geometry/analysis.&lt;/p&gt;
&lt;h3 id=&#34;theorem-statement&#34;&gt;Theorem Statement&lt;/h3&gt;
$$
\left( \sum_{i = 1}^{n} x_{i}y_{i} \right) ^{2} \leq \left( \sum_{i= 1}^{n} x^{2}_{i} \right)  \left( \sum_{i = 1}^{n} y^{2}_{i} \right)
$$$$
\lvert \langle u, v \rangle  \rvert ^{2} \leq \langle u, u \rangle  \cdot \langle v, v \rangle 
$$&lt;p&gt;
with $u = \left( x_{1}, \dots, x_{n} \right)$ and  $v = \left( y_{1}, \dots, y_{n} \right)$  and the $\langle \cdot, \cdot \rangle$ operator is the &lt;a href=&#34;https://flecart.github.io/notes/inner-product-spaces&#34;&gt;inner product&lt;/a&gt;.
We have equality if and only if $u$ and $v$ are linearly dependent (this one is easy to prove if seen from the vectorial view).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Inner product spaces</title>
      <link>https://flecart.github.io/notes/inner-product-spaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/inner-product-spaces/</guid>
      <description>&lt;p&gt;This set of notes tries to fix what I haven&amp;rsquo;t learned in 2021 course in algebra. It&amp;rsquo;s about inner product spaces.
A good online reference on the topic is &lt;a href=&#34;https://rich-d-wilkinson.github.io/MATH3030/2.3-linalg-innerprod.html&#34;&gt;wilkinson&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;definitions&#34;&gt;Definitions&lt;/h2&gt;
&lt;h3 id=&#34;inner-product-space&#34;&gt;Inner product space&lt;/h3&gt;
&lt;p&gt;We define the &lt;a href=&#34;https://flecart.github.io/notes/spazi-vettoriali&#34;&gt;vector space&lt;/a&gt; $V$ to be a inner product space, if we define a inner product operator ($\langle \cdot, \cdot \rangle : V \times V \to R$) such that the following are valid:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It is linear on both arguments:
$$
\langle \alpha x_{1} + \beta x_{2}, y \rangle = \alpha \langle x_{1}, y \rangle  + \beta \langle x_{2}, y \rangle 
$$&lt;/li&gt;
&lt;li&gt;It is a &lt;strong&gt;symmetric operator&lt;/strong&gt;: $\langle x, y \rangle = \langle y, x \rangle$&lt;/li&gt;
&lt;li&gt;It is &lt;strong&gt;positive definite&lt;/strong&gt; that is we have $\forall x \in V: \langle x, x \rangle \geq 0$ with equality only if $x = \boldsymbol{0}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An example of such operator is the classical &lt;strong&gt;cosine distance&lt;/strong&gt; which is just the angle, or &lt;strong&gt;euclidean distance&lt;/strong&gt;. Also all $p-\text{norms}$ are inner products.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Algebra modulare</title>
      <link>https://flecart.github.io/notes/algebra-modulare/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/algebra-modulare/</guid>
      <description>&lt;h1 id=&#34;algebra-modulare&#34;&gt;Algebra modulare&lt;/h1&gt;
&lt;h2 id=&#34;assunzioni&#34;&gt;Assunzioni&lt;/h2&gt;
&lt;p&gt;Andiamo ora ad assumere  l&amp;rsquo;esistenza e correttezza di alcune cose di base. (in teoria si possono dimostrare da cose più di base, ma non ho tempo).&lt;/p&gt;
&lt;h3 id=&#34;teorema-fondamentale-dellalgebra&#34;&gt;Teorema fondamentale dell&amp;rsquo;algebra&lt;/h3&gt;
&lt;p&gt;Ogni numero intero si fattorizza in modo unico.&lt;/p&gt;
&lt;h3 id=&#34;algoritmo-di-euclide&#34;&gt;Algoritmo di Euclide&lt;/h3&gt;
&lt;p&gt;La conseguenza più importante di questo teorema, dovuto ad Euclide è che
se ho $a, b \in \mathbb{Z}$ allora esistono resto e dividendo fra i due. Ossia
$\exists q, p : a\mid b = qk + p$ per qualche $k$ intero&lt;/p&gt;</description>
    </item>
    <item>
      <title>Applicazioni lineari</title>
      <link>https://flecart.github.io/notes/applicazioni-lineari/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/applicazioni-lineari/</guid>
      <description>&lt;h2 id=&#34;31-introduzione-e-definizione&#34;&gt;3.1 Introduzione e definizione&lt;/h2&gt;
&lt;p&gt;Si definisce applicazione lineare una funzione (omomorfica) che preserva la struttura dello spazio vettoriale, ossia vale che&lt;/p&gt;
$$
f:V \to W, \text{ tale che } \\
f(u + v) = f(u) +f(v)\\,
f(\lambda v) = \lambda f(v)
$$&lt;p&gt;
Vengono mantenute alcune caratteristiche principali.
In modo simile si possono definire omomorfismi per tutte le altre strutture algebriche, la cosa importante è che lo spazio d&amp;rsquo;arrivo possieda ancora tutte le stesse operazioni.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Base e dimensione</title>
      <link>https://flecart.github.io/notes/base-e-dimensione/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/base-e-dimensione/</guid>
      <description>&lt;h2 id=&#34;21-basi&#34;&gt;2.1 Basi&lt;/h2&gt;
&lt;h3 id=&#34;211-definizione&#34;&gt;2.1.1 Definizione&lt;/h3&gt;
&lt;p&gt;Un insieme di vettori $v_1,...,v_n$ sono basi di uno spazio vettoriale $V$ se sono soddisfatte queste proprietà&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$V = \langle v_1,...,v_n\rangle$&lt;/li&gt;
&lt;li&gt;$v_1,...,v_n$ sono linearmente indipendenti&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Dalla proprietà 2 potremmo anche dire che è il minimo insieme di vettori necessario per avere questa base.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finitamente generato&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Se l&amp;rsquo;insieme dei vettori nella base è finito allora posso dire che è finitamente generato&lt;/p&gt;
&lt;p&gt;Ma possiamo trovare anche spazi che non sono finitamente generati come $\R[x]$ che non hanno un numero finito di basi (perché dipende dal grado dei polinomi che può essere infinito).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cambio di Base</title>
      <link>https://flecart.github.io/notes/cambio-di-base/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/cambio-di-base/</guid>
      <description>&lt;h2 id=&#34;nozioni-da-avere-prima-di-cambio-di-base&#34;&gt;Nozioni da avere prima di Cambio di Base&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://flecart.github.io/notes/applicazioni-lineari&#34;&gt;Applicazioni lineari&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;La definizione di applicazione lineare&lt;/li&gt;
&lt;li&gt;La matrice associata&lt;/li&gt;
&lt;li&gt;L&amp;rsquo;esistenza e unicità di una applicazione lineare rispetto a una base&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Le coordinate di un punto rispetto a una base.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;matrice-del-cambio-di-base&#34;&gt;Matrice del Cambio di Base&lt;/h2&gt;
&lt;p&gt;Se ho due spazi vettoriali&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Intuizione in $R$
Le coordinate dei punti in $R$ sono uguali a $V$ per le basi canoniche, ma questo vale solamente per $R$, ora vogliamo andare a dire una cosa più forte, il cambio di base
Poi sarà importantissimo questa nozione, applicazione di base in ML è Principal Component Analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Se ho una applicazione lineare $F: V \to W$ e un insieme di basi del dominio e del codominio, allora esiste una matrice $A \in M_{m \times n} (\mathbb{R})$ tali che vale il cambio di base.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduzione algebra</title>
      <link>https://flecart.github.io/notes/introduzione-algebra/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/introduzione-algebra/</guid>
      <description>&lt;p&gt;Tutta sta parte si fa in modo formale in &lt;a href=&#34;https://flecart.github.io/notes/sistemi-lineari-e-determinanti&#34;&gt;Sistemi Lineari e determinanti&lt;/a&gt;, quindi potresti saltarla totalmente&lt;/p&gt;
&lt;h2 id=&#34;equazioni-lineari&#34;&gt;Equazioni lineari&lt;/h2&gt;
&lt;p&gt;L&amp;rsquo;obiettivo dell&amp;rsquo;algebra lineare è risolvere n equazioni con n sconosciuti di primo grado.
Cosa che ci riesce con grandissimo successo! Andiamo ora a definire meglio cosa è una equazione lineare&lt;/p&gt;
&lt;h3 id=&#34;definizione&#34;&gt;Definizione&lt;/h3&gt;
&lt;p&gt;Una equazione lineare è una equazione a coefficienti appartenenti a un certo campo (che può essere R) e incognite il cui grado è 1 e che siano indipendenti:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sistemi Lineari e determinanti</title>
      <link>https://flecart.github.io/notes/sistemi-lineari-e-determinanti/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/sistemi-lineari-e-determinanti/</guid>
      <description>&lt;h2 id=&#34;41-sistemi-lineari&#34;&gt;4.1 Sistemi lineari&lt;/h2&gt;
&lt;p&gt;La cosa buona è che possiamo analizzare il sistema lineare utilizzando tutti i teoremi che abbiamo sviluppato finora, quindi siamo molto più potenti per attaccare questo problema.&lt;/p&gt;
&lt;p&gt;Definiamo un sistema lineare così&lt;/p&gt;
&lt;p&gt;$Ax = b$ con A la matrice associata.&lt;/p&gt;
&lt;h3 id=&#34;411-preimmagine&#34;&gt;4.1.1 Preimmagine&lt;/h3&gt;
&lt;p&gt;Data una applicazione lineare $F:V \to W$, allora la controimmagine è l&amp;rsquo;insieme dei vettori di V che fanno a finire in quel punto, in matematichese:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
