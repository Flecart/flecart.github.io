<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ðŸ¥˜Deep-Learning on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/deep-learning/</link>
    <description>Recent content in ðŸ¥˜Deep-Learning on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Logistic Regression</title>
      <link>https://flecart.github.io/notes/logistic-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/logistic-regression/</guid>
      <description>Introduzione alla logistic regression Giustificazione del metodo Questo Ã¨ uno dei modelli classici, creati da Minsky qualche decennio fa In questo caso andiamo direttamente a computare il valore di $P(Y|X)$ durante l&amp;rsquo;inferenza, quindi si parla di modello discriminativo.
Introduzione al problema Supponiamo che
$Y$ siano variabili booleane $X_{i}$ siano variabili continue $X_{i}$ siano indipendenti uno dall&amp;rsquo;altro. $P(X_{i}| Y= k)$ sono modellate tramite distribuzioni gaussiane $\mathbb{N}(\mu_{ik}, \sigma_{i})$ NOTA! la varianza non dipende dalle feature!</description>
    </item>
    <item>
      <title>Neural Networks</title>
      <link>https://flecart.github.io/notes/neural-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/neural-networks/</guid>
      <description>Introduction: a neuron I am lazy, so I&amp;rsquo;m skipping the introduction for this set of notes. Look at Andrew Ng&amp;rsquo;s Coursera course for this part. Historical notes are (Rosenblatt 1958).
Structure A single layer of a function can be written in the following way:
$$ F(\theta)(x) = \phi(Wx + b) $$ Which can be summarized by: linear part + activation function. Where $F(\theta)$ is a partial function that returns another function, $\theta = (W, b)$ a vector, this is just a way to separate the bias with the parameters.</description>
    </item>
  </channel>
</rss>
