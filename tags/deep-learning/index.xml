<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ðŸ¥˜Deep-Learning on X. Angelo Huang&#39;s Blog</title>
    <link>https://flecart.github.io/tags/deep-learning/</link>
    <description>Recent content in ðŸ¥˜Deep-Learning on X. Angelo Huang&#39;s Blog</description>
    <image>
      <title>X. Angelo Huang&#39;s Blog</title>
      <url>https://flecart.github.io/images/papermod-cover.png</url>
      <link>https://flecart.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.143.1</generator>
    <language>en</language>
    <atom:link href="https://flecart.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Neural Networks</title>
      <link>https://flecart.github.io/notes/neural-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/neural-networks/</guid>
      <description>&lt;h2 id=&#34;introduction-a-neuron&#34;&gt;Introduction: a neuron&lt;/h2&gt;
&lt;p&gt;I am lazy, so I&amp;rsquo;m skipping the introduction for this set of notes. Look at Andrew Ng&amp;rsquo;s Coursera course for this part. Historical notes are &lt;a href=&#34;https://psycnet.apa.org/record/1959-09865-001&#34;&gt;(Rosenblatt 1958)&lt;/a&gt;.
One can view a perceptron to be a &lt;a href=&#34;https://flecart.github.io/notes/log-linear-models&#34;&gt;Log Linear Models&lt;/a&gt; with the temperature of the softmax that goes to 0 (so that it is an argmax).
Trained with a stochastic gradient descent with a batch of 1 (this is called the perceptron update rule).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logistic Regression</title>
      <link>https://flecart.github.io/notes/logistic-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://flecart.github.io/notes/logistic-regression/</guid>
      <description>&lt;p&gt;Queste note sono molto di base. Per cose leggermente piÃ¹ avanzate bisogna guardare &lt;a href=&#34;https://flecart.github.io/notes/bayesian-linear-regression&#34;&gt;Bayesian Linear Regression&lt;/a&gt;, &lt;a href=&#34;https://flecart.github.io/notes/linear-regression-methods&#34;&gt;Linear Regression methods&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;introduzione-alla-logistic-regression&#34;&gt;Introduzione alla logistic regression&lt;/h2&gt;
&lt;h3 id=&#34;giustificazione-del-metodo&#34;&gt;Giustificazione del metodo&lt;/h3&gt;
&lt;p&gt;Questo Ã¨ uno dei modelli classici, creati da &lt;strong&gt;Minsky&lt;/strong&gt; qualche decennio fa
In questo caso andiamo direttamente a computare il valore di $P(Y|X)$ durante l&amp;rsquo;inferenza, quindi si parla di modello &lt;strong&gt;discriminativo&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;introduzione-al-problema&#34;&gt;Introduzione al problema&lt;/h4&gt;
&lt;p&gt;Supponiamo che&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Y$ siano variabili booleane&lt;/li&gt;
&lt;li&gt;$X_{i}$ siano variabili continue&lt;/li&gt;
&lt;li&gt;$X_{i}$ siano indipendenti uno dall&amp;rsquo;altro.&lt;/li&gt;
&lt;li&gt;$P(X_{i}| Y= k)$ sono modellate tramite distribuzioni gaussiane $\mathbb{N}(\mu_{ik}, \sigma_{i})$
&lt;ul&gt;
&lt;li&gt;NOTA! la varianza non dipende dalle feature!, questo mi permetterebbe di poi togliere la cosa quadratico dopo, rendendo poi l&amp;rsquo;approssimazione lineare&lt;/li&gt;
&lt;li&gt;Per esempio se utilizziamo nelle immagini, avrebbe senso normalizzare pixel by pixel, e non image wide con un unico valore, Ã¨ una assunzione, che se funziona dovrebbe poi far andare meglio la regressione logistica!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$Y$ Ã¨ una distribuzione bernoulliana.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ci chiediamo come Ã¨ fatto $P(Y|X)$?&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
