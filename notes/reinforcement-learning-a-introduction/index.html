<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Reinforcement Learning, a introduction | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="➕probabilistic-artificial-intelligence">
<meta name="description" content="The main difference between reinforcement learning and other machine learning, pattern inference methods is that reinforcement learning takes the concept of actions into its core: models developed in this field can be actively developed to have an effect in its environment, while other methods are mainly used to summarize interesting data or generating sort of reports.

Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. ~Wikipedia page.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/reinforcement-learning-a-introduction/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/reinforcement-learning-a-introduction/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/reinforcement-learning-a-introduction/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Reinforcement Learning, a introduction">
  <meta property="og:description" content="The main difference between reinforcement learning and other machine learning, pattern inference methods is that reinforcement learning takes the concept of actions into its core: models developed in this field can be actively developed to have an effect in its environment, while other methods are mainly used to summarize interesting data or generating sort of reports.
Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. ~Wikipedia page.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="➕Probabilistic-Artificial-Intelligence">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Reinforcement Learning, a introduction">
<meta name="twitter:description" content="The main difference between reinforcement learning and other machine learning, pattern inference methods is that reinforcement learning takes the concept of actions into its core: models developed in this field can be actively developed to have an effect in its environment, while other methods are mainly used to summarize interesting data or generating sort of reports.

Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. ~Wikipedia page.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Reinforcement Learning, a introduction",
      "item": "https://flecart.github.io/notes/reinforcement-learning-a-introduction/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Reinforcement Learning, a introduction",
  "name": "Reinforcement Learning, a introduction",
  "description": "The main difference between reinforcement learning and other machine learning, pattern inference methods is that reinforcement learning takes the concept of actions into its core: models developed in this field can be actively developed to have an effect in its environment, while other methods are mainly used to summarize interesting data or generating sort of reports.\nReinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. ~Wikipedia page.\n",
  "keywords": [
    "➕probabilistic-artificial-intelligence"
  ],
  "articleBody": "The main difference between reinforcement learning and other machine learning, pattern inference methods is that reinforcement learning takes the concept of actions into its core: models developed in this field can be actively developed to have an effect in its environment, while other methods are mainly used to summarize interesting data or generating sort of reports.\nReinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. ~Wikipedia page.\nNote: there is a big gap between theory and practise in this field.\nIntroduzione Una delle idee migliori riguardanti questo campo del reinforcement learning è il focus sul processo decisionale del singolo agente, condizionato al reward che l’ambiente esterno gli dà (feedback). Il setting classico di questo genere di problemi è un caso speciale della caratterizzazione presente in l’intelligenza.\nAbbiamo in questo caso un agente all’interno del suo ambiente. L’agente è in grado di interagire col suo ambiente attraverso alcune azioni ben definite, e l’ambiente restituisce un feedback ad ogni azione. L’agente si regola di conseguenza, nel tentativo di massimizzare il reward che riceve.\nÈ da notare che questa impostazione è molto diversa rispetto al machine learning classico, seppur si può comunque collocare al suo interno. Classifcamente nei modelli di machine learning supervised si cerca di minimizzare un errore con alcuni dataset etichettati, mentre qui non abbiamo nessuna etichetta, mentre nel unsupervised proviamo a trovare alcuni pattern nei dati, mentre qui non cerchiamo nessun pattern. Si potrebbe dire che questo sia un terzo paradigma di machine learning.\nNOTA: questi appunti riassumono concetti dai primi 4 capitoli del Sutton and Barto 2020\nUn problema classico: n-bandit Vedere N-Bandit Problem.\nSetting classico (Model Policy Reward) Quando andiamo a parlare di Reinforcement learning andiamo a considerare un setting classico di agente che interagisce con un ambiente attraverso delle azioni, e l’ambiente che risponde attraverso i reward. L’agente osserva quindi lo stato (se è full-observable vede lo stato esterno, altrimenti partially observable vede solamente parte delle informazioni dello stato dell’ambiente) e insieme al reward percepito prova a eseguire delle altre azioni.\nSono particolarmente importanti quindi 3 parole chiave utili per descrivere una delle 3 frecce in immagine\nModel Il modello dell’ambiente lo indichiamo anche come dinamica o sistema di transizione dell’ambiente. nel modello sono definite tutte le distribuzioni di probabilità che portano uno stato a un altro: $P(s'|s)$, questo possiamo dire, ossia partendo da uno stato s, quanto è probabile finire in uno stato s’ ??\n$$ \\begin{align} P: S \\times \\mathcal{A} \\to \\Delta(S) \\\\ r: S \\times \\mathcal{A} \\to [0, 1] \\end{align} $$ Dove $\\Delta$ è una distribuzione su $S$.\nPolicy La policy è un indicatore delle azioni del singolo agente, ci dice quanto è probabile che l’agente esegua una certa azione, dato che sia sopra un certo stato s, lo indichiamo solitamente con $\\pi(a | s)$. Nel caso in cui è una policy deterministica, nel senso che a uno stato corrisponde uno e un solo azione, potremmo scrivere qualcosa del tipo $\\pi (s) = a$ Quindi è una funzione $\\pi: S \\to \\Delta(\\mathcal{A})$.\nReward Il reward descrive il feedback che l’ambiente ritorna al giocatore una volta che una azione è stata eseguita, spesso lo indichiamo in questi modi\n$$ r(s, a) \\\\ r(s, a, s') \\\\ r(s) $$A seconda di quanto vogliamo esprimere (quindi il reward atteso dopo aver fatto una azione da unc erto stato, il reward atteso dopo aver fatto una azione da un certo stato ed essere arrivati a un certo stao e così via\nThe Value function $$ v_{i}(S_{j}) = \\mathbf{E} [r_{i} + r_{i + 1} + \\dots | S_{j}] $$$$ v_{i}(S_{j}) = \\mathbf{E} [r_{i} + v_{i+1}(S) | S_{j}] $$ Con $S$ uno stato su cui puoi essere al passo successivo.\nAll components are functions:\nPolicies: $\\pi: S \\rightarrow A$ (or to probabilities over A) Value functions: $v: S \\rightarrow R$ Models: $m: S \\rightarrow S$ and/or $r: S \\rightarrow R$ State update: $u: S \\times O \\rightarrow S$ Categorie di agenti Policy - Value categorization Value Based ha solamente value based, la sua policy è basata sul suo valore (in modo greedy va a cercare quale sia lo stato con valore maggiore) Esempi sono Monte Carlo, SARSA, Q-learning, DQN.\nPolicy based Il contrario, non ha value function, ma solamente la policy, tenta direttamente Policy Gradient, NPG, TRPO, PPO.\nActor Critic Ha entrambi, ha sia policy (l’attore) e il critico che cerca di aiutare. Questi sono anche chiamati model based.\nModels Model free Se hanno policy o value, ma non hanno nessun modello sull’ambiente in cui sono presenti Solitamente sono molto semplici, e permettono di imparare direttamente la policy migliore possibili per questo ambiente. Secondo il professor Buhmann non ha senso parlare di model free perché qualunque modello implicitamente ne ha uno.\nModel based Hanno il modello dell’ambiente, e non necessariamente hanno policy o value function. Questi potremmo anche chiamarli (Ha \u0026 Schmidhuber 2018). Solitamente questi permettono di utilizzare l’esperienza meglio (+ sample efficient).\nOther definitions Prediction and control Prediction è la capacità di sapere come sarà il futuro Control è la capacità di ottimizzare la propria value function. Solitamente sono molto legati fra di loro.\nEpisodic and Non-episodic Episodic -\u003e We have a collection of episodes, each of which gave us new trajectories about it. We can reset the environment as we like Non-episodic -\u003e We learn this online, each one yields a single trajectory, we can’t reset.\nOnline vs Offline RL Per i modelli online possiamo andare direttamente ad agire sull’ambiente per ottenere dei dati. Si parla di exploitation exploration tradeoff. Solo che bisogna stare attenti perché ci può essere un rischio per certe azioni. Per esempio una macchina potrebbe schiantarsi quando esplora, perché lo sta facendo nel mondo reale. Per i modelli offline abbiamo già collezionato un sacco di dati, e possiamo usare questo per cercare di creare un modello ed imparare. Sono anche chiamati batched RL.\nOn-policy vs Off-policy RL Con on-policy è sempre un RL online, in cui andiamo ad imparare utilizzando la policy attuale. Con off-policy stiamo usando una policy diversa per andare ad imparare la nostra policy finale. Magari abbiamo un buffer in questo caso che utilizziamo per memorizzare in modo temporaneo le nostre informazioni.\nModel based and Model free Model-based -\u003e We want to create a world model, and try to estimate the state transitions and the rewards Model-free -\u003e We just try to learn enough to act well: we just estimate the value of a single state.\nMarkov chains Dovrebbe essere approfondito meglio in Markov Chains\nReferences [1] Ha \u0026 Schmidhuber “World Models” 2018\n",
  "wordCount" : "1107",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/reinforcement-learning-a-introduction/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Reinforcement Learning, a introduction
    </h1>
    <div class="post-meta">6 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#introduzione" aria-label="Introduzione">Introduzione</a><ul>
                        
                <li>
                    <a href="#un-problema-classico-n-bandit" aria-label="Un problema classico: n-bandit">Un problema classico: n-bandit</a></li>
                <li>
                    <a href="#setting-classico-model-policy-reward" aria-label="Setting classico (Model Policy Reward)">Setting classico (Model Policy Reward)</a><ul>
                        
                <li>
                    <a href="#model" aria-label="Model">Model</a></li>
                <li>
                    <a href="#policy" aria-label="Policy">Policy</a></li>
                <li>
                    <a href="#reward" aria-label="Reward">Reward</a></li>
                <li>
                    <a href="#the-value-function" aria-label="The Value function">The Value function</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#categorie-di-agenti" aria-label="Categorie di agenti">Categorie di agenti</a><ul>
                        
                <li>
                    <a href="#policy---value-categorization" aria-label="Policy - Value categorization">Policy - Value categorization</a><ul>
                        
                <li>
                    <a href="#value-based" aria-label="Value Based">Value Based</a></li>
                <li>
                    <a href="#policy-based" aria-label="Policy based">Policy based</a></li>
                <li>
                    <a href="#actor-critic" aria-label="Actor Critic">Actor Critic</a></li></ul>
                </li>
                <li>
                    <a href="#models" aria-label="Models">Models</a><ul>
                        
                <li>
                    <a href="#model-free" aria-label="Model free">Model free</a></li>
                <li>
                    <a href="#model-based" aria-label="Model based">Model based</a></li></ul>
                </li>
                <li>
                    <a href="#other-definitions" aria-label="Other definitions">Other definitions</a><ul>
                        
                <li>
                    <a href="#prediction-and-control" aria-label="Prediction and control">Prediction and control</a></li>
                <li>
                    <a href="#episodic-and-non-episodic" aria-label="Episodic and Non-episodic">Episodic and Non-episodic</a></li>
                <li>
                    <a href="#online-vs-offline-rl" aria-label="Online vs Offline RL">Online vs Offline RL</a></li>
                <li>
                    <a href="#on-policy-vs-off-policy-rl" aria-label="On-policy vs Off-policy RL">On-policy vs Off-policy RL</a></li>
                <li>
                    <a href="#model-based-and-model-free" aria-label="Model based and Model free">Model based and Model free</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#markov-chains" aria-label="Markov chains">Markov chains</a></li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>The main difference between reinforcement learning and other machine learning, pattern inference methods is that reinforcement learning takes the concept of <strong>actions</strong> into its core: models developed in this field can be actively developed to have an effect in its environment, while other methods are mainly used to summarize interesting data or generating sort of reports.</p>
<blockquote>
<p><strong>Reinforcement learning</strong> (<strong>RL</strong>) is an interdisciplinary area of <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> and <a href="https://en.wikipedia.org/wiki/Optimal_control">optimal control</a> concerned with how an <a href="https://en.wikipedia.org/wiki/Intelligent_agent">intelligent agent</a> ought to take <a href="https://en.wikipedia.org/wiki/Action_selection">actions</a> in a dynamic environment in order to maximize the <a href="https://en.wikipedia.org/wiki/Reward-based_selection">cumulative reward</a>. <em>~Wikipedia page</em>.</p></blockquote>
<p>Note: there is a big gap between theory and practise in this field.</p>
<h2 id="introduzione">Introduzione<a hidden class="anchor" aria-hidden="true" href="#introduzione">#</a></h2>
<p>Una delle idee migliori riguardanti questo campo del reinforcement learning è il focus sul processo decisionale del singolo agente, condizionato al reward che l’ambiente esterno gli dà (feedback). Il setting classico di questo genere di problemi è un caso speciale della caratterizzazione presente in l’intelligenza.</p>
<p>Abbiamo in questo caso un agente all’interno del suo ambiente. L’agente è in grado di interagire col suo ambiente attraverso alcune azioni ben definite, e l’ambiente restituisce un feedback ad ogni azione. L’agente si regola di conseguenza, nel tentativo di massimizzare il reward che riceve.</p>
<p>È da notare che questa impostazione è molto diversa rispetto al machine learning classico, seppur si può comunque collocare al suo interno. Classifcamente nei modelli di machine learning supervised si cerca di minimizzare un errore con alcuni dataset etichettati, mentre qui non abbiamo nessuna etichetta, mentre nel unsupervised proviamo a trovare alcuni pattern nei dati, mentre qui non cerchiamo nessun pattern. Si potrebbe dire che questo sia un terzo paradigma di machine learning.</p>
<p>NOTA: questi appunti riassumono concetti dai primi 4 capitoli del Sutton and Barto 2020</p>
<h3 id="un-problema-classico-n-bandit">Un problema classico: n-bandit<a hidden class="anchor" aria-hidden="true" href="#un-problema-classico-n-bandit">#</a></h3>
<p>Vedere N-Bandit Problem.</p>
<h3 id="setting-classico-model-policy-reward">Setting classico (Model Policy Reward)<a hidden class="anchor" aria-hidden="true" href="#setting-classico-model-policy-reward">#</a></h3>
<p>Quando andiamo a parlare di Reinforcement learning andiamo a considerare un setting classico di agente che interagisce con un ambiente attraverso delle azioni, e l’ambiente che risponde attraverso i reward. L’agente osserva quindi lo stato (se è full-observable vede lo stato esterno, altrimenti partially observable vede solamente parte delle informazioni dello stato dell’ambiente) e insieme al reward percepito prova a eseguire delle altre azioni.</p>
<img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled">
<p>Sono particolarmente importanti quindi 3 parole chiave utili per descrivere una delle 3 frecce in immagine</p>
<h4 id="model">Model<a hidden class="anchor" aria-hidden="true" href="#model">#</a></h4>
<p>Il modello dell&rsquo;ambiente lo indichiamo anche come dinamica o sistema di transizione dell’ambiente. nel modello sono definite tutte le distribuzioni di probabilità che portano uno stato a un altro: $P(s'|s)$, questo possiamo dire, ossia partendo da uno stato s, quanto è probabile finire in uno stato s’ ??</p>
$$
\begin{align}
P: S \times \mathcal{A} \to \Delta(S) \\
r: S \times \mathcal{A} \to [0, 1]
\end{align}
$$<p>
Dove $\Delta$ è una distribuzione su $S$.</p>
<h4 id="policy">Policy<a hidden class="anchor" aria-hidden="true" href="#policy">#</a></h4>
<p>La policy è un indicatore delle azioni del singolo agente, ci dice quanto è probabile che l’agente esegua una certa azione, dato che sia sopra un certo stato s, lo indichiamo solitamente con $\pi(a | s)$. Nel caso in cui è una policy deterministica, nel senso che a uno stato corrisponde uno e un solo azione, potremmo scrivere qualcosa del tipo $\pi (s) = a$
Quindi è una funzione $\pi: S \to \Delta(\mathcal{A})$.</p>
<h4 id="reward">Reward<a hidden class="anchor" aria-hidden="true" href="#reward">#</a></h4>
<p>Il reward descrive il feedback che l’ambiente ritorna al giocatore una volta che una azione è stata eseguita, spesso lo indichiamo in questi modi</p>
$$
r(s, a) \\ r(s, a, s') \\ r(s)
$$<p>A seconda di quanto vogliamo esprimere (quindi il reward atteso dopo aver fatto una azione da unc erto stato, il reward atteso dopo aver fatto una azione da un certo stato ed essere arrivati a un certo stao e così via</p>
<h4 id="the-value-function">The Value function<a hidden class="anchor" aria-hidden="true" href="#the-value-function">#</a></h4>
$$
v_{i}(S_{j}) = \mathbf{E} [r_{i} + r_{i + 1} + \dots | S_{j}]
$$$$
v_{i}(S_{j}) = \mathbf{E} [r_{i} + v_{i+1}(S) | S_{j}]
$$<p>
Con $S$ uno stato su cui puoi essere al passo successivo.</p>
<p>All components are functions:</p>
<ul>
<li>Policies: $\pi: S \rightarrow A$ (or to probabilities over A)</li>
<li>Value functions: $v: S \rightarrow R$</li>
<li>Models: $m: S \rightarrow S$ and/or $r: S \rightarrow R$</li>
<li>State update: $u: S \times O \rightarrow S$</li>
</ul>
<h2 id="categorie-di-agenti">Categorie di agenti<a hidden class="anchor" aria-hidden="true" href="#categorie-di-agenti">#</a></h2>
<h3 id="policy---value-categorization">Policy - Value categorization<a hidden class="anchor" aria-hidden="true" href="#policy---value-categorization">#</a></h3>
<h4 id="value-based">Value Based<a hidden class="anchor" aria-hidden="true" href="#value-based">#</a></h4>
<p>ha solamente value based, la sua policy è basata sul suo valore (in modo greedy va a cercare quale sia lo stato con valore maggiore)
Esempi sono Monte Carlo, SARSA, Q-learning, DQN.</p>
<h4 id="policy-based">Policy based<a hidden class="anchor" aria-hidden="true" href="#policy-based">#</a></h4>
<p>Il contrario, non ha value function, ma solamente la policy, tenta direttamente
Policy Gradient, NPG, TRPO, PPO.</p>
<h4 id="actor-critic">Actor Critic<a hidden class="anchor" aria-hidden="true" href="#actor-critic">#</a></h4>
<p>Ha entrambi, ha sia policy (l&rsquo;attore) e il critico che cerca di aiutare. Questi sono anche chiamati <strong>model based</strong>.</p>
<h3 id="models">Models<a hidden class="anchor" aria-hidden="true" href="#models">#</a></h3>
<img src="/images/notes/Reinforcement Learning, a introduction-20240908223656566.webp" style="width: 100%" class="center" alt="Reinforcement Learning, a introduction-20240908223656566">
<h4 id="model-free">Model free<a hidden class="anchor" aria-hidden="true" href="#model-free">#</a></h4>
<p>Se hanno policy o value, ma non hanno <strong>nessun modello sull&rsquo;ambiente</strong> in cui sono presenti
Solitamente sono molto semplici, e permettono di imparare direttamente la policy migliore possibili per questo ambiente.
Secondo il professor Buhmann non ha senso parlare di model free perché qualunque modello implicitamente ne ha uno.</p>
<h4 id="model-based">Model based<a hidden class="anchor" aria-hidden="true" href="#model-based">#</a></h4>
<p>Hanno il modello dell&rsquo;ambiente, e non necessariamente hanno policy o value function. Questi potremmo anche chiamarli <a href="http://arxiv.org/abs/1803.10122">(Ha &amp; Schmidhuber 2018)</a>.
Solitamente questi permettono di utilizzare l&rsquo;esperienza meglio (+ <em>sample efficient</em>).</p>
<h3 id="other-definitions">Other definitions<a hidden class="anchor" aria-hidden="true" href="#other-definitions">#</a></h3>
<h4 id="prediction-and-control">Prediction and control<a hidden class="anchor" aria-hidden="true" href="#prediction-and-control">#</a></h4>
<p><strong>Prediction</strong> è la capacità di sapere come sarà il futuro
<strong>Control</strong> è la capacità di ottimizzare la propria value function.
Solitamente sono molto legati fra di loro.</p>
<h4 id="episodic-and-non-episodic">Episodic and Non-episodic<a hidden class="anchor" aria-hidden="true" href="#episodic-and-non-episodic">#</a></h4>
<p>Episodic -&gt; We have a collection of episodes, each of which gave us new trajectories about it. We can reset the environment as we like
Non-episodic -&gt; We learn this online, each one yields a single trajectory, we can&rsquo;t reset.</p>
<h4 id="online-vs-offline-rl">Online vs Offline RL<a hidden class="anchor" aria-hidden="true" href="#online-vs-offline-rl">#</a></h4>
<p>Per i modelli <strong>online</strong> possiamo andare direttamente ad agire sull&rsquo;ambiente per ottenere dei dati. Si parla di exploitation exploration tradeoff. Solo che bisogna stare attenti perché ci può essere un rischio per certe azioni. Per esempio una macchina potrebbe schiantarsi quando esplora, perché  lo sta facendo nel mondo reale.
Per i modelli <strong>offline</strong> abbiamo già collezionato un sacco di dati, e possiamo usare questo per cercare di creare un modello ed imparare. Sono anche chiamati <strong>batched</strong> RL.</p>
<h4 id="on-policy-vs-off-policy-rl">On-policy vs Off-policy RL<a hidden class="anchor" aria-hidden="true" href="#on-policy-vs-off-policy-rl">#</a></h4>
<p>Con <strong>on-policy</strong> è sempre un RL online, in cui andiamo ad imparare utilizzando la policy attuale.
Con <strong>off-policy</strong> stiamo usando una policy diversa per andare ad imparare la nostra policy finale. Magari abbiamo un buffer in questo caso che utilizziamo per memorizzare in modo temporaneo le nostre informazioni.</p>
<h4 id="model-based-and-model-free">Model based and Model free<a hidden class="anchor" aria-hidden="true" href="#model-based-and-model-free">#</a></h4>
<p>Model-based -&gt; We want to create a world model, and try to estimate the state transitions and the rewards
Model-free -&gt; We just try to <em>learn enough to act well</em>: we just estimate the value of a single state.</p>
<h2 id="markov-chains">Markov chains<a hidden class="anchor" aria-hidden="true" href="#markov-chains">#</a></h2>
<p>Dovrebbe essere approfondito meglio in <a href="/notes/markov-chains">Markov Chains</a></p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Ha &amp; Schmidhuber <a href="http://arxiv.org/abs/1803.10122">“World Models”</a>  2018</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">➕Probabilistic-Artificial-Intelligence</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on x"
            href="https://x.com/intent/tweet/?text=Reinforcement%20Learning%2c%20a%20introduction&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2freinforcement-learning-a-introduction%2f&amp;hashtags=%e2%9e%95probabilistic-artificial-intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2freinforcement-learning-a-introduction%2f&amp;title=Reinforcement%20Learning%2c%20a%20introduction&amp;summary=Reinforcement%20Learning%2c%20a%20introduction&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2freinforcement-learning-a-introduction%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2freinforcement-learning-a-introduction%2f&title=Reinforcement%20Learning%2c%20a%20introduction">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2freinforcement-learning-a-introduction%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on whatsapp"
            href="https://api.whatsapp.com/send?text=Reinforcement%20Learning%2c%20a%20introduction%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2freinforcement-learning-a-introduction%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on telegram"
            href="https://telegram.me/share/url?text=Reinforcement%20Learning%2c%20a%20introduction&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2freinforcement-learning-a-introduction%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Reinforcement%20Learning%2c%20a%20introduction&u=https%3a%2f%2fflecart.github.io%2fnotes%2freinforcement-learning-a-introduction%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
