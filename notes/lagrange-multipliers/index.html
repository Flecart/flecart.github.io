<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Lagrange Multipliers | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="📈optimization, machinelearning">
<meta name="description" content="This is also known as Lagrange Optimization or undetermined multipliers. Some of these notes are based on Appendix E of (Bishop 2006), others were found when studying bits of rational mechanics. Also (Boyd &amp; Vandenberghe 2004) chapter 5 should be a good resource on this topic.
Let&rsquo;s consider a standard linear optimization problem $$ \begin{array} \\ \min f_{0}(x) \\ \text{subject to } f_{i}(x) \leq 0 \\ h_{j}(x) = 0 \end{array} $$ Lagrangian function And let&rsquo;s consider the Lagrangian function associated to this problem defined as $$ \mathcal{L}(x, \lambda, \nu) = f_{0}(x) &#43; \sum \lambda_{i}f_{i}(x) &#43; \sum\nu_{j}h_{j}(x) $$ We want to say something about this function, because it is able to simplify the optimization problem a lot, but first we want to study this mathematically.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/lagrange-multipliers/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/lagrange-multipliers/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Lagrange Multipliers" />
<meta property="og:description" content="This is also known as Lagrange Optimization or undetermined multipliers. Some of these notes are based on Appendix E of (Bishop 2006), others were found when studying bits of rational mechanics. Also (Boyd &amp; Vandenberghe 2004) chapter 5 should be a good resource on this topic.
Let&rsquo;s consider a standard linear optimization problem $$ \begin{array} \\ \min f_{0}(x) \\ \text{subject to } f_{i}(x) \leq 0 \\ h_{j}(x) = 0 \end{array} $$ Lagrangian function And let&rsquo;s consider the Lagrangian function associated to this problem defined as $$ \mathcal{L}(x, \lambda, \nu) = f_{0}(x) &#43; \sum \lambda_{i}f_{i}(x) &#43; \sum\nu_{j}h_{j}(x) $$ We want to say something about this function, because it is able to simplify the optimization problem a lot, but first we want to study this mathematically." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/lagrange-multipliers/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Lagrange Multipliers"/>
<meta name="twitter:description" content="This is also known as Lagrange Optimization or undetermined multipliers. Some of these notes are based on Appendix E of (Bishop 2006), others were found when studying bits of rational mechanics. Also (Boyd &amp; Vandenberghe 2004) chapter 5 should be a good resource on this topic.
Let&rsquo;s consider a standard linear optimization problem $$ \begin{array} \\ \min f_{0}(x) \\ \text{subject to } f_{i}(x) \leq 0 \\ h_{j}(x) = 0 \end{array} $$ Lagrangian function And let&rsquo;s consider the Lagrangian function associated to this problem defined as $$ \mathcal{L}(x, \lambda, \nu) = f_{0}(x) &#43; \sum \lambda_{i}f_{i}(x) &#43; \sum\nu_{j}h_{j}(x) $$ We want to say something about this function, because it is able to simplify the optimization problem a lot, but first we want to study this mathematically."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Lagrange Multipliers",
      "item": "https://flecart.github.io/notes/lagrange-multipliers/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Lagrange Multipliers",
  "name": "Lagrange Multipliers",
  "description": "This is also known as Lagrange Optimization or undetermined multipliers. Some of these notes are based on Appendix E of (Bishop 2006), others were found when studying bits of rational mechanics. Also (Boyd \u0026amp; Vandenberghe 2004) chapter 5 should be a good resource on this topic.\nLet\u0026rsquo;s consider a standard linear optimization problem $$ \\begin{array} \\\\ \\min f_{0}(x) \\\\ \\text{subject to } f_{i}(x) \\leq 0 \\\\ h_{j}(x) = 0 \\end{array} $$ Lagrangian function And let\u0026rsquo;s consider the Lagrangian function associated to this problem defined as $$ \\mathcal{L}(x, \\lambda, \\nu) = f_{0}(x) + \\sum \\lambda_{i}f_{i}(x) + \\sum\\nu_{j}h_{j}(x) $$ We want to say something about this function, because it is able to simplify the optimization problem a lot, but first we want to study this mathematically.",
  "keywords": [
    "📈optimization", "machinelearning"
  ],
  "articleBody": "This is also known as Lagrange Optimization or undetermined multipliers. Some of these notes are based on Appendix E of (Bishop 2006), others were found when studying bits of rational mechanics. Also (Boyd \u0026 Vandenberghe 2004) chapter 5 should be a good resource on this topic.\nLet’s consider a standard linear optimization problem $$ \\begin{array} \\\\ \\min f_{0}(x) \\\\ \\text{subject to } f_{i}(x) \\leq 0 \\\\ h_{j}(x) = 0 \\end{array} $$ Lagrangian function And let’s consider the Lagrangian function associated to this problem defined as $$ \\mathcal{L}(x, \\lambda, \\nu) = f_{0}(x) + \\sum \\lambda_{i}f_{i}(x) + \\sum\\nu_{j}h_{j}(x) $$ We want to say something about this function, because it is able to simplify the optimization problem a lot, but first we want to study this mathematically.\nWe call $\\lambda_{i}, \\nu_{i}$ the lagrange multipliers associated with their function, where we have that $\\lambda_{i} \\geq 0$ and $\\nu_{i}$ are free. We can also define the dual function as $g(\\lambda, \\nu) = \\min_{x} L(x, \\lambda, \\nu)$.\nMotivation In this section we try to explain step by step why this method works. We will find that it is easier than we could expect this. With this, we will evaluate the maximum (for the minimum just take the dual)\nSingle Equation constraint In this case we have our function $f$ along with a constraint $g(x) = b$ where $b$ is a constant in $\\mathbb{R}$ and $x$ is a vector. Note: we use $g(x) = b$ but it’s the same if $b =0$ just define $h(x) = g(x) - b$ and you have your other function compared to a 0, which is usually easier in this context. Let’s consider the solutions of $g(x) = b$, this usually gives us a set of points (sometimes contiguous) where we have to look for a solution of $f$, we want to find among these points the one that minimizes the function $f$. The constraint on $g$ is somewhat similar to convex analysis’ level sets. One thing that is easily observable (this way is very physicist’s way to motivate things, you do something similar when analyzing the parallel electric field in conductors see Condensatori nel vuoto), is that if you have a $g(x)$ and you move slightly along the path of $g(x + \\varepsilon)$ the value is the same, so the derivative along this direction is null. This allows us to say that the derivative of $g$ is perpendicular to the direction of the curve. Formally, we can write the Taylor expansion of $g$ around some point $x$: $$ g(x + \\varepsilon) \\approx g(x) + \\nabla g(x) \\varepsilon $$ Then we say that $g(x + \\varepsilon) - g(x) = 0$ because they both lie in the same contour surface which implies $\\nabla g(x)\\varepsilon = 0$ in that direction. So, only the perpedicular component of the gradient remains. Then we can also say something about the gradient of $f$, because if it is not perfectly perpendicular to that surface, we would have that moving slightly along the surface could increase or decrease the value of $f$. So we need the two to be parallel or anti-parallel. Which motivates us to write $\\nabla f + \\lambda \\nabla g = 0$ which in turn motivates the creation of the so called lagrangian function: $$ \\mathcal{L}(x, \\lambda) = f(x) + \\lambda g(x) $$ We observe that its stationary point needs to have its partial derivatives set to zero, so we would have the original partial derivative condition, and also the inequality constraint for $\\lambda \\neq 0$. In this way we can find both the $x$ and the $\\lambda$.\nIn the case we want to minimize, we can choose the lagrangian to be $$ \\mathcal{L}(x, \\lambda) = f(x) + \\lambda g(x) $$ So that the derivative has opposite direction. (actually I don’t have understood this point).\nInequality constraints If instead of having bounds like $g(x) = b$ we have bounds like $g(x) \\geq b$ it’s just a little bit more complicated, we have more points, and could be useful to divide the case when $g(x)= b$ with $g(x) \u003e b$. In the latter case, we just set $\\lambda = 0$ (because this is what we get if we take the derivative w.r.t. to $\\lambda$ and set $\\lambda g(x) = 0$) and maximize the lagrangian (doesn’t matter the direction of the gradient of $f$ now), when the solution is border, we should take a little bit more care: we want the gradient of $f$ to be away from the region $g$, which motivates us to have an equation like $\\lambda \\nabla g(x) = -\\nabla f(x)$ and $\\lambda \u003e 0$. So we have the same Lagrangian as before, but we have some other constraints, those are called the Karush-Kuhn-Tucker conditions: $$ \\begin{cases} g(x) \\geq 0 \\\\ \\lambda \\geq 0 \\\\ \\lambda g(x) = 0 \\end{cases} $$ The last two conditions are called complementary slackness conditions.\nWe can work the four cases intuitively and get some other intuition about lagrange optimizations.\nPlaybook for Lagrange Multipliers Given an optimization problem $$ \\begin{array} \\\\ \\min_{w} f(w) \\\\ \\text{ s.t. } g_{i}(w) \\leq 0 \\\\ h_{i}(w) = 0 \\end{array} $$ Write the Lagrangian function $$ \\mathcal{L}(w, \\lambda, \\nu) = f(w) + \\sum_{i} \\lambda_{i}g_{i}(w) + \\sum_{i}\\nu_{i}h_{i}(w) $$ with $\\lambda_{i} \\geq 0$ these are the classical conditions, motivated above (just note the minimization problem, so we inverted one condition). Check for Slater’s condition which is check if there is a $w$ such that for every $i$ $g_{i}(w) \u003c 0$ and $h_{i}(w) = 0$ Solve $\\nabla_{w} \\mathcal{L}$, $\\lambda_{i}g_{i}(w) = 0$ and $h_{i}(w) = 0$ Duality Primal problem The Lagrangian Multiplier method allows us to define the concept of dual optimization problem also in this context. If we consider this classical optimization problem $$ \\begin{array} \\\\ \\min_{w} f(w) \\\\ \\text{ s.t. } g_{i}(w) \\leq 0 \\\\ h_{i}(w) = 0 \\end{array} $$ We can build the Lagrange Multiplier as $$ \\mathcal{L}(w, \\lambda, \\nu) = f(w) + \\sum_{i} \\lambda_{i}g_{i}(w) + \\sum_{i}\\nu_{i}h_{i}(w) $$ with $\\lambda_{i} \\geq 0$, these are the classical conditions, motivated above. Now consider this primal value: $$ \\theta_{\\mathcal{P}}(w) = \\max_{\\lambda, \\nu : \\lambda_{i} \\geq 0} \\mathcal{L}(w, \\lambda, \\nu) $$ We observe that if any of the condition $g_{i}(w) \\leq 0$ or $h_{i}(w) = 0$ is violated, then it’s easy to say that $\\theta_{\\mathcal{P}} = +\\infty$. So we say that this formulation is the same as the above: $$ p^{*} = \\min_{w} \\theta_{\\mathcal{P}}(w) = \\min_{w} \\max_{\\lambda, \\nu: \\lambda_{i} \\geq 0} \\mathcal{L}(w, \\lambda, \\nu) $$ The dual of this optimization problem is just inverting the values in some manner.\nDual problem Dual problem of the Lagrange function is the following: $$ \\theta_{\\mathcal{D}}(\\lambda, \\nu) = \\min_{\\omega} \\mathcal{L}(w, \\lambda, \\nu) $$ We can prove some properties of this new function. Note that the chosen $w$ could also be a not admissible solution in this case.\nSo now the optimization problem is $$ d^{*} = \\max_{\\lambda, \\nu: \\lambda_{i} \\geq 0} \\theta_{\\mathcal{D}}(\\lambda, \\nu) = \\max_{\\lambda, \\nu: \\lambda_{i} \\geq 0} \\min_{w} \\mathcal{L}(w, \\lambda, \\nu) $$ We see a nice symmetry with the primal optimization problem. This is the same as solving $$ \\begin{align} \\max_{\\lambda, \\nu} \\theta_{\\mathcal{D}}(\\lambda, \\nu) \\\\ \\text{ s.t. } \\forall i, \\lambda_{i} \\geq 0 \\end{align} $$ Boundness of the dual function Consider the Lagrangian given the optimization problem above\n$$ \\mathcal{L}(x, \\lambda, \\nu) = f_{0}(x) + \\sum \\lambda_{i}f_{i}(x) + \\sum\\nu_{j}h_{j}(x) $$ Given $x^{*}$ a candidate of the optimization problem, we say that, after fixing $\\lambda \\geq 0$ and $\\nu$ then it’s always true that $$ \\theta_{D}(\\lambda, \\nu) = min_{x} \\mathcal{L}(x, \\lambda, \\nu) \\leq \\mathcal{L}(x^{*}, \\lambda, \\nu) \\leq f_{0}(x^{*}) $$ This is called weak duality: we observe that each possible solution is bounded below by the dual function. When Equality holds for the best $\\lambda$ and $\\nu$ we say that we have strong duality, which implies that the admissible $x$ that we have found is actually the best solution possible, and that solving the dual problem actually gives you the best solution for the primal problem.\nSlater’s condition Slater’s condition enables us to assert that strong duality is possible, and thus we have a manner to find the best solution for the optimization problem by optimizing the dual problem. if given the following optimization problem $$ \\begin{array} \\\\ \\min f(w), \u0026 w \\in \\mathbb{R}^{d} \\\\ \\text{ s.t. } g_{i}(w) = 0, \u0026 i \\leq m \\\\ h_{j}(w) \\leq 0 \u0026 j \\leq n \\end{array} $$ And we have that $f, h_{j}$ are all convex and $g_{i}$ is affine, then this theorem asserts that\nif there is a feasible $w$ such that $h_{j}(w) \u003c 0$ for all $j \\leq n$ then we have strong duality.\nWhen we have strong duality, we can solve the dual problem instead of the original problem.\nReferences [1] Bishop “Pattern Recognition and Machine Learning” Springer 2006\n[2] Boyd \u0026 Vandenberghe “Convex Optimization – Boyd and Vandenberghe” 2004\n",
  "wordCount" : "1458",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/lagrange-multipliers/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Lagrange Multipliers
    </h1>
    <div class="post-meta">7 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#lagrangian-function" aria-label="Lagrangian function">Lagrangian function</a></li></ul>
                    
                <li>
                    <a href="#motivation" aria-label="Motivation">Motivation</a><ul>
                        
                <li>
                    <a href="#single-equation-constraint" aria-label="Single Equation constraint">Single Equation constraint</a></li>
                <li>
                    <a href="#inequality-constraints" aria-label="Inequality constraints">Inequality constraints</a><ul>
                        
                <li>
                    <a href="#playbook-for-lagrange-multipliers" aria-label="Playbook for Lagrange Multipliers">Playbook for Lagrange Multipliers</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#duality" aria-label="Duality">Duality</a><ul>
                        <ul>
                        
                <li>
                    <a href="#primal-problem" aria-label="Primal problem">Primal problem</a></li>
                <li>
                    <a href="#dual-problem" aria-label="Dual problem">Dual problem</a></li>
                <li>
                    <a href="#boundness-of-the-dual-function" aria-label="Boundness of the dual function">Boundness of the dual function</a></li>
                <li>
                    <a href="#slaters-condition" aria-label="Slater&rsquo;s condition">Slater&rsquo;s condition</a></li></ul>
                    </ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>This is also known as <em>Lagrange Optimization</em> or <em>undetermined multipliers</em>. Some of these notes are based on Appendix E of (Bishop 2006), others were found when studying bits of rational mechanics.
Also <a href="https://web.stanford.edu/~boyd/cvxbook/">(Boyd &amp; Vandenberghe 2004)</a> chapter 5 should be a good resource on this topic.</p>
<p>Let&rsquo;s consider a standard linear optimization problem
</p>
$$
\begin{array} \\
\min f_{0}(x)  \\
\text{subject to } f_{i}(x) \leq 0 \\
h_{j}(x) = 0
\end{array}
$$
<h3 id="lagrangian-function">Lagrangian function<a hidden class="anchor" aria-hidden="true" href="#lagrangian-function">#</a></h3>
<p>And let&rsquo;s consider the Lagrangian function associated to this problem defined as
</p>
$$
\mathcal{L}(x, \lambda, \nu) = f_{0}(x) + \sum \lambda_{i}f_{i}(x) + \sum\nu_{j}h_{j}(x)
$$
<p>
We want to say something about this function, because it is able to simplify the optimization problem a lot, but first we want to study this mathematically.</p>
<p>We call $\lambda_{i}, \nu_{i}$ the <em>lagrange multipliers</em> associated with their function, where we have that $\lambda_{i} \geq 0$ and $\nu_{i}$ are free. We can also define the <strong>dual function</strong> as $g(\lambda, \nu) = \min_{x} L(x, \lambda, \nu)$.</p>
<h2 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h2>
<p>In this section we try to explain step by step why this method works. We will find that it is easier than we could expect this.
With this, we will evaluate the maximum (for the minimum just take the dual)</p>
<h3 id="single-equation-constraint">Single Equation constraint<a hidden class="anchor" aria-hidden="true" href="#single-equation-constraint">#</a></h3>
<p>In this case we have our function $f$ along with a constraint $g(x) = b$ where $b$ is a constant in $\mathbb{R}$ and $x$ is a vector.
Note: we use $g(x) = b$ but it&rsquo;s the same if $b =0$ just define $h(x) = g(x) - b$ and you have your other function compared to a 0, which is usually easier in this context.
Let&rsquo;s consider the solutions of $g(x) = b$, this usually gives us a set of points (sometimes contiguous) where we have to look for a solution of $f$, we want to find among these points the one that minimizes the function $f$. The constraint on $g$ is somewhat similar to <a href="/notes/analisi-di-convessit%C3%A0/#sub-level-set">convex analysis&rsquo; level sets</a>.
One thing that is easily observable (this way is very physicist&rsquo;s way to motivate things, you do something similar when analyzing the parallel electric field in conductors see <a href="/notes/condensatori-nel-vuoto/">Condensatori nel vuoto</a>), is that if you have a $g(x)$ and you move slightly along the path of $g(x + \varepsilon)$ the value is the same, so the derivative along this direction is null. This allows us to say that the derivative of $g$ is perpendicular to the direction of the curve. Formally, we can write the <a href="/notes/hopital-taylor-peano/">Taylor</a> expansion of $g$ around some point $x$:
</p>
$$
g(x + \varepsilon) \approx g(x) + \nabla g(x) \varepsilon 
$$
<p>
Then we say that $g(x + \varepsilon) - g(x) = 0$ because they both lie in the same contour surface which implies $\nabla g(x)\varepsilon = 0$ in that direction. So, only the perpedicular component of the gradient remains.
<img src="/images/notes/Lagrange Multipliers-20240806125816880.webp" width="344" alt="Lagrange Multipliers-20240806125816880">
Then we can also say something about the gradient of $f$, because if it is not perfectly perpendicular to that surface, we would have that moving slightly along the surface could increase or decrease the value of $f$. So we <em>need</em> the two to be parallel or anti-parallel. Which motivates us to write $\nabla f + \lambda \nabla g = 0$ which in turn motivates the creation of the so called <strong>lagrangian</strong> function:
</p>
$$
\mathcal{L}(x, \lambda) = f(x) + \lambda g(x)
$$
<p>
We observe that its stationary point needs to have its partial derivatives set to zero, so we would have the original partial derivative condition, and also the inequality constraint for $\lambda \neq 0$. In this way we can find both the $x$ and the $\lambda$.</p>
<p>In the case we want to minimize, we can choose the lagrangian to be
</p>
$$
\mathcal{L}(x, \lambda) = f(x) + \lambda g(x)
$$
<p>
So that the derivative has opposite direction. (actually I don&rsquo;t have understood this point).</p>
<h3 id="inequality-constraints">Inequality constraints<a hidden class="anchor" aria-hidden="true" href="#inequality-constraints">#</a></h3>
<p>If instead of having bounds like $g(x) = b$ we have bounds like $g(x) \geq b$ it&rsquo;s just a little bit more complicated, we have more points, and could be useful to divide the case when $g(x)= b$ with $g(x) > b$. In the latter case, we just set $\lambda = 0$ (because this is what we get if we take the derivative w.r.t. to $\lambda$ and set $\lambda g(x) = 0$) and maximize the lagrangian (doesn&rsquo;t matter the direction of the gradient of $f$ now), when the solution is border, we should take a little bit more care: we want the gradient of $f$ to be away from the region $g$, which motivates us to have an equation like $\lambda \nabla g(x) = -\nabla f(x)$ and $\lambda > 0$. So we have the same Lagrangian as before, but we have some other constraints, those are called the <strong>Karush-Kuhn-Tucker</strong> conditions:
</p>
$$
\begin{cases}
g(x) \geq 0 \\
\lambda \geq 0 \\
\lambda g(x) = 0
\end{cases}
$$
<p>
The last two conditions are called <strong>complementary</strong> slackness conditions.</p>
<p>We can work the four cases intuitively and get some other intuition about lagrange optimizations.</p>
<h4 id="playbook-for-lagrange-multipliers">Playbook for Lagrange Multipliers<a hidden class="anchor" aria-hidden="true" href="#playbook-for-lagrange-multipliers">#</a></h4>
<p>Given an optimization problem
</p>
$$
\begin{array}
 \\
\min_{w} f(w) \\
\text{ s.t. } g_{i}(w) \leq 0 \\
h_{i}(w) = 0
\end{array}
$$
<ol>
<li>Write the Lagrangian function
$$
\mathcal{L}(w, \lambda, \nu) = f(w) + \sum_{i} \lambda_{i}g_{i}(w) + \sum_{i}\nu_{i}h_{i}(w)
$$
with $\lambda_{i} \geq 0$  these are the classical conditions, motivated above (just note the minimization problem, so we inverted one condition).</li>
<li>Check for <strong>Slater&rsquo;s condition</strong> which is check if there is a $w$ such that for every $i$ $g_{i}(w) < 0$ and $h_{i}(w) = 0$</li>
<li>Solve $\nabla_{w} \mathcal{L}$, $\lambda_{i}g_{i}(w) = 0$ and $h_{i}(w) = 0$</li>
</ol>
<h2 id="duality">Duality<a hidden class="anchor" aria-hidden="true" href="#duality">#</a></h2>
<h4 id="primal-problem">Primal problem<a hidden class="anchor" aria-hidden="true" href="#primal-problem">#</a></h4>
<p>The Lagrangian Multiplier method allows us to define the concept of dual optimization problem also in this context. If we consider this classical optimization problem
</p>
$$
\begin{array}
 \\
\min_{w} f(w) \\
\text{ s.t. } g_{i}(w) \leq 0 \\
h_{i}(w) = 0
\end{array}
$$
<p>
We can build the Lagrange Multiplier as
</p>
$$
\mathcal{L}(w, \lambda, \nu) = f(w) + \sum_{i} \lambda_{i}g_{i}(w) + \sum_{i}\nu_{i}h_{i}(w)
$$
<p>
with $\lambda_{i} \geq 0$, these are the classical conditions, motivated above.
Now consider this <strong>primal value</strong>:
</p>
$$
\theta_{\mathcal{P}}(w) = \max_{\lambda, \nu : \lambda_{i} \geq 0} \mathcal{L}(w, \lambda, \nu)
$$
<p>
We observe that if any of the condition $g_{i}(w) \leq 0$ or $h_{i}(w) = 0$ is violated, then it&rsquo;s easy to say that $\theta_{\mathcal{P}} = +\infty$. So we say that this formulation is the same as the above:
</p>
$$
p^{*} = \min_{w} \theta_{\mathcal{P}}(w) = \min_{w} \max_{\lambda, \nu: \lambda_{i} \geq 0} \mathcal{L}(w, \lambda, \nu)
$$
<p>The dual of this optimization problem is just inverting the values in some manner.</p>
<h4 id="dual-problem">Dual problem<a hidden class="anchor" aria-hidden="true" href="#dual-problem">#</a></h4>
<p>Dual problem of the Lagrange function is the following:
</p>
$$
\theta_{\mathcal{D}}(\lambda, \nu) = \min_{\omega} \mathcal{L}(w, \lambda, \nu)
$$
<p>
We can prove some properties of this new function. Note that the chosen $w$ could also be a not admissible solution in this case.</p>
<p>So now the optimization problem is
</p>
$$
d^{*} = \max_{\lambda, \nu: \lambda_{i} \geq 0} \theta_{\mathcal{D}}(\lambda, \nu) = \max_{\lambda, \nu: \lambda_{i} \geq 0} \min_{w} \mathcal{L}(w, \lambda, \nu)
$$
<p>
We see a nice symmetry with the primal optimization problem.
This is the same as solving
</p>
$$
\begin{align}
\max_{\lambda, \nu} \theta_{\mathcal{D}}(\lambda, \nu)  \\
\text{ s.t. } \forall i, \lambda_{i} \geq 0
\end{align}
$$
<h4 id="boundness-of-the-dual-function">Boundness of the dual function<a hidden class="anchor" aria-hidden="true" href="#boundness-of-the-dual-function">#</a></h4>
<p>Consider the Lagrangian given the optimization problem above</p>
$$
\mathcal{L}(x, \lambda, \nu) = f_{0}(x) + \sum \lambda_{i}f_{i}(x) + \sum\nu_{j}h_{j}(x)
$$
<p>
Given $x^{*}$ a candidate of the optimization problem, we say that, after fixing $\lambda \geq 0$ and $\nu$  then it&rsquo;s always true that
</p>
$$
\theta_{D}(\lambda, \nu) =  min_{x} \mathcal{L}(x, \lambda, \nu)  \leq \mathcal{L}(x^{*}, \lambda, \nu) \leq f_{0}(x^{*})
$$
<p>
This is called <strong>weak duality</strong>: we observe that each possible solution is bounded below by the dual function. When Equality holds for the best $\lambda$ and $\nu$ we say that we have <strong>strong duality</strong>, which implies that the admissible $x$ that we have found is actually the best solution possible, and that solving the dual problem actually gives you the best solution for the primal problem.</p>
<h4 id="slaters-condition">Slater&rsquo;s condition<a hidden class="anchor" aria-hidden="true" href="#slaters-condition">#</a></h4>
<p>Slater&rsquo;s condition enables us to assert that strong duality is possible, and thus we have a manner to find the best solution for the optimization problem by optimizing the dual problem.
if given the following optimization problem
</p>
$$
\begin{array}
 \\
\min f(w),  & w \in \mathbb{R}^{d} \\
\text{ s.t.  } g_{i}(w) = 0, &  i \leq m \\
h_{j}(w) \leq 0 &  j \leq n
\end{array}
$$
<p>
And we have that $f, h_{j}$ are all convex and $g_{i}$ is affine, then this theorem asserts that</p>
<blockquote>
<p>if there is a feasible $w$ such that $h_{j}(w) < 0$ for all $j \leq n$ then we have strong duality.</p>
</blockquote>
<p>When we have strong duality, we can solve the dual problem instead of the original problem.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Bishop “Pattern Recognition and Machine Learning” Springer 2006</p>
<p>[2] Boyd &amp; Vandenberghe <a href="https://web.stanford.edu/~boyd/cvxbook/">“Convex Optimization – Boyd and Vandenberghe”</a>  2004</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/optimization/">📈Optimization</a></li>
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Lagrange Multipliers on x"
            href="https://x.com/intent/tweet/?text=Lagrange%20Multipliers&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2flagrange-multipliers%2f&amp;hashtags=%f0%9f%93%88optimization%2cmachinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Lagrange Multipliers on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2flagrange-multipliers%2f&amp;title=Lagrange%20Multipliers&amp;summary=Lagrange%20Multipliers&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2flagrange-multipliers%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Lagrange Multipliers on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2flagrange-multipliers%2f&title=Lagrange%20Multipliers">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Lagrange Multipliers on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2flagrange-multipliers%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Lagrange Multipliers on whatsapp"
            href="https://api.whatsapp.com/send?text=Lagrange%20Multipliers%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2flagrange-multipliers%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Lagrange Multipliers on telegram"
            href="https://telegram.me/share/url?text=Lagrange%20Multipliers&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2flagrange-multipliers%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Lagrange Multipliers on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Lagrange%20Multipliers&u=https%3a%2f%2fflecart.github.io%2fnotes%2flagrange-multipliers%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
