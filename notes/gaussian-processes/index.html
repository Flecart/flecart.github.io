<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Gaussian Processes | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="âž•probabilistic-artificial-intelligence, machinelearning">
<meta name="description" content="Gaussian processes can be viewed through a Bayesian lens of the function space: rather than sampling over individual data points, we are now sampling over entire functions. They extend the idea of bayesian linear regression by introducing an infinite number of feature functions for the input XXX.
In geostatistics, Gaussian processes are referred to as kriging regressions, and many other models, such as Kalman Filters or radial basis function networks, can be understood as special cases of Gaussian processes.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/gaussian-processes/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/gaussian-processes/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Gaussian Processes" />
<meta property="og:description" content="Gaussian processes can be viewed through a Bayesian lens of the function space: rather than sampling over individual data points, we are now sampling over entire functions. They extend the idea of bayesian linear regression by introducing an infinite number of feature functions for the input XXX.
In geostatistics, Gaussian processes are referred to as kriging regressions, and many other models, such as Kalman Filters or radial basis function networks, can be understood as special cases of Gaussian processes." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/gaussian-processes/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Gaussian Processes"/>
<meta name="twitter:description" content="Gaussian processes can be viewed through a Bayesian lens of the function space: rather than sampling over individual data points, we are now sampling over entire functions. They extend the idea of bayesian linear regression by introducing an infinite number of feature functions for the input XXX.
In geostatistics, Gaussian processes are referred to as kriging regressions, and many other models, such as Kalman Filters or radial basis function networks, can be understood as special cases of Gaussian processes."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Gaussian Processes",
      "item": "https://flecart.github.io/notes/gaussian-processes/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Gaussian Processes",
  "name": "Gaussian Processes",
  "description": "Gaussian processes can be viewed through a Bayesian lens of the function space: rather than sampling over individual data points, we are now sampling over entire functions. They extend the idea of bayesian linear regression by introducing an infinite number of feature functions for the input XXX.\nIn geostatistics, Gaussian processes are referred to as kriging regressions, and many other models, such as Kalman Filters or radial basis function networks, can be understood as special cases of Gaussian processes.",
  "keywords": [
    "âž•probabilistic-artificial-intelligence", "machinelearning"
  ],
  "articleBody": "Gaussian processes can be viewed through a Bayesian lens of the function space: rather than sampling over individual data points, we are now sampling over entire functions. They extend the idea of bayesian linear regression by introducing an infinite number of feature functions for the input XXX.\nIn geostatistics, Gaussian processes are referred to as kriging regressions, and many other models, such as Kalman Filters or radial basis function networks, can be understood as special cases of Gaussian processes. In this framework, certain functions are more likely than others, and we aim to model this probability distribution.\nGaussian processes retain epistemic uncertainty, meaning we are still modeling what we donâ€™t know about the function, while remaining tractable. Essentially, they provide a normal distribution over functions.\nIf we index this infinite dimensional Gaussian, thanks to the properties of Gaussians with marginalization, we still have another Gaussian. We need a way to parametrize now. In this case we do it using a covariance function $k(x, x') = \\text{Cov} (f(x), f(x'))$\nThis seems a quite nice resource on the topic.\nFormal definition of Gaussian Process A Gaussian process is an infinite set of random variables such that any finite number of them are jointly Gaussian.\nThe informal ideas is that we use the probability of the joints, given by our training dataset, to do some inference.\nA Gaussian Process is a (infinite) set of random variables, indexed by some set $X$ (domain of the function) and for each $x \\in X$ there is a random variable $f_{x}$ such that for all $A \\subseteq X$ and $A = \\left\\{ x_{1}, \\dots, x_{m} \\right\\}$ it holds that $f_{A} = [f_{x_{1}}, \\dots, f_{x_{m}}] \\sim \\mathcal{N}(\\mu_{A}, K_{AA})$ , this is called noise-free prediction, to have the value of $y$ just add some Gaussians with a noise variance. You can see that GPs are very sensitive to adversarial data, it will be considered in the prediction and will condition the mean and Covariance of the prediction. Where $$ K_{A A} = \\begin{pmatrix} k(x_{1}, x_{1}), \\dots, k(x_{1}, k_{m}) \\\\ \\vdots \\\\ k(x_{m}, x_{1}), \\dots, k(x_{m}, k_{m}) \\end{pmatrix} \\text{ and } \\mu_A = \\begin{pmatrix} \\mu(x_{1}) \\\\ \\vdots \\\\ \\mu(x_{m}) \\end{pmatrix} $$ $k : X \\times X \\to \\mathbb{R}$ is called the covariance (kernel) function and $\\mu : X \\to \\mathbb{R}$ is called the mean function. We write $f \\sim GP(\\mu, k)$ and define $f(x) := f_{x}$ See Kernel Methods for more about kernels.\nInference for Gaussian Processes ðŸŸ© Inference is easy if you understand the conditional Gaussian distribution, see Gaussians. You already have the old points defined, you just need to get the conditioned Gaussian on the previous data points.\nHere is the setting, and itâ€™s very similar to the Bayesian Linear Regression function view inference $$ \\bar{f} = \\begin{bmatrix} f \\\\ f* \\end{bmatrix}, \\, \\bar{\\Sigma} =\\begin{bmatrix} \\Sigma_{AA} + \\sigma_{p}^{2}I \u0026 k(x^{*}, A)^{T} \\\\ k(x^{*}, A) \u0026 k(x^{*}, x^{*}) \\end{bmatrix}, \\mu = \\begin{bmatrix} \\mu_{A} \\\\ \\mu_{f^{*}} \\end{bmatrix} $$ We just use the conditional Gaussian to have that our wanted distribution $f^{*} \\mid f$ has the following distribution:\n$$ \\begin{cases} \\mu = \\mu_{f^{*}} + k(x^{*}, A)(\\Sigma_{AA} + \\sigma^{2}_{p}I)^{-1}(A - \\mu_{A}) \\\\ \\Sigma = k(x^{*}, x^{*}) - k(x^{*}, A)(\\Sigma_{AA} + \\sigma_{p}^{2}I)^{-1}k(x^{*}, A)^{T} \\end{cases} $$ And then you just sample with this, which we will solve in the next section. Where $k(x^{*}, A) = \\left[ k(x^{*}, x_{1}), \\dots,k(x^{*}, x_{n}) \\right]$\nIf we have only the GP prior then the mean $\\mu_{f^{*}} = 0$, this is often a convention because the prior mean is only used to compute the posterior mean, and can be easily added back in. One note: the posterior mean depends on the observations $y$ while the variance does not.\nSampling for Gaussian Processes Standard Gaussian Sampling ðŸŸ©â€“ One simple solution is just sampling using $X = \\Sigma^{1/2}\\varepsilon + \\mu$, but taking the square root of a matrix takes $\\mathcal{O}(n^{3})$ time. This samples every needed point immediately. This is possible because by assumption we have that the GP is jointly Gaussian, meaning: $$ \\begin{array} \\\\ f_{1:n} \\sim \\mathcal{N}(\\mu_{f}, K_{f}) \\\\ \\end{array} $$ Forward Sampling ðŸŸ© Another approach, called forward sampling just samples everything one by one. Meaning $$ \\begin{array} \\\\ f_{1} \\sim p(f_{1}) \\\\ f_{2} \\sim p(f_{2} \\mid f_{1}) \\\\ \\vdots \\\\ f_{n} \\sim p(f_{n} \\mid f_{1:n-1}) \\end{array} $$ But we need to compute the matrix inverse in this case, which also takes $\\mathcal{O}(n^{3})$.\nLearning Gaussians Processes Kernel optimization ðŸŸ¨â€“ Could be also useful too look at the Kernel Methods. We want to optimize the following equation: $$ \\begin{split} \\ \\hat{\\theta_{MLE}} = \\arg \\max_{\\theta} p(y_{\\text{train} }\\mid x_{\\text{train}}, \\theta) = \\arg \\max_{\\theta} \\int p(y_{\\text{train}} \\mid f, x_{\\text{train}}, \\theta)p(f \\mid \\theta) , df \\ = \\arg \\max_{\\theta} \\int p(y_{1:n} \\mid f_{1:n}, x_{1:n}, \\theta)p(f_{1:n} \\mid \\theta) , df \\ = \\arg \\max_{\\theta} \\int \\prod_{i = 1}^{n} \\mathcal{N}(y_{i} \\mid f_{i}; 0, \\sigma^{2}{n} )\\mathcal{N}(f{1:n} ; 0, K_{f}(\\theta)) df \\ = \\arg \\max_{\\theta} \\mathcal{N}(y_{1:n} ; 0, K_{y}(\\theta)) \\ = \\arg \\max_{\\theta} \\frac{1}{\\sqrt{ (2\\pi)^{n} \\lvert K_{y}(\\theta) \\rvert }}\\exp\\left( - \\frac{1}{2} y_{1:n}^{T} K^{-1}{y}(\\theta) y{1:n} \\right) , \\end{split}\n$$ Now let's take the log and we will get: $$ \\begin{align} \\ = \\arg \\min_{\\theta} - \\log \\mathcal{N}(y_{1:n}; 0, K_{y}(\\theta)) \\ = \\arg \\min_{\\theta} \\frac{n}{2} \\log 2\\pi + \\frac{1}{2} \\log \\lvert K_{y}(\\theta) \\rvert + \\frac{1}{2} y_{1:n}^{T} K^{-1}{y}(\\theta) y{1:n} \\ = \\arg \\min_{\\theta} \\frac{1}{2} \\log \\lvert K_{y}(\\theta) \\rvert + \\frac{1}{2} y_{1:n}^{T} K^{-1}{y}(\\theta) y{1:n} \\end{align} $$ Where $$ K_{f}(\\theta) = K_{f_{1:n}}(\\theta) = \\begin{bmatrix} k_{\\theta}(x_{1}, x_{1}), \u0026 \\dots \u0026 , k_{\\theta}(x_{1}, x_{n}) \\ \\vdots \u0026 \u0026 \\vdots \\ k_{\\theta}(x_{n}, x_{1}), \u0026 \\dots \u0026 , k_{\\theta}(x_{n}, x_{n}) \\end{bmatrix} $$ And $$ K_{y}(\\theta) = K_{f}(\\theta) + \\sigma^{2}_{n}I $$\nOne can also prove that the gradient of our likelihood function is $$ \\frac{ \\partial \\log p(y_{1:n} \\mid x_{1:n}, \\theta) }{ \\partial \\theta_{j} } = \\frac{1}{2} tr\\left( (\\alpha \\alpha^{T} - K^{-1}_{y, \\theta}) \\frac{ \\partial K_{y, \\theta} }{ \\partial \\theta_{j}} \\right) $$ With $\\alpha = K_{y, \\theta}^{-1}y$ Now letâ€™s take some time to analyze what we good after all of this derivation: The quadratic form is interpretable as the likelihood, or the goodness of the fit. While the log of the Kernel matrix is the volume of the possible prediction, which is kinda the prior. Minimizing the above loss with gradient methods or similars can give us the best kernel parameters.\nWe can also obtain a Map Estimate if we place a prior on the kernel parameters $p(\\theta)$.\nHyper-priors ðŸŸ¨+ Another way to optimize is using hyper-priors. Basically, we assume the priors have parameters too, and we try to optimize over those parameters. Usually a good way to choose kernels is posterior agreement. Given the data, we would like to make the probability of the given data as high as possible. This is very important but I have not understood this well\nApproximating GPs Leaning a Gaussian Process is usually quite slow. We will introduce some local methods for faster approximation, and a technique introduced in Rahimi, Recht NeurIPs 2007, it is a general algorithm for simplifying stationary kernels.\nLocal methods ðŸŸ© One way is only use points such that satisfy $\\lvert k(x, x') \\rvert \\geq \\tau$, the idea is that point that are far away usually have not such a great influence. This is still expensive if there are too many points close. Basically we can restrict ourselves to a subset of the Covariance matrix, as most other points will be set to $0$. This method essentially cuts off the tails of the distribution.\nKernel function approximation ðŸŸ¥+ This technique is quite hard to understand well. The idea is to reduce GPs to linear regression, then the cost would be $\\mathcal{O}(nm^{2} + m^{3})$ instead of $\\mathcal{n^{3}}$. One simple way to do it, is estimate the kernel function as some $$ k(x, x') \\approx \\phi(x)^{T}\\phi(x') $$ Such that $\\phi(x) \\in \\mathbb{R}^{m}$, and $m$ is a much smaller dimensionality (easier to compute). Then itâ€™s just GP with a linear kernel, or a Bayesian linear regression.\nFourier Features We make use of Bochnerâ€™s theorem which states that a Kernel is a valid Mercer Kernel if its Fourier transform $p(\\omega)$ is non-negative (which implies it is a valid probability distribution). When $p$ is a valid function, it is called spectral density\nConsider a shift invariant (stationary kernel), as they can be seen as function in one variable, these kernels have a Fourier transform, see Fourier Series. $$ k(x, x') = k(x - x') = \\int _{\\mathbb{R}^{d}} p(\\omega) e^{j\\omega^{T}(x - x')} \\, d\\omega = \\mathbb{E}_{\\omega, b}[z_{\\omega, b}(x) \\cdot z_{\\omega, b}(x')] $$ Where $\\omega \\sim p(\\omega), b \\sim U([0, 2\\pi])$, and define $z_{\\omega, b} = \\sqrt{ 2 } \\cos(\\omega^{T}x + b)$ where we are reinterpreting the integral as an expectation, one can show that these two are equivalent.\nThen we can approximate this expectation as follows: $$ \\mathbb{E}_{\\omega, b}[z_{\\omega, b}(x) \\cdot z_{\\omega, b}(x')] = \\frac{1}{m} \\sum_{i = 1}^{m} z_{\\omega, b}(x) \\cdot z_{\\omega, b}(x') = z(x)^{T}z(x) $$ Where $z(x) = \\frac{1}{\\sqrt{ m }}[z_{\\omega_{1, b_{1}}}(x), \\dots, z_{\\omega_{m, b_{m}}}(x)]$ Now we have an explicit random feature map, where $m$ works as a parameter for quality of the approximation.\nOne can also have some error analysis, telling you how close is the approximation to the original kernel. The theoretical thing is that this approximates the kernel function uniformly well, we use uniform convergence in this setting, which we donâ€™t know exactly what this is.\nInducing points ðŸŸ¥ Introduced in (Quinonero-Candela and Rasmussen, 2005), we want to summarize the points of the training set into the inducing points, and then use these points for inference.\n",
  "wordCount" : "1566",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/gaussian-processes/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;Â»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Gaussian Processes
    </h1>
    <div class="post-meta">8 min&nbsp;Â·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#formal-definition-of-gaussian-process" aria-label="Formal definition of Gaussian Process">Formal definition of Gaussian Process</a></li>
                <li>
                    <a href="#inference-for-gaussian-processes-" aria-label="Inference for Gaussian Processes ðŸŸ©">Inference for Gaussian Processes ðŸŸ©</a></li></ul>
                    
                <li>
                    <a href="#sampling-for-gaussian-processes" aria-label="Sampling for Gaussian Processes">Sampling for Gaussian Processes</a><ul>
                        
                <li>
                    <a href="#standard-gaussian-sampling---" aria-label="Standard Gaussian Sampling ðŸŸ©&ndash;">Standard Gaussian Sampling ðŸŸ©&ndash;</a></li>
                <li>
                    <a href="#forward-sampling-" aria-label="Forward Sampling ðŸŸ©">Forward Sampling ðŸŸ©</a></li></ul>
                </li>
                <li>
                    <a href="#learning-gaussians-processes" aria-label="Learning Gaussians Processes">Learning Gaussians Processes</a><ul>
                        
                <li>
                    <a href="#kernel-optimization---" aria-label="Kernel optimization ðŸŸ¨&ndash;">Kernel optimization ðŸŸ¨&ndash;</a></li>
                <li>
                    <a href="#hyper-priors-" aria-label="Hyper-priors ðŸŸ¨&#43;">Hyper-priors ðŸŸ¨+</a></li></ul>
                </li>
                <li>
                    <a href="#approximating-gps" aria-label="Approximating GPs">Approximating GPs</a><ul>
                        
                <li>
                    <a href="#local-methods-" aria-label="Local methods ðŸŸ©">Local methods ðŸŸ©</a></li>
                <li>
                    <a href="#kernel-function-approximation-" aria-label="Kernel function approximation ðŸŸ¥&#43;">Kernel function approximation ðŸŸ¥+</a></li>
                <li>
                    <a href="#fourier-features" aria-label="Fourier Features">Fourier Features</a></li>
                <li>
                    <a href="#inducing-points-" aria-label="Inducing points ðŸŸ¥">Inducing points ðŸŸ¥</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Gaussian processes can be viewed through a Bayesian lens of the function space: rather than sampling over individual data points, we are now sampling over entire functions. They extend the idea of <a href="/notes/bayesian-linear-regression/">bayesian linear regression</a> by introducing an infinite number of feature functions for the input XXX.</p>
<p>In geostatistics, Gaussian processes are referred to as <em>kriging</em> regressions, and many other models, such as <a href="/notes/kalman-filters/">Kalman Filters</a> or radial basis function networks, can be understood as special cases of Gaussian processes. In this framework, certain functions are more likely than others, and we aim to model this probability distribution.</p>
<p>Gaussian processes retain epistemic uncertainty, meaning we are still modeling what we don&rsquo;t know about the function, while remaining tractable. Essentially, they provide a <strong>normal distribution over functions</strong>.</p>
<p>If we index this infinite dimensional Gaussian, thanks to the properties of Gaussians with marginalization, we still have another Gaussian. We need a way to parametrize now. In this case we do it using a covariance function $k(x, x') = \text{Cov} (f(x), f(x'))$</p>
<p><a href="https://distill.pub/2019/visual-exploration-gaussian-processes/">This</a> seems a quite nice resource on the topic.</p>
<h4 id="formal-definition-of-gaussian-process">Formal definition of Gaussian Process<a hidden class="anchor" aria-hidden="true" href="#formal-definition-of-gaussian-process">#</a></h4>
<blockquote>
<p>A Gaussian process is an infinite set of random variables such that any finite number of them are jointly
Gaussian.</p>
</blockquote>
<p>The informal ideas is that we use the probability of the joints, given by our training dataset, to do some inference.</p>
<p>A <strong>Gaussian Process</strong> is a (infinite) set of random variables, indexed by some set $X$ (domain of the function) and for each $x \in X$ there is a random variable $f_{x}$ such that for all $A \subseteq X$ and $A = \left\{ x_{1}, \dots, x_{m} \right\}$ it holds that $f_{A} = [f_{x_{1}}, \dots, f_{x_{m}}] \sim \mathcal{N}(\mu_{A}, K_{AA})$ , this is called <strong>noise-free</strong> prediction, to have the value of $y$ just add some <a href="/notes/gaussians/">Gaussians</a> with a noise variance. You can see that GPs are very sensitive to adversarial data, it will be considered in the prediction and will condition the mean and Covariance of the prediction.
Where
</p>
$$
K_{A A} = \begin{pmatrix}
k(x_{1}, x_{1}), \dots, k(x_{1}, k_{m}) \\
\vdots \\
k(x_{m}, x_{1}), \dots, k(x_{m}, k_{m})
\end{pmatrix} 
\text{ and }
\mu_A = \begin{pmatrix}
\mu(x_{1})  \\
\vdots \\
\mu(x_{m})
\end{pmatrix}
$$
<p>$k : X \times X \to \mathbb{R}$ is called the <strong>covariance (kernel)</strong> function and $\mu : X \to \mathbb{R}$ is called the mean function.
We write $f \sim GP(\mu, k)$ and define $f(x) := f_{x}$
See <a href="/notes/kernel-methods/">Kernel Methods</a> for more about kernels.</p>
<h4 id="inference-for-gaussian-processes-">Inference for Gaussian Processes ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#inference-for-gaussian-processes-">#</a></h4>
<p>Inference is easy if you understand the conditional Gaussian distribution, see <a href="/notes/gaussians/">Gaussians</a>.
You already have the old points defined, you just need to get the conditioned Gaussian on the previous data points.</p>
<p>Here is the setting, and it&rsquo;s very similar to the <a href="/notes/bayesian-linear-regression/">Bayesian Linear Regression</a> function view inference
</p>
$$
\bar{f} = \begin{bmatrix}
f \\
f*
\end{bmatrix}, \, \bar{\Sigma}  =\begin{bmatrix}
\Sigma_{AA} + \sigma_{p}^{2}I  & k(x^{*}, A)^{T} \\
k(x^{*}, A)  & k(x^{*}, x^{*})
\end{bmatrix},
\mu = \begin{bmatrix}
\mu_{A} \\
\mu_{f^{*}}
\end{bmatrix}
$$
<p>
We just use the conditional Gaussian to have that our wanted distribution $f^{*} \mid f$ has the following distribution:</p>
$$
\begin{cases}
\mu = \mu_{f^{*}} + k(x^{*}, A)(\Sigma_{AA} + \sigma^{2}_{p}I)^{-1}(A - \mu_{A}) \\
\Sigma = k(x^{*}, x^{*}) - k(x^{*}, A)(\Sigma_{AA} + \sigma_{p}^{2}I)^{-1}k(x^{*}, A)^{T}
\end{cases}
$$
<p>
And then you just sample with this, which we will solve in the next section.
Where $k(x^{*}, A) = \left[ k(x^{*}, x_{1}), \dots,k(x^{*}, x_{n})  \right]$</p>
<p>If we have only the GP prior then the mean $\mu_{f^{*}} = 0$, this is often a convention because the prior mean is only used to compute the posterior mean, and can be easily added back in.
One note: the posterior mean depends on the observations $y$ while the variance does not.</p>
<h3 id="sampling-for-gaussian-processes">Sampling for Gaussian Processes<a hidden class="anchor" aria-hidden="true" href="#sampling-for-gaussian-processes">#</a></h3>
<h4 id="standard-gaussian-sampling---">Standard Gaussian Sampling ðŸŸ©&ndash;<a hidden class="anchor" aria-hidden="true" href="#standard-gaussian-sampling---">#</a></h4>
<p>One simple solution is just sampling using $X = \Sigma^{1/2}\varepsilon + \mu$, but taking the square root of a matrix takes $\mathcal{O}(n^{3})$ time. This samples every needed point immediately.
This is possible because by assumption we have that the GP is jointly Gaussian, meaning:
</p>
$$
\begin{array}
 \\
f_{1:n} \sim \mathcal{N}(\mu_{f}, K_{f}) \\
\end{array}
$$
<h4 id="forward-sampling-">Forward Sampling ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#forward-sampling-">#</a></h4>
<p>Another approach, called <em>forward sampling</em> just samples everything one by one. Meaning
</p>
$$
\begin{array}
 \\
f_{1} \sim p(f_{1}) \\
f_{2} \sim p(f_{2} \mid f_{1}) \\
\vdots \\
f_{n} \sim p(f_{n} \mid f_{1:n-1})
\end{array}
$$
<p>
But we need to compute the matrix inverse in this case, which also takes $\mathcal{O}(n^{3})$.</p>
<h3 id="learning-gaussians-processes">Learning Gaussians Processes<a hidden class="anchor" aria-hidden="true" href="#learning-gaussians-processes">#</a></h3>
<h4 id="kernel-optimization---">Kernel optimization ðŸŸ¨&ndash;<a hidden class="anchor" aria-hidden="true" href="#kernel-optimization---">#</a></h4>
<p>Could be also useful too look at the <a href="/notes/kernel-methods/">Kernel Methods</a>.
We want to optimize the following equation:
$$
\begin{split}
\
\hat{\theta_{MLE}} = \arg \max_{\theta} p(y_{\text{train} }\mid  x_{\text{train}}, \theta)
= \arg \max_{\theta} \int p(y_{\text{train}} \mid f, x_{\text{train}}, \theta)p(f \mid \theta) , df \
= \arg \max_{\theta} \int p(y_{1:n} \mid f_{1:n}, x_{1:n}, \theta)p(f_{1:n} \mid \theta) , df  \
=  \arg \max_{\theta} \int \prod_{i = 1}^{n} \mathcal{N}(y_{i} \mid f_{i}; 0, \sigma^{2}<em>{n} )\mathcal{N}(f</em>{1:n} ; 0, K_{f}(\theta)) df  \
= \arg \max_{\theta}   \mathcal{N}(y_{1:n} ; 0, K_{y}(\theta))  \
= \arg \max_{\theta}   \frac{1}{\sqrt{ (2\pi)^{n} \lvert K_{y}(\theta) \rvert }}\exp\left(  - \frac{1}{2} y_{1:n}^{T} K^{-1}<em>{y}(\theta) y</em>{1:n} \right) ,
\end{split}</p>
$$
Now let's take the log and we will get:
$$
<p>
\begin{align}
\
= \arg \min_{\theta} - \log \mathcal{N}(y_{1:n}; 0, K_{y}(\theta)) \
= \arg \min_{\theta} \frac{n}{2} \log 2\pi + \frac{1}{2} \log \lvert K_{y}(\theta) \rvert + \frac{1}{2} y_{1:n}^{T} K^{-1}<em>{y}(\theta) y</em>{1:n} \
= \arg \min_{\theta} \frac{1}{2} \log \lvert K_{y}(\theta) \rvert + \frac{1}{2} y_{1:n}^{T} K^{-1}<em>{y}(\theta) y</em>{1:n}
\end{align}
</p>
$$
Where 
$$
<p>
K_{f}(\theta) = K_{f_{1:n}}(\theta) = \begin{bmatrix}
k_{\theta}(x_{1}, x_{1}),  &amp;  \dots  &amp; , k_{\theta}(x_{1}, x_{n})  \
\vdots  &amp;   &amp; \vdots \
k_{\theta}(x_{n}, x_{1}),  &amp;  \dots  &amp; , k_{\theta}(x_{n}, x_{n})
\end{bmatrix}
</p>
$$
And
$$
<p>
K_{y}(\theta) = K_{f}(\theta) + \sigma^{2}_{n}I
$$</p>
<p>One can also prove that the gradient of our likelihood function is
</p>
$$
\frac{ \partial \log p(y_{1:n} \mid x_{1:n}, \theta) }{ \partial \theta_{j} }  = \frac{1}{2} tr\left( (\alpha \alpha^{T} - K^{-1}_{y, \theta}) \frac{ \partial K_{y, \theta} }{ \partial \theta_{j}}  \right) 
$$
<p>
With $\alpha = K_{y, \theta}^{-1}y$
Now let&rsquo;s take some time to analyze what we good after all of this derivation:
The quadratic form is interpretable as the <strong>likelihood</strong>, or the goodness of the fit. While the log of the Kernel matrix is the <strong>volume</strong> of the possible prediction, which is kinda the prior.
Minimizing the above loss with gradient methods or similars can give us the best kernel parameters.</p>
<p>We can also obtain a Map Estimate if we place a prior on the kernel parameters $p(\theta)$.</p>
<h4 id="hyper-priors-">Hyper-priors ðŸŸ¨+<a hidden class="anchor" aria-hidden="true" href="#hyper-priors-">#</a></h4>
<p>Another way to optimize is using <strong>hyper-priors</strong>. Basically, we assume the priors have parameters too, and we try to optimize over those parameters.
Usually a good way to choose kernels is <strong>posterior agreement</strong>. Given the data, we would like to make the probability of the given data as high as possible. This is very important but I have not understood this well</p>
<h3 id="approximating-gps">Approximating GPs<a hidden class="anchor" aria-hidden="true" href="#approximating-gps">#</a></h3>
<p>Leaning a Gaussian Process is usually quite slow. We will introduce some <strong>local methods</strong> for faster approximation, and a technique introduced in Rahimi, Recht NeurIPs 2007, it is a general algorithm for simplifying stationary kernels.</p>
<h4 id="local-methods-">Local methods ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#local-methods-">#</a></h4>
<p>One way is only use points such that satisfy $\lvert k(x, x') \rvert \geq \tau$, the idea is that point that are far away usually have not such a great influence. This is still expensive if there are too many points close.
Basically we can restrict ourselves to a subset of the Covariance matrix, as most other points will be set to $0$.
This method essentially <strong>cuts off the tails of the distribution</strong>.</p>
<h4 id="kernel-function-approximation-">Kernel function approximation ðŸŸ¥+<a hidden class="anchor" aria-hidden="true" href="#kernel-function-approximation-">#</a></h4>
<p>This technique is quite hard to understand well.
The idea is to reduce GPs to linear regression, then the cost would be $\mathcal{O}(nm^{2} + m^{3})$ instead of $\mathcal{n^{3}}$.
One simple way to do it, is estimate the kernel function as some
</p>
$$
k(x, x') \approx \phi(x)^{T}\phi(x')
$$
<p>
Such that $\phi(x) \in \mathbb{R}^{m}$, and $m$ is a much smaller dimensionality (easier to compute).
Then it&rsquo;s just GP with a linear kernel, or a Bayesian linear regression.</p>
<h4 id="fourier-features">Fourier Features<a hidden class="anchor" aria-hidden="true" href="#fourier-features">#</a></h4>
<p>We make use of <strong>Bochner&rsquo;s theorem</strong> which states that a Kernel is a valid Mercer Kernel if its Fourier transform $p(\omega)$ is non-negative (which implies it is a valid probability distribution). When $p$ is a valid function, it is called <strong>spectral density</strong></p>
<p>Consider a shift invariant (stationary kernel), as they can be seen as function in one variable, these kernels have a <strong>Fourier</strong> transform, see <a href="/notes/fourier-series/">Fourier Series</a>.
</p>
$$
k(x, x') = k(x - x') = \int _{\mathbb{R}^{d}} p(\omega) e^{j\omega^{T}(x - x')} \, d\omega 
= \mathbb{E}_{\omega, b}[z_{\omega, b}(x) \cdot z_{\omega, b}(x')]
$$
<p>
Where $\omega \sim p(\omega), b \sim U([0, 2\pi])$, and define $z_{\omega, b} = \sqrt{ 2 } \cos(\omega^{T}x + b)$ where we are reinterpreting the integral as an expectation, one can show that these two are equivalent.</p>
<p>Then we can approximate this expectation as follows:
</p>
$$
\mathbb{E}_{\omega, b}[z_{\omega, b}(x) \cdot z_{\omega, b}(x')]
= \frac{1}{m} \sum_{i = 1}^{m} z_{\omega, b}(x) \cdot z_{\omega, b}(x')
= z(x)^{T}z(x)
$$
<p>
Where $z(x) = \frac{1}{\sqrt{ m }}[z_{\omega_{1, b_{1}}}(x), \dots, z_{\omega_{m, b_{m}}}(x)]$
Now we have an explicit random feature map, where $m$ works as a parameter for quality of the approximation.</p>
<p>One can also have some error analysis, telling you how close is the approximation to the original kernel. The theoretical thing is that this approximates the kernel function <strong>uniformly well</strong>, we use uniform convergence in this setting, which we don&rsquo;t know exactly what this is.</p>
<h4 id="inducing-points-">Inducing points ðŸŸ¥<a hidden class="anchor" aria-hidden="true" href="#inducing-points-">#</a></h4>
<p>Introduced in (Quinonero-Candela and Rasmussen, 2005), we want to summarize the points of the training set into the <strong>inducing points</strong>, and then use these points for inference.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">âž•Probabilistic-Artificial-Intelligence</a></li>
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussian Processes on x"
            href="https://x.com/intent/tweet/?text=Gaussian%20Processes&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussian-processes%2f&amp;hashtags=%e2%9e%95probabilistic-artificial-intelligence%2cmachinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussian Processes on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussian-processes%2f&amp;title=Gaussian%20Processes&amp;summary=Gaussian%20Processes&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussian-processes%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussian Processes on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussian-processes%2f&title=Gaussian%20Processes">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussian Processes on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussian-processes%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussian Processes on whatsapp"
            href="https://api.whatsapp.com/send?text=Gaussian%20Processes%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fgaussian-processes%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussian Processes on telegram"
            href="https://telegram.me/share/url?text=Gaussian%20Processes&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussian-processes%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussian Processes on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Gaussian%20Processes&u=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussian-processes%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
