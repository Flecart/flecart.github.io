<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Introduction to statistical learning | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="no-tags">
<meta name="description" content="#statistics
Introduzione This is a short introduction to statistical learning, made with the help of the book ISLP (mi sento positivo ad affrontare la lettura di questo libro, ora che sta in python non lo vedo più come un libro solamente per statistici)
statistical learning refers to a set of approaches for estimating $f$ .
Utilizzi del statistical learning Solitamente sono due gli utilizzi Predizione e inferenza. Per predizione intendiamo il miglior modello che possa produrre le Y che ancora non conosciamo.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="flecart.github.io/notes/introduction-to-statistical-learning/">
<link crossorigin="anonymous" href="flecart.github.io/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="mask-icon" href="flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="flecart.github.io/notes/introduction-to-statistical-learning/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Introduction to statistical learning" />
<meta property="og:description" content="#statistics
Introduzione This is a short introduction to statistical learning, made with the help of the book ISLP (mi sento positivo ad affrontare la lettura di questo libro, ora che sta in python non lo vedo più come un libro solamente per statistici)
statistical learning refers to a set of approaches for estimating $f$ .
Utilizzi del statistical learning Solitamente sono due gli utilizzi Predizione e inferenza. Per predizione intendiamo il miglior modello che possa produrre le Y che ancora non conosciamo." />
<meta property="og:type" content="article" />
<meta property="og:url" content="flecart.github.io/notes/introduction-to-statistical-learning/" />
<meta property="og:image" content="flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Introduction to statistical learning"/>
<meta name="twitter:description" content="#statistics
Introduzione This is a short introduction to statistical learning, made with the help of the book ISLP (mi sento positivo ad affrontare la lettura di questo libro, ora che sta in python non lo vedo più come un libro solamente per statistici)
statistical learning refers to a set of approaches for estimating $f$ .
Utilizzi del statistical learning Solitamente sono due gli utilizzi Predizione e inferenza. Per predizione intendiamo il miglior modello che possa produrre le Y che ancora non conosciamo."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Introduction to statistical learning",
      "item": "flecart.github.io/notes/introduction-to-statistical-learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Introduction to statistical learning",
  "name": "Introduction to statistical learning",
  "description": "#statistics\nIntroduzione This is a short introduction to statistical learning, made with the help of the book ISLP (mi sento positivo ad affrontare la lettura di questo libro, ora che sta in python non lo vedo più come un libro solamente per statistici)\nstatistical learning refers to a set of approaches for estimating $f$ .\nUtilizzi del statistical learning Solitamente sono due gli utilizzi Predizione e inferenza. Per predizione intendiamo il miglior modello che possa produrre le Y che ancora non conosciamo.",
  "keywords": [
    "no-tags"
  ],
  "articleBody": "#statistics\nIntroduzione This is a short introduction to statistical learning, made with the help of the book ISLP (mi sento positivo ad affrontare la lettura di questo libro, ora che sta in python non lo vedo più come un libro solamente per statistici)\nstatistical learning refers to a set of approaches for estimating $f$ .\nUtilizzi del statistical learning Solitamente sono due gli utilizzi Predizione e inferenza. Per predizione intendiamo il miglior modello che possa produrre le Y che ancora non conosciamo. Per inferenza significa il miglior modello f per predire Y che conosciamo.\nNel primo caso la funzione migliore $f$ potrebbe benissimo restare sconosciuta, ci importa solamente che predica con accuratezza, mentre nel secondo caso vorremmo anche conoscere $f$.\nIn un certo senso l’inferenza ci permette di calcolare una specie di legge fisica, (è incorretto chiamarlo in questo modo, ma credo dia l’idea), ossia permette la comprensione del perché avendo questi input, potrò avere quei output. NOTA: avendo questa comprensione potrei anche fare predizioni su dati che non esistono, credo.\nErrore riducibile e irriducibile Questo è un concetto statistico abbastanza nuovo, utile per parlare di stima di modelli statistici. Supponiamo di avere una serie di dati solitamente indicati con $X$ , vogliamo con questi andare a predire la variabile dipendente $Y$. Solitamente andiamo anche ad introdurre un errore rappresentato con $\\varepsilon$ . Questo è un errore indipendente da $X$, ossia non possiamo predirlo utilizzando X.\nLa branca dell’apprendimento statistico vuole utilizzare X per andare a predire Y. Però il meglio che può fare resterebbe sempre solamente una funzione $f$ che abbia quell’errore che lo faccia variare. Quell’errore potrebbe essere dovuto a informazioni che non conosciamo oppure solamente a cose che non possono essere misurate (elementi che sono principalmente dovuti al caso, che difficilmente avremmo potuto utilizzare per misurarlo)\nFormalizzazione di sopra Tutto si potrebbe formalizzare con questo ragionamento $$ E[Y - \\hat{Y}]^2 = E[(f(X) + \\varepsilon) - \\hat{f}(X)]^2 = [f(X) - \\hat{f}(X)]^2 + var(\\varepsilon) $$ Osservando che X è una costante in questo caso, e ci interessa solamente la varianza. Si potrebbe intendere che la prima parte è un errore riducibile (può essere fino a 0), mentre la varianza dell’errore è parte di errore irriducibile.\nTipologie di modelli Principalmente esistono due tipologie di modelli, proveremo a parlarne estensivamente qui sotto:\nModelli parametrici L’inferenza/predizione con questo genere di modelli si fa in due step:\nScelta del modello (lineare? Quadratico? rete neurale? allora architettura della rete) Algoritmo di scelta dei parametri del modello Vedere sul libro pagina 31 per degli esempi. La caratteristica negativa di questo è che è fortemente dipendente dal nostro modello scelto, che non sempre può risultare essere la migliore per stimare la funzione che abbiamo:\nThe potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of $f$ .\nModelli non parametrici Non-parametric methods do not make explicit assumptions about the functional form of $f$ . Instead they seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly.\nUn esempio di questo genere di modelli potrebbero essere splines, che sono molto più variabili di un modello parametrico\nVantaggio principale sui parametrici by avoiding the assumption of a particular functional form for f , they have the potential to accurately fit a wider range of possible shapes for f .\nSvantaggio principale Dato che non devo imparare solamente dei parametri, ho bisogno di molto più dati affinché io sia accurato nella predizione\nLessico importante Variabile dipendente e indipendente Predittori, input e output e response Errore riducibile ed irriducibile ",
  "wordCount" : "601",
  "inLanguage": "en",
  "image": "flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "flecart.github.io/notes/introduction-to-statistical-learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="flecart.github.io/flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Introduction to statistical learning
    </h1>
    <div class="post-meta">3 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduzione" aria-label="Introduzione">Introduzione</a><ul>
                        
                <li>
                    <a href="#utilizzi-del-statistical-learning" aria-label="Utilizzi del statistical learning">Utilizzi del statistical learning</a></li></ul>
                </li>
                <li>
                    <a href="#errore-riducibile-e-irriducibile" aria-label="Errore riducibile e irriducibile">Errore riducibile e irriducibile</a><ul>
                        
                <li>
                    <a href="#formalizzazione-di-sopra" aria-label="Formalizzazione di sopra">Formalizzazione di sopra</a></li></ul>
                </li>
                <li>
                    <a href="#tipologie-di-modelli" aria-label="Tipologie di modelli">Tipologie di modelli</a><ul>
                        
                <li>
                    <a href="#modelli-parametrici" aria-label="Modelli parametrici">Modelli parametrici</a></li>
                <li>
                    <a href="#modelli-non-parametrici" aria-label="Modelli non parametrici">Modelli non parametrici</a><ul>
                        
                <li>
                    <a href="#vantaggio-principale-sui-parametrici" aria-label="Vantaggio principale sui parametrici">Vantaggio principale sui parametrici</a></li>
                <li>
                    <a href="#svantaggio-principale" aria-label="Svantaggio principale">Svantaggio principale</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#lessico-importante" aria-label="Lessico importante">Lessico importante</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>#statistics</p>
<h2 id="introduzione">Introduzione<a hidden class="anchor" aria-hidden="true" href="#introduzione">#</a></h2>
<p>This is a short introduction to statistical learning, made with the help of the book ISLP (mi sento positivo ad affrontare la lettura di questo libro, ora che sta in python non lo vedo più come un libro solamente per statistici)</p>
<blockquote>
<p>statistical learning refers to a set of approaches for estimating $f$ .</p>
</blockquote>
<h3 id="utilizzi-del-statistical-learning">Utilizzi del statistical learning<a hidden class="anchor" aria-hidden="true" href="#utilizzi-del-statistical-learning">#</a></h3>
<p>Solitamente sono due gli utilizzi <strong>Predizione</strong> e <strong>inferenza</strong>. Per predizione intendiamo il miglior modello che possa produrre le Y che ancora non conosciamo.
Per inferenza significa il miglior modello f per predire Y che conosciamo.</p>
<p>Nel primo caso la funzione migliore $f$ potrebbe benissimo restare sconosciuta, ci importa solamente che predica con accuratezza, mentre nel secondo caso vorremmo anche conoscere $f$.</p>
<p>In un certo senso l&rsquo;inferenza ci permette di calcolare una specie di <em>legge fisica</em>, (è incorretto chiamarlo in questo modo, ma credo dia l&rsquo;idea), ossia permette la comprensione del <em>perché</em> avendo questi input, potrò avere quei output. NOTA: avendo questa comprensione potrei anche fare predizioni su dati che non esistono, credo.</p>
<h2 id="errore-riducibile-e-irriducibile">Errore riducibile e irriducibile<a hidden class="anchor" aria-hidden="true" href="#errore-riducibile-e-irriducibile">#</a></h2>
<p>Questo è un concetto statistico abbastanza nuovo, utile per parlare di stima di modelli statistici.
Supponiamo di avere una serie di dati solitamente indicati con $X$ , vogliamo con questi andare a predire la variabile dipendente $Y$. Solitamente andiamo anche ad introdurre un errore rappresentato con $\varepsilon$ . Questo è un errore <strong>indipendente da $X$</strong>, ossia non possiamo predirlo utilizzando X.</p>
<p>La branca dell&rsquo;apprendimento statistico vuole utilizzare X per andare a predire Y. Però il meglio che può fare resterebbe sempre solamente una funzione $f$ che abbia quell&rsquo;errore che lo faccia variare. Quell&rsquo;errore potrebbe essere dovuto a informazioni che non conosciamo oppure solamente a cose che non possono essere misurate (elementi che sono principalmente dovuti al caso, che difficilmente avremmo potuto utilizzare per misurarlo)</p>
<h3 id="formalizzazione-di-sopra">Formalizzazione di sopra<a hidden class="anchor" aria-hidden="true" href="#formalizzazione-di-sopra">#</a></h3>
<p>Tutto si potrebbe formalizzare con questo ragionamento
$$
E[Y - \hat{Y}]^2 = E[(f(X) + \varepsilon) - \hat{f}(X)]^2 = [f(X) - \hat{f}(X)]^2 + var(\varepsilon)
$$
Osservando che X è una costante in questo caso, e ci interessa solamente la varianza.
Si potrebbe intendere che la prima parte è un errore riducibile (può essere fino a 0), mentre la varianza dell&rsquo;errore è parte di errore irriducibile.</p>
<h2 id="tipologie-di-modelli">Tipologie di modelli<a hidden class="anchor" aria-hidden="true" href="#tipologie-di-modelli">#</a></h2>
<p>Principalmente esistono due tipologie di modelli, proveremo a parlarne estensivamente qui sotto:</p>
<h3 id="modelli-parametrici">Modelli parametrici<a hidden class="anchor" aria-hidden="true" href="#modelli-parametrici">#</a></h3>
<p>L&rsquo;inferenza/predizione con questo genere di modelli si fa in due step:</p>
<ol>
<li>Scelta del modello (lineare? Quadratico? rete neurale? allora architettura della rete)</li>
<li>Algoritmo di scelta dei parametri del modello
Vedere sul libro pagina 31 per degli esempi.</li>
</ol>
<p>La caratteristica negativa di questo è che è fortemente dipendente dal nostro modello scelto, che non sempre può risultare essere la migliore per stimare la funzione che abbiamo:</p>
<blockquote>
<p>The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of $f$ .</p>
</blockquote>
<h3 id="modelli-non-parametrici">Modelli non parametrici<a hidden class="anchor" aria-hidden="true" href="#modelli-non-parametrici">#</a></h3>
<blockquote>
<p>Non-parametric methods do not make explicit assumptions about the functional form of $f$ . Instead t<em>hey seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly</em>.</p>
</blockquote>
<p>Un esempio di questo genere di modelli potrebbero essere splines, che sono molto più variabili di un modello parametrico</p>
<h4 id="vantaggio-principale-sui-parametrici">Vantaggio principale sui parametrici<a hidden class="anchor" aria-hidden="true" href="#vantaggio-principale-sui-parametrici">#</a></h4>
<blockquote>
<p>by avoiding the assumption of a particular functional form for f , they have the potential
to accurately fit a wider range of possible shapes for f .</p>
</blockquote>
<h4 id="svantaggio-principale">Svantaggio principale<a hidden class="anchor" aria-hidden="true" href="#svantaggio-principale">#</a></h4>
<p>Dato che non devo imparare solamente dei parametri, ho bisogno di molto più dati affinché io sia accurato nella predizione</p>
<h2 id="lessico-importante">Lessico importante<a hidden class="anchor" aria-hidden="true" href="#lessico-importante">#</a></h2>
<ul>
<li>Variabile dipendente e indipendente</li>
<li>Predittori, input e output e response</li>
<li>Errore riducibile ed irriducibile</li>
<li></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="flecart.github.io/tags/no-tags/">No-Tags</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to statistical learning on x"
            href="https://x.com/intent/tweet/?text=Introduction%20to%20statistical%20learning&amp;url=flecart.github.io%2fnotes%2fintroduction-to-statistical-learning%2f&amp;hashtags=no-tags">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to statistical learning on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=flecart.github.io%2fnotes%2fintroduction-to-statistical-learning%2f&amp;title=Introduction%20to%20statistical%20learning&amp;summary=Introduction%20to%20statistical%20learning&amp;source=flecart.github.io%2fnotes%2fintroduction-to-statistical-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to statistical learning on reddit"
            href="https://reddit.com/submit?url=flecart.github.io%2fnotes%2fintroduction-to-statistical-learning%2f&title=Introduction%20to%20statistical%20learning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to statistical learning on facebook"
            href="https://facebook.com/sharer/sharer.php?u=flecart.github.io%2fnotes%2fintroduction-to-statistical-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to statistical learning on whatsapp"
            href="https://api.whatsapp.com/send?text=Introduction%20to%20statistical%20learning%20-%20flecart.github.io%2fnotes%2fintroduction-to-statistical-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to statistical learning on telegram"
            href="https://telegram.me/share/url?text=Introduction%20to%20statistical%20learning&amp;url=flecart.github.io%2fnotes%2fintroduction-to-statistical-learning%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to statistical learning on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Introduction%20to%20statistical%20learning&u=flecart.github.io%2fnotes%2fintroduction-to-statistical-learning%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
