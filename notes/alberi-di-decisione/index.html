<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Alberi di decisione | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning">
<meta name="description" content="Introduzione agli alberi di decisione Setting del problema ðŸŸ©- Spazio delle ipotesi Definizione spazio ipotesi ðŸŸ©&mdash; Per spazio delle ipotesi andiamo a considerare l&rsquo;insieme delle funzioni rappresentabili dal nostro modello. Questo implica che l&rsquo;allenamento ricerca l&rsquo;ipotesi ossia la parametrizzazione ottimale del nostro modello, ottimale in quanto minimizza l&rsquo;errore che viene compiuto nel training set.
L&rsquo;insieme iniziale si puÃ² anche considerare come inductive bias ossia il restringimento solamente a certe ipotesi e non tutte.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/alberi-di-decisione/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/alberi-di-decisione/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Alberi di decisione" />
<meta property="og:description" content="Introduzione agli alberi di decisione Setting del problema ðŸŸ©- Spazio delle ipotesi Definizione spazio ipotesi ðŸŸ©&mdash; Per spazio delle ipotesi andiamo a considerare l&rsquo;insieme delle funzioni rappresentabili dal nostro modello. Questo implica che l&rsquo;allenamento ricerca l&rsquo;ipotesi ossia la parametrizzazione ottimale del nostro modello, ottimale in quanto minimizza l&rsquo;errore che viene compiuto nel training set.
L&rsquo;insieme iniziale si puÃ² anche considerare come inductive bias ossia il restringimento solamente a certe ipotesi e non tutte." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/alberi-di-decisione/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Alberi di decisione"/>
<meta name="twitter:description" content="Introduzione agli alberi di decisione Setting del problema ðŸŸ©- Spazio delle ipotesi Definizione spazio ipotesi ðŸŸ©&mdash; Per spazio delle ipotesi andiamo a considerare l&rsquo;insieme delle funzioni rappresentabili dal nostro modello. Questo implica che l&rsquo;allenamento ricerca l&rsquo;ipotesi ossia la parametrizzazione ottimale del nostro modello, ottimale in quanto minimizza l&rsquo;errore che viene compiuto nel training set.
L&rsquo;insieme iniziale si puÃ² anche considerare come inductive bias ossia il restringimento solamente a certe ipotesi e non tutte."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Alberi di decisione",
      "item": "https://flecart.github.io/notes/alberi-di-decisione/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Alberi di decisione",
  "name": "Alberi di decisione",
  "description": "Introduzione agli alberi di decisione Setting del problema ðŸŸ©- Spazio delle ipotesi Definizione spazio ipotesi ðŸŸ©\u0026mdash; Per spazio delle ipotesi andiamo a considerare l\u0026rsquo;insieme delle funzioni rappresentabili dal nostro modello. Questo implica che l\u0026rsquo;allenamento ricerca l\u0026rsquo;ipotesi ossia la parametrizzazione ottimale del nostro modello, ottimale in quanto minimizza l\u0026rsquo;errore che viene compiuto nel training set.\nL\u0026rsquo;insieme iniziale si puÃ² anche considerare come inductive bias ossia il restringimento solamente a certe ipotesi e non tutte.",
  "keywords": [
    "machinelearning"
  ],
  "articleBody": "Introduzione agli alberi di decisione Setting del problema ðŸŸ©- Spazio delle ipotesi Definizione spazio ipotesi ðŸŸ©â€” Per spazio delle ipotesi andiamo a considerare lâ€™insieme delle funzioni rappresentabili dal nostro modello. Questo implica che lâ€™allenamento ricerca lâ€™ipotesi ossia la parametrizzazione ottimale del nostro modello, ottimale in quanto minimizza lâ€™errore che viene compiuto nel training set.\nLâ€™insieme iniziale si puÃ² anche considerare come inductive bias ossia il restringimento solamente a certe ipotesi e non tutte. Altrimenti abbiamo no free lunch.\nEspressivitÃ  ðŸŸ© In pratica ci andiamo a chiedere\nPer quali $h$ esistono modelli di alberi di decisione? Per tutti Dato un albero per una ipotesi $h$, lâ€™albero Ã¨ unico? Se non Ã¨ unico abbiamo una preferenza? Overfitting and underfitting ðŸŸ© Sono dei fenomeni molto comuni nel campo dellâ€™apprendimento statistico. Si potrebbe dire in modo intuitivo che:\nUnderfitting quando il modello non Ã¨ ancora stato allenato, quindi possiede un bias molto alto per quanto riguarda la precisione del modello\nOverfitting quando il modello Ã¨ stato allenato troppo, tanto che ha imparato del rumore presente sul training set, questo non permette la generalizzazione sul test set.\nSi potrebbe parlare in modo piÃ¹ formale di overfitting come il verificarsi allo stesso tempo di due condizioni $$ error_{D}(h) \u003e error_{D}(hâ€™) $$ $$ error_{train}(h) \u003c error_{train}(hâ€™) $$ Ossia ho un errore basso nel training set, ma non riesco a generalizzare sul validation set.\n### Esempio di struttura albero decisionale Vorremmo cercare di modellare alcuni modelli di regressione o classificazione seguendo un albero di decisione come in figura Il problema sarebbe capire come creare lâ€™albero in automatico, a seconda di un training set labellato: Input:\n$$ coppie di training set. Output Un ipotesi $h$ che Ã¨ un albero di decisione. Entropia Definizione entropia ðŸŸ© $$ H(X) = - \\sum_{i=1}^{n}P(X = i) \\log_{2}P(X=i) $$ In cui se sono uguali hanno entropia massima, segue il grafico di questo genere\nCâ€™Ã¨ un apparato teorico non da poco per questo, perchÃ© Ã¨ stato utilizzato molto nella teoria della comunicazione. Una cosa che riguarda la probabilitÃ  del singolo dato, se Ã¨ sempre uguale ho entropia piÃ¹ alta.\nInformation Gain ðŸŸ¨+ Una definizione che segue lâ€™intuizione Ã¨ che la probabilitÃ  dellâ€™avvenimento influenzi il concetto di informazione, ossia se un evento compare sempre (tipo il sole che sorge), non ha molto informazione, perchÃ© Ã¨ sempre uguale.\nProbabilitÃ  1 ha zero informazione given two independent events with probabilities p1 and p2 their joint probability is p1p2 but the information acquired is the sum of the informations of the two independent events, so $I(p_{1}p_{2}) = I(p_{1}) + I(p_{2})$ Le due propreitÃ  di sopra giustificano il fatto di definire in modo abbastanza naturale che $$ I(p) = -\\log(p) $$ ProprietÃ  importanti (4) ðŸŸ¨++ Algoritmo di costruzione dellâ€™albero Descrizione algoritmo di costruzione ðŸŸ©- Bisogna introdurre il concetto di entropia per poter discriminare, vorremmo cercare il punto che diminuisca di piÃ¹ lâ€™entropia.\nRiduzione overfitting Pruning dellâ€™albero (2) ðŸŸ©â€“ Posso fare early stopping, ossia smetto di alllenarmi (ossia di creare altre branch), dopo che ne ho create un tot. Posso fare post-pruning, ossia dopo che ho creato tutto lâ€™albero (probabilmente facendo overfitting), mi metto ad eliminare alcune branches di poco conto. Questo si utilizza il validation set per vedere in che modo potare puÃ² influenzare la performance su questo (se migliora allora di fa, lo facciamo in modo greedy) Index di ImpuritÃ  di Gini ðŸŸ©- Questo Ã¨ un indice che Ã¨ stato usato in economia per studiare la disuguaglianza fra le persone, nel nostro caso lo utilizziamo per capire in che modo fare branching con lâ€™albero di decisione\nGiniâ€™s impurity measures the probability that a generic element get misclassified according to the current classification (an alternative to entropy).\n$$ I_{G}(F) = \\sum_{i=1}^{m} f_{i}(1 - f_{i}) = \\sum_{i=1}^{m} (f_{i} - f_{i}^{2}) = 1 - \\sum_{i= 1}^{m}f_{i}^{2} $$ Dove $f_{i}$ Ã¨ la frazione del dataset che appartiene ad $i$ Questo Ã¨ quindi un modo alternativo per fare split ad un nodo dellâ€™albero.\nRandom forests Vengono costruiti molti alberi e si utilizzano le loro decisioni assieme per avere un risultato finale, questo Ã¨ una tecnica ensemble perchÃ© vengono messe assieme conoscenze di tutti gli alberi.\nEnsemble models ðŸŸ© Ensemble techniques exploits the principle that a large number of relatively uncorrelated models (e.g. trees) operating as a committee will typically outperform any of the individual constituent models.\nMetodi di differenziazione (2) ðŸŸ©â€“ Chiaramente non abbiamo molto vantaggio se tutti gli alberi che vengono cosÃ¬ creati sono tutti uguali fra di loro, Ã¨ quindi utile utilizzare tecniche che li differenzino fra di loro:\nBagging uso input random. Feature randomness (uso subset di features per predire) Conclusioni Aspetti positivi degli alberi di decisione (4) ðŸŸ©- Molto veloci Non hanno bisogno di grandi quantitÃ  di dati Facile capire perchÃ© viene fatto la decisione (posso plottare le immagini) Adatti sia a problemi continui che discreti Aspetti negativi (3) ðŸŸ©- Molto facile andare in overfitting Esistono molti alberi per lo stesso dataset (instabile con le features e struttura dellâ€™albero) Possono diventare molto unbalanced se câ€™Ã¨ qualche predittore forte (andare a guardare questo). Side notes (altro) ",
  "wordCount" : "836",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/alberi-di-decisione/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;Â»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Alberi di decisione
    </h1>
    <div class="post-meta">4 min&nbsp;Â·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduzione-agli-alberi-di-decisione" aria-label="Introduzione agli alberi di decisione">Introduzione agli alberi di decisione</a><ul>
                        
                <li>
                    <a href="#setting-del-problema--" aria-label="Setting del problema ðŸŸ©-">Setting del problema ðŸŸ©-</a></li>
                <li>
                    <a href="#spazio-delle-ipotesi" aria-label="Spazio delle ipotesi">Spazio delle ipotesi</a><ul>
                        
                <li>
                    <a href="#definizione-spazio-ipotesi----" aria-label="Definizione spazio ipotesi ðŸŸ©&mdash;">Definizione spazio ipotesi ðŸŸ©&mdash;</a></li>
                <li>
                    <a href="#espressivit%c3%a0-" aria-label="EspressivitÃ  ðŸŸ©">EspressivitÃ  ðŸŸ©</a></li>
                <li>
                    <a href="#overfitting-and-underfitting-" aria-label="Overfitting and underfitting ðŸŸ©">Overfitting and underfitting ðŸŸ©</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#entropia" aria-label="Entropia">Entropia</a><ul>
                        
                <li>
                    <a href="#definizione-entropia-" aria-label="Definizione entropia ðŸŸ©">Definizione entropia ðŸŸ©</a></li>
                <li>
                    <a href="#information-gain-" aria-label="Information Gain ðŸŸ¨&#43;">Information Gain ðŸŸ¨+</a><ul>
                        
                <li>
                    <a href="#propriet%c3%a0-importanti-4-" aria-label="ProprietÃ  importanti (4) ðŸŸ¨&#43;&#43;">ProprietÃ  importanti (4) ðŸŸ¨++</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#algoritmo-di-costruzione-dellalbero" aria-label="Algoritmo di costruzione dell&rsquo;albero">Algoritmo di costruzione dell&rsquo;albero</a><ul>
                        
                <li>
                    <a href="#descrizione-algoritmo-di-costruzione--" aria-label="Descrizione algoritmo di costruzione ðŸŸ©-">Descrizione algoritmo di costruzione ðŸŸ©-</a></li>
                <li>
                    <a href="#riduzione-overfitting" aria-label="Riduzione overfitting">Riduzione overfitting</a><ul>
                        
                <li>
                    <a href="#pruning-dellalbero-2---" aria-label="Pruning dell&rsquo;albero (2) ðŸŸ©&ndash;">Pruning dell&rsquo;albero (2) ðŸŸ©&ndash;</a></li>
                <li>
                    <a href="#index-di-impurit%c3%a0-di-gini--" aria-label="Index di ImpuritÃ  di Gini ðŸŸ©-">Index di ImpuritÃ  di Gini ðŸŸ©-</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#random-forests" aria-label="Random forests">Random forests</a><ul>
                        
                <li>
                    <a href="#ensemble-models-" aria-label="Ensemble models ðŸŸ©">Ensemble models ðŸŸ©</a></li>
                <li>
                    <a href="#metodi-di-differenziazione-2---" aria-label="Metodi di differenziazione (2) ðŸŸ©&ndash;">Metodi di differenziazione (2) ðŸŸ©&ndash;</a></li></ul>
                </li>
                <li>
                    <a href="#conclusioni" aria-label="Conclusioni">Conclusioni</a><ul>
                        
                <li>
                    <a href="#aspetti-positivi-degli-alberi-di-decisione-4--" aria-label="Aspetti positivi degli alberi di decisione (4) ðŸŸ©-">Aspetti positivi degli alberi di decisione (4) ðŸŸ©-</a></li>
                <li>
                    <a href="#aspetti-negativi-3--" aria-label="Aspetti negativi (3) ðŸŸ©-">Aspetti negativi (3) ðŸŸ©-</a></li></ul>
                </li>
                <li>
                    <a href="#side-notes-altro" aria-label="Side notes (altro)">Side notes (altro)</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduzione-agli-alberi-di-decisione">Introduzione agli alberi di decisione<a hidden class="anchor" aria-hidden="true" href="#introduzione-agli-alberi-di-decisione">#</a></h2>
<h3 id="setting-del-problema--">Setting del problema ðŸŸ©-<a hidden class="anchor" aria-hidden="true" href="#setting-del-problema--">#</a></h3>
<img src="/images/notes/Alberi di decisione-1696950382366.jpeg" alt="Alberi di decisione-1696950382366">
<h3 id="spazio-delle-ipotesi">Spazio delle ipotesi<a hidden class="anchor" aria-hidden="true" href="#spazio-delle-ipotesi">#</a></h3>
<h4 id="definizione-spazio-ipotesi----">Definizione spazio ipotesi ðŸŸ©&mdash;<a hidden class="anchor" aria-hidden="true" href="#definizione-spazio-ipotesi----">#</a></h4>
<p>Per spazio delle ipotesi andiamo a considerare l&rsquo;insieme delle <em>funzioni rappresentabili dal nostro modello</em>.
Questo implica che <strong>l&rsquo;allenamento ricerca l&rsquo;ipotesi</strong> ossia la parametrizzazione <em>ottimale</em> del nostro modello, ottimale in quanto <em>minimizza</em> l&rsquo;errore che viene compiuto nel training set.</p>
<p>L&rsquo;insieme iniziale si puÃ² anche considerare come <strong>inductive bias</strong> ossia il restringimento solamente a certe ipotesi e non tutte. Altrimenti abbiamo no free lunch.</p>
<h4 id="espressivitÃ -">EspressivitÃ  ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#espressivitÃ -">#</a></h4>
<p>In pratica ci andiamo a chiedere</p>
<ul>
<li>Per quali $h$ esistono modelli di alberi di decisione? Per tutti</li>
<li>Dato un albero per una ipotesi $h$, l&rsquo;albero Ã¨ unico?</li>
<li>Se non Ã¨ unico abbiamo una preferenza?</li>
</ul>
<h4 id="overfitting-and-underfitting-">Overfitting and underfitting ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#overfitting-and-underfitting-">#</a></h4>
<p>Sono dei fenomeni molto comuni nel campo dell&rsquo;apprendimento statistico. Si potrebbe dire in modo intuitivo che:</p>
<ul>
<li>
<p><strong>Underfitting</strong> quando il modello non Ã¨ ancora stato allenato, quindi possiede un bias molto alto per quanto riguarda la precisione del modello</p>
</li>
<li>
<p><strong>Overfitting</strong> quando il modello Ã¨ stato allenato troppo, tanto che ha imparato del rumore presente sul training set, questo non permette la generalizzazione sul test set.</p>
</li>
</ul>
<p>Si potrebbe parlare in modo piÃ¹ formale di overfitting come il verificarsi allo stesso tempo di due condizioni
$$
error_{D}(h) &gt; error_{D}(h&rsquo;)
$$
$$
error_{train}(h) &lt; error_{train}(h&rsquo;)
$$
Ossia ho un errore basso nel training set, ma non riesco a generalizzare sul validation set.</p>
<img src="/images/notes/Alberi di decisione-1696962538331.jpeg" alt="Alberi di decisione-1696962538331">
### Esempio di struttura albero decisionale
<p>Vorremmo cercare di modellare alcuni modelli di <em>regressione</em> o <em>classificazione</em> seguendo un albero di decisione come in figura
<img src="/images/notes/Alberi di decisione-1696854233904.jpeg" alt="Alberi di decisione-1696854233904">
Il problema sarebbe capire <strong>come creare l&rsquo;albero in automatico</strong>, a seconda di un training set labellato:
<strong>Input</strong>:</p>
<ul>
<li>$&lt;x_{i}, y_{i}&gt;$ coppie di training set.
<strong>Output</strong></li>
<li>Un ipotesi $h$ che Ã¨ un albero di decisione.</li>
</ul>
<h2 id="entropia">Entropia<a hidden class="anchor" aria-hidden="true" href="#entropia">#</a></h2>
<h3 id="definizione-entropia-">Definizione entropia ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#definizione-entropia-">#</a></h3>
<p>$$
H(X) = - \sum_{i=1}^{n}P(X = i) \log_{2}P(X=i)
$$
In cui se sono uguali hanno entropia massima, segue il grafico di questo genere</p>
<img src="/images/notes/Alberi di decisione-1696855880485.jpeg" alt="Alberi di decisione-1696855880485">
<p>C&rsquo;Ã¨ un apparato teorico non da poco per questo, perchÃ© Ã¨ stato utilizzato molto nella teoria della comunicazione.
Una cosa che riguarda la <strong>probabilitÃ  del singolo dato</strong>, se Ã¨ sempre uguale ho entropia piÃ¹ alta.</p>
<h3 id="information-gain-">Information Gain ðŸŸ¨+<a hidden class="anchor" aria-hidden="true" href="#information-gain-">#</a></h3>
<p>Una definizione che segue l&rsquo;intuizione Ã¨ che la <em>probabilitÃ </em> dell&rsquo;avvenimento influenzi il concetto di informazione, ossia se un evento compare sempre (tipo il sole che sorge), non ha molto informazione, perchÃ© Ã¨ sempre uguale.</p>
<ol>
<li>ProbabilitÃ  1 ha zero informazione</li>
<li>given two independent events with probabilities p1 and p2 their joint probability is p1p2 but the information acquired is the sum of the informations of the two independent events, so $I(p_{1}p_{2}) = I(p_{1}) + I(p_{2})$
Le due propreitÃ  di sopra giustificano il fatto di definire in modo abbastanza naturale che
$$
I(p) = -\log(p)
$$</li>
</ol>
<h4 id="proprietÃ -importanti-4-">ProprietÃ  importanti (4) ðŸŸ¨++<a hidden class="anchor" aria-hidden="true" href="#proprietÃ -importanti-4-">#</a></h4>
<img src="/images/notes/Alberi di decisione-1696961446542.jpeg" alt="Alberi di decisione-1696961446542">
<h2 id="algoritmo-di-costruzione-dellalbero">Algoritmo di costruzione dell&rsquo;albero<a hidden class="anchor" aria-hidden="true" href="#algoritmo-di-costruzione-dellalbero">#</a></h2>
<h3 id="descrizione-algoritmo-di-costruzione--">Descrizione algoritmo di costruzione ðŸŸ©-<a hidden class="anchor" aria-hidden="true" href="#descrizione-algoritmo-di-costruzione--">#</a></h3>
<img src="/images/notes/Alberi di decisione-1696855730494.jpeg" alt="Alberi di decisione-1696855730494">
<p>Bisogna introdurre il concetto di <strong>entropia</strong> per poter discriminare, vorremmo cercare il punto che <strong>diminuisca di piÃ¹ l&rsquo;entropia</strong>.</p>
<h3 id="riduzione-overfitting">Riduzione overfitting<a hidden class="anchor" aria-hidden="true" href="#riduzione-overfitting">#</a></h3>
<h4 id="pruning-dellalbero-2---">Pruning dell&rsquo;albero (2) ðŸŸ©&ndash;<a hidden class="anchor" aria-hidden="true" href="#pruning-dellalbero-2---">#</a></h4>
<ul>
<li>Posso fare <strong>early stopping</strong>, ossia smetto di alllenarmi (ossia di creare altre branch), dopo che ne ho create un tot.</li>
<li>Posso fare <strong>post-pruning</strong>, ossia dopo che ho creato tutto l&rsquo;albero (probabilmente facendo overfitting), mi metto ad eliminare alcune branches di poco conto.
<ul>
<li>Questo si utilizza il validation set per vedere in che modo potare puÃ² influenzare la performance su questo (se migliora allora di fa, lo facciamo in modo <em>greedy</em>)</li>
</ul>
</li>
</ul>
<h4 id="index-di-impuritÃ -di-gini--">Index di ImpuritÃ  di Gini ðŸŸ©-<a hidden class="anchor" aria-hidden="true" href="#index-di-impuritÃ -di-gini--">#</a></h4>
<p>Questo Ã¨ un indice che Ã¨ stato usato in economia per studiare la disuguaglianza fra le persone, nel nostro caso lo utilizziamo per capire in che modo fare branching con l&rsquo;albero di decisione</p>
<blockquote>
<p>Giniâ€™s impurity measures the probability that a generic element get misclassified according to the current classification (an alternative to entropy).</p>
</blockquote>
<p>$$
I_{G}(F) = \sum_{i=1}^{m} f_{i}(1 - f_{i}) = \sum_{i=1}^{m} (f_{i} - f_{i}^{2}) = 1 - \sum_{i= 1}^{m}f_{i}^{2}
$$
Dove $f_{i}$ Ã¨ la <em>frazione del dataset che appartiene ad $i$</em>
Questo Ã¨ quindi un modo alternativo per fare split ad un nodo dell&rsquo;albero.</p>
<h2 id="random-forests">Random forests<a hidden class="anchor" aria-hidden="true" href="#random-forests">#</a></h2>
<p>Vengono costruiti molti alberi e si utilizzano le loro decisioni assieme per avere un risultato finale, questo Ã¨ una tecnica <strong>ensemble</strong> perchÃ© vengono messe assieme conoscenze di tutti gli alberi.</p>
<h3 id="ensemble-models-">Ensemble models ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#ensemble-models-">#</a></h3>
<blockquote>
<p>Ensemble techniques exploits the principle that a large number of relatively uncorrelated models (e.g. trees) operating as a committee will typically outperform any of the individual constituent models.</p>
</blockquote>
<h3 id="metodi-di-differenziazione-2---">Metodi di differenziazione (2) ðŸŸ©&ndash;<a hidden class="anchor" aria-hidden="true" href="#metodi-di-differenziazione-2---">#</a></h3>
<p>Chiaramente non abbiamo molto vantaggio se tutti gli alberi che vengono cosÃ¬ creati sono tutti uguali fra di loro, Ã¨ quindi utile utilizzare tecniche che li differenzino fra di loro:</p>
<ul>
<li><strong>Bagging</strong> uso input random.</li>
<li><strong>Feature randomness</strong> (uso subset di features per predire)</li>
</ul>
<h2 id="conclusioni">Conclusioni<a hidden class="anchor" aria-hidden="true" href="#conclusioni">#</a></h2>
<h3 id="aspetti-positivi-degli-alberi-di-decisione-4--">Aspetti positivi degli alberi di decisione (4) ðŸŸ©-<a hidden class="anchor" aria-hidden="true" href="#aspetti-positivi-degli-alberi-di-decisione-4--">#</a></h3>
<ul>
<li>Molto veloci</li>
<li>Non hanno bisogno di grandi quantitÃ  di dati</li>
<li>Facile capire perchÃ© viene fatto la decisione (posso plottare le immagini)</li>
<li>Adatti sia a problemi continui che discreti</li>
</ul>
<h3 id="aspetti-negativi-3--">Aspetti negativi (3) ðŸŸ©-<a hidden class="anchor" aria-hidden="true" href="#aspetti-negativi-3--">#</a></h3>
<ul>
<li>Molto facile andare in overfitting</li>
<li>Esistono molti alberi per lo stesso dataset (instabile con le features e struttura dell&rsquo;albero)</li>
<li>Possono diventare molto unbalanced se c&rsquo;Ã¨ qualche predittore forte (andare a guardare questo).</li>
</ul>
<h2 id="side-notes-altro">Side notes (altro)<a hidden class="anchor" aria-hidden="true" href="#side-notes-altro">#</a></h2>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Alberi di decisione on x"
            href="https://x.com/intent/tweet/?text=Alberi%20di%20decisione&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2falberi-di-decisione%2f&amp;hashtags=machinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Alberi di decisione on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2falberi-di-decisione%2f&amp;title=Alberi%20di%20decisione&amp;summary=Alberi%20di%20decisione&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2falberi-di-decisione%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Alberi di decisione on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2falberi-di-decisione%2f&title=Alberi%20di%20decisione">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Alberi di decisione on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2falberi-di-decisione%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Alberi di decisione on whatsapp"
            href="https://api.whatsapp.com/send?text=Alberi%20di%20decisione%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2falberi-di-decisione%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Alberi di decisione on telegram"
            href="https://telegram.me/share/url?text=Alberi%20di%20decisione&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2falberi-di-decisione%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Alberi di decisione on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Alberi%20di%20decisione&u=https%3a%2f%2fflecart.github.io%2fnotes%2falberi-di-decisione%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
