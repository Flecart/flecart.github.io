<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Egocentric Vision | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machine-perception">
<meta name="description" content="Egocentric vision is a sub-field of computer vision that studies vision understanding from a centered point of view, that typical of animals.
One historical thing is MIT 1997 they had to bring around very heavy cameras. Now we have glasses. Other examples of egocentric vision are cars with cameras that see their surrounding, or robots equipped with cameras mimicking human vision.
The difference of egocentric vision compared to standard vision techniques is the high variability and instability of the video, and the concept of movement and interactions inside the image. Standard computer vision is disembodied and controlled field of view.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/egocentric-vision/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/egocentric-vision/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/egocentric-vision/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Egocentric Vision">
  <meta property="og:description" content="Egocentric vision is a sub-field of computer vision that studies vision understanding from a centered point of view, that typical of animals. One historical thing is MIT 1997 they had to bring around very heavy cameras. Now we have glasses. Other examples of egocentric vision are cars with cameras that see their surrounding, or robots equipped with cameras mimicking human vision. The difference of egocentric vision compared to standard vision techniques is the high variability and instability of the video, and the concept of movement and interactions inside the image. Standard computer vision is disembodied and controlled field of view.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="Machine-Perception">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Egocentric Vision">
<meta name="twitter:description" content="Egocentric vision is a sub-field of computer vision that studies vision understanding from a centered point of view, that typical of animals.
One historical thing is MIT 1997 they had to bring around very heavy cameras. Now we have glasses. Other examples of egocentric vision are cars with cameras that see their surrounding, or robots equipped with cameras mimicking human vision.
The difference of egocentric vision compared to standard vision techniques is the high variability and instability of the video, and the concept of movement and interactions inside the image. Standard computer vision is disembodied and controlled field of view.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Egocentric Vision",
      "item": "https://flecart.github.io/notes/egocentric-vision/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Egocentric Vision",
  "name": "Egocentric Vision",
  "description": "Egocentric vision is a sub-field of computer vision that studies vision understanding from a centered point of view, that typical of animals. One historical thing is MIT 1997 they had to bring around very heavy cameras. Now we have glasses. Other examples of egocentric vision are cars with cameras that see their surrounding, or robots equipped with cameras mimicking human vision. The difference of egocentric vision compared to standard vision techniques is the high variability and instability of the video, and the concept of movement and interactions inside the image. Standard computer vision is disembodied and controlled field of view.\n",
  "keywords": [
    "machine-perception"
  ],
  "articleBody": "Egocentric vision is a sub-field of computer vision that studies vision understanding from a centered point of view, that typical of animals. One historical thing is MIT 1997 they had to bring around very heavy cameras. Now we have glasses. Other examples of egocentric vision are cars with cameras that see their surrounding, or robots equipped with cameras mimicking human vision. The difference of egocentric vision compared to standard vision techniques is the high variability and instability of the video, and the concept of movement and interactions inside the image. Standard computer vision is disembodied and controlled field of view.\nComparison between fixed and wearable camera Camera Type Pros Cons Wearable Camera ✓ Dynamic uncurated content ✗ Occlusions ✓ Long-form video stream ✗ Increased Motion blur ✓ Driven by goals, interactions, and attention (embodied) Driven by goals interactions and attention Fixed Camera ✓ Static setup ✗ Curated moment in time ✓ Controlled Field of View ✗ Disembodied Vannevar Bush’s Vision and Memex Seminal paper, he saw that human knowledge would have expanded a lot, and he says we need new methods to manage this technology. He imagines a machine he calls Memex to store the knowledge in books, see this video. This article, (Bush 1945), is often cited as one of the intellectual origins of the web and modern knowledge systems. He anticipated Hypertext, personal computing, wearable technology and speech recognition.\nHistory of Egocentric Vision Research attempts In 1968 we have one of the first head mounted displays, and Steve Mann’s inventions in 1970 has pioneered some of these wearable computing. In public media we have Terminator 2 that has given some ideas of egocentric vision. Then you have MIT’s media lab going on with these works, Oxfords’ robotics research group. We also have Sixth sense in 2009, presented in a TED talk, which was quite impressive.\nCommercialization attempts Other commercial attempts where Hololens, Google Glasses, more recent apple glasses, or Meta’s Aria glasses. We are also able to track gaze now.\nDatasets Epic Kitchens is a famous dataset for egocentric vision. EGO4D is one of the most recent datasets, with a HUGE speedup. These are unscripted activities This dataset benchmarks episodic memory, social interaction, hands and objects interactions and audio-visual diarization and future forecasting. So you see it is important to create highly curated datasets. HD-EPIC is another dataset, but egocentric (increasingly more and more). Now that you have more dataset, you can build models and try techniques to solve it.\nApplications Problems common to this field of egocentric vision are:\nLocalisation (where am I) Scene understanding Action recognition, anticipation and gaze understanding and prediction. Social behavior understanding Full body pose estimation, see Parametric Human Body Models. Hand and hand-object interactions, which is a quite complex problem. Person identification Privacy Visual question answering Summarization Hand and Hand-Object Interaction Internet scale datasets Collect lot of hand data (3k hours of video, all annotated). Extract hand state information about these videos. Bounding box all the hands (left and right annotation), and also objects and its contact (none, self/person/portable/non-portable). They claim\n90% accuracy across datasets Automated 3D hand mesh reconstruction A nice application is how hands would grasp objects that they never seen. DexYCB This is a dataset of videos that grab things:\n2D object and keypoints detection. 3D hand pose estimation 6D object pose estimation (translation and rotation variables). They claim this dataset is useful to learn object manipulation for robotic hands.\nHaMeR Is the state of the art end-to-end hand reconstruction framework. They use Visual transformers to estimate the parameters of the hand model (shape, pose and camera) to reconstruct and reproject it to the image. Action Recognition and Anticipation We want to be able to predict goals, interactions and attention from dynamic uncurated embodied video data. Predicting this part should be useful to recognize and predict actions.\nThere are lots of works in psychology about intention, close to theory of mind, and my own paper in (Huang et al. 2024).\nSummarize the past to predict the future Short object interactions from EGO 4D benchmarks. Predict interaction object, the action and time to contact. They try to summarize important parts of the environment (like, are you in kitchen?, are you doing specific things, for example have you just picked up chopsticks?) This is extracted by language context extraction, then put it into TransFusion and predict next parts. They just do part of speech tagging to get noun and verbs and create mos t probable action annotation for the data. Then they use CLIP to match words and the objects in the image. Everything is then fused together and they use a fast R-CNN RoI heads to make noun, verb, box classification.\nThey show that dynamic information is hard to predict using this framework. This is why the TTC value is a little bit less accurate. Using language they had better generalization ability.\nPALM: Prediction Actions through Language Models They use a transformer to predict the future action. They use the past and the future to predict the action. They use a language model to predict the action, similar to transfusion, but they move to the language domain before predicting the next action.\nThey see that if you have many objects, the LM has a big difficulty in predicting the action.\nGaze Understanding and Prediction With this technology we can estimate the visual attention from egocentric videos. They add the global-local correlation module.\nDriving applications One important application is for driving, you can predict ego-trajectory prediction using these techniques. GPS locations inside the car are quite off, it is better to put them outside.\nThey can used shared embedding models and the feed it into a time-series transformer. One problem is being able to use long-tail distributional data. Visual Loss was to regularize the network.\nPath Complexity Index Most driving datasets are long tail distributions. The idea of this index is to take\ntake speed of the last moment and have a simple linear interpolation, and compare it with the real path, and have a difficulty measure in this manner, by comparing difference of linear and real path:\n$$ PCI(\\mathcal{T}_{target} | \\mathcal{T}_{input}) = \\| \\mathcal{T}_{target}(t) - \\mathcal{T}_{simple}(t) \\| $$$$ \\begin{aligned} \\mathcal{T}_{simple}(t) \u0026= \\mathcal{T}_{input}(T) + v_{final} \\cdot t, \\quad t \\ge T \\\\ v_{final} \u0026= \\mathcal{T}_{input}(T') - \\mathcal{T}_{input}(T' - 1) \\end{aligned} $$Indeed it is correlated with turning angles: They also show that adding GAZE improves with most complex datasets using this measure.\n3D Scene Understanding There some hard problems:\nCamera motion and narrow vision that make it hard to understand what you are doing Contextualization of the scene 3D interpretation of the camera motion. EgoGaussian You have a 3D representation where the object is moving around. They segment background and object, then they add the changes of the object and complete the shape of the object.\nRobotic Applications Kitten Carousel One of the most famous experiment in psychology was the Kitten Carousel in the 1963. Here we have a active cat that moves around and see the world, and the passive cat can just see but cannot move. It was to study the visual development of cats. Passive cats cannot develop their visual system. This is a very important experiment in psychology.\nThis suggests that to really being able to navigate the real world, you need to be able to navigate it (self motion + feedback), not just passively observe it.\nVisual encoder for Dexterous manipulation They use a visual encoder for features extraction, which is then used to policy training which is then used in the real world.\nMAPLE Another useful work is MAPLE: using manipulation priors (contact points) that are learned from egocentric videos to attack these kinds of problems\nThey use a ViT encoder to return the contact points and hand poses as tokens, then you move it to a diffusion policy to extract everything. They also have a label extraction pipeline.\nGeneralizable Ego-Vision World Model (Gao et al. 2024) is an example of these world models. You want to be able to predict next frames and being able to make prediction based on these predictions.\nReferences [1] Bush “As We May Think” The Atlantic 1945 [2] Gao et al. “Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability” arXiv preprint arXiv:2405.17398 2024 [3] Huang et al. “A Notion of Complexity for Theory of Mind via Discrete World Models” Association for Computational Linguistics 2024 ",
  "wordCount" : "1398",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/egocentric-vision/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Egocentric Vision
    </h1>
    <div class="post-meta">7 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul><ul>
                <li>
                    <a href="#comparison-between-fixed-and-wearable-camera" aria-label="Comparison between fixed and wearable camera">Comparison between fixed and wearable camera</a></li>
                <li>
                    <a href="#vannevar-bushs-vision-and-memex" aria-label="Vannevar Bush&rsquo;s Vision and Memex">Vannevar Bush&rsquo;s Vision and Memex</a></li></ul>
                    
                <li>
                    <a href="#history-of-egocentric-vision" aria-label="History of Egocentric Vision">History of Egocentric Vision</a><ul>
                        
                <li>
                    <a href="#research-attempts" aria-label="Research attempts">Research attempts</a></li>
                <li>
                    <a href="#commercialization-attempts" aria-label="Commercialization attempts">Commercialization attempts</a></li>
                <li>
                    <a href="#datasets" aria-label="Datasets">Datasets</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#applications" aria-label="Applications">Applications</a><ul>
                        
                <li>
                    <a href="#hand-and-hand-object-interaction" aria-label="Hand and Hand-Object Interaction">Hand and Hand-Object Interaction</a><ul>
                        
                <li>
                    <a href="#internet-scale-datasets" aria-label="Internet scale datasets">Internet scale datasets</a></li>
                <li>
                    <a href="#dexycb" aria-label="DexYCB">DexYCB</a></li>
                <li>
                    <a href="#hamer" aria-label="HaMeR">HaMeR</a></li></ul>
                </li>
                <li>
                    <a href="#action-recognition-and-anticipation" aria-label="Action Recognition and Anticipation">Action Recognition and Anticipation</a><ul>
                        
                <li>
                    <a href="#summarize-the-past-to-predict-the-future" aria-label="Summarize the past to predict the future">Summarize the past to predict the future</a></li>
                <li>
                    <a href="#palm-prediction-actions-through-language-models" aria-label="PALM: Prediction Actions through Language Models">PALM: Prediction Actions through Language Models</a></li></ul>
                </li>
                <li>
                    <a href="#gaze-understanding-and-prediction" aria-label="Gaze Understanding and Prediction">Gaze Understanding and Prediction</a><ul>
                        
                <li>
                    <a href="#driving-applications" aria-label="Driving applications">Driving applications</a></li>
                <li>
                    <a href="#path-complexity-index" aria-label="Path Complexity Index">Path Complexity Index</a></li></ul>
                </li>
                <li>
                    <a href="#3d-scene-understanding" aria-label="3D Scene Understanding">3D Scene Understanding</a><ul>
                        
                <li>
                    <a href="#egogaussian" aria-label="EgoGaussian">EgoGaussian</a></li></ul>
                </li>
                <li>
                    <a href="#robotic-applications" aria-label="Robotic Applications">Robotic Applications</a><ul>
                        
                <li>
                    <a href="#kitten-carousel" aria-label="Kitten Carousel">Kitten Carousel</a></li>
                <li>
                    <a href="#visual-encoder-for-dexterous-manipulation" aria-label="Visual encoder for Dexterous manipulation">Visual encoder for Dexterous manipulation</a></li>
                <li>
                    <a href="#maple" aria-label="MAPLE">MAPLE</a></li>
                <li>
                    <a href="#generalizable-ego-vision-world-model" aria-label="Generalizable Ego-Vision World Model">Generalizable Ego-Vision World Model</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Egocentric vision is a sub-field of computer vision that studies vision understanding from a centered point of view, that typical of animals.
One historical thing is MIT 1997 they had to bring around very heavy cameras. Now we have glasses. Other examples of egocentric vision are cars with cameras that see their surrounding, or robots equipped with cameras mimicking human vision.
The difference of egocentric vision compared to standard vision techniques is the high <em>variability</em> and <em>instability</em> of the video, and the concept of movement and interactions inside the image. Standard computer vision is <em>disembodied</em> and controlled field of view.</p>
<h4 id="comparison-between-fixed-and-wearable-camera">Comparison between fixed and wearable camera<a hidden class="anchor" aria-hidden="true" href="#comparison-between-fixed-and-wearable-camera">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Camera Type</th>
          <th>Pros</th>
          <th>Cons</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Wearable Camera</td>
          <td>✓ Dynamic uncurated content</td>
          <td>✗ Occlusions</td>
      </tr>
      <tr>
          <td></td>
          <td>✓ Long-form video stream</td>
          <td>✗ Increased Motion blur</td>
      </tr>
      <tr>
          <td></td>
          <td>✓ Driven by goals, interactions, and attention (<strong>embodied</strong>)</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>Driven by goals interactions and attention</td>
          <td></td>
      </tr>
      <tr>
          <td>Fixed Camera</td>
          <td>✓ Static setup</td>
          <td>✗ Curated moment in time</td>
      </tr>
      <tr>
          <td></td>
          <td>✓ Controlled Field of View</td>
          <td>✗ Disembodied</td>
      </tr>
  </tbody>
</table>
<h4 id="vannevar-bushs-vision-and-memex">Vannevar Bush&rsquo;s Vision and Memex<a hidden class="anchor" aria-hidden="true" href="#vannevar-bushs-vision-and-memex">#</a></h4>
<p>Seminal paper, he saw that human knowledge would have expanded a lot, and he says we need new methods to manage this technology. He imagines a machine he calls Memex to store the knowledge in books, see <a href="https://www.youtube.com/watch?v=c539cK58ees&ab_channel=MarkSanderson-researcher">this video</a>. This article, <a href="https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/">(Bush 1945)</a>, is often cited as one of the <strong>intellectual origins of the web</strong> and <strong>modern knowledge systems</strong>.
He anticipated Hypertext, personal computing, wearable technology and speech recognition.</p>
<h3 id="history-of-egocentric-vision">History of Egocentric Vision<a hidden class="anchor" aria-hidden="true" href="#history-of-egocentric-vision">#</a></h3>
<h4 id="research-attempts">Research attempts<a hidden class="anchor" aria-hidden="true" href="#research-attempts">#</a></h4>
<p>In 1968 we have one of the first head mounted displays, and Steve Mann&rsquo;s inventions in 1970 has pioneered some of these wearable computing. In public media we have Terminator 2 that has given some ideas of egocentric vision.
Then you have MIT&rsquo;s media lab going on with these works, Oxfords&rsquo; robotics research group.
We also have Sixth sense in 2009, presented in a TED talk, which was quite impressive.</p>
<h4 id="commercialization-attempts">Commercialization attempts<a hidden class="anchor" aria-hidden="true" href="#commercialization-attempts">#</a></h4>
<p>Other commercial attempts where Hololens, Google Glasses, more recent apple glasses, or Meta&rsquo;s Aria glasses. We are also able to track gaze now.</p>
<h4 id="datasets">Datasets<a hidden class="anchor" aria-hidden="true" href="#datasets">#</a></h4>
<p>Epic Kitchens is a famous dataset for egocentric vision. EGO4D is one of the most recent datasets, with a HUGE speedup. These are <strong>unscripted</strong> activities
This dataset benchmarks episodic memory, social interaction, hands and objects interactions and audio-visual diarization and future forecasting. So you see it is important to create highly curated datasets.
HD-EPIC is another dataset, but egocentric (increasingly more and more).
Now that you have more dataset, you can build models and try techniques to solve it.</p>
<h2 id="applications">Applications<a hidden class="anchor" aria-hidden="true" href="#applications">#</a></h2>
<p>Problems common to this field of egocentric vision are:</p>
<ul>
<li>Localisation (where am I)</li>
<li>Scene understanding</li>
<li>Action recognition, anticipation and gaze understanding and prediction.</li>
<li>Social behavior understanding</li>
<li>Full body pose estimation, see <a href="/notes/parametric-human-body-models">Parametric Human Body Models</a>.</li>
<li>Hand and hand-object interactions, which is a quite complex problem.</li>
<li>Person identification</li>
<li>Privacy</li>
<li>Visual question answering</li>
<li>Summarization</li>
</ul>
<h3 id="hand-and-hand-object-interaction">Hand and Hand-Object Interaction<a hidden class="anchor" aria-hidden="true" href="#hand-and-hand-object-interaction">#</a></h3>
<h4 id="internet-scale-datasets">Internet scale datasets<a hidden class="anchor" aria-hidden="true" href="#internet-scale-datasets">#</a></h4>
<ul>
<li>Collect  lot of hand data (3k hours of video, all annotated).</li>
<li>Extract hand state information about these videos.
<ul>
<li>Bounding box all the hands (left and right annotation), and also objects and its contact (none, self/person/portable/non-portable).</li>
</ul>
</li>
</ul>
<img src="/images/notes/Egocentric Vision-20250517130812660.webp" style="width: 100%" class="center" alt="Egocentric Vision-20250517130812660">
<p>They claim</p>
<ul>
<li>90% accuracy across datasets</li>
<li>Automated 3D hand mesh reconstruction</li>
<li>A nice application is how hands would grasp objects that they never seen.</li>
</ul>
<h4 id="dexycb">DexYCB<a hidden class="anchor" aria-hidden="true" href="#dexycb">#</a></h4>
<p>This is a dataset of videos that grab things:</p>
<ul>
<li>2D object and keypoints detection.</li>
<li>3D hand pose estimation</li>
<li>6D object pose estimation (translation and rotation variables).</li>
</ul>
<p>They claim this dataset is useful to learn object manipulation for robotic hands.</p>
<h4 id="hamer">HaMeR<a hidden class="anchor" aria-hidden="true" href="#hamer">#</a></h4>
<p>Is the state of the art <strong>end-to-end</strong>  hand reconstruction framework. They use Visual transformers to estimate the parameters of the hand model (shape, pose and camera) to reconstruct and reproject it to the image.
<img src="/images/notes/Egocentric Vision-20250517131227460.webp" style="width: 100%" class="center" alt="Egocentric Vision-20250517131227460"></p>
<h3 id="action-recognition-and-anticipation">Action Recognition and Anticipation<a hidden class="anchor" aria-hidden="true" href="#action-recognition-and-anticipation">#</a></h3>
<p>We want to be able to predict goals, interactions and attention from dynamic uncurated embodied video data. Predicting this part should be useful to recognize and predict actions.</p>
<p>There are lots of works in psychology about intention, close to theory of mind, and my own paper in <a href="https://aclanthology.org/2024.findings-emnlp.167/">(Huang et al. 2024)</a>.</p>
<h4 id="summarize-the-past-to-predict-the-future">Summarize the past to predict the future<a hidden class="anchor" aria-hidden="true" href="#summarize-the-past-to-predict-the-future">#</a></h4>
<p>Short object interactions from EGO 4D benchmarks. Predict interaction object, the action and time to contact.
They try to summarize important parts of the environment (like, are you in kitchen?, are you doing specific things, for example have you just picked up chopsticks?)
This is extracted by language context extraction, then put it into <strong>TransFusion</strong> and predict next parts.
They just do part of speech tagging to get noun and verbs and create mos t probable action annotation for the data.
Then they use CLIP to match words and the objects in the image.
Everything is then fused together and they use a fast R-CNN RoI heads to make noun, verb, box classification.</p>
<p>They show that dynamic information is hard to predict using this framework. This is why the TTC value is a little bit less accurate. Using language they had better generalization ability.</p>
<h4 id="palm-prediction-actions-through-language-models">PALM: Prediction Actions through Language Models<a hidden class="anchor" aria-hidden="true" href="#palm-prediction-actions-through-language-models">#</a></h4>
<p>They use a transformer to predict the future action. They use the past and the future to predict the action. They use a language model to predict the action, similar to transfusion, but they move to the language domain before predicting the next action.</p>
<p>They see that if you have many objects, the LM has a big difficulty in predicting the action.</p>
<h3 id="gaze-understanding-and-prediction">Gaze Understanding and Prediction<a hidden class="anchor" aria-hidden="true" href="#gaze-understanding-and-prediction">#</a></h3>
<p>With this technology we can estimate the <strong>visual attention</strong> from egocentric videos.
They add the <strong>global-local</strong> correlation module.</p>
<h4 id="driving-applications">Driving applications<a hidden class="anchor" aria-hidden="true" href="#driving-applications">#</a></h4>
<p>One important application is for driving, you can predict ego-trajectory prediction using these techniques.
GPS locations inside the car are quite off, it is better to put them outside.</p>
<img src="/images/notes/Egocentric Vision-20250517144733606.webp" style="width: 100%" class="center" alt="Egocentric Vision-20250517144733606">
<p>They can used shared embedding models and the feed it into a time-series transformer. One problem is being able to use long-tail distributional data. Visual Loss was to regularize the network.</p>
<h4 id="path-complexity-index">Path Complexity Index<a hidden class="anchor" aria-hidden="true" href="#path-complexity-index">#</a></h4>
<p>Most driving datasets are long tail distributions. The idea of this index is to take</p>
<p>take speed of the last moment and have a simple linear interpolation, and compare it with the real path, and have a difficulty measure in this manner, by comparing difference of linear and real path:</p>
$$
PCI(\mathcal{T}_{target} | \mathcal{T}_{input}) = \| \mathcal{T}_{target}(t) - \mathcal{T}_{simple}(t) \|
$$$$
\begin{aligned}
\mathcal{T}_{simple}(t) &= \mathcal{T}_{input}(T) + v_{final} \cdot t, \quad t \ge T \\
v_{final} &= \mathcal{T}_{input}(T') - \mathcal{T}_{input}(T' - 1)
\end{aligned}
$$<p>Indeed it is correlated with turning angles:
<img src="/images/notes/Egocentric Vision-20250517145152819.webp" style="width: 100%" class="center" alt="Egocentric Vision-20250517145152819"></p>
<p>They also show that adding GAZE improves with most complex datasets using this measure.</p>
<h3 id="3d-scene-understanding">3D Scene Understanding<a hidden class="anchor" aria-hidden="true" href="#3d-scene-understanding">#</a></h3>
<p>There some hard problems:</p>
<ol>
<li>Camera motion and narrow vision that make it hard to understand what you are doing</li>
<li>Contextualization of the scene</li>
<li>3D interpretation of the camera motion.</li>
</ol>
<h4 id="egogaussian">EgoGaussian<a hidden class="anchor" aria-hidden="true" href="#egogaussian">#</a></h4>
<p>You have a 3D representation where the object is moving around. They segment background and object, then they add the changes of the object and <em>complete the shape</em> of the object.</p>
<h3 id="robotic-applications">Robotic Applications<a hidden class="anchor" aria-hidden="true" href="#robotic-applications">#</a></h3>
<h4 id="kitten-carousel">Kitten Carousel<a hidden class="anchor" aria-hidden="true" href="#kitten-carousel">#</a></h4>
<p>One of the most famous experiment in psychology was the Kitten Carousel in the 1963. Here we have a <em>active cat</em> that moves around and see the world, and the <em>passive cat</em> can just see but cannot move. It was to study the visual development of cats. Passive cats cannot develop their visual system. This is a very important experiment in psychology.</p>
<img src="/images/notes/Egocentric Vision-20250517150238291.webp" style="width: 100%" class="center" alt="Egocentric Vision-20250517150238291">
<p>This suggests that to really being able to navigate the real world, you need to be able to navigate it (self motion + feedback), not just passively observe it.</p>
<h4 id="visual-encoder-for-dexterous-manipulation">Visual encoder for Dexterous manipulation<a hidden class="anchor" aria-hidden="true" href="#visual-encoder-for-dexterous-manipulation">#</a></h4>
<p>They use a visual encoder for features extraction, which is then used to policy training which is then used in the real world.</p>
<h4 id="maple">MAPLE<a hidden class="anchor" aria-hidden="true" href="#maple">#</a></h4>
<p>Another useful work is MAPLE: using <strong>manipulation priors</strong> (contact points) that are learned from egocentric videos to attack these kinds of problems</p>
<img src="/images/notes/Egocentric Vision-20250517150612415.webp" style="width: 100%" class="center" alt="Egocentric Vision-20250517150612415">
<p>They use a ViT encoder to return the contact points and hand poses as tokens, then you move it to a diffusion policy to extract everything.
They also have a label extraction pipeline.</p>
<h4 id="generalizable-ego-vision-world-model">Generalizable Ego-Vision World Model<a hidden class="anchor" aria-hidden="true" href="#generalizable-ego-vision-world-model">#</a></h4>
<p><a href="http://arxiv.org/abs/2405.17398">(Gao et al. 2024)</a> is an example of these world models. You want to be able to predict next frames and being able to make prediction based on these predictions.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=bushWeMayThink1945>[1] Bush <a href="https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/">“As We May Think”</a> The Atlantic 1945
 </p>
<p id=gaoVistaGeneralizableDriving2024>[2] Gao et al. <a href="http://arxiv.org/abs/2405.17398">“Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability”</a> arXiv preprint arXiv:2405.17398 2024
 </p>
<p id=huangNotionComplexityTheory2024>[3] Huang et al. <a href="https://aclanthology.org/2024.findings-emnlp.167/">“A Notion of Complexity for Theory of Mind via Discrete World Models”</a> Association for Computational Linguistics  2024
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machine-perception/">Machine-Perception</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Egocentric Vision on x"
            href="https://x.com/intent/tweet/?text=Egocentric%20Vision&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fegocentric-vision%2f&amp;hashtags=machine-perception">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Egocentric Vision on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fegocentric-vision%2f&amp;title=Egocentric%20Vision&amp;summary=Egocentric%20Vision&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fegocentric-vision%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Egocentric Vision on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fegocentric-vision%2f&title=Egocentric%20Vision">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Egocentric Vision on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fegocentric-vision%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Egocentric Vision on whatsapp"
            href="https://api.whatsapp.com/send?text=Egocentric%20Vision%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fegocentric-vision%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Egocentric Vision on telegram"
            href="https://telegram.me/share/url?text=Egocentric%20Vision&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fegocentric-vision%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Egocentric Vision on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Egocentric%20Vision&u=https%3a%2f%2fflecart.github.io%2fnotes%2fegocentric-vision%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
