<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Advanced 3D Representations | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machine-perception">
<meta name="description" content="3D representations
In this section, we present some of the most common 3D representations used in computer graphics and computer vision. Each representation has its own advantages and disadvantages, and the choice of representation often depends on the specific application.
Voxels
With voxels we discretize 3D space into a 3d grid, it is an intuitive manner to represent the data, but it has limited resolution.
It needs $\mathcal{O}(n^{3})$ memory.
Points and Volumetric primitives
We can discretize surfaces into 3D points.
Yet, this does not model connectivity, and might vary from frame to frame if it is a video.">
<meta name="author" content="
By Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/advanced-3d-representations/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f790d9af969c56c079c1ce2d5972a04486bf3d6144295d5fba319830e1e55a7a.css" integrity="sha256-95DZr5acVsB5wc4tWXKgRIa/PWFEKV1fujGYMOHlWno=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/advanced-3d-representations/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/advanced-3d-representations/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Advanced 3D Representations">
  <meta property="og:description" content="3D representations In this section, we present some of the most common 3D representations used in computer graphics and computer vision. Each representation has its own advantages and disadvantages, and the choice of representation often depends on the specific application.
Voxels With voxels we discretize 3D space into a 3d grid, it is an intuitive manner to represent the data, but it has limited resolution. It needs $\mathcal{O}(n^{3})$ memory.
Points and Volumetric primitives We can discretize surfaces into 3D points. Yet, this does not model connectivity, and might vary from frame to frame if it is a video.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="Machine-Perception">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Advanced 3D Representations">
<meta name="twitter:description" content="3D representations
In this section, we present some of the most common 3D representations used in computer graphics and computer vision. Each representation has its own advantages and disadvantages, and the choice of representation often depends on the specific application.
Voxels
With voxels we discretize 3D space into a 3d grid, it is an intuitive manner to represent the data, but it has limited resolution.
It needs $\mathcal{O}(n^{3})$ memory.
Points and Volumetric primitives
We can discretize surfaces into 3D points.
Yet, this does not model connectivity, and might vary from frame to frame if it is a video.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Advanced 3D Representations",
      "item": "https://flecart.github.io/notes/advanced-3d-representations/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Advanced 3D Representations",
  "name": "Advanced 3D Representations",
  "description": "3D representations In this section, we present some of the most common 3D representations used in computer graphics and computer vision. Each representation has its own advantages and disadvantages, and the choice of representation often depends on the specific application.\nVoxels With voxels we discretize 3D space into a 3d grid, it is an intuitive manner to represent the data, but it has limited resolution. It needs $\\mathcal{O}(n^{3})$ memory.\nPoints and Volumetric primitives We can discretize surfaces into 3D points. Yet, this does not model connectivity, and might vary from frame to frame if it is a video.\n",
  "keywords": [
    "machine-perception"
  ],
  "articleBody": "3D representations In this section, we present some of the most common 3D representations used in computer graphics and computer vision. Each representation has its own advantages and disadvantages, and the choice of representation often depends on the specific application.\nVoxels With voxels we discretize 3D space into a 3d grid, it is an intuitive manner to represent the data, but it has limited resolution. It needs $\\mathcal{O}(n^{3})$ memory.\nPoints and Volumetric primitives We can discretize surfaces into 3D points. Yet, this does not model connectivity, and might vary from frame to frame if it is a video.\nMany hardware cameras (like LiDAR) use this representation, and it is also used in some 3D reconstruction methods.\nMeshes We discretize vertices and faces, we know about connectivity. GPUs were made to handle this kind of data for example. Yet we have a limited number of vertices and faces (fixed resolution, which means we have an approximation error). Unreal Engine 5 Nanite has a way to decide when to use more triangles (more detail), or less (example long fields). Usually most fine-grained description is around one triangle per pixel (more is useless, you cannot render it). But as scientists, we like to represent it as continuous functions.\nImplicit Representations We can represent surfaces as the level-set of a continuous function =\u003e We have arbitrary topologies (for intro to topology see Topological Spaces). We use neural networks to approximate these implicit functions and shapes. Many many complex shapes can be represented with these implicit functions, but they need to be converted to representations above to be visualized.\nSignal Distance Fields (SDF) Normally, SDF (signal distance field $f_{\\theta} : \\mathbb{R}^{3} \\times \\mathcal{X} \\to R$) values are stored in a grid (this is quite memory consuming), meaning every cell stores the signed distance from the surface (negative means inside, positive means outside).\nNeural Implicit Representations Here we have neural networks that represent space information (if the shape is inside or outside).\nTwo representations - Occupancy Networks and DeepSDF We use simple MLP (see Neural Networks) to represent these images.\nOccupancy networks are just classification problems (you sample points, and decide if a point is inside or outside).\nDeepSDF is a regression problem, you sample points and predict the signed distance from the surface (you find level sets to know the surface).\nThe other idea is that we can use these neural fields also for color, lighting, radiance, and similar things (meaning more information)! Another nice thing is that normals are **gradients of the level set**, during backpropagation we have this information for free. The only problem when the translation to meshes happens is that at vertices, the normals are not well defined. Marching Cubes This is a rendering technique.\nMarching Cubes can extract renderable meshes from these implicit shapes, it is a way to convert the implicit representation to a mesh. This looks like a good web resource to give a brief idea of the method. It divides the space into voxels, and queries for inside and outside for the vertices, after it gets the configuration of the inside outside, it can render some triangles for the mesh.\nLearning Implicit Shapes This section explores most common ways to learn the implicit representations mentioned above.\nWatertight meshes We can create some sort of a surrogate model after having a first easily renderable (but more memory costly model) in the following way:\nWith these kinds of meshes we sample 3D points in space we query occupancy to the model (this is some ground value). After creating this dataset, then you can use a cross entropy loss to train the implicit shape. $$ \\mathcal{L}(\\theta, \\phi) = \\sum_{j = 1}^{K} \\text{BCE}(f_{\\theta}(p_{ij}, z_{i}), o_{ij}) $$ Where $\\text{BCE}$ is the binary cross entropy, $f_{\\theta}$ is the implicit function, $p_{ij}$ are the sampled points, $z_{i}$ are the latent variables (if you have some), and $o_{ij}$ are the occupancy labels (inside or outside).\nLearning from Points See this paper -\u003e (Gropp et al. 2020). The main problem with this format is that we have no connectivity information, and we need to learn it from the data. For example if you have some points in a circle, it is not obvious to define whether if a points is actually inside or outside the circle.\nFor example, both of the following solutions are plausible: $$ \\lVert \\nabla f_{\\theta}(\\boldsymbol{x}) \\rVert_{2} = 1 $$ We won’t explain the maths behind this, but it is a way to enforce the model to learn the distance field.\n$$ \\mathcal{L}(\\theta) = \\sum_{i \\in I} \\lvert f_{\\theta}(\\boldsymbol{x}_{i}) \\rvert^{2} + \\lambda \\sum_{j \\in J} \\left( \\lVert \\nabla f_{\\theta}(\\boldsymbol{x}_{j}) \\rVert_{2} - 1\\right)^{2} $$ This was introduced from a paper in 2020 cited above.\nLearning from Images The following is the general idea of this technique:\nWe usually have video from 2D images. We model geometry and appearance with neural implicit fields. We can use a differentiable renderer to render the images, and then optimize the parameters of the implicit field with a loss function that compares the rendered images with the original ones. Once you have this renderer trained, we can use it to render new images from different viewpoints. Forward Rendering How can we develop a differentiable rendering engine? From the camera point, we shoot the ray for all pixels $\\boldsymbol{u}$ and render using the predicted occupancy and texture data, using the texture field at the found point. (To find intersection they use root finding, also called secant method).\nSecant Root Method $$ f_{\\theta}(x_{j}) \u003c \\tau \\leq f_{\\theta}(x_{j+1}) $$ Then you can use the description of the algorithm above and do it several times until it converges to the value.\nBackward pass The idea is now we have a rendering, we can have an image back and compare it with the original.\nImage Observation $\\mathbf{\\hat{I}}$ Loss: $\\mathcal{L}(\\hat{\\mathbf{I}}, \\mathbf{I}) = \\sum_{\\mathbf{u}} \\| \\hat{\\mathbf{I}}_{\\mathbf{u}} - \\mathbf{I}_{\\mathbf{u}} \\|$ Gradient of loss function $$ \\begin{align*} \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \u0026= \\sum_{\\mathbf{u}} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{I}}_{\\mathbf{u}}} \\cdot \\frac{\\partial \\hat{\\mathbf{I}}_{\\mathbf{u}}}{\\partial \\theta} \\\\ \\frac{\\partial \\hat{\\mathbf{I}}_{\\mathbf{u}}}{\\partial \\theta} \u0026= \\frac{\\partial t_\\theta(\\hat{\\mathbf{p}})}{\\partial \\theta} + \\frac{\\partial t_\\theta(\\hat{\\mathbf{p}})}{\\partial \\hat{\\mathbf{p}}} \\cdot \\frac{\\partial \\hat{\\mathbf{p}}}{\\partial \\theta} \\end{align*} $$Differentiation of $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial \\theta}$ is tricky because $\\hat{\\mathbf{p}}$ is only implicitly defined and found iteratively via the Secant method. You can find an analytic solution for $p$ so that also that part is differentiable. Consider the ray: $\\hat{\\mathbf{p}} = \\mathbf{r}_0 + \\hat{d} \\mathbf{w}$\nCondition for surface-ray intersection: $f_\\theta(\\hat{\\mathbf{p}}) = \\tau$\nImplicit differentiation on both sides with respect to $\\theta$:\n$$ \\frac{\\partial f_\\theta(\\hat{\\mathbf{p}})}{\\partial \\theta} + \\frac{\\partial f_\\theta(\\hat{\\mathbf{p}})}{\\partial \\hat{\\mathbf{p}}} \\cdot \\frac{\\partial \\hat{\\mathbf{p}}}{\\partial \\theta} = 0 $$Substitute $\\hat{\\mathbf{p}} = \\mathbf{r}_0 + \\hat{d} \\mathbf{w}$:\n$$ \\frac{\\partial f_\\theta(\\hat{\\mathbf{p}})}{\\partial \\theta} + \\frac{\\partial f_\\theta(\\hat{\\mathbf{p}})}{\\partial \\hat{\\mathbf{p}}} \\cdot \\mathbf{w} \\frac{\\partial \\hat{d}}{\\partial \\theta} = 0 $$ Solve for $\\frac{\\partial \\hat{d}}{\\partial \\theta}$:\n$$ \\frac{\\partial \\hat{\\mathbf{p}}}{\\partial \\theta} = \\mathbf{w} \\frac{\\partial \\hat{d}}{\\partial \\theta} = -\\mathbf{w} \\left(\\frac{\\partial f_\\theta(\\hat{\\mathbf{p}})}{\\partial \\hat{\\mathbf{p}}} \\cdot \\mathbf{w} \\right)^{-1} \\frac{\\partial f_\\theta(\\hat{\\mathbf{p}})}{\\partial \\theta} $$ We leave out the implementation details, the nice thing is that this is analytical.\nNeural Radiance Fields Neural Radiance Fields (NeRF) is a method for representing 3D scenes using neural networks. It is particularly useful for rendering novel views of a scene from a sparse set of input images. The view can have a drastic change in appearance, NeRF is a solution that attacks this kind of problems.\nIntroduction to NeRF NeRF starts with a set of different views of a scene. For every scene it sends a ray and samples some points, getting its RGB value for every point and integrating. Then you combine this information from multiple views to create the final scene.\nWe add parameters of the viewing direction to the fully connected network, so we can have a different output for different viewing directions, and we also output the density value of the point.\nDensity $\\sigma$ describes how solid or transpared a 3D point is and is independent of viewing direction. We only condition with the view direction at the end, since it should not impact the underlying geometry much (we assume early layers model that).\nAlpha Compositing Alpha compositing is a technique used to combine images with transparency. In the context of NeRF, it is used to blend the colors of sampled points along a ray to produce the final pixel color. If a point has a high density, it means the alpha value is quite high (probaby the ray ends there).\n$$ \\begin{align*} \\text{Alpha}: \u0026 \\alpha_{i} = 1 - e^{-\\sigma_{i} \\delta_{i}}, \\delta_{i} = \\| \\mathbf{p}_{i} - \\mathbf{p}_{i-1} \\|_{2} \\\\ \\text{Transmittance}: \u0026 T_{i} = e^{-\\sum_{j=1}^{i-1} \\sigma_{j} \\delta_{j}} \\\\ \\text{Color}: \u0026 C_{i} = \\mathbf{c}_{i} \\cdot T_{i} \\cdot \\alpha_{i} \\\\ \\end{align*} $$There is a big cost in adding these samples, one way to make it cheaper is hierarchical sampling, to sample it more coarsely, and just taking into account where weights are high.\nTraining NeRF Also NeRF use the image reconstruction as its loss (like image implicit shape rendering approach). The thing is that we have view information, and this can be combined to create a good representation of the static scene.\nPositional Encoding with Fourier Features These are the same features we have seen in Gaussian Processes when making them run faster, also similar with positional encodings in Transformers. This is needed since NN are biased towards learning lower frequency functions (I currently don’t know why this is true).\nLow and high frequency is also the same idea used for JPEG image format compression techniques. Some forms of parameterization Quick Small Volume Rendering We said that NeRF wastes a lot of computation when doing Alpha Compositing. The idea is to make smaller volumes close to the figure that we want to approximate. This makes that part more efficient, but you need to first describe the volumes (problem is shifted to another domain, usually you don’t have the volumes).\nSpherical Primitives Instead of using pixels, we can use spheres, which are well supported by existing cuda kernels. The problem is that with sphere based reconstruction sometimes it is hard to have high quality for thin structures. But this can be solved with elipsoid primitives, which are more flexible and can represent thin structures better.\nStrenghts and weaknesses Strengths:\nCan render novel views of a scene from a sparse set of input images. Can handle complex lighting and reflections. Can produce high-quality images with realistic details. NeRF is a more flexible representation compared to implicit representation, since it has information about transparency and thin structure. Weaknesses:\nRequires a large amount of calibrated views to train and render a scene effectively. Slow rendering times, especially for high-resolution images. Not suitable for dynamic scenes (NeRF is designed for static scenes). Can produce artifacts such as floating objects or worse geometry compared to explicit representations like meshes. Doesn’t produce a geometry that can be easily manipulated or edited, it has lots of noise, floaters compared to implicit surfaces. So this method is good if you want nice static images. 3D Gaussian splatting This technique has been proposed in 2023, and has been quite influential now.\nIt is a method for representing 3D scenes using Gaussian splats, which are 3D Gaussian distributions that can be rendered efficiently. It is particularly useful for rendering novel views of a scene from a sparse set of input images.\nOne of their main advantages is fast inference.\nNo MLP evaluation No ray marching or point sampling Almost real time 60fps rendering. Able to represent thin structures, NeRFs have problems with those. Overview of the Technique 3D Gaussian splatting (See (Kerbl et al. 2023)) works by representing a scene as a collection of 3D Gaussian distributions, each with its own position, color, and opacity. These Gaussians can be rendered efficiently using GPU acceleration, allowing for real-time rendering of complex scenes.\nImages from the paper.\nAdaptive density control is used to add more detail to areas of the scene that require it, while reducing detail in areas that do not. This allows for efficient rendering without sacrificing quality. Examples are removing (if transparency is too low) or adding some Gaussians\nUnder reconstruction: if you cannot fit the shape well, you add a gaussian, and then use the gradient information to fill the loss function over reconstruction: remove the gaussian (probably if the gradient is too low, at the end this is the signal.). The gradient is the signal used to decide how to add or remove new Gaussians.\nThe Model Gaussian Splatting uses 3D gaussian primitives. Each Gaussian has four parts:\nA center $\\mu \\in \\mathbb{R}^{3}$ A covariance matrix $\\Sigma \\in \\mathbb{R}^{3 \\times 3}$, often expressed as rotation and diagonal scaling to maintain positive semi-definitivenes $\\Sigma = RSS^{T}R^{T}$. Color $c \\in \\mathbb{R}^{3}$, sometimes replaced with spherical harmonics $\\in \\mathbb{R}^{9}$ especially for view dependent formats. And an opacity value $o \\in\\mathbb{R}$. Rendering with Gaussian Splatting We only consider Gaussians close to the point in2D space, sort it in vicinance order with GPU (so O(n) sorts), and then use alpha compositing to render the scene.\n$$ \\alpha = o \\cdot \\exp(-0.5 (x - \\mu')^{T}\\Sigma^{'-1}(x- \\mu')) $$$$ C = \\sum_{i} \\alpha_{i} c_{i} \\prod_{j=0}^{i-1} (1 - \\alpha_{j}) $$ The main difference is that here we use Gaussians instead of MLPs, and we can render them in parallel (makes this very fast).\nApplications in Human Body Reconstruction This theme is important for the lab that is giving the course, also take a look at Parametric Human Body Models.\nSNARF We have 3D meshes, and we map it to a canonical pose (also called star pose). Then you compute the skinning weights to pose the implicit shape to unseen poses, now you can animate it to new poses, which is a quite cool industry application.\nThe difficult part is how to map the starting image to the canonical space. The idea here is to predict skinning weights, and use pose parameters to check if it matches the canonical pose or not. This is called forward warping. Vid2Avatar From single monocular video we can:\nReconstruct geometry Separate human from background Reconstruct human’s appearance. Image from the paper\nThe main point here is that you can use the ideas above to make these beautiful reconstructions.\nMulti-View Reconstruction and Tracking If you attach some Gaussians to some meshes, you can track how the object moves, this is the basic idea. They show you can keep tracking the movement also after drastic changes!\nReferences [1] Gropp et al. “Implicit Geometric Regularization for Learning Shapes” PMLR 2020 [2] Kerbl et al. “3D Gaussian Splatting for Real-Time Radiance Field Rendering” arXiv preprint arXiv:2308.04079 2023 ",
  "wordCount" : "2395",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/advanced-3d-representations/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Advanced 3D Representations
    </h1>
    <div class="post-meta">Reading Time: 12 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#3d-representations" aria-label="3D representations">3D representations</a><ul>
                        
                <li>
                    <a href="#voxels" aria-label="Voxels">Voxels</a></li>
                <li>
                    <a href="#points-and-volumetric-primitives" aria-label="Points and Volumetric primitives">Points and Volumetric primitives</a></li>
                <li>
                    <a href="#meshes" aria-label="Meshes">Meshes</a></li>
                <li>
                    <a href="#implicit-representations" aria-label="Implicit Representations">Implicit Representations</a></li>
                <li>
                    <a href="#signal-distance-fields-sdf" aria-label="Signal Distance Fields (SDF)">Signal Distance Fields (SDF)</a></li></ul>
                </li>
                <li>
                    <a href="#neural-implicit-representations" aria-label="Neural Implicit Representations">Neural Implicit Representations</a><ul>
                        
                <li>
                    <a href="#two-representations---occupancy-networks-and-deepsdf" aria-label="Two representations - Occupancy Networks and DeepSDF">Two representations - Occupancy Networks and DeepSDF</a></li>
                <li>
                    <a href="#marching-cubes" aria-label="Marching Cubes">Marching Cubes</a></li></ul>
                </li>
                <li>
                    <a href="#learning-implicit-shapes" aria-label="Learning Implicit Shapes">Learning Implicit Shapes</a><ul>
                        
                <li>
                    <a href="#watertight-meshes" aria-label="Watertight meshes">Watertight meshes</a></li>
                <li>
                    <a href="#learning-from-points" aria-label="Learning from Points">Learning from Points</a></li>
                <li>
                    <a href="#learning-from-images" aria-label="Learning from Images">Learning from Images</a></li>
                <li>
                    <a href="#forward-rendering" aria-label="Forward Rendering">Forward Rendering</a></li>
                <li>
                    <a href="#secant-root-method" aria-label="Secant Root Method">Secant Root Method</a></li>
                <li>
                    <a href="#backward-pass" aria-label="Backward pass">Backward pass</a></li></ul>
                </li>
                <li>
                    <a href="#neural-radiance-fields" aria-label="Neural Radiance Fields">Neural Radiance Fields</a><ul>
                        
                <li>
                    <a href="#introduction-to-nerf" aria-label="Introduction to NeRF">Introduction to NeRF</a></li>
                <li>
                    <a href="#alpha-compositing" aria-label="Alpha Compositing">Alpha Compositing</a></li>
                <li>
                    <a href="#training-nerf" aria-label="Training NeRF">Training NeRF</a></li>
                <li>
                    <a href="#positional-encoding-with-fourier-features" aria-label="Positional Encoding with Fourier Features">Positional Encoding with Fourier Features</a></li>
                <li>
                    <a href="#some-forms-of-parameterization" aria-label="Some forms of parameterization">Some forms of parameterization</a></li>
                <li>
                    <a href="#strenghts-and-weaknesses" aria-label="Strenghts and weaknesses">Strenghts and weaknesses</a></li></ul>
                </li>
                <li>
                    <a href="#3d-gaussian-splatting" aria-label="3D Gaussian splatting">3D Gaussian splatting</a><ul>
                        
                <li>
                    <a href="#overview-of-the-technique" aria-label="Overview of the Technique">Overview of the Technique</a></li>
                <li>
                    <a href="#the-model" aria-label="The Model">The Model</a></li>
                <li>
                    <a href="#rendering-with-gaussian-splatting" aria-label="Rendering with Gaussian Splatting">Rendering with Gaussian Splatting</a></li></ul>
                </li>
                <li>
                    <a href="#applications-in-human-body-reconstruction" aria-label="Applications in Human Body Reconstruction">Applications in Human Body Reconstruction</a><ul>
                        
                <li>
                    <a href="#snarf" aria-label="SNARF">SNARF</a></li>
                <li>
                    <a href="#vid2avatar" aria-label="Vid2Avatar">Vid2Avatar</a></li>
                <li>
                    <a href="#multi-view-reconstruction-and-tracking" aria-label="Multi-View Reconstruction and Tracking">Multi-View Reconstruction and Tracking</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="3d-representations">3D representations<a hidden class="anchor" aria-hidden="true" href="#3d-representations">#</a></h3>
<p>In this section, we present some of the most common 3D representations used in computer graphics and computer vision. Each representation has its own advantages and disadvantages, and the choice of representation often depends on the specific application.</p>
<h4 id="voxels">Voxels<a hidden class="anchor" aria-hidden="true" href="#voxels">#</a></h4>
<p>With voxels we <strong>discretize</strong> 3D space into a 3d grid, it is an <em>intuitive</em> manner to represent the data, but it has limited resolution.
It needs $\mathcal{O}(n^{3})$ memory.</p>
<h4 id="points-and-volumetric-primitives">Points and Volumetric primitives<a hidden class="anchor" aria-hidden="true" href="#points-and-volumetric-primitives">#</a></h4>
<p>We can discretize surfaces into 3D points.
Yet, this does not model <em>connectivity</em>, and might vary from frame to frame if it is a video.</p>
<p>Many hardware cameras (like LiDAR) use this representation, and it is also used in some 3D reconstruction methods.</p>
<h4 id="meshes">Meshes<a hidden class="anchor" aria-hidden="true" href="#meshes">#</a></h4>
<p>We discretize vertices and faces, we know about <em>connectivity</em>. GPUs were made to handle this kind of data for example.
Yet we have a limited number of vertices and faces (<strong>fixed resolution</strong>, which means we have an approximation error).
Unreal Engine 5 Nanite has a way to decide when to use more triangles (more detail), or less (example long fields). Usually most fine-grained description is around one triangle per pixel (more is useless, you cannot render it).
But as scientists, we like to represent it as continuous functions.</p>
<h4 id="implicit-representations">Implicit Representations<a hidden class="anchor" aria-hidden="true" href="#implicit-representations">#</a></h4>
<p>We can represent surfaces as the level-set of a continuous function =&gt; We have arbitrary topologies (for intro to topology see <a href="/notes/topological-spaces">Topological Spaces</a>).
We use neural networks to approximate these <em>implicit</em> functions and shapes.
Many many complex shapes can be represented with these implicit functions, but they need to be converted to representations above to be visualized.</p>
<h4 id="signal-distance-fields-sdf">Signal Distance Fields (SDF)<a hidden class="anchor" aria-hidden="true" href="#signal-distance-fields-sdf">#</a></h4>
<p>Normally, SDF (signal distance field $f_{\theta} : \mathbb{R}^{3} \times \mathcal{X} \to R$) values are stored in a grid (this is quite memory consuming), meaning every cell stores the signed distance from the surface (negative means inside, positive means outside).</p>
<h3 id="neural-implicit-representations">Neural Implicit Representations<a hidden class="anchor" aria-hidden="true" href="#neural-implicit-representations">#</a></h3>
<p>Here we have neural networks that represent space information (if the shape is inside or outside).</p>
<h4 id="two-representations---occupancy-networks-and-deepsdf">Two representations - Occupancy Networks and DeepSDF<a hidden class="anchor" aria-hidden="true" href="#two-representations---occupancy-networks-and-deepsdf">#</a></h4>
<p>We use simple MLP (see <a href="/notes/neural-networks">Neural Networks</a>) to represent these images.</p>
<ul>
<li>
<p>Occupancy networks are just classification problems (you sample points, and decide if a point is inside or outside).</p>
</li>
<li>
<p>DeepSDF is a regression problem, you sample points and predict the signed distance from the surface (you find level sets to know the surface).</p>
</li>
</ul>
<img src="/images/notes/Advanced 3D Representations-20250524193309961.webp" style="width: 100%" class="center" alt="Advanced 3D Representations-20250524193309961">
The other idea is that we can use these neural fields also for color, lighting, radiance, and similar things (meaning more information)!
Another nice thing is that normals are **gradients of the level set**, during backpropagation we have this information for free. The only problem when the translation to meshes happens is that at vertices, the normals are not well defined.
<h4 id="marching-cubes">Marching Cubes<a hidden class="anchor" aria-hidden="true" href="#marching-cubes">#</a></h4>
<p>This is a rendering technique.</p>
<p><strong>Marching Cubes</strong> can extract renderable meshes from these implicit shapes, it is a way to convert the implicit representation to a mesh. <a href="https://graphics.stanford.edu/~mdfisher/MarchingCubes.html">This</a> looks like a good web resource to give a brief idea of the method.
It divides the space into voxels, and queries for inside and outside for the vertices, after it gets the configuration of the inside outside, it can render some triangles for the mesh.</p>
<h3 id="learning-implicit-shapes">Learning Implicit Shapes<a hidden class="anchor" aria-hidden="true" href="#learning-implicit-shapes">#</a></h3>
<p>This section explores most common ways to learn the implicit representations mentioned above.</p>
<h4 id="watertight-meshes">Watertight meshes<a hidden class="anchor" aria-hidden="true" href="#watertight-meshes">#</a></h4>
<p>We can create some sort of a <em>surrogate model</em> after having a first easily renderable (but more memory costly model) in the following way:</p>
<ul>
<li>With these kinds of meshes we sample 3D points in space</li>
<li>we query occupancy to the model (this is some ground value).</li>
<li>After creating this dataset, then you can use a cross entropy loss to train the implicit shape.</li>
</ul>
$$
\mathcal{L}(\theta, \phi) = \sum_{j = 1}^{K} \text{BCE}(f_{\theta}(p_{ij}, z_{i}), o_{ij})
$$<p>
Where $\text{BCE}$ is the binary cross entropy, $f_{\theta}$ is the implicit function, $p_{ij}$ are the sampled points, $z_{i}$ are the latent variables (if you have some), and $o_{ij}$ are the occupancy labels (inside or outside).</p>
<h4 id="learning-from-points">Learning from Points<a hidden class="anchor" aria-hidden="true" href="#learning-from-points">#</a></h4>
<p>See this paper -&gt; <a href="https://proceedings.mlr.press/v119/gropp20a.html">(Gropp et al. 2020)</a>.
The main problem with this format is that we have no connectivity information, and we need to learn it from the data. For example if you have some points in a circle, it is not obvious to define whether if a points is actually inside or outside the circle.</p>
<p>For example, both of the following solutions are plausible:
<img src="/images/notes/Advanced 3D Representations-20250524194839528.webp" style="width: 100%" class="center" alt="Advanced 3D Representations-20250524194839528"></p>
$$
\lVert \nabla f_{\theta}(\boldsymbol{x}) \rVert_{2} = 1
$$<p>
We won&rsquo;t explain the maths behind this, but it is a way to enforce the model to learn the distance field.</p>
$$
\mathcal{L}(\theta) = \sum_{i \in I} \lvert f_{\theta}(\boldsymbol{x}_{i}) \rvert^{2} + \lambda \sum_{j \in J} \left(  \lVert \nabla f_{\theta}(\boldsymbol{x}_{j}) \rVert_{2}  - 1\right)^{2}
$$<p>
This was introduced from a paper in 2020 cited above.</p>
<h4 id="learning-from-images">Learning from Images<a hidden class="anchor" aria-hidden="true" href="#learning-from-images">#</a></h4>
<p>The following is the general idea of this technique:</p>
<ul>
<li>We usually have video from 2D images.</li>
<li>We model geometry and appearance with neural implicit fields.</li>
<li>We can use a differentiable renderer to render the images, and then optimize the parameters of the implicit field with a loss function that compares the rendered images with the original ones.
Once you have this renderer trained, we can use it to render new images from different viewpoints.</li>
</ul>
<h4 id="forward-rendering">Forward Rendering<a hidden class="anchor" aria-hidden="true" href="#forward-rendering">#</a></h4>
<p>How can we develop a differentiable rendering engine?
<img src="/images/notes/Advanced 3D Representations-20250525112707944.webp" style="width: 100%" class="center" alt="Advanced 3D Representations-20250525112707944">
From the camera point, we shoot the ray for all pixels $\boldsymbol{u}$ and render using the predicted occupancy and texture data, using the texture field at the found point. (To find intersection they use <em>root finding</em>, also called secant method).</p>
<h4 id="secant-root-method">Secant Root Method<a hidden class="anchor" aria-hidden="true" href="#secant-root-method">#</a></h4>
$$
f_{\theta}(x_{j}) < \tau \leq f_{\theta}(x_{j+1})
$$<p>
Then you can use the description of the algorithm above and do it several times until it converges to the value.</p>
<h4 id="backward-pass">Backward pass<a hidden class="anchor" aria-hidden="true" href="#backward-pass">#</a></h4>
<p>The idea is now we have a rendering, we can have an image back and compare it with the original.</p>
<ul>
<li><strong>Image Observation</strong> $\mathbf{\hat{I}}$</li>
<li><strong>Loss</strong>: $\mathcal{L}(\hat{\mathbf{I}}, \mathbf{I}) = \sum_{\mathbf{u}} \| \hat{\mathbf{I}}_{\mathbf{u}} - \mathbf{I}_{\mathbf{u}} \|$</li>
<li><strong>Gradient of loss function</strong></li>
</ul>
$$
 \begin{align*}
  \frac{\partial \mathcal{L}}{\partial \theta} &= \sum_{\mathbf{u}} \frac{\partial \mathcal{L}}{\partial \hat{\mathbf{I}}_{\mathbf{u}}} \cdot \frac{\partial \hat{\mathbf{I}}_{\mathbf{u}}}{\partial \theta} \\
  \frac{\partial \hat{\mathbf{I}}_{\mathbf{u}}}{\partial \theta} &= \frac{\partial t_\theta(\hat{\mathbf{p}})}{\partial \theta} + \frac{\partial t_\theta(\hat{\mathbf{p}})}{\partial \hat{\mathbf{p}}} \cdot \frac{\partial \hat{\mathbf{p}}}{\partial \theta}
\end{align*}
$$<p>Differentiation of $\frac{\partial \hat{\mathbf{p}}}{\partial \theta}$ is tricky because $\hat{\mathbf{p}}$ is only implicitly defined and found iteratively via the Secant method.
You can find an analytic solution for $p$ so that also that part is differentiable.
Consider the ray: $\hat{\mathbf{p}} = \mathbf{r}_0 + \hat{d} \mathbf{w}$</p>
<p>Condition for surface-ray intersection:  $f_\theta(\hat{\mathbf{p}}) = \tau$</p>
<p>Implicit differentiation on both sides with respect to $\theta$:</p>
$$
\frac{\partial f_\theta(\hat{\mathbf{p}})}{\partial \theta} + \frac{\partial f_\theta(\hat{\mathbf{p}})}{\partial \hat{\mathbf{p}}} \cdot \frac{\partial \hat{\mathbf{p}}}{\partial \theta} = 0
$$<p>Substitute $\hat{\mathbf{p}} = \mathbf{r}_0 + \hat{d} \mathbf{w}$:</p>
$$
\frac{\partial f_\theta(\hat{\mathbf{p}})}{\partial \theta} + \frac{\partial f_\theta(\hat{\mathbf{p}})}{\partial \hat{\mathbf{p}}} \cdot \mathbf{w} \frac{\partial \hat{d}}{\partial \theta} = 0
$$<p>
Solve for $\frac{\partial \hat{d}}{\partial \theta}$:</p>
$$
\frac{\partial \hat{\mathbf{p}}}{\partial \theta} = \mathbf{w} \frac{\partial \hat{d}}{\partial \theta} = -\mathbf{w}
\left(\frac{\partial f_\theta(\hat{\mathbf{p}})}{\partial \hat{\mathbf{p}}} \cdot \mathbf{w}
\right)^{-1} \frac{\partial f_\theta(\hat{\mathbf{p}})}{\partial \theta}
$$<p>
We leave out the implementation details, the nice thing is that this is analytical.</p>
<h3 id="neural-radiance-fields">Neural Radiance Fields<a hidden class="anchor" aria-hidden="true" href="#neural-radiance-fields">#</a></h3>
<p>Neural Radiance Fields (NeRF) is a method for representing 3D scenes using neural networks. It is particularly useful for rendering novel views of a scene from a sparse set of input images.
The view can have a <em>drastic change</em> in appearance, NeRF is a solution that attacks this kind of problems.</p>
<h4 id="introduction-to-nerf">Introduction to NeRF<a hidden class="anchor" aria-hidden="true" href="#introduction-to-nerf">#</a></h4>
<p>NeRF starts with a set of different views of a scene. For every scene it sends a ray and samples some points, getting its RGB value for every point and integrating. Then you combine this information from multiple views to create the final scene.</p>
<p>We add parameters of the viewing direction to the fully connected network, so we can have a different output for different viewing directions, and we also output the density value of the point.</p>
<figure class="center">
<img src="/images/notes/Advanced 3D Representations-20250525114205478.webp" style="width: 100%"   alt="Advanced 3D Representations-20250525114205478" title="Advanced 3D Representations-20250525114205478"/>
<figcaption><p style="text-align:center;"> Density $\sigma$ describes how solid or transpared a 3D point is and is independent of viewing direction. We only condition with the view direction at the end, since it should not impact the underlying geometry much (we assume early layers model that).</p></figcaption>
</figure>
<h4 id="alpha-compositing">Alpha Compositing<a hidden class="anchor" aria-hidden="true" href="#alpha-compositing">#</a></h4>
<p>Alpha compositing is a technique used to combine images with transparency. In the context of NeRF, it is used to blend the colors of sampled points along a ray to produce the final pixel color.
If a point has a high density, it means the alpha value is quite high (probaby the ray ends there).</p>
$$
\begin{align*}
\text{Alpha}: & \alpha_{i} = 1 - e^{-\sigma_{i} \delta_{i}}, \delta_{i} = \| \mathbf{p}_{i} - \mathbf{p}_{i-1} \|_{2} \\
\text{Transmittance}: & T_{i} = e^{-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}} \\
\text{Color}: & C_{i} = \mathbf{c}_{i} \cdot T_{i} \cdot \alpha_{i} \\
\end{align*}
$$<p>There is a big cost in adding these samples, one way to make it cheaper is <strong>hierarchical sampling</strong>, to sample it more coarsely, and just taking into account where weights are high.</p>
<h4 id="training-nerf">Training NeRF<a hidden class="anchor" aria-hidden="true" href="#training-nerf">#</a></h4>
<p>Also NeRF use the image reconstruction as its loss (like image implicit shape rendering approach).
The  thing is that we have view information, and this can be combined to create a good representation of the <strong>static scene</strong>.</p>
<h4 id="positional-encoding-with-fourier-features">Positional Encoding with Fourier Features<a hidden class="anchor" aria-hidden="true" href="#positional-encoding-with-fourier-features">#</a></h4>
<p>These are the same features we have seen in <a href="/notes/gaussian-processes">Gaussian Processes</a> when making them run faster, also similar with positional encodings in <a href="/notes/transformers">Transformers</a>.
This is needed since NN are <em>biased towards learning lower frequency functions</em> (I currently don&rsquo;t know why this is true).</p>
<img src="/images/notes/Advanced 3D Representations-20250525120254638.webp" style="width: 100%" class="center" alt="Advanced 3D Representations-20250525120254638">
Low and high frequency is also the same idea used for JPEG image format compression techniques.
<h4 id="some-forms-of-parameterization">Some forms of parameterization<a hidden class="anchor" aria-hidden="true" href="#some-forms-of-parameterization">#</a></h4>
<p><strong>Quick Small Volume Rendering</strong>
We said that NeRF wastes a lot of computation when doing Alpha Compositing. The idea is to make smaller volumes <strong>close</strong> to the figure that we want to approximate.
This makes that part more efficient, but you need to first describe the volumes (problem is shifted to another domain, usually you don&rsquo;t have the volumes).</p>
<p><strong>Spherical Primitives</strong>
Instead of using pixels, we can use spheres, which are well supported by existing cuda kernels. The problem is that with sphere based reconstruction sometimes it is hard to have high quality for <em>thin</em> structures.
<img src="/images/notes/Advanced 3D Representations-20250525123230614.webp" style="width: 100%" class="center" alt="Advanced 3D Representations-20250525123230614"></p>
<p>But this can be solved with elipsoid primitives, which are more flexible and can represent thin structures better.</p>
<h4 id="strenghts-and-weaknesses">Strenghts and weaknesses<a hidden class="anchor" aria-hidden="true" href="#strenghts-and-weaknesses">#</a></h4>
<ul>
<li>
<p><strong>Strengths</strong>:</p>
<ul>
<li>Can render novel views of a scene from a sparse set of input images.</li>
<li>Can handle complex lighting and reflections.</li>
<li>Can produce high-quality images with realistic details.
<ul>
<li>NeRF is a more flexible representation compared to implicit representation, since it has information about transparency and thin structure.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Weaknesses</strong>:</p>
<ul>
<li>Requires a large amount of <strong>calibrated views</strong> to train and render a scene effectively.</li>
<li>Slow rendering times, especially for high-resolution images.</li>
<li>Not suitable for dynamic scenes (NeRF is designed for <strong>static</strong> scenes).</li>
<li>Can produce artifacts such as floating objects or worse geometry compared to explicit representations like meshes.</li>
<li>Doesn&rsquo;t produce a geometry that can be easily manipulated or edited, it has lots of noise, floaters compared to implicit surfaces. So this method is good if you want nice static images.</li>
</ul>
</li>
</ul>
<h3 id="3d-gaussian-splatting">3D Gaussian splatting<a hidden class="anchor" aria-hidden="true" href="#3d-gaussian-splatting">#</a></h3>
<p>This technique has been proposed in 2023, and has been quite influential now.</p>
<p>It is a method for representing 3D scenes using Gaussian splats, which are 3D Gaussian distributions that can be rendered efficiently. It is particularly useful for rendering novel views of a scene from a sparse set of input images.</p>
<p>One of their main advantages is <strong>fast inference</strong>.</p>
<ul>
<li>No MLP evaluation</li>
<li>No ray marching or point sampling</li>
<li>Almost real time 60fps rendering.</li>
<li>Able to represent thin structures, NeRFs have problems with those.</li>
</ul>
<h4 id="overview-of-the-technique">Overview of the Technique<a hidden class="anchor" aria-hidden="true" href="#overview-of-the-technique">#</a></h4>
<p>3D Gaussian splatting (See <a href="http://arxiv.org/abs/2308.04079">(Kerbl et al. 2023)</a>) works by representing a scene as a collection of 3D Gaussian distributions, each with its own position, color, and opacity. These Gaussians can be rendered efficiently using GPU acceleration, allowing for real-time rendering of complex scenes.</p>
<figure class="center">
<img src="/images/notes/Advanced 3D Representations-20250525123521170.webp" style="width: 100%"   alt="Advanced 3D Representations-20250525123521170" title="Advanced 3D Representations-20250525123521170"/>
<figcaption><p style="text-align:center;"> Images from the paper.</p></figcaption>
</figure>
<p><strong>Adaptive density control</strong> is used to add more detail to areas of the scene that require it, while reducing detail in areas that do not. This allows for efficient rendering without sacrificing quality. Examples are removing (if transparency is too low) or adding some Gaussians</p>
<ul>
<li>Under reconstruction: if you cannot fit the shape well, you add a gaussian, and then use the gradient information to fill the loss function</li>
<li>over reconstruction: remove the gaussian (probably if the gradient is too low, at the end this is the signal.).</li>
</ul>
<p>The gradient is the signal used to decide how to add or remove new Gaussians.</p>
<h4 id="the-model">The Model<a hidden class="anchor" aria-hidden="true" href="#the-model">#</a></h4>
<p>Gaussian Splatting uses 3D gaussian primitives. Each Gaussian has four parts:</p>
<ol>
<li>A center $\mu \in \mathbb{R}^{3}$</li>
<li>A covariance matrix $\Sigma \in \mathbb{R}^{3 \times 3}$, often expressed as rotation and diagonal scaling to maintain positive semi-definitivenes $\Sigma = RSS^{T}R^{T}$.</li>
<li>Color $c \in \mathbb{R}^{3}$, sometimes replaced with <em>spherical harmonics</em> $\in \mathbb{R}^{9}$ especially for view dependent formats.</li>
<li>And an opacity value $o \in\mathbb{R}$.</li>
</ol>
<h4 id="rendering-with-gaussian-splatting">Rendering with Gaussian Splatting<a hidden class="anchor" aria-hidden="true" href="#rendering-with-gaussian-splatting">#</a></h4>
<p>We <strong>only consider Gaussians close to the point</strong> in2D space, sort it in vicinance order with GPU (so O(n) sorts), and then use alpha compositing to render the scene.</p>
$$
\alpha = o \cdot \exp(-0.5 (x - \mu')^{T}\Sigma^{'-1}(x- \mu'))
$$$$
C = \sum_{i} \alpha_{i} c_{i} \prod_{j=0}^{i-1} (1 - \alpha_{j})
$$<p>
The main difference is that here we use Gaussians instead of MLPs, and we can render them in parallel (makes this very fast).</p>
<h3 id="applications-in-human-body-reconstruction">Applications in Human Body Reconstruction<a hidden class="anchor" aria-hidden="true" href="#applications-in-human-body-reconstruction">#</a></h3>
<p>This theme is important for the lab that is giving the course, also take a look at <a href="/notes/parametric-human-body-models">Parametric Human Body Models</a>.</p>
<h4 id="snarf">SNARF<a hidden class="anchor" aria-hidden="true" href="#snarf">#</a></h4>
<p>We have 3D meshes, and we map it to a canonical pose (also called star pose). Then you compute the skinning weights to pose the implicit shape to unseen poses, now you can animate it to new poses, which is a quite cool industry application.</p>
<p>The difficult part is how to map the starting image to the canonical space. The idea here is to predict skinning weights, and use pose parameters to check if it matches the canonical pose or not. This is called <strong>forward warping</strong>.
<img src="/images/notes/Advanced 3D Representations-20250525121552673.webp" style="width: 100%" class="center" alt="Advanced 3D Representations-20250525121552673"></p>
<h4 id="vid2avatar">Vid2Avatar<a hidden class="anchor" aria-hidden="true" href="#vid2avatar">#</a></h4>
<p>From single monocular video we can:</p>
<ul>
<li>Reconstruct geometry</li>
<li>Separate human from background</li>
<li>Reconstruct human&rsquo;s appearance.</li>
</ul>
<figure class="center">
<img src="/images/notes/Advanced 3D Representations-20250525122036048.webp" style="width: 100%"   alt="Advanced 3D Representations-20250525122036048" title="Advanced 3D Representations-20250525122036048"/>
<figcaption><p style="text-align:center;">Image from the paper</p></figcaption>
</figure>
<p>The main point here is that you can use the ideas above to make these beautiful reconstructions.</p>
<h4 id="multi-view-reconstruction-and-tracking">Multi-View Reconstruction and Tracking<a hidden class="anchor" aria-hidden="true" href="#multi-view-reconstruction-and-tracking">#</a></h4>
<p>If you attach some Gaussians to some meshes, you can track how the object moves, this is the basic idea.
They show you can keep tracking the movement also after <strong>drastic</strong> changes!</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=groppImplicitGeometricRegularization2020>[1] Gropp et al. <a href="https://proceedings.mlr.press/v119/gropp20a.html">“Implicit Geometric Regularization for Learning Shapes”</a> PMLR  2020
 </p>
<p id=kerbl3DGaussianSplatting2023>[2] Kerbl et al. <a href="http://arxiv.org/abs/2308.04079">“3D Gaussian Splatting for Real-Time Radiance Field Rendering”</a> arXiv preprint arXiv:2308.04079 2023
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machine-perception/">Machine-Perception</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Advanced 3D Representations on x"
            href="https://x.com/intent/tweet/?text=Advanced%203D%20Representations&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fadvanced-3d-representations%2f&amp;hashtags=machine-perception">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Advanced 3D Representations on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fadvanced-3d-representations%2f&amp;title=Advanced%203D%20Representations&amp;summary=Advanced%203D%20Representations&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fadvanced-3d-representations%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Advanced 3D Representations on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fadvanced-3d-representations%2f&title=Advanced%203D%20Representations">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Advanced 3D Representations on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fadvanced-3d-representations%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Advanced 3D Representations on whatsapp"
            href="https://api.whatsapp.com/send?text=Advanced%203D%20Representations%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fadvanced-3d-representations%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Advanced 3D Representations on telegram"
            href="https://telegram.me/share/url?text=Advanced%203D%20Representations&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fadvanced-3d-representations%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Advanced 3D Representations on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Advanced%203D%20Representations&u=https%3a%2f%2fflecart.github.io%2fnotes%2fadvanced-3d-representations%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
