<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Gaussians | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="➕probabilistic-artificial-intelligence">
<meta name="description" content="Gaussians are one of the most important family of probability distributions. They arise naturally in the law of large numbers and have some nice properties that we will briefly present and prove here in this note. They are also quite common for Gaussian Processes and the Clustering algorithm. They have also something to say about Maximum Entropy Principle. The best thing if you want to learn this part actually well is section 2.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/gaussians/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/gaussians/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Gaussians" />
<meta property="og:description" content="Gaussians are one of the most important family of probability distributions. They arise naturally in the law of large numbers and have some nice properties that we will briefly present and prove here in this note. They are also quite common for Gaussian Processes and the Clustering algorithm. They have also something to say about Maximum Entropy Principle. The best thing if you want to learn this part actually well is section 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/gaussians/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Gaussians"/>
<meta name="twitter:description" content="Gaussians are one of the most important family of probability distributions. They arise naturally in the law of large numbers and have some nice properties that we will briefly present and prove here in this note. They are also quite common for Gaussian Processes and the Clustering algorithm. They have also something to say about Maximum Entropy Principle. The best thing if you want to learn this part actually well is section 2."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Gaussians",
      "item": "https://flecart.github.io/notes/gaussians/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Gaussians",
  "name": "Gaussians",
  "description": "Gaussians are one of the most important family of probability distributions. They arise naturally in the law of large numbers and have some nice properties that we will briefly present and prove here in this note. They are also quite common for Gaussian Processes and the Clustering algorithm. They have also something to say about Maximum Entropy Principle. The best thing if you want to learn this part actually well is section 2.",
  "keywords": [
    "➕probabilistic-artificial-intelligence"
  ],
  "articleBody": "Gaussians are one of the most important family of probability distributions. They arise naturally in the law of large numbers and have some nice properties that we will briefly present and prove here in this note. They are also quite common for Gaussian Processes and the Clustering algorithm. They have also something to say about Maximum Entropy Principle. The best thing if you want to learn this part actually well is section 2.3 of (Bishop 2006), so go there my friend :)\nThe Density Function The single variable Gaussian is as follows: $$ \\mathcal{N}(\\mu, \\sigma) = \\frac{1}{\\sqrt{ 2\\pi } \\sigma} \\exp\\left( -\\frac{(x - \\mu)^{2}}{2\\sigma^{2}} \\right) $$ This can be generalized for the multi variable case $$ \\mathcal{N}(\\mu, \\Sigma) = \\frac{1}{(2\\pi )^{d/2}\\sqrt{ \\lvert \\Sigma \\rvert } } \\exp\\left( -\\frac{1}{2} (x - \\mu)^{T} \\Sigma^{-1} (x - \\mu) \\right) $$ Where $d$ is the dimensionality for the multidimensional Gaussian.\nIntegral is 1 🟥++ We now prove that the integral of the Gaussian PDF is 1, this is a requirements needed to be considered a probability distribution function.\nFirst, let’s prove a famous equality: $$ I = \\int_{-\\infty}^{\\infty} \\exp( - x^{2}) \\, dx = \\sqrt{ \\pi } $$ This is kinda surprising, we need some care to prove it: $$ \\begin{align} \\\\ I^{2} \u0026 = \\int_{-\\infty}^{\\infty} \\exp( - x^{2} - y^{2}) \\, dx dy \\\\ \u0026 = \\int_{0}^{2\\pi} \\int_{0}^{\\infty} r\\exp( - r^{2}) \\, dr d\\theta \\\\ \u0026 = 2\\pi \\cdot \\left( -\\frac{1}{2} \\right) \\int_{0}^{\\infty} -2r\\exp( - r^{2}) \\, dr \\\\ \u0026 = -\\frac{2\\pi}{2} \\exp(-r^{2}) \\bigg\\vert_{0}^{\\infty} = \\pi \\\\ \u0026 \\implies I = \\sqrt{ \\pi } \\end{align} $$ Note that in the second step we changed variables: This is quite interesting. If we do the same derivation with $\\exp\\left( - \\frac{(x - \\mu)^{2}}{2\\sigma^{2}} \\right)$, first doing a change of variables $y = (x - \\mu)$ where we have $dy = dx$, then doing another change of variables $z = \\frac{y}{\\sqrt{ 2 \\sigma^{2} }}$ where we get $\\sqrt{ 2\\sigma^{2} }dz = dy$ now we have the same integral, plus an added constant multiplicative term. So we have\n$$ \\int _{-\\infty}^{\\infty} \\exp\\left( -\\frac{(x - \\mu)^{2}}{2\\sigma^{2}} \\right) \\, dx = \\sqrt{ 2\\sigma^{2} } \\int _{-\\infty}^{\\infty} \\exp(-z^{2})\\, dz = \\sqrt{ 2\\pi \\sigma^{2}} $$ Which finishes the derivation of the normalizing term.\nError Function 🟨– Sometimes, for example calculating the mean of the folded Gaussian, is useful to consider the error function. This is defined as $$ \\text{erf}(x) = \\frac{2}{\\sqrt{ \\pi }} \\int_{0}^{x} \\exp(-t^{2}) \\, dt $$ Sometimes this is also written, using the symmetry over the $x-$axis as $$ \\text{erf}(x) = \\frac{1}{\\sqrt{ \\pi }} \\int_{-x}^{x} \\exp(-t^{2}) \\, dt $$ We observe that the limit $x \\to +\\infty$ is 1, and that $x \\to -\\infty$ is -1.\nAnother useful relation is with the Gaussian CDF: $$ \\text{erf}\\left( \\frac{x}{\\sqrt{ 2 }} \\right) = 2\\Phi(x) - 1 $$ We also note that it is anti-symmetric: $$ \\text{erf}(-x) = -\\text{erf}(x) $$ Some properties of Gaussians The conditional Gaussian If we have $X,Y$ which are jointly Gaussian, then the distribution $p(X = x \\mid Y = y)$ is a gaussian with the following mean and variance:\n$$ \\mu_{X \\mid Y = y} = \\mu_{X} + \\Sigma_{XY} \\Sigma_{YY}^{-1}(y - \\mu_{Y}) $$ And $$ \\Sigma_{X \\mid Y} = \\Sigma_{XX} - \\Sigma_{XY} \\Sigma^{-1}_{YY} \\Sigma_{YX} $$ The proof is presented in section 2.3 of (Bishop 2006)\nProduct of Gaussians are Gaussian 🟨++ This is a little more difficult to detail, see this chatgpt response. It’s just an Unnormalized Gaussian.\nMarginals are Gaussians 🟨 One can prove that any finite marginals of Gaussians are still multivariate Gaussians.\nLet’s now write a closed for for this. Let’s assume we have these two random variables: $$ p(A, B) = \\begin{bmatrix} A \\\\ B \\end{bmatrix} \\sim \\mathcal{N}(\\mu, \\Sigma) $$ Where: $$ \\mu = \\begin{bmatrix} \\mu_{A} \\\\ \\mu_{B} \\end{bmatrix}, \\Sigma = \\begin{bmatrix} \\Sigma_{AA} \u0026 \\Sigma_{AB} \\\\ \\Sigma_{BA} \u0026 \\Sigma_{BB} \\end{bmatrix} $$ We want to find the value of $p(A)$ and of $p(B \\mid A)$. To prove this it is useful to remember the value of the following matrix: $$ \\Sigma^{-1} = V = \\begin{bmatrix} V_{11} \u0026 V_{12} \\\\ V_{21} \u0026 V_{22} \\end{bmatrix} = \\begin{bmatrix} I \u0026 V_{12}V_{22}^{-1} \\\\ 0 \u0026 I \\end{bmatrix} \\cdot \\begin{bmatrix} V_{11} - V_{12}V_{22}^{-1}V_{21} \u0026 0 \\\\ 0 \u0026 V_{22} \\end{bmatrix} \\cdot \\begin{bmatrix} I \u0026 0 \\\\ V_{22}^{-1}V_{21} \u0026 I \\end{bmatrix} $$ Then the inverse $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$ which is equal to: $$ V^{-1} =\n\\begin{bmatrix} I \u0026 0 \\\nV_{22}^{-1}V_{21} \u0026 I \\end{bmatrix} \\cdot \\begin{bmatrix} (V_{11} - V_{12}V_{22}^{-1}V_{21})^{-1} \u0026 0 \\ 0 \u0026 V_{22}^{-1} \\end{bmatrix} \\cdot\\begin{bmatrix} I \u0026 -V_{12}V_{22}^{-1} \\ 0 \u0026 I \\end{bmatrix} = \\begin{bmatrix} (V_{11} - V_{12}V_{22}^{-1}V_{21})^{-1} \u0026 -\\Sigma_{11}V_{12}V_{22}^{-1} \\ -\\Sigma_{11}V_{22}^{-1}V_{22} \u0026 V_{22}^{-1}(V_{12}V_{22}^{-1}V_{21}\\Sigma_{11} + 1) \\end{bmatrix} $$ One can note now with the inverse thing that $V_{22} = (\\Sigma_{22} - \\Sigma_{21}\\Sigma^{-1}_{11}\\Sigma_{12})^{-1}$ This allows to write $V_{12}$ nicely as $$ \\Sigma_{12} = -\\Sigma_{11}V_{12}V_{22}^{-1} \\implies V_{12} = -\\Sigma_{11}\\Sigma_{12}^{-1} V_{22}^{-1} =-\\Sigma_{11}\\Sigma_{12}^{-1} (\\Sigma_{22} - \\Sigma_{21}\\Sigma^{-1}{11}\\Sigma{12} $$ Because then it is easily invertible and one can observe that $\\Sigma_{11} = (V_{11} - V_{12}V_{22}^{-1}V_{21})^{-1}$, this is used for the marginalization calculation. One can find in this manner that $$ A \\sim \\mathcal{N}(\\mu_{A}, \\Sigma_{AA}) $$ And that $$ B \\mid A \\sim \\mathcal{N}(\\mu_{B} - V_{BB}^{-1}V_{AB} (A - \\mu_{A}), V_{BB}) $$ If you are a student ad ETH watch [this](https://video.ethz.ch/lectures/d-infk/2024/autumn/263-5210-00L/0d924ef2-af34-4cb1-96cb-ba9a63c1f15b.html) for the derivation, minute 46. Rewriting with the above properties for $V_{BB}$ and $V_{AB}$ we obtain: $$ B \\mid A \\sim \\mathcal{N}(\\mu_{B} + \\Sigma_{21}\\Sigma_{11}^{-1}(A - \\mu_{A}),\\Sigma_{22} - \\Sigma_{21}\\Sigma^{-1}{11}\\Sigma{12}) $$ Which is a ok form, but very very long to derive. Gaussian characteristic function 🟨++ Characteristic functions are sometimes useful to prove that two distributions are the same as each other. One can prove that the characteristic function for Gaussians is\n$$ \\mathbb{E}[\\exp(itX)] = \\exp\\left( \\mu it - \\frac{1}{2} t^{T} \\Sigma t \\right) $$ Let’s prove the uni-variate case, we will see that it will be exactly this value. We need to compute the value: $$ \\mathbb{E}[\\exp(itX)] = \\int _{-\\infty}^{\\infty} \\frac{1}{\\sqrt{ 2\\pi } \\sigma} \\exp\\left( -\\frac{(x - \\mu)^{2}}{2\\sigma^{2}} + itx \\right) \\, dx $$ The idea is to complete the square, and the by knowing the value of the integral of the completed square, we simplify. $$ \\begin{align} \\\\ \\int _{-\\infty}^{\\infty} \\frac{1}{\\sqrt{ 2\\pi } \\sigma} \\exp\\left( -\\frac{(x - \\mu)^{2}}{2\\sigma^{2}} + itx \\right) \\, dx \\\\ = \\int _{-\\infty}^{\\infty} \\frac{1}{\\sqrt{ 2\\pi } \\sigma} \\exp\\left( -\\frac{(x - (\\mu + \\sigma^{2}it))^{2} - 2\\mu it\\sigma^{2} + \\sigma^{4}t^{2} }{2\\sigma^{2}} \\right) \\, dx \\\\ = \\exp\\left( \\mu it - \\frac{1}{2} \\sigma^{2}t^{2} \\right) \\cdot \\int _{-\\infty}^{\\infty} \\frac{1}{\\sqrt{ 2\\pi } \\sigma} \\exp\\left( -\\frac{(x - (\\mu + \\sigma^{2}it))^{2} }{2\\sigma^{2}} \\right) \\, dx \\\\ = \\exp\\left( \\mu it - \\frac{1}{2} \\sigma^{2}t^{2} \\right) \\end{align} $$ Sum Gaussians are Gaussian 🟩 This is easily provable, if we have $X \\sim \\mathcal{N}(\\mu_{X}, \\Sigma_{X})$ and a compatible distribution $Y \\sim \\mathcal{N}(\\mu_{Y}, \\Sigma_{Y})$ then we have that the distribution $X +Y = \\mathcal{N} (\\mu_{X} + \\mu_{Y}, \\Sigma_{X} + \\Sigma_{Y})$ The proof should use characteristic functions in the line of linear Gaussians.\n$$ \\begin{align} \\mathbb{E}[\\exp(it (X + Y))] = \\\\ \u0026= \\mathbb{E}[\\exp(itX)]\\mathbb{E}[\\exp(itY)] \\\\ \u0026= \\exp\\left( \\mu_{X} it - \\frac{1}{2} t^{T} \\Sigma_{X} t \\right) \\exp\\left( \\mu_{Y} it - \\frac{1}{2} t^{T} \\Sigma_{Y} t \\right) \\\\ \u0026= \\exp\\left( (\\mu_{X} + \\mu_{Y}) it - \\frac{1}{2} t^{T} (\\Sigma_{X} + \\Sigma_{Y}) t \\right) \\end{align} $$ Which finishes the proof. One can also extend this result to every linear combination of Gaussians.\nProperties to remember 🟩 Compact representation of high dimensional joint distributions: instead of using $2^{n}$ variables we just need $n^{2}$, this is why Gaussian Processes are analytically handy. Closed form inference (I think about the Conjugacy of itself, this is because Gaussians are in the The Exponential Family.) Confidence Intervals Gaussians are a nice distribution. We have listed many of its properties by now. But one of the most over-utilized feature is the ease in computing $1 - \\alpha$ confidence intervals where $\\alpha$ is called significance level: meaning we want to find the interval where our prediction lies there with $1 - \\alpha$ probability. This is usually easy to compute with $z$ tables. The Standard Error of a Gaussian is $\\frac{\\sigma}{\\sqrt{ n }}$ and is related to the square root of the mean variance.\nSo after we have computed this values, the confidence interval for a prediction is just $$ \\bar{x} \\pm z \\cdot SE $$ Where $\\bar{x}$ is the expected value for our prediction.\nInformation theoretic properties =#### Entropy of a Gaussian distribution 🟩 We compute here the Entropy of a Univariate Gaussian distribution $\\mathcal{N}(x; \\mu, \\sigma^{2})$. So we need to compute the following value:\n$$ \\begin{align} \\int p(x) \\log \\frac{1}{p(x)} , dx \u0026= -\\int \\frac{1}{\\sqrt{ 2\\pi \\sigma^{2} }} \\exp\\left( -\\frac{(x - \\mu)^{2}}{2\\sigma^{2}} \\right) \\cdot \\left( -\\frac{1}{2} \\log(2\\pi \\sigma^{2}) -\\frac{1}{2\\sigma^{2}}(x - \\mu)^{2} \\right) , dx \\ \u0026= \\frac{1}{2}\\log(2\\pi \\sigma^{2}) +\\frac{1}{2\\sigma^{2}} \\mathbb{E}_{x} [(x - \\mu)^{2}] \\ \u0026= \\frac{1}{2}\\log(2\\pi \\sigma^{2}) + \\frac{1}{2} \\ \u0026= \\frac{1}{2} \\log(2\\pi \\sigma^{2}e) \\end{align}\n$$ With just the above proof one can prove that Gaussians are the distributions with maximum entropy for a given mean and variance. See Maximum Entropy Principle.\nWe can extend this to the multivariate case, observing the following: $$ \\begin{align} \\mathbb{E}_{x \\sim p}[-\\log p(x)] \u0026 = \\mathbb{E}_{x \\sim p}\\left[ \\frac{d}{2} \\log(2\\pi ) + \\log \\det \\Sigma + \\frac{1}{2}(x - \\mu)^{T}\\Sigma^{-1}(x - \\mu) \\right] \\\\ \u0026 = \\frac{d}{2} \\log(2\\pi ) + \\log \\det \\Sigma + \\frac{1}{2} \\mathbb{E}_{x \\sim p}[(x - \\mu)^{T}\\Sigma^{-1}(x - \\mu)] \\\\ \u0026 =\\frac{d}{2}(1 + \\log(2\\pi)) + \\log \\det \\Sigma \\\\ \u0026= \\frac{d}{2} \\log(2\\pi e) + \\log \\det \\Sigma \\\\ \u0026= \\frac{1}{2} \\log((2\\pi e)^{d} \\lvert \\Sigma \\rvert ) \\end{align} $$ Where in the last step we used this equality: $$ \\begin{align} \\mathbb{E}{x \\sim p}[(x - \\mu)^{T}\\Sigma^{-1}(x - \\mu)] \u0026 = \\mathbb{E}{x \\sim p}[\\text{tr}((x - \\mu)^{T}\\Sigma^{-1}(x - \\mu))] \u0026 \\text{ trace of real number}\\\n\u0026 = \\mathbb{E}{x \\sim p}[\\text{tr} (\\Sigma^{-1}(x - \\mu)(x - \\mu)^{T})] \u0026 \\text{ eq. 16 Matrix Cookbook} \\ \u0026 = \\text{tr}(\\mathbb{E}{x \\sim p}[\\Sigma^{-1}(x - \\mu)^{T}(x - \\mu)]) \u0026 \\text{ linearity of trace} \\ \u0026 = \\text{tr}(\\Sigma^{-1} \\mathbb{E}_{x \\sim p}[(x - \\mu)(x - \\mu)^{T}]) \u0026 \\text{ linearity of expectation} \\ \u0026 = \\text{tr}(\\Sigma^{-1} \\Sigma) \u0026 \\text{ definition of covariance} \\ \u0026 = d \u0026 \\text{ trace of identity matrix} \\ \\end{align} $$ The Matrix Cookbook refers to this resource.\nMutual information of Gaussians Suppose we have a Gaussian $X \\sim \\mathcal{N}(\\mu, \\Sigma)$ and $Y = X + \\varepsilon, \\varepsilon\\sim \\mathcal{N}(0, \\sigma^{2}_{n}I)$ TODO\nWe will see that this is equal to: $$ I(X, Y) = \\frac{1}{2} \\log\\lvert I + \\sigma^{-2}_{n}\\Sigma \\rvert $$ General KL divergence between Gaussians The KL divergence between two Gaussians is given by: $$ KL(p \\mid \\mid q) = \\frac{1}{2} \\left( \\log \\frac{\\lvert \\Sigma_{q} \\rvert}{\\lvert \\Sigma_{p} \\rvert} - d + tr(\\Sigma_{q}^{-1}\\Sigma_{p}) + (\\mu_{q} - \\mu_{p})^{T}\\Sigma_{q}^{-1}(\\mu_{q} - \\mu_{p}) \\right) $$ This is a good resource for a proof.\nForward KL One can prove that the forward KL divergence between two Gaussians defined as $p \\sim \\mathcal{N}(\\mu_{1}, diag\\left\\{ \\sigma^{2}_{1}, \\dots, \\sigma^{2}_{d} \\right\\})$ and $q = \\mathcal{N}(0, 1)$ is given by:\n$$ KL(p \\mid \\mid q) = \\frac{1}{2} \\sum_{i = 1}^{d} \\left( \\sigma_{i}^{2} + \\mu_{i}^{2} - \\log \\sigma_{i}^{2} - 1 \\right) $$ Let’s interpret this. The $\\mu$ term works to pull the mean toward zero. The $\\sigma$ term introduces a penalty for high variance values, while the $\\log \\sigma^2$ term imposes a cost for low values of $\\sigma$. This forward KL is what is used for Autoencoders.\nReverse KL Given the same assumptions we have that the KL of $q$ over $p$ is given by: $$ KL(q \\mid \\mid p) = \\frac{1}{2} \\sum_{i = 1}^{d} \\left( \\frac{\\mu_{i}^{2}}{\\sigma_{i}^{2}} + \\sigma_{i}^{-2} + \\log \\sigma_{i}^{2} - 1 \\right) $$ References [1] Bishop “Pattern Recognition and Machine Learning” Springer 2006\n",
  "wordCount" : "1907",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/gaussians/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Gaussians
    </h1>
    <div class="post-meta">9 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#the-density-function" aria-label="The Density Function">The Density Function</a><ul>
                        
                <li>
                    <a href="#integral-is-1-" aria-label="Integral is 1 🟥&#43;&#43;">Integral is 1 🟥++</a></li>
                <li>
                    <a href="#error-function---" aria-label="Error Function 🟨&ndash;">Error Function 🟨&ndash;</a></li></ul>
                </li>
                <li>
                    <a href="#some-properties-of-gaussians" aria-label="Some properties of Gaussians">Some properties of Gaussians</a><ul>
                        
                <li>
                    <a href="#the-conditional-gaussian" aria-label="The conditional Gaussian">The conditional Gaussian</a></li>
                <li>
                    <a href="#product-of-gaussians-are-gaussian-" aria-label="Product of Gaussians are Gaussian 🟨&#43;&#43;">Product of Gaussians are Gaussian 🟨++</a></li>
                <li>
                    <a href="#marginals-are-gaussians-" aria-label="Marginals are Gaussians 🟨">Marginals are Gaussians 🟨</a></li>
                <li>
                    <a href="#gaussian-characteristic-function-" aria-label="Gaussian characteristic function 🟨&#43;&#43;">Gaussian characteristic function 🟨++</a></li>
                <li>
                    <a href="#sum-gaussians-are-gaussian-" aria-label="Sum Gaussians are Gaussian 🟩">Sum Gaussians are Gaussian 🟩</a></li>
                <li>
                    <a href="#properties-to-remember-" aria-label="Properties to remember 🟩">Properties to remember 🟩</a></li>
                <li>
                    <a href="#confidence-intervals" aria-label="Confidence Intervals">Confidence Intervals</a></li></ul>
                </li>
                <li>
                    <a href="#information-theoretic-properties" aria-label="Information theoretic properties">Information theoretic properties</a><ul>
                        
                <li>
                    <a href="#mutual-information-of-gaussians" aria-label="Mutual information of Gaussians">Mutual information of Gaussians</a></li>
                <li>
                    <a href="#general-kl-divergence-between-gaussians" aria-label="General KL divergence between Gaussians">General KL divergence between Gaussians</a></li>
                <li>
                    <a href="#forward-kl" aria-label="Forward KL">Forward KL</a></li>
                <li>
                    <a href="#reverse-kl" aria-label="Reverse KL">Reverse KL</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Gaussians are one of the most important family of probability distributions.
They arise naturally in the <a href="/notes/central-limit-theorem-and-law-of-large-numbers/">law of large numbers</a> and have some nice properties that we will briefly present and prove here in this note. They are also quite common for <a href="/notes/gaussian-processes/">Gaussian Processes</a> and the <a href="/notes/clustering/">Clustering</a> algorithm. They have also something to say about <a href="/notes/maximum-entropy-principle/">Maximum Entropy Principle</a>.
The best thing if you want to learn this part actually well is section 2.3 of (Bishop 2006), so go there my friend :)</p>
<h3 id="the-density-function">The Density Function<a hidden class="anchor" aria-hidden="true" href="#the-density-function">#</a></h3>
<p>The single variable Gaussian is as follows:
</p>
$$
\mathcal{N}(\mu, \sigma) = \frac{1}{\sqrt{ 2\pi } \sigma} \exp\left( -\frac{(x - \mu)^{2}}{2\sigma^{2}} \right)
$$
<p>
This can be generalized for the multi variable case
</p>
$$
\mathcal{N}(\mu, \Sigma) = \frac{1}{(2\pi )^{d/2}\sqrt{ \lvert \Sigma \rvert } } \exp\left( -\frac{1}{2} (x - \mu)^{T} \Sigma^{-1} (x - \mu) \right)
$$
<p>
Where $d$ is the dimensionality for the multidimensional Gaussian.</p>
<h4 id="integral-is-1-">Integral is 1 🟥++<a hidden class="anchor" aria-hidden="true" href="#integral-is-1-">#</a></h4>
<p>We now prove that the integral of the Gaussian PDF is 1, this is a requirements needed to be considered a probability distribution function.</p>
<p>First, let&rsquo;s prove a famous equality:
</p>
$$
I = \int_{-\infty}^{\infty} \exp( - x^{2}) \, dx = \sqrt{ \pi } 
$$
<p>
This is kinda surprising, we need some care to prove it:
</p>
$$
\begin{align}
 \\
I^{2}  & = \int_{-\infty}^{\infty} \exp( - x^{2} - y^{2}) \, dx dy \\
 & = \int_{0}^{2\pi} \int_{0}^{\infty} r\exp( - r^{2}) \, dr d\theta \\
& = 2\pi \cdot \left( -\frac{1}{2} \right) \int_{0}^{\infty} -2r\exp( - r^{2}) \, dr  \\
 & = -\frac{2\pi}{2}  \exp(-r^{2}) \bigg\vert_{0}^{\infty} = \pi \\
 & \implies I = \sqrt{ \pi }
\end{align}
$$
<p>Note that in the second step we changed variables:
This is quite interesting.
If we do the same derivation with $\exp\left( - \frac{(x - \mu)^{2}}{2\sigma^{2}} \right)$, first doing a change of variables $y = (x - \mu)$ where we have  $dy = dx$, then doing another change of variables $z = \frac{y}{\sqrt{ 2 \sigma^{2} }}$ where we get $\sqrt{ 2\sigma^{2} }dz = dy$ now we have the same integral, plus an added constant multiplicative term. So we have</p>
$$
\int _{-\infty}^{\infty} \exp\left( -\frac{(x - \mu)^{2}}{2\sigma^{2}} \right) \, dx = \sqrt{ 2\sigma^{2} }  \int _{-\infty}^{\infty} \exp(-z^{2})\, dz = \sqrt{ 2\pi  \sigma^{2}}
$$
<p>
Which finishes the derivation of the normalizing term.</p>
<h4 id="error-function---">Error Function 🟨&ndash;<a hidden class="anchor" aria-hidden="true" href="#error-function---">#</a></h4>
<p>Sometimes, for example calculating the mean of the folded Gaussian, is useful to consider the error function. This is defined as
</p>
$$
\text{erf}(x) = \frac{2}{\sqrt{ \pi }} \int_{0}^{x} \exp(-t^{2}) \, dt
$$
<p>
Sometimes this is also written, using the symmetry over the $x-$axis as
</p>
$$
\text{erf}(x) = \frac{1}{\sqrt{ \pi }} \int_{-x}^{x} \exp(-t^{2}) \, dt
$$
<p>
We observe that the limit $x \to +\infty$ is 1, and that $x \to -\infty$ is -1.</p>
<p>Another useful relation is with the Gaussian CDF:
</p>
$$
\text{erf}\left( \frac{x}{\sqrt{ 2 }} \right) = 2\Phi(x) - 1
$$
<p>We also note that it is <strong>anti-symmetric</strong>:
</p>
$$
\text{erf}(-x) = -\text{erf}(x)
$$
<h3 id="some-properties-of-gaussians">Some properties of Gaussians<a hidden class="anchor" aria-hidden="true" href="#some-properties-of-gaussians">#</a></h3>
<h4 id="the-conditional-gaussian">The conditional Gaussian<a hidden class="anchor" aria-hidden="true" href="#the-conditional-gaussian">#</a></h4>
<p>If we have $X,Y$ which are jointly Gaussian, then the distribution $p(X = x \mid Y = y)$ is a gaussian with the following mean and variance:</p>
$$
\mu_{X \mid Y = y} = \mu_{X} + \Sigma_{XY} \Sigma_{YY}^{-1}(y - \mu_{Y}) 
$$
<p>
And
</p>
$$
\Sigma_{X \mid Y} = \Sigma_{XX} - \Sigma_{XY} \Sigma^{-1}_{YY} \Sigma_{YX}
$$
<p>The proof is presented in section 2.3 of (Bishop 2006)</p>
<h4 id="product-of-gaussians-are-gaussian-">Product of Gaussians are Gaussian 🟨++<a hidden class="anchor" aria-hidden="true" href="#product-of-gaussians-are-gaussian-">#</a></h4>
<p>This is a little more difficult to detail, see <a href="https://chatgpt.com/share/66f58324-ed18-8009-a72b-30da0c7c0f63">this</a> chatgpt response.
It&rsquo;s just an Unnormalized Gaussian.</p>
<h4 id="marginals-are-gaussians-">Marginals are Gaussians 🟨<a hidden class="anchor" aria-hidden="true" href="#marginals-are-gaussians-">#</a></h4>
<p>One can prove that any finite marginals of Gaussians are still multivariate Gaussians.</p>
<p>Let&rsquo;s now write a closed for for this.
Let&rsquo;s assume we have these two random variables:
</p>
$$
p(A, B) = \begin{bmatrix}
A \\
B
\end{bmatrix} \sim
\mathcal{N}(\mu, \Sigma)
$$
<p>
Where:
</p>
$$
\mu = \begin{bmatrix}
\mu_{A} \\
\mu_{B}
\end{bmatrix}, \Sigma = \begin{bmatrix}
 \Sigma_{AA}  &  \Sigma_{AB} \\
\Sigma_{BA}  &  \Sigma_{BB}
\end{bmatrix}
$$
<p>We want to find the value of $p(A)$ and of $p(B \mid A)$.
To prove this it is useful to remember the value of the following matrix:
</p>
$$
\Sigma^{-1} = V = \begin{bmatrix}
V_{11}  & V_{12} \\
V_{21} & V_{22}
\end{bmatrix} = \begin{bmatrix}
I  &  V_{12}V_{22}^{-1} \\
0  & I
\end{bmatrix} \cdot \begin{bmatrix}
V_{11} - V_{12}V_{22}^{-1}V_{21}  & 0 \\
0  &  V_{22} 
\end{bmatrix} \cdot \begin{bmatrix}
I  & 0 \\
V_{22}^{-1}V_{21}  &  I
\end{bmatrix}
$$
<p>
Then the inverse $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$ which is equal to:
$$
V^{-1} =</p>
<p>\begin{bmatrix}
I  &amp; 0 \</p>
<ul>
<li>V_{22}^{-1}V_{21}  &amp;  I
\end{bmatrix} \cdot
\begin{bmatrix}
(V_{11} - V_{12}V_{22}^{-1}V_{21})^{-1}  &amp; 0 \
0  &amp;  V_{22}^{-1}
\end{bmatrix}  \cdot\begin{bmatrix}
I  &amp;  -V_{12}V_{22}^{-1} \
0  &amp; I
\end{bmatrix} = \begin{bmatrix}
(V_{11} - V_{12}V_{22}^{-1}V_{21})^{-1}   &amp;  -\Sigma_{11}V_{12}V_{22}^{-1}  \
-\Sigma_{11}V_{22}^{-1}V_{22}  &amp; V_{22}^{-1}(V_{12}V_{22}^{-1}V_{21}\Sigma_{11} + 1)
\end{bmatrix}
$$
One can note now with the inverse thing that $V_{22} = (\Sigma_{22} - \Sigma_{21}\Sigma^{-1}_{11}\Sigma_{12})^{-1}$ This allows to write $V_{12}$ nicely as
$$
\Sigma_{12} = -\Sigma_{11}V_{12}V_{22}^{-1} \implies V_{12} = -\Sigma_{11}\Sigma_{12}^{-1} V_{22}^{-1}
=-\Sigma_{11}\Sigma_{12}^{-1} (\Sigma_{22} - \Sigma_{21}\Sigma^{-1}<em>{11}\Sigma</em>{12}
$$
Because then it is easily invertible and one can observe that $\Sigma_{11} = (V_{11} - V_{12}V_{22}^{-1}V_{21})^{-1}$, this is used for the marginalization calculation.
One can find in this manner that 
$$
A \sim \mathcal{N}(\mu_{A}, \Sigma_{AA})
$$
And that
$$
B \mid A \sim \mathcal{N}(\mu_{B} - V_{BB}^{-1}V_{AB} (A - \mu_{A}), V_{BB})
$$
If you are a student ad ETH watch [this](https://video.ethz.ch/lectures/d-infk/2024/autumn/263-5210-00L/0d924ef2-af34-4cb1-96cb-ba9a63c1f15b.html) for the derivation, minute 46.
Rewriting with the above properties for $V_{BB}$ and $V_{AB}$ we obtain:
$$
B \mid A \sim \mathcal{N}(\mu_{B} + \Sigma_{21}\Sigma_{11}^{-1}(A - \mu_{A}),\Sigma_{22} - \Sigma_{21}\Sigma^{-1}<em>{11}\Sigma</em>{12})
$$
Which is a ok form, but very very long to derive.</li>
</ul>
<h4 id="gaussian-characteristic-function-">Gaussian characteristic function 🟨++<a hidden class="anchor" aria-hidden="true" href="#gaussian-characteristic-function-">#</a></h4>
<p>Characteristic functions are sometimes useful to prove that two distributions are the same as each other.
One can prove that the characteristic function for Gaussians is</p>
$$
\mathbb{E}[\exp(itX)] = \exp\left( \mu it - \frac{1}{2} t^{T} \Sigma t \right)
$$
<p>Let&rsquo;s prove the uni-variate case, we will see that it will be exactly this value.
We need to compute the value:
</p>
$$
\mathbb{E}[\exp(itX)] = \int _{-\infty}^{\infty} \frac{1}{\sqrt{ 2\pi } \sigma} \exp\left( -\frac{(x - \mu)^{2}}{2\sigma^{2}} + itx \right) \, dx 
$$
<p>
The idea is to complete the square, and the by knowing the value of the integral of the completed square, we simplify.
</p>
$$
\begin{align}
 \\
 \int _{-\infty}^{\infty} \frac{1}{\sqrt{ 2\pi } \sigma} \exp\left( -\frac{(x - \mu)^{2}}{2\sigma^{2}} + itx \right) \, dx  \\
=  \int _{-\infty}^{\infty} \frac{1}{\sqrt{ 2\pi } \sigma} \exp\left( -\frac{(x - (\mu + \sigma^{2}it))^{2} - 2\mu it\sigma^{2} + \sigma^{4}t^{2} }{2\sigma^{2}}  \right) \, dx  \\
= \exp\left( \mu it - \frac{1}{2} \sigma^{2}t^{2} \right) \cdot \int _{-\infty}^{\infty} \frac{1}{\sqrt{ 2\pi } \sigma} \exp\left( -\frac{(x - (\mu + \sigma^{2}it))^{2} }{2\sigma^{2}}  \right) \, dx  \\
= \exp\left( \mu it - \frac{1}{2} \sigma^{2}t^{2} \right)
\end{align}
$$
<h4 id="sum-gaussians-are-gaussian-">Sum Gaussians are Gaussian 🟩<a hidden class="anchor" aria-hidden="true" href="#sum-gaussians-are-gaussian-">#</a></h4>
<p>This is easily provable, if we have $X \sim \mathcal{N}(\mu_{X}, \Sigma_{X})$ and a compatible distribution $Y  \sim \mathcal{N}(\mu_{Y}, \Sigma_{Y})$ then we have that the distribution $X +Y = \mathcal{N} (\mu_{X} + \mu_{Y}, \Sigma_{X} + \Sigma_{Y})$
The proof should use characteristic functions in the line of linear Gaussians.</p>
$$
\begin{align}
\mathbb{E}[\exp(it (X + Y))]  = \\
&= \mathbb{E}[\exp(itX)]\mathbb{E}[\exp(itY)]  \\
&= \exp\left( \mu_{X} it - \frac{1}{2} t^{T} \Sigma_{X} t \right) \exp\left( \mu_{Y} it - \frac{1}{2} t^{T} \Sigma_{Y} t \right)  \\
&= \exp\left( (\mu_{X} + \mu_{Y}) it - \frac{1}{2} t^{T} (\Sigma_{X} + \Sigma_{Y}) t \right)
\end{align}
$$
<p>
Which finishes the proof.
One can also extend this result to every linear combination of Gaussians.</p>
<h4 id="properties-to-remember-">Properties to remember 🟩<a hidden class="anchor" aria-hidden="true" href="#properties-to-remember-">#</a></h4>
<ul>
<li><strong>Compact representation</strong> of high dimensional joint distributions: instead of using $2^{n}$ variables we just need $n^{2}$, this is why <a href="/notes/gaussian-processes/">Gaussian Processes</a> are analytically handy.</li>
<li>Closed form inference (I think about the Conjugacy of itself, this is because Gaussians are in the <a href="/notes/the-exponential-family/">The Exponential Family</a>.)</li>
</ul>
<h4 id="confidence-intervals">Confidence Intervals<a hidden class="anchor" aria-hidden="true" href="#confidence-intervals">#</a></h4>
<p>Gaussians are a nice distribution. We have listed many of its properties by now. But one of the most over-utilized feature is the ease in computing $1 - \alpha$ confidence intervals where $\alpha$ is called <em>significance level</em>: meaning we want to find the interval where our prediction lies there with $1 - \alpha$ probability. This is usually easy to compute with $z$ tables.
The Standard Error of a Gaussian is $\frac{\sigma}{\sqrt{ n }}$ and is related to the square root of the mean variance.</p>
<p>So after we have computed this values, the confidence interval for a prediction is just
</p>
$$
\bar{x} \pm z \cdot SE
$$
<p>
Where $\bar{x}$ is the expected value for our prediction.</p>
<h3 id="information-theoretic-properties">Information theoretic properties<a hidden class="anchor" aria-hidden="true" href="#information-theoretic-properties">#</a></h3>
<p>=#### Entropy of a Gaussian distribution 🟩
We compute here the <a href="/notes/entropy/">Entropy</a> of a Univariate Gaussian distribution $\mathcal{N}(x; \mu, \sigma^{2})$. So we need to compute the following value:</p>
<p>$$
\begin{align}
\int p(x) \log \frac{1}{p(x)} , dx  &amp;= -\int \frac{1}{\sqrt{ 2\pi \sigma^{2} }} \exp\left( -\frac{(x - \mu)^{2}}{2\sigma^{2}} \right) \cdot \left( -\frac{1}{2} \log(2\pi \sigma^{2}) -\frac{1}{2\sigma^{2}}(x - \mu)^{2} \right) , dx  \
&amp;= \frac{1}{2}\log(2\pi \sigma^{2}) +\frac{1}{2\sigma^{2}}  \mathbb{E}_{x} [(x - \mu)^{2}] \
&amp;= \frac{1}{2}\log(2\pi \sigma^{2}) + \frac{1}{2} \
&amp;= \frac{1}{2} \log(2\pi \sigma^{2}e)
\end{align}</p>
<p>$$
With just the above proof one can prove that Gaussians are the distributions with maximum entropy for a given mean and variance. See <a href="/notes/maximum-entropy-principle/">Maximum Entropy Principle</a>.</p>
<p>We can extend this to the multivariate case, observing the following:
</p>
$$
\begin{align}
\mathbb{E}_{x \sim p}[-\log p(x)]  & = \mathbb{E}_{x \sim p}\left[ \frac{d}{2} \log(2\pi ) + \log \det \Sigma + \frac{1}{2}(x - \mu)^{T}\Sigma^{-1}(x - \mu) \right] \\ 
 &  = \frac{d}{2} \log(2\pi ) + \log \det \Sigma + \frac{1}{2} \mathbb{E}_{x \sim p}[(x - \mu)^{T}\Sigma^{-1}(x - \mu)] \\   
 & =\frac{d}{2}(1 + \log(2\pi)) + \log \det \Sigma  \\
&= \frac{d}{2} \log(2\pi e) + \log \det \Sigma \\
&= \frac{1}{2} \log((2\pi e)^{d} \lvert \Sigma \rvert )
\end{align}
$$
<p>
Where in the last step we used this equality:
$$
\begin{align}
\mathbb{E}<em>{x \sim p}[(x - \mu)^{T}\Sigma^{-1}(x - \mu)]  &amp; = \mathbb{E}</em>{x \sim p}[\text{tr}((x - \mu)^{T}\Sigma^{-1}(x - \mu))] &amp; \text{ trace of real number}\</p>
<p>&amp; = \mathbb{E}<em>{x \sim p}[\text{tr} (\Sigma^{-1}(x - \mu)(x - \mu)^{T})] &amp; \text{ eq. 16 Matrix Cookbook} \
&amp; = \text{tr}(\mathbb{E}</em>{x \sim p}[\Sigma^{-1}(x - \mu)^{T}(x - \mu)]) &amp; \text{ linearity of trace} \ <br>
&amp; =  \text{tr}(\Sigma^{-1} \mathbb{E}_{x \sim p}[(x - \mu)(x - \mu)^{T}]) &amp; \text{ linearity of expectation} \
&amp; = \text{tr}(\Sigma^{-1} \Sigma) &amp; \text{ definition of covariance}  \
&amp; = d &amp; \text{ trace of identity matrix} \
\end{align}
$$
The Matrix Cookbook refers to <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">this</a> resource.</p>
<h4 id="mutual-information-of-gaussians">Mutual information of Gaussians<a hidden class="anchor" aria-hidden="true" href="#mutual-information-of-gaussians">#</a></h4>
<p>Suppose we have a Gaussian $X \sim \mathcal{N}(\mu, \Sigma)$ and $Y = X + \varepsilon, \varepsilon\sim \mathcal{N}(0, \sigma^{2}_{n}I)$
TODO</p>
<p>We will see that this is equal to:
</p>
$$
I(X, Y) = \frac{1}{2} \log\lvert I + \sigma^{-2}_{n}\Sigma \rvert 
$$
<h4 id="general-kl-divergence-between-gaussians">General KL divergence between Gaussians<a hidden class="anchor" aria-hidden="true" href="#general-kl-divergence-between-gaussians">#</a></h4>
<p>The KL divergence between two Gaussians is given by:
</p>
$$
KL(p \mid \mid q) = \frac{1}{2} \left( \log \frac{\lvert \Sigma_{q} \rvert}{\lvert \Sigma_{p} \rvert} - d + tr(\Sigma_{q}^{-1}\Sigma_{p}) + (\mu_{q} - \mu_{p})^{T}\Sigma_{q}^{-1}(\mu_{q} - \mu_{p}) \right)
$$
<p><a href="https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/">This</a> is a good resource for a proof.</p>
<h4 id="forward-kl">Forward KL<a hidden class="anchor" aria-hidden="true" href="#forward-kl">#</a></h4>
<p>One can prove that the forward KL divergence between two Gaussians defined as $p \sim \mathcal{N}(\mu_{1}, diag\left\{ \sigma^{2}_{1}, \dots, \sigma^{2}_{d} \right\})$ and $q = \mathcal{N}(0, 1)$ is given by:</p>
$$
KL(p \mid \mid q) = \frac{1}{2} \sum_{i = 1}^{d} \left( \sigma_{i}^{2} + \mu_{i}^{2} - \log \sigma_{i}^{2} - 1 \right)
$$
<p>
Let’s interpret this. The $\mu$ term works to pull the mean toward zero. The $\sigma$ term introduces a penalty for high variance values, while the $\log \sigma^2$ term imposes a cost for low values of $\sigma$.
This forward KL is what is used for <a href="/notes/autoencoders/">Autoencoders</a>.</p>
<h4 id="reverse-kl">Reverse KL<a hidden class="anchor" aria-hidden="true" href="#reverse-kl">#</a></h4>
<p>Given the same assumptions we have that the KL of $q$ over $p$ is given by:
</p>
$$
KL(q \mid \mid p) = \frac{1}{2} \sum_{i = 1}^{d} \left( \frac{\mu_{i}^{2}}{\sigma_{i}^{2}} + \sigma_{i}^{-2} + \log \sigma_{i}^{2} - 1 \right)
$$
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Bishop “Pattern Recognition and Machine Learning” Springer 2006</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">➕Probabilistic-Artificial-Intelligence</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussians on x"
            href="https://x.com/intent/tweet/?text=Gaussians&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussians%2f&amp;hashtags=%e2%9e%95probabilistic-artificial-intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussians on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussians%2f&amp;title=Gaussians&amp;summary=Gaussians&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussians%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussians on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussians%2f&title=Gaussians">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussians on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussians%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussians on whatsapp"
            href="https://api.whatsapp.com/send?text=Gaussians%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fgaussians%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussians on telegram"
            href="https://telegram.me/share/url?text=Gaussians&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussians%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Gaussians on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Gaussians&u=https%3a%2f%2fflecart.github.io%2fnotes%2fgaussians%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
