<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Principal Component Analysis | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning">
<meta name="description" content="Principal Component Analysis is a technique used to reduce the dimensionality of a dataset. So we can view this as an algorithm that tries to compress the data while retaining some of the most important information. It is one of the earliest and most simple techniques in machine learning. Single Layer Autoencoders learn to approximate this kind of transformation.
The main idea is to find the directions with the most variance in a dataset.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/principal-component-analysis/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/principal-component-analysis/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Principal Component Analysis" />
<meta property="og:description" content="Principal Component Analysis is a technique used to reduce the dimensionality of a dataset. So we can view this as an algorithm that tries to compress the data while retaining some of the most important information. It is one of the earliest and most simple techniques in machine learning. Single Layer Autoencoders learn to approximate this kind of transformation.
The main idea is to find the directions with the most variance in a dataset." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/principal-component-analysis/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Principal Component Analysis"/>
<meta name="twitter:description" content="Principal Component Analysis is a technique used to reduce the dimensionality of a dataset. So we can view this as an algorithm that tries to compress the data while retaining some of the most important information. It is one of the earliest and most simple techniques in machine learning. Single Layer Autoencoders learn to approximate this kind of transformation.
The main idea is to find the directions with the most variance in a dataset."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Principal Component Analysis",
      "item": "https://flecart.github.io/notes/principal-component-analysis/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Principal Component Analysis",
  "name": "Principal Component Analysis",
  "description": "Principal Component Analysis is a technique used to reduce the dimensionality of a dataset. So we can view this as an algorithm that tries to compress the data while retaining some of the most important information. It is one of the earliest and most simple techniques in machine learning. Single Layer Autoencoders learn to approximate this kind of transformation.\nThe main idea is to find the directions with the most variance in a dataset.",
  "keywords": [
    "machinelearning"
  ],
  "articleBody": "Principal Component Analysis is a technique used to reduce the dimensionality of a dataset. So we can view this as an algorithm that tries to compress the data while retaining some of the most important information. It is one of the earliest and most simple techniques in machine learning. Single Layer Autoencoders learn to approximate this kind of transformation.\nThe main idea is to find the directions with the most variance in a dataset. Those will be called principal components.\nSome resources and trivia: There is a very easy derivation present in (Bishop \u0026 Bishop 2024). It is also known as the Kosambi-Karhunen-Loève transform (you will probably like this name more if you are from physics). One good resource is See https://peterbloem.nl/blog/pca series. Another good resource is the Wilkinson.\nTechnique setting Le’t say we have $\\left\\{ \\boldsymbol{x}_{n} \\right\\}$ observations of dimension $D$ and $n \\in \\left\\{ 1, \\dots, N \\right\\}$. We want to project this onto a dimension $M$, with the constraint of maximizing the variance of the projected data. Let’s say $u$ is our basis matrix.\nMaximum variance approach If $M=1$ then we have a single direction, and it’s easy to see that the projected points are $u^{T}\\boldsymbol{x}_{n}$. Then we can easily calculate the statistics. Mean: $$ \\bar{x} = \\frac{1}{N} \\sum_{n=1}^{N} x_{n} \\implies \\text{ mean is }u^{T}\\bar{x} $$ The variance: $$ \\frac{1}{N} \\sum_{n = 1}^{N} \\left( u^{T}x_{n} - u^{T}\\bar{x} \\right) ^{2} = u^{T}Su $$ With $S$ the covariance matrix defined as $S_{ij} = \\text{ Cov}(x_{i}, x_{j}) = \\frac{1}{N} \\sum_{n = 1}^{N} (x_{n} - \\bar{x}) (x_{n} - \\bar{x})^{T}$. And then we can try to maximize $u^{T}Su$ with respect to $u$. With the condition that $u^{T}u = 1$ so that it is a normalized direction, we can use the Lagrange Multipliers technique explained in Duality Theory#Lagrangian function. So our optimization problem is $$ \\text{ minimize } f(u, \\lambda) = u^{T}Su - \\lambda(1 - u^{T}u) $$ And we use the multi-variable optimization tools, derived in Massimi minimi multi-variabile to see that $$ \\frac{ \\partial f(u, \\lambda) }{ \\partial u } = 2Su - 2\\lambda u $$ Which equated to zero for a stationary point we have $Su = \\lambda u$ which is then just try to find the eigenvalues of $S$, explained here Determinanti. Now if we use some maths and the property that $u^{T}u = 1$ we observe that the variance is exactly the Lagrange multiplier: $u^{T}Su = \\lambda$. So the eigenvectors of $S$ describe the variance, and this variance will be the largest when we chose the biggest eigenvector. This is also why we call it first principal component.\nThe above is also known as the Rayleigh quotient optimization problem, where the covariance matrix is a special case.\nDerivation of the covariance property Before going into the details here take this easier way into consideration. If you are not familiar with matrix calculation and are unsure about why this is true check the following argument:\nWe know that: $$ \\left( u^{T}x_{n} - u^{T}\\bar{x} \\right) ^{2} = (u_{1}x_{1} + \\dots + u_{M}x_{M} - u_{1}\\bar{x}{1} - \\dots - u{M}\\bar{x}_{M})^{2} $$ $$ = \\sum_{i = 1}^{M} (u^{2}{i}x^{2}{i} + u_{i}^{2}\\bar{x}^{2}_{i})\n2\\sum_{i\\neq j}^{M} u_{i}u_{j}x_{i}x_{j} 2\\sum_{i,j=1, 1}^{M} u_{i}u_{j}x_{i}\\bar{x}_{j} 2\\sum_{i \\neq j}^{M} u_{i}u_{j}\\bar{x}{i}\\bar{x}{j} $$ Note: first i used $n$ to indicate the nth vector, then i used $i$ to index the $n$-th vector, take care of this thing. Now let’s compare the last result with the covariance matrix. We have in the covariance matrix what $$ S_{ij} = (x_{n} - \\bar{x})_{i} (x_{n} - \\bar{x})_{j} = x_{i}x_{j} - x_{i}\\bar{x}_{j} - \\bar{x}_{i}x_{j} + \\bar{x}_{i}\\bar{x}_{j} $$ And summing $S_{ij} + S_{ji}$ we have $2(x_{i}x_{j} + \\bar{x}_{i}\\bar{x}_{j} - x_{i}\\bar{x}_{j} - \\bar{x}_{i}x_{j})$ Which takes care of all the sums where $i \\neq j$ when we calculate the $u_{i}S_{ij}u_{j} + u_{j}S_{ji}u_{i}$ When $i = j$ we observe that $S_{ii} = x_{i}^{2} + \\bar{x}_{i}^{2} - 2x_{i}\\bar{x}_{i}$. This proves the relation, just a lot of sums.\nCheck Multi Variable Derivatives for derivatives of vectors and matrices if you are unconfortable.\nGeneralization of the Reduction Now that we know the general idea in the special case where $M = 1$ we want to generalize this approach. Let’s say $u$ is the eigenvector we have found at the first step and consider the new dataset: $$ X_{1} = \\left\\{ x - (u^{T}x)\\cdot u : x \\in \\mathcal{X} \\right\\} $$ We see that the mean of the dataset is $\\bar{x} - (u^{T}\\bar{x})\\cdot u$ while the new variance is $$ \\Sigma_{1} = \\Sigma - u\\Sigma u^{T} $$ This is easily shown by applying the variance formula. Then you observe that the new matrix has exactly the same eigenvectors as the previous, minus $u$, which means that if you continue to apply the variance procedure then you will get the second highest eigenvalue, and so on.\nApproach by Induction This section aims to provide a proof of induction of the maximum variance approach. But this has not been checked and so you should read this quite carefully.\nWe have the base case of our induction done, we assume inductively that the $\\lambda_{1}, \\dots \\lambda_{N}$ the ordered eigenvectors of the covariance matrix are the best solution to our optimization problem when we still take the next eigenvector. By induction we know that those eigenvectors are orthonormal and that the projection matrix defined by them, that we call $w$ is the one that maximizes the variance, now defined as $u^{T}Su$. with $w : \\mathbb{R}^{D} \\to \\mathbb{R}^{M}$. Let’s add a column to this projection matrix. We have other conditions caused by the fact that we want orthonormal vectors. Let’s call $v_{1}, v_{2}, \\dots, v_{N}$ the eigenvectors associated to the found eigenvalues, then the Lagrange Multipliers technique tells us to $$ \\text{ maximize } u^{T}Su + \\lambda(1 - u^{T}u) + \\sum_{k = 1}^{N} \\mu_{k} (0 - u^{T}v_{k}) $$ We derive with respect to $w$ and we have $$ 2Su - 2\\lambda u - \\sum_{k= 1}^{N}\\mu_{k}v_{k} \\implies \\lambda = $$ Now to continue this multiplication, the trick is to multiply this by $v_{l}$ and then we have $$ 2v_{l}Su - 2\\lambda v_{l}u - \\sum_{k=1}^{N}\\mu_{k}v_{l}v_{k} = 0 - 0 - \\mu_{l}v_{l}^{2} $$ which implies that $\\mu_{l}=0$ we can repeat this for all $\\mu$ and show that all of those coefficients should be 0, this falls back to the original proof, and in this way we know that it is the next possible eigenvector, because if not it would not be orthonormal.\nReferences [1] Bishop \u0026 Bishop “Deep Learning: Foundations and Concepts” Springer International Publishing 2024\n",
  "wordCount" : "1069",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/principal-component-analysis/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Principal Component Analysis
    </h1>
    <div class="post-meta">6 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#technique-setting" aria-label="Technique setting">Technique setting</a></li>
                <li>
                    <a href="#maximum-variance-approach" aria-label="Maximum variance approach">Maximum variance approach</a><ul>
                        
                <li>
                    <a href="#derivation-of-the-covariance-property" aria-label="Derivation of the covariance property">Derivation of the covariance property</a></li>
                <li>
                    <a href="#generalization-of-the-reduction" aria-label="Generalization of the Reduction">Generalization of the Reduction</a></li>
                <li>
                    <a href="#approach-by-induction" aria-label="Approach by Induction">Approach by Induction</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Principal Component Analysis is a technique used to reduce the dimensionality of a dataset. So we can view this as an algorithm that tries to compress the data while retaining some of the most important information.
It is one of the earliest and most simple techniques in machine learning. Single Layer <a href="/notes/autoencoders/">Autoencoders</a> learn to approximate this kind of transformation.</p>
<p>The main idea is to find the directions with the most variance in a dataset. Those will be called <strong>principal components.</strong></p>
<p>Some resources and trivia:
There is a very easy derivation present in <a href="https://link.springer.com/10.1007/978-3-031-45468-4">(Bishop &amp; Bishop 2024)</a>.
It is also known as the <em>Kosambi-Karhunen-Loève</em> transform (you will probably like this name more if you are from physics).
One good resource is See <a href="https://peterbloem.nl/blog/pca">https://peterbloem.nl/blog/pca</a> series. Another good resource is the <a href="https://rich-d-wilkinson.github.io/MATH3030/4.2-pca-a-formal-description-with-proofs.html">Wilkinson</a>.</p>
<h3 id="technique-setting">Technique setting<a hidden class="anchor" aria-hidden="true" href="#technique-setting">#</a></h3>
<p>Le&rsquo;t say we have $\left\{ \boldsymbol{x}_{n} \right\}$ observations of dimension $D$ and $n \in \left\{ 1, \dots, N \right\}$. We want to project this onto a dimension $M$, with the constraint of maximizing the variance of the projected data.
Let&rsquo;s say $u$ is our basis matrix.</p>
<h3 id="maximum-variance-approach">Maximum variance approach<a hidden class="anchor" aria-hidden="true" href="#maximum-variance-approach">#</a></h3>
<p>If $M=1$ then we have a single direction, and it&rsquo;s easy to see that the projected points are $u^{T}\boldsymbol{x}_{n}$. Then we can easily calculate the statistics.
Mean:
</p>
$$
\bar{x} = \frac{1}{N} \sum_{n=1}^{N} x_{n} \implies \text{ mean is }u^{T}\bar{x}
$$
<p>
The variance:
</p>
$$
\frac{1}{N} \sum_{n = 1}^{N} \left( u^{T}x_{n} - u^{T}\bar{x} \right) ^{2} = u^{T}Su
$$
<p>
With $S$ the covariance matrix defined as $S_{ij} =  \text{ Cov}(x_{i}, x_{j}) = \frac{1}{N} \sum_{n = 1}^{N} (x_{n} - \bar{x}) (x_{n} - \bar{x})^{T}$.
And then we can try to maximize $u^{T}Su$ with respect to $u$.
With the condition that $u^{T}u = 1$ so that it is a normalized direction, we can use the Lagrange Multipliers technique explained in <a href="/notes/duality-theory/#lagrangian-function">Duality Theory#Lagrangian function</a>.
So our optimization problem is
</p>
$$
\text{ minimize } f(u, \lambda) = u^{T}Su - \lambda(1 - u^{T}u)
$$
<p>
And we use the multi-variable optimization tools, derived in <a href="/notes/massimi-minimi-multi-variabile/">Massimi minimi multi-variabile</a> to see that
</p>
$$
\frac{ \partial f(u, \lambda) }{ \partial u }  = 2Su - 2\lambda u
$$
<p>
Which equated to zero for a stationary point we have $Su = \lambda u$ which is then just try to find the eigenvalues of $S$, explained here <a href="/notes/determinanti/">Determinanti</a>.
Now if we use some maths and the property that $u^{T}u = 1$ we observe that the variance is exactly the Lagrange multiplier: $u^{T}Su = \lambda$. So the eigenvectors of $S$ describe the variance, and this variance will be the largest when we chose the biggest eigenvector. This is also why we call it <strong>first principal component</strong>.</p>
<p>The above is also known as the <a href="https://en.wikipedia.org/wiki/Rayleigh_quotient">Rayleigh quotient</a> optimization problem, where the covariance matrix is a special case.</p>
<h4 id="derivation-of-the-covariance-property">Derivation of the covariance property<a hidden class="anchor" aria-hidden="true" href="#derivation-of-the-covariance-property">#</a></h4>
<p>Before going into the details here take <a href="https://chatgpt.com/share/66fea0e8-4694-8009-a892-33a2d6785107">this</a> easier way into consideration.
If you are not familiar with matrix calculation and are unsure about why this is true check the following argument:</p>
<h1 id="-u_1x_1--dots--u_mx_m---u_1barx_1---dots---u_mbarx_m2">We know that:
$$
\left( u^{T}x_{n} - u^{T}\bar{x} \right) ^{2}
= (u_{1}x_{1} + \dots + u_{M}x_{M} - u_{1}\bar{x}<em>{1} - \dots - u</em>{M}\bar{x}_{M})^{2}</h1>
$$
$$
<p>
= \sum_{i = 1}^{M} (u^{2}<em>{i}x^{2}</em>{i} + u_{i}^{2}\bar{x}^{2}_{i})</p>
<ul>
<li>2\sum_{i\neq j}^{M} u_{i}u_{j}x_{i}x_{j}</li>
</ul>
<ul>
<li>2\sum_{i,j=1, 1}^{M} u_{i}u_{j}x_{i}\bar{x}_{j}</li>
</ul>
<ul>
<li>2\sum_{i \neq j}^{M} u_{i}u_{j}\bar{x}<em>{i}\bar{x}</em>{j}
$$
Note: first i used $n$ to indicate the nth vector, then i used $i$ to index the $n$-th vector, take care of this thing.</li>
</ul>
<p>Now let&rsquo;s compare the last result with the covariance matrix. We have in the covariance matrix what
</p>
$$
S_{ij} =  (x_{n} - \bar{x})_{i} (x_{n} - \bar{x})_{j} 
= x_{i}x_{j} - x_{i}\bar{x}_{j} - \bar{x}_{i}x_{j} + \bar{x}_{i}\bar{x}_{j}
$$
<p>
And summing $S_{ij} + S_{ji}$ we have $2(x_{i}x_{j} + \bar{x}_{i}\bar{x}_{j} - x_{i}\bar{x}_{j} - \bar{x}_{i}x_{j})$
Which takes care of all the sums  where $i \neq j$ when we calculate the $u_{i}S_{ij}u_{j} + u_{j}S_{ji}u_{i}$
When $i = j$ we observe that $S_{ii} = x_{i}^{2} + \bar{x}_{i}^{2} - 2x_{i}\bar{x}_{i}$.
This proves the relation, just a lot of sums.</p>
<p>Check <a href="/notes/multi-variable-derivatives/">Multi Variable Derivatives</a> for derivatives of vectors and matrices if you are unconfortable.</p>
<h4 id="generalization-of-the-reduction">Generalization of the Reduction<a hidden class="anchor" aria-hidden="true" href="#generalization-of-the-reduction">#</a></h4>
<p>Now that we know the general idea in the special case where $M = 1$ we want to generalize this approach.
Let&rsquo;s say $u$ is the eigenvector we have found at the first step and consider the new dataset:
</p>
$$
X_{1} = \left\{ x - (u^{T}x)\cdot u : x \in \mathcal{X} \right\} 
$$
<p>
We see that the mean of the dataset is $\bar{x} - (u^{T}\bar{x})\cdot u$ while the new variance is
</p>
$$
\Sigma_{1} = \Sigma - u\Sigma u^{T}
$$
<p>
This is easily shown by applying the variance formula. Then you observe that the new matrix has exactly the same eigenvectors as the previous, minus $u$, which means that if you continue to apply the variance procedure then you will get the second highest eigenvalue, and so on.</p>
<h4 id="approach-by-induction">Approach by Induction<a hidden class="anchor" aria-hidden="true" href="#approach-by-induction">#</a></h4>
<p>This section aims to provide a proof of induction of the maximum variance approach. But this has not been checked and so you should read this quite carefully.</p>
<p>We have the base case of our induction done, we assume inductively that the $\lambda_{1}, \dots \lambda_{N}$ the ordered eigenvectors of the covariance matrix are the best solution to our optimization problem when we still take the next eigenvector. By induction we know that those eigenvectors are orthonormal and that the projection matrix defined by them, that we call $w$ is the one that maximizes the variance, now defined as $u^{T}Su$. with $w : \mathbb{R}^{D} \to \mathbb{R}^{M}$. Let&rsquo;s add a column to this projection matrix.
We have other conditions caused by the fact that we want orthonormal vectors. Let&rsquo;s call $v_{1}, v_{2}, \dots, v_{N}$ the eigenvectors associated to the found eigenvalues, then the <a href="/notes/lagrange-multipliers/">Lagrange Multipliers</a> technique tells us to
</p>
$$
\text{ maximize } u^{T}Su + \lambda(1 - u^{T}u) + \sum_{k = 1}^{N} \mu_{k} (0 - u^{T}v_{k})
$$
<p>
We derive with respect to $w$ and we have
</p>
$$
2Su - 2\lambda u - \sum_{k= 1}^{N}\mu_{k}v_{k} \implies \lambda = 
$$
<p>
Now to continue this multiplication, the trick is to multiply this by $v_{l}$ and then we have
</p>
$$
2v_{l}Su - 2\lambda v_{l}u - \sum_{k=1}^{N}\mu_{k}v_{l}v_{k} = 0 - 0 - \mu_{l}v_{l}^{2}
$$
<p>
which implies that $\mu_{l}=0$ we can repeat this for all $\mu$ and show that all of those coefficients should be 0, this falls back to the original proof, and in this way we know that it is the next possible eigenvector, because if not it would not be orthonormal.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Bishop &amp; Bishop <a href="https://link.springer.com/10.1007/978-3-031-45468-4">“Deep Learning: Foundations and Concepts”</a> Springer International Publishing 2024</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on x"
            href="https://x.com/intent/tweet/?text=Principal%20Component%20Analysis&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fprincipal-component-analysis%2f&amp;hashtags=machinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fprincipal-component-analysis%2f&amp;title=Principal%20Component%20Analysis&amp;summary=Principal%20Component%20Analysis&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fprincipal-component-analysis%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fprincipal-component-analysis%2f&title=Principal%20Component%20Analysis">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fprincipal-component-analysis%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on whatsapp"
            href="https://api.whatsapp.com/send?text=Principal%20Component%20Analysis%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fprincipal-component-analysis%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on telegram"
            href="https://telegram.me/share/url?text=Principal%20Component%20Analysis&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fprincipal-component-analysis%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Principal%20Component%20Analysis&u=https%3a%2f%2fflecart.github.io%2fnotes%2fprincipal-component-analysis%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
