<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bayesian Linear Regression | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="âž•probabilistic-artificial-intelligence">
<meta name="description" content="We have a prior $p(\text{model})$, we have a posterior $p(\text{model} \mid \text{data})$, a likelihood $p(\text{data} \mid \text{model})$ and $p(\text{data})$ is called the evidence.
Classical Linear regression Let&rsquo;s start with a classical regression. In this setting we need to estimate a model that is generated from this kind of data: $$ y = w^{T}x &#43; \varepsilon $$ Where $\varepsilon \sim \mathcal{N}(0, \sigma_{n}^{2}I)$ and it&rsquo;s the irreducible noise, an error that cannot be eliminated by any model in the model class, this is also called aleatoric uncertainty.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/bayesian-linear-regression/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/bayesian-linear-regression/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Bayesian Linear Regression" />
<meta property="og:description" content="We have a prior $p(\text{model})$, we have a posterior $p(\text{model} \mid \text{data})$, a likelihood $p(\text{data} \mid \text{model})$ and $p(\text{data})$ is called the evidence.
Classical Linear regression Let&rsquo;s start with a classical regression. In this setting we need to estimate a model that is generated from this kind of data: $$ y = w^{T}x &#43; \varepsilon $$ Where $\varepsilon \sim \mathcal{N}(0, \sigma_{n}^{2}I)$ and it&rsquo;s the irreducible noise, an error that cannot be eliminated by any model in the model class, this is also called aleatoric uncertainty." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/bayesian-linear-regression/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Bayesian Linear Regression"/>
<meta name="twitter:description" content="We have a prior $p(\text{model})$, we have a posterior $p(\text{model} \mid \text{data})$, a likelihood $p(\text{data} \mid \text{model})$ and $p(\text{data})$ is called the evidence.
Classical Linear regression Let&rsquo;s start with a classical regression. In this setting we need to estimate a model that is generated from this kind of data: $$ y = w^{T}x &#43; \varepsilon $$ Where $\varepsilon \sim \mathcal{N}(0, \sigma_{n}^{2}I)$ and it&rsquo;s the irreducible noise, an error that cannot be eliminated by any model in the model class, this is also called aleatoric uncertainty."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bayesian Linear Regression",
      "item": "https://flecart.github.io/notes/bayesian-linear-regression/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bayesian Linear Regression",
  "name": "Bayesian Linear Regression",
  "description": "We have a prior $p(\\text{model})$, we have a posterior $p(\\text{model} \\mid \\text{data})$, a likelihood $p(\\text{data} \\mid \\text{model})$ and $p(\\text{data})$ is called the evidence.\nClassical Linear regression Let\u0026rsquo;s start with a classical regression. In this setting we need to estimate a model that is generated from this kind of data: $$ y = w^{T}x + \\varepsilon $$ Where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{n}^{2}I)$ and it\u0026rsquo;s the irreducible noise, an error that cannot be eliminated by any model in the model class, this is also called aleatoric uncertainty.",
  "keywords": [
    "âž•probabilistic-artificial-intelligence"
  ],
  "articleBody": "We have a prior $p(\\text{model})$, we have a posterior $p(\\text{model} \\mid \\text{data})$, a likelihood $p(\\text{data} \\mid \\text{model})$ and $p(\\text{data})$ is called the evidence.\nClassical Linear regression Letâ€™s start with a classical regression. In this setting we need to estimate a model that is generated from this kind of data: $$ y = w^{T}x + \\varepsilon $$ Where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{n}^{2}I)$ and itâ€™s the irreducible noise, an error that cannot be eliminated by any model in the model class, this is also called aleatoric uncertainty. One could write this as follows: $y \\sim \\mathcal{N}(w^{T}x, \\sigma^{2}_{n}I)$ and itâ€™s the exact same thing as the previous, so if we look for the MLE estimate now we get\n$$ \\hat{w}_{\\text{MLE}} = \\arg \\max_{w \\in \\mathbb{R}^{d}} \\sum_{i = 1}^{N} \\log p (y_{i} \\mid x_{i}, w) = \\arg \\min_{w \\in \\mathbb{R}^{d}} (y - w^{T}x)^{2} $$ And this result is quite nice, because we just discovered that with this assumptions the best model for classical linear regression, the minimizer for the ordinary least squares problem, gives the same result with the maximum likelihood estimator.\nRidge Regression Before diving into this section, you should know Ridge Regression quite well.\nMaximum a posteriori estimate leads to Ridge Regression ðŸŸ© We assume a prior $p(w) = \\mathcal{N}(0, \\sigma_{p}^{2}I)$ (this is the special thing about Bayesian learning, which gives us confidence bounds, we choose Gaussians because of nice properties, see Gaussians and the Maximum Entropy Principle), and we also assume that $w$ is independent of the $x_{1:n}$, this further assumption acts as a regularization, as we will see briefly. The conditional likelihood is $p(y_{i:n} \\mid w_{i}x_{i:n}) = \\prod_{i = 1}^{n} p(y_{i} \\mid w_{i}x_{i})$ so we assume that the $y_{i:n}$ are conditionally independent given $w_{i}x_{i:n}$. And we further assume that $$ p(y_{i} \\mid w_{i}x_{i}) = \\mathcal{N}(y_{i}; w^{T}x_{i}, \\sigma^{2}_{n}) $$ Which is the same as saying that $y_{i} = w^{T}x_{i} + \\varepsilon_{i}$ where $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2}_{n})$.\nNow we can model the posterior: $$ p(w \\mid x_{1:n}, y_{1:n}) = \\frac{1}{z} p(w \\mid x_{1:n}) p(y_{1:n} \\mid w_{i},x_{1:n}) $$ And $z = \\int p(w) p(y_{1:n} \\mid w_{i}, x_{1:n}) \\, dw$.\nNow letâ€™s expand everything: we will have:\n$$ \\frac{1}{z} \\cdot \\frac{1}{z_{p}} \\exp\\left( -\\frac{1}{2\\sigma^{2}_{p}} \\lVert w \\rVert ^{2}_{2} \\right) \\cdot \\frac{1}{z_{l}} \\prod_{i = 1}^{n} \\exp\\left( -\\frac{1}{2\\sigma_{n}^{2}} (y_{i} - w^{T}x_{i})^{2} \\right) $$ Letâ€™s call $z'=z \\cdot z_{p} \\cdot z_{l}$ and now we have this: $$ = \\frac{1}{z'} \\exp\\left( -\\left[ \\frac{1}{2\\sigma^{2}_{p} }\\lVert w \\rVert^{2}_{2} + \\frac{1}{2\\sigma^{2}_{n}} \\sum_{i = 1}^{n} (y_{i} - w^{T}x_{i})^{2} \\right] \\right) $$ Now we do Max. a posteriori (MAP) estimate for $\\hat{w}$ and we get this: $$\n\\begin{align} \\hat{w} = \\arg\\max_{w} \\log p(w \\mid x_{1:n}y_{1:n}) \\ \\ = \\arg \\max_{w} \\log p(y_{1:n} \\mid x_{1:n}, w) + \\log p(w)\\ = \\arg \\min_{w} \\sum_{i = 1}^{k} (y_{i} - w^{T}x_{i})^{2} + \\frac{\\sigma_{n}^{2}}{\\sigma^{2}{p}} \\lVert w \\rVert ^{2}{2} \\end{align} $$ Which we note is the same as Ridge regression with $\\lambda = \\frac{\\sigma_{n}^{2}}{\\sigma^{2}_{p}}$. So with those Gaussian assumptions we just go back to a standard regression case! Remember that if $\\lambda = 0$ we donâ€™t have basically a regularizer, if $\\lambda$ is high we practically donâ€™t care much about the data (if noise is high we want to regularize, but if the prior variance is small we regularize more)\nThe MAP estimate simply collapses all mass of the posterior around its mode. This can be harmful when we are highly unsure about the best model, e.g., because we have observed insufficient data.\nThis why usually bayesian methods are preferred in other times.\nAnd interesting observation is that one could derive the lasso regression if we assume the underlying distribution of $p(w)$ to be a Laplacian instead of a Gaussian. This should enable us to perform variable selection. But the lasso optimization is usually not derivable (computed using LARS or similars), and the form is not exactly nice. For completeness, we report it here anyways: The prior on the weights should be in this case $$ p(w) = \\prod_{i = 1}^{d} \\frac{\\sigma^{2}_{p}}{4 \\sigma^{2}_{n}} \\exp\\left( -\\frac{\\sigma^{2}_{p}}{2\\sigma^{2}_{n}} \\lvert w_{i} \\rvert \\right) $$ No clue how to derive it though. I took this from slide 12 here (you need eth credentials to access the resource).\nPosterior is Gaussian ðŸŸ© In theory this result is easy if you know what are The Exponential Family and see that Gaussian is a Conjugate prior of itself. We observe that we can rewrite the argument part of the exponent in the following equation to be a gaussian: $$ p(w \\mid x_{1:n}, y_{1:n}) = \\frac{1}{z'} \\exp\\left( -\\left[ \\frac{1}{2\\sigma^{2}_{p} }\\lVert w \\rVert^{2}_{2} + \\frac{1}{2\\sigma^{2}_{n}} \\sum_{i = 1}^{n} (y_{i} - w^{T}x_{i})^{2} \\right] \\right) $$ We can rewrite it as $(w - \\bar{\\mu})^{T} \\bar{\\Sigma} (w - \\bar{\\mu})$ where $$ \\begin{array} \\\\ \\bar{\\Sigma} = \\left( \\frac{1}{\\sigma_{n}^{2}} X^{T}X + \\frac{1}{\\sigma_{p}^{2}}I \\right)^{-1} = \\sigma_{n}^{2}\\left( X^{T}X + \\frac{\\sigma^{2}_{n}}{\\sigma_{p}^{2}} I \\right) ^{-1} \\\\ \\bar{\\mu} = \\frac{1}{\\sigma_{n}^{2}} \\bar{\\Sigma} X^{T}y = (X^{T}X + \\lambda I)^{-1} X^{T}y \\end{array} $$ Where $\\lambda$ is as before, and this is the link with linear regression!\nInference Inference in Bayesian Linear Regression ðŸŸ© $$ p(y^{*} \\mid x^{*} x_{1:n} y_{1:n}) = \\int p(y^{*}, w \\mid x^{*}, x_{1:n} ,y_{1:n}) \\, dw = \\int p(y^{*} \\mid w ,x^{*}) p(w \\mid x_{1:n} y_{1:n}) \\, dw $$ We first used the sum rule, then we used product rule and independence assumption . Letâ€™s suppose $f^{*} = x^{*T}w$ so we use the multiplication of Gaussians sule and get $$ p(f^{*} \\mid x^{*}, x_{1:n}, y_{1:n}) = \\mathcal{N}(f^{*}; \\bar{\\mu} ^{T}x^{*} , x^{*T}\\bar{\\Sigma}x^{*}) $$ And assuming $y^{*} = f^{*} + \\varepsilon$ we have sum of Gaussian is Gaussian so we have $$ p(y^{} \\mid x^{}, x_{1:n}, y_{1:n}) =\n\\int p(y^{} \\mid w ,x^{}) p(w \\mid x_{1:n} y_{1:n}) , dw =\\mathcal{N}(y^{}; \\bar{\\mu}^{T} x^{}, x^{T}\\bar{\\Sigma}x^{} + \\sigma^{2}_{n}I) $$ So compared to ridge regression, the Bayesian linear regression has a variance term that is added. Here we are using all possible models, weighted by their probability. Conceptually bayesian is better than MLE.\nIt is possible to see later with the law of total variance, the decomposition of the inference variance into epistemic and aleatoric variance.\nRelation between BLR and Ridge Regression ðŸŸ© Ridge regression can be viewed as approximating the full posterior by placing all mass on its mode (also other models can be seen similarly, one example is MLE, analyzed in Parametric Models).\nSo itâ€™s a nice approximation: $$ \\begin{align}\np(y^{} \\mid x^{}, x_{1:n}, y_{1:n}) =\n\\int p(y^{} \\mid w ,x^{}) p(w \\mid x_{1:n} y_{1:n}) , dw \\ \\approx \\int p(y^{} \\mid w ,x^{}) \\delta_{\\hat{w}}(w) , dw \\ = p(y^{} \\mid x^{}, \\hat{w}) \\end{align} $$ Remember that for delta direct distribution we have $$ \\mathbb{E}[f(x)] = \\int f(x) \\delta_{xâ€™}(x) , dx = f(xâ€™) $$\nEpistemic and aleatoric uncertainty ðŸŸ© We now start a philosophical discussion on epistemic and aleatoric uncertainty.\nEpistemic uncertainty: Uncertainty about the model due to the lack of data Aleatoric uncertainty: Irreducible noise The $x^{*T}\\bar{\\Sigma}x^{*}$ is the epistemic uncertainty about $f^{*}$ while the $\\sigma^{2}_{n}$ factor is the noise/uncertainty about $y^{*}$ given $f^{*}$, meaning we need to change $f^{*}$, or hypothesis class if we want to lower this uncertainty I think (not sure though).\nThe law of total variance can give another intuition about the decomposition between epistemic and aleatoric uncertainty:\n$$ Var(y \\mid x) = \\text{ aleatoric } + \\text{ epistemic } = \\mathbb{E}_{\\theta}\\left[ \\text{Var}_{y}(y \\mid x, \\theta) \\right] + \\text{Var}_{\\theta}(\\mathbb{E}_{y}[y \\mid x, \\theta]) $$ Choosing hyper-parameters ðŸŸ¨++ We would like a manner to compute the values of $\\sigma^{2}_{n}$ and $\\sigma^{2}_{p}$. Covariance of the prior and variance of the noise need to be chosen (TODO understand why) before we can apply these methods. One way is just finding $\\lambda = \\frac{\\sigma^{2}_{n}}{\\sigma^{2}_{p}}$ with cross-validation, then estimate the variance from data and get the prior variance in this manner.\nThen itâ€™s possible to have graphical models for bayesian linear regression.\nRecursive bayesian updates ðŸŸ¨+ We want to incorporate the data we have today as future prior. The posterior we have today will be the prior of tomorrow. Letâ€™s suppose we have $p(w)$ prior and we have observations $y_{1:n}$ such that $p(y_{1:n} \\mid w) = \\prod_{i = 1}^{N} p(y_{i} \\mid w)$ We can define some intermediate posteriors: $$ p^{(j)} (w) := p(w \\mid y_{1: j}), p^{(0)}(w) = p(w) $$ If we have the posterior for the previous $j - 1$ data points and now we have $y_{j}$, then we can compute the next prior (we used Bayes rule here and i.i.d. property of $y_{i}$): $$ p^{(j)}(w) = p( w \\mid y_{1: j}) = \\frac{1}{z} p(w) \\cdot p(y_{1} \\mid w) \\dots p(y_{j} \\mid w) = \\frac{1}{z} \\left( z \\cdot p^{(j - 1)}(w) \\right) p(y_{j} \\mid w) = f(p^{(j - 1)}(w), y_{j}) $$ So we would just need to integrate the new information given by $y_{j}$ and then use the previous knowledge.\nOnline Bayesian regression ðŸŸ¨ This section concerns in building a bayesian regression algorithm whose memory does not grow with the number of samples. It can be conceived as a filtering process, composed of two phased: conditioning (updating over the priors) and prediction.\nWe have discovered a manner to learn the best parameters for bayesian linear regression which is just this closed form: $$ p(w \\mid X, y) = \\mathcal{N}(w; (X^{T}X +\\sigma_{n}^{2}I)^{-1}X^{T}y, (\\sigma^{-2}_{n}X^{T}X + I)^{-1}) $$ Assuming $\\sigma_{p}^{2} = 1$.\nNow we want to do online linear regression, assuming the data is coming one by one, we would like to update the corresponding matrices of interest, which are $X^{T}X$ and $X^{T}y$. So, letâ€™s say we are given some matrices $$ X^{(t)} = \\begin{bmatrix} x_{1}^{T} \\\\ \\vdots \\\\ x_{t}^{T} \\end{bmatrix} \\in \\mathbb{R}^{t \\times d} \\,\\,\\, Y^{(t)} = \\begin{bmatrix} y_{1} \\\\ \\vdots \\\\ y_{t} \\end{bmatrix} \\in \\mathbb{R}^{t \\times 1} $$ We would like to efficiently compute $X^{(t + 1)T}X^{(t + 1)}$ and $X^{(t + 1)T}Y^{(t+1)}$ assuming the known matrices above. The latter is easy to compute because: $$ X^{(t + 1)T}Y^{(t+1)} = \\sum_{i = 1}^{t} x_{i}y_{i} + x_{t + 1}y_{t + 1} = X^{(t)}Y^{(t)} + x_{t + 1}y_{t + 1} $$ Now letâ€™s try to decompose the other product $$ X^{(t + 1)T} X^{(t+ 1)} = \\begin{pmatrix} x_{1} , \\dots x_{t} \\end{pmatrix} \\cdot \\begin{pmatrix} x_{1} \\\\ \\vdots \\\\ x_{t} \\end{pmatrix} = \\sum_{i = 1}^{t} x_{i}x_{i}^{T} + x_{t + 1}x_{t + 1}^{T} = X^{(t)T}X^{(t)} + x_{t+1}x_{t + 1}^{T} $$ This allows to compute the parameters in a way that the memory requirement is constant and does not grow as $\\mathcal{O(t)}$.\nAnother problem that we have is computing the inverse of $X^{T}X + \\sigma_{n}^{2}I$ is very expensive, usually takes $\\mathcal{O}(d^{3})$ but we can do it in $\\mathcal{O}(d^{2})$ with the use of Woodbury Identities: If $A \\in \\mathbb{R}^{n \\times n}$ and $u, v \\in \\mathbb{R}^{n \\times 1}$ then the following is true: $$ (A + uv^{T})^{-1} = A^{-1} - \\frac{A^{-1}uv^{T}A^{-1}}{1 + v^{T}A^{-1}u} $$ And we can interpret the value $X^{(t)T}X^{(t)} + \\sigma_{n}^{2}I$ as our $A$ and the $x_{t+1}x_{t + 1}^{T}$ as our two vectors. In this manner we just need to compute matrix multiplications which bring down the complexity to $\\mathcal{O}(d^{2})$.\nThis is all for analysis in the online bayesian setting.\nLogistic regression case ðŸŸ© With this we just have the likelihood from a Bernoulli distribution (could also be extended for many dimensions, not just 0 or 1 labels, but now we have just the binary case). So we write $$ P(y \\mid x, w) = Ber(y; \\sigma(w^{T}x)) $$ Where $\\sigma$ is the Sigmoid function which gives an idea of confidence of a prediction.\nRelation with dimensionality ðŸŸ© We can still apply linear methods on non-linearly transformed data, we just do something like $$ f(x) = \\sum_{i = 1}^{N} w_{i} \\phi_{i}(x) $$ The problem is that the number of new features grows exponentially with the number of the dimensions, so itâ€™s not usually feasible using this trick. For example if we consider polynomials of degree $m$ of $d$ dimensions, then we will have the following variables: $$ \\phi = [1, x_{1}, \\dots, x_{d}, x_{1}^{2}, .. x_{2}^{2}, x_{1}x_{2}, \\dots, x_{1}x_{2}\\dots x_{d}] $$ which has dimension $$ \\lvert \\phi \\rvert = \\begin{pmatrix} d + m \\\\ d \\end{pmatrix}$$ The kernel trick ðŸŸ© We first want to replace the problem using inner products then we want to replace inner products using kernels. If we are able to do this, then we can compute inner products in feature space quite efficiently. For example if $\\phi(x) = [\\text{ all monomials up to } m \\text{ variables} ]$ then we can compute $\\phi(x)^{T}\\phi(x)$ by just computing $(1 + x^{T}x)^{m}$. This is quite important to understand well. See Kernel Methods for more. This leads us in an attempt to kernelize the bayesian linear regression.\nFunction view In the previous sections we have derived the Bayesian Linear Regression based on priors on the weights and likelihoods. It is possible to derive something very similar by just observing inputs and outputs, so having a distribution on the space of the functions. This part builds the fundamental ideas that will be later needed for Gaussian Processes so it is quite important to understand this quite well.\nAssuming this structure $f = \\Phi w$ we can assume a structure on the space of functions: $$ f \\mid X \\sim \\mathcal{N}(\\Phi \\mathbb{E}[w], \\Phi\\mathbb{E}[w]\\Phi^{T}) = \\mathcal{N}(0, \\sigma^{2}_{p}\\Phi\\Phi^{T}) $$ With this formulation the feature map is implicit in the choice of the Kernel.\nPredictions in function view Letâ€™s suppose we have a new point $x^{*}$, we need to show that with the function view we can do inference without too many problems: Letâ€™s define $$ \\bar{\\Phi} = \\begin{bmatrix} \\Phi \\\\ \\phi(x^{*})^{T} \\end{bmatrix}, \\, \\bar{y} =\\begin{bmatrix} y \\\\ y^{*} \\end{bmatrix}, \\bar{f} = \\begin{bmatrix} f \\\\ f^{*} \\end{bmatrix} $$ At the end the inference process is very similar to the one in the linear regression setting: $\\bar{f} = \\bar{\\Phi} w$ which is a normal with variance $\\bar{K} = \\sigma^{2}_{p}\\bar{\\Phi}\\bar{\\Phi}^{T}$ and mean 0. Then we just add the aleatoric noise and obtain $$ \\hat{y}\\mid X, x^{*} \\sim \\mathcal{N}(0, \\bar{K} + \\sigma^{2}_{n}I) $$ ",
  "wordCount" : "2285",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/bayesian-linear-regression/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;Â»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Bayesian Linear Regression
    </h1>
    <div class="post-meta">11 min&nbsp;Â·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#classical-linear-regression" aria-label="Classical Linear regression">Classical Linear regression</a></li></ul>
                    
                <li>
                    <a href="#ridge-regression" aria-label="Ridge Regression">Ridge Regression</a><ul>
                        
                <li>
                    <a href="#maximum-a-posteriori-estimate-leads-to-ridge-regression-" aria-label="Maximum a posteriori estimate leads to Ridge Regression ðŸŸ©">Maximum a posteriori estimate leads to Ridge Regression ðŸŸ©</a></li>
                <li>
                    <a href="#posterior-is-gaussian-" aria-label="Posterior is Gaussian ðŸŸ©">Posterior is Gaussian ðŸŸ©</a></li></ul>
                </li>
                <li>
                    <a href="#inference" aria-label="Inference">Inference</a><ul>
                        
                <li>
                    <a href="#inference-in-bayesian-linear-regression-" aria-label="Inference in Bayesian Linear Regression ðŸŸ©">Inference in Bayesian Linear Regression ðŸŸ©</a></li>
                <li>
                    <a href="#relation-between-blr-and-ridge-regression-" aria-label="Relation between BLR and Ridge Regression ðŸŸ©">Relation between BLR and Ridge Regression ðŸŸ©</a></li>
                <li>
                    <a href="#epistemic-and-aleatoric-uncertainty-" aria-label="Epistemic and aleatoric uncertainty ðŸŸ©">Epistemic and aleatoric uncertainty ðŸŸ©</a></li>
                <li>
                    <a href="#choosing-hyper-parameters-" aria-label="Choosing hyper-parameters ðŸŸ¨&#43;&#43;">Choosing hyper-parameters ðŸŸ¨++</a></li>
                <li>
                    <a href="#recursive-bayesian-updates-" aria-label="Recursive bayesian updates ðŸŸ¨&#43;">Recursive bayesian updates ðŸŸ¨+</a></li>
                <li>
                    <a href="#online-bayesian-regression-" aria-label="Online Bayesian regression ðŸŸ¨">Online Bayesian regression ðŸŸ¨</a></li>
                <li>
                    <a href="#logistic-regression-case-" aria-label="Logistic regression case ðŸŸ©">Logistic regression case ðŸŸ©</a></li></ul>
                </li>
                <li>
                    <a href="#relation-with-dimensionality-" aria-label="Relation with dimensionality ðŸŸ©">Relation with dimensionality ðŸŸ©</a><ul>
                        
                <li>
                    <a href="#the-kernel-trick-" aria-label="The kernel trick ðŸŸ©">The kernel trick ðŸŸ©</a></li></ul>
                </li>
                <li>
                    <a href="#function-view" aria-label="Function view">Function view</a><ul>
                        
                <li>
                    <a href="#predictions-in-function-view" aria-label="Predictions in function view">Predictions in function view</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>We have a prior $p(\text{model})$, we have a posterior $p(\text{model} \mid \text{data})$, a likelihood $p(\text{data} \mid \text{model})$ and $p(\text{data})$ is called the <em>evidence</em>.</p>
<h4 id="classical-linear-regression">Classical Linear regression<a hidden class="anchor" aria-hidden="true" href="#classical-linear-regression">#</a></h4>
<p>Let&rsquo;s start with a classical regression. In this setting we need to estimate a model that is generated from this kind of data:
</p>
$$
y = w^{T}x + \varepsilon
$$
<p>
Where $\varepsilon \sim \mathcal{N}(0, \sigma_{n}^{2}I)$ and it&rsquo;s the irreducible noise, an error that cannot be eliminated by any model in the model class, this is also called <strong>aleatoric uncertainty</strong>.
One could write this as follows: $y \sim \mathcal{N}(w^{T}x, \sigma^{2}_{n}I)$ and it&rsquo;s the exact same thing as the previous, so if we look for the MLE estimate now we get</p>
$$
\hat{w}_{\text{MLE}} = \arg \max_{w \in \mathbb{R}^{d}} \sum_{i = 1}^{N} \log p (y_{i} \mid x_{i}, w) = \arg \min_{w \in \mathbb{R}^{d}} (y - w^{T}x)^{2}
$$
<p>
And this result is quite nice, because we just discovered that with this assumptions the best model for classical linear regression, the minimizer for the ordinary least squares problem, gives the same result with the maximum likelihood estimator.</p>
<h3 id="ridge-regression">Ridge Regression<a hidden class="anchor" aria-hidden="true" href="#ridge-regression">#</a></h3>
<p>Before diving into this section, you should know <a href="/notes/linear-regression-methods/">Ridge Regression</a> quite well.</p>
<h4 id="maximum-a-posteriori-estimate-leads-to-ridge-regression-">Maximum a posteriori estimate leads to Ridge Regression ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#maximum-a-posteriori-estimate-leads-to-ridge-regression-">#</a></h4>
<p>We assume a prior $p(w) = \mathcal{N}(0, \sigma_{p}^{2}I)$ (this is the special thing about Bayesian learning, which gives us confidence bounds, we choose Gaussians because of nice properties, see <a href="/notes/gaussians/">Gaussians</a> and the <a href="/notes/maximum-entropy-principle/">Maximum Entropy Principle</a>), and we also assume that $w$ is independent of the $x_{1:n}$, this further assumption acts as a regularization, as we will see briefly.
The conditional likelihood is $p(y_{i:n} \mid w_{i}x_{i:n}) = \prod_{i = 1}^{n} p(y_{i} \mid w_{i}x_{i})$ so we assume that the $y_{i:n}$ are conditionally independent given $w_{i}x_{i:n}$.
And we further assume that
</p>
$$
p(y_{i} \mid w_{i}x_{i}) = \mathcal{N}(y_{i}; w^{T}x_{i}, \sigma^{2}_{n})
$$
<p>
Which is the same as saying that $y_{i} = w^{T}x_{i} + \varepsilon_{i}$  where $\varepsilon_{i} \sim \mathcal{N}(0, \sigma^{2}_{n})$.</p>
<p>Now we can model the posterior:
</p>
$$
p(w \mid x_{1:n}, y_{1:n}) = \frac{1}{z} p(w \mid x_{1:n}) p(y_{1:n} \mid w_{i},x_{1:n})
$$
<p>
And $z = \int p(w) p(y_{1:n} \mid w_{i}, x_{1:n}) \, dw$.</p>
<p>Now let&rsquo;s expand everything: we will have:</p>
$$
\frac{1}{z} \cdot \frac{1}{z_{p}} \exp\left( -\frac{1}{2\sigma^{2}_{p}} \lVert w \rVert ^{2}_{2} \right) \cdot \frac{1}{z_{l}} \prod_{i = 1}^{n} \exp\left( -\frac{1}{2\sigma_{n}^{2}} (y_{i} - w^{T}x_{i})^{2} \right) 
$$
<p>
Let&rsquo;s call $z'=z \cdot z_{p} \cdot z_{l}$ and now we have this:
</p>
$$
= \frac{1}{z'} \exp\left( -\left[  \frac{1}{2\sigma^{2}_{p}  }\lVert w \rVert^{2}_{2} + \frac{1}{2\sigma^{2}_{n}} \sum_{i = 1}^{n} (y_{i} - w^{T}x_{i})^{2} \right] \right)
$$
<p>
Now we do Max. a posteriori (MAP) estimate for $\hat{w}$ and we get this:
$$</p>
<p>\begin{align}
\hat{w} = \arg\max_{w} \log p(w \mid x_{1:n}y_{1:n})  \ \
= \arg \max_{w}  \log p(y_{1:n} \mid x_{1:n}, w) + \log p(w)\
= \arg \min_{w} \sum_{i = 1}^{k} (y_{i} - w^{T}x_{i})^{2} + \frac{\sigma_{n}^{2}}{\sigma^{2}<em>{p}} \lVert w \rVert ^{2}</em>{2}
\end{align}
$$
Which we note is the same as <strong>Ridge regression</strong> with $\lambda = \frac{\sigma_{n}^{2}}{\sigma^{2}_{p}}$. So with those Gaussian assumptions we just go back to a standard regression case!
Remember that if $\lambda = 0$ we don&rsquo;t have basically a regularizer, if $\lambda$ is high we practically don&rsquo;t care much about the  data (if noise is high we want to regularize, but if the prior variance is small we regularize more)</p>
<blockquote>
<p>The MAP estimate simply collapses all mass of the posterior around its mode. This can be
harmful when we are highly unsure about the best model, e.g., because
we have observed insufficient data.</p>
</blockquote>
<p>This why usually bayesian methods are preferred in other times.</p>
<p>And interesting observation is that one could <strong>derive the lasso regression</strong> if we assume the underlying distribution of $p(w)$ to be a Laplacian instead of a Gaussian. This should enable us to perform variable selection. But the lasso optimization is usually not derivable (computed using LARS or similars), and the form is not exactly nice.
For completeness, we report it here anyways: The prior on the weights should be in this case
</p>
$$
p(w) = \prod_{i = 1}^{d} \frac{\sigma^{2}_{p}}{4 \sigma^{2}_{n}} \exp\left( -\frac{\sigma^{2}_{p}}{2\sigma^{2}_{n}} \lvert w_{i} \rvert \right)
$$
<p>
No clue how to derive it though. I took this from slide 12 <a href="https://moodle-app2.let.ethz.ch/pluginfile.php/2179617/mod_resource/content/1/aml24_lecture_04_RegSumJB.pdf">here</a> (you need eth credentials to access the resource).</p>
<h4 id="posterior-is-gaussian-">Posterior is Gaussian ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#posterior-is-gaussian-">#</a></h4>
<p>In theory this result is easy if you know what are <a href="/notes/the-exponential-family/">The Exponential Family</a> and see that Gaussian is a Conjugate prior of itself.
We observe that we can rewrite the argument part of the exponent in the following equation to be a gaussian:
</p>
$$
p(w \mid x_{1:n}, y_{1:n}) = \frac{1}{z'} \exp\left( -\left[  \frac{1}{2\sigma^{2}_{p}  }\lVert w \rVert^{2}_{2} + \frac{1}{2\sigma^{2}_{n}} \sum_{i = 1}^{n} (y_{i} - w^{T}x_{i})^{2} \right] \right)
$$
<p>
We can rewrite it as $(w - \bar{\mu})^{T} \bar{\Sigma} (w - \bar{\mu})$ where
</p>
$$
\begin{array}
 \\
\bar{\Sigma} = \left( \frac{1}{\sigma_{n}^{2}} X^{T}X + \frac{1}{\sigma_{p}^{2}}I \right)^{-1} = \sigma_{n}^{2}\left( X^{T}X + \frac{\sigma^{2}_{n}}{\sigma_{p}^{2}} I \right) ^{-1} \\
\bar{\mu} = \frac{1}{\sigma_{n}^{2}} \bar{\Sigma} X^{T}y = (X^{T}X + \lambda I)^{-1} X^{T}y
\end{array}
$$
<p>
Where $\lambda$ is as before, and this is the link with linear regression!</p>
<h3 id="inference">Inference<a hidden class="anchor" aria-hidden="true" href="#inference">#</a></h3>
<h4 id="inference-in-bayesian-linear-regression-">Inference in Bayesian Linear Regression ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#inference-in-bayesian-linear-regression-">#</a></h4>
$$
p(y^{*} \mid x^{*} x_{1:n} y_{1:n}) = \int p(y^{*}, w \mid x^{*}, x_{1:n} ,y_{1:n}) \, dw = \int p(y^{*} \mid w ,x^{*}) p(w \mid x_{1:n} y_{1:n}) \, dw 
$$
<p>
We first used the sum rule, then we used product rule and independence assumption .
Let&rsquo;s suppose $f^{*} = x^{*T}w$ so we use the multiplication of <a href="/notes/gaussians/">Gaussians</a> sule and get
</p>
$$
p(f^{*} \mid x^{*}, x_{1:n}, y_{1:n}) = \mathcal{N}(f^{*}; \bar{\mu} ^{T}x^{*} , x^{*T}\bar{\Sigma}x^{*})
$$
<p>
And assuming $y^{*} = f^{*} + \varepsilon$ we have sum of Gaussian is Gaussian so we have
$$
p(y^{<em>} \mid x^{</em>}, x_{1:n}, y_{1:n}) =</p>
<p>\int p(y^{<em>} \mid w ,x^{</em>}) p(w \mid x_{1:n} y_{1:n}) , dw
=\mathcal{N}(y^{<em>}; \bar{\mu}^{T} x^{</em>}, x^{<em>T}\bar{\Sigma}x^{</em>} + \sigma^{2}_{n}I)
$$
So compared to ridge regression, the Bayesian linear regression has a variance term that is added.
Here we are using all possible models, weighted by their probability. Conceptually bayesian is better than MLE.</p>
<p>It is possible to see later with the law of total variance, the decomposition of the inference variance into epistemic and aleatoric variance.</p>
<h4 id="relation-between-blr-and-ridge-regression-">Relation between BLR and Ridge Regression ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#relation-between-blr-and-ridge-regression-">#</a></h4>
<blockquote>
<p>Ridge regression can be viewed as approximating the full posterior by placing all mass on its mode (also other models can be seen similarly, one example is MLE, analyzed in <a href="/notes/parametric-models/">Parametric Models</a>).</p>
</blockquote>
<p>So it&rsquo;s a nice approximation:
$$
\begin{align}</p>
<p>p(y^{<em>} \mid x^{</em>}, x_{1:n}, y_{1:n}) =</p>
<p>\int p(y^{<em>} \mid w ,x^{</em>}) p(w \mid x_{1:n} y_{1:n}) , dw  \
\approx
\int p(y^{<em>} \mid w ,x^{</em>}) \delta_{\hat{w}}(w) , dw  \
= p(y^{<em>} \mid x^{</em>}, \hat{w})
\end{align}
</p>
$$
Remember that for delta direct distribution we have
$$
<p>
\mathbb{E}[f(x)] = \int f(x) \delta_{x&rsquo;}(x) , dx = f(x&rsquo;)
$$</p>
<h4 id="epistemic-and-aleatoric-uncertainty-">Epistemic and aleatoric uncertainty ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#epistemic-and-aleatoric-uncertainty-">#</a></h4>
<p>We now start a philosophical discussion on epistemic and aleatoric uncertainty.</p>
<ul>
<li>Epistemic uncertainty: Uncertainty about the model due to the lack of data</li>
<li>Aleatoric uncertainty: Irreducible noise</li>
</ul>
<p>The $x^{*T}\bar{\Sigma}x^{*}$ is the epistemic uncertainty about $f^{*}$ while the $\sigma^{2}_{n}$ factor is the noise/uncertainty about $y^{*}$ given $f^{*}$, meaning we need to change $f^{*}$, or hypothesis class if we want to lower this uncertainty I think (not sure though).</p>
<p>The law of total variance can give another intuition about the decomposition between epistemic and aleatoric uncertainty:</p>
$$
Var(y \mid x) = \text{ aleatoric } + \text{ epistemic } = \mathbb{E}_{\theta}\left[ \text{Var}_{y}(y \mid x, \theta) \right]  + \text{Var}_{\theta}(\mathbb{E}_{y}[y \mid x, \theta])
$$
<h4 id="choosing-hyper-parameters-">Choosing hyper-parameters ðŸŸ¨++<a hidden class="anchor" aria-hidden="true" href="#choosing-hyper-parameters-">#</a></h4>
<p>We would like a manner to compute the values of $\sigma^{2}_{n}$ and $\sigma^{2}_{p}$.
Covariance of the prior and variance of the noise need to be chosen (TODO understand why) before we can apply these methods.
One way is just finding $\lambda = \frac{\sigma^{2}_{n}}{\sigma^{2}_{p}}$ with cross-validation, then estimate the variance from data and get the prior variance in this manner.</p>
<p>Then it&rsquo;s possible to have <strong>graphical models</strong> for bayesian linear regression.</p>
<h4 id="recursive-bayesian-updates-">Recursive bayesian updates ðŸŸ¨+<a hidden class="anchor" aria-hidden="true" href="#recursive-bayesian-updates-">#</a></h4>
<p>We want to incorporate the data we have today as future prior. The posterior we have today will be the prior of tomorrow.
Let&rsquo;s suppose we have $p(w)$ prior and we have observations $y_{1:n}$ such that $p(y_{1:n} \mid w) = \prod_{i = 1}^{N} p(y_{i} \mid w)$
We can define some intermediate posteriors:
</p>
$$
p^{(j)} (w) := p(w \mid y_{1: j}), p^{(0)}(w) = p(w)
$$
<p>
If we have the posterior for the previous $j - 1$ data points and now we have $y_{j}$, then we can compute the next prior (we used Bayes rule here and i.i.d. property of $y_{i}$):
</p>
$$
p^{(j)}(w) = p( w \mid y_{1: j}) = \frac{1}{z} p(w) \cdot p(y_{1} \mid w) \dots p(y_{j} \mid w)  = \frac{1}{z} \left( z \cdot p^{(j - 1)}(w) \right) p(y_{j} \mid w) = f(p^{(j - 1)}(w), y_{j})
$$
<p>So we would just need to integrate the new information given by $y_{j}$ and then use the previous knowledge.</p>
<h4 id="online-bayesian-regression-">Online Bayesian regression ðŸŸ¨<a hidden class="anchor" aria-hidden="true" href="#online-bayesian-regression-">#</a></h4>
<p>This section concerns in building a bayesian regression algorithm whose memory does not grow with the number of samples. It can be conceived as a filtering process, composed of two phased: <em>conditioning</em> (updating over the priors) and <em>prediction</em>.</p>
<p>We have discovered a manner to learn the best parameters for bayesian linear regression which is just this closed form:
</p>
$$
p(w \mid X, y) = \mathcal{N}(w; (X^{T}X +\sigma_{n}^{2}I)^{-1}X^{T}y, (\sigma^{-2}_{n}X^{T}X + I)^{-1})
$$
<p>
Assuming $\sigma_{p}^{2} = 1$.</p>
<p>Now we want to do online linear regression, assuming the data is coming one by one, we would like to update the corresponding matrices of interest, which are $X^{T}X$ and $X^{T}y$.
So, let&rsquo;s say we are given some matrices
</p>
$$
X^{(t)} = \begin{bmatrix}
x_{1}^{T}  \\
\vdots \\
x_{t}^{T}
\end{bmatrix} \in \mathbb{R}^{t \times d} \,\,\, Y^{(t)} = \begin{bmatrix}
y_{1}  \\
\vdots \\
y_{t}
\end{bmatrix} \in \mathbb{R}^{t \times 1}
$$
<p>
We would like to efficiently compute $X^{(t + 1)T}X^{(t + 1)}$ and $X^{(t + 1)T}Y^{(t+1)}$ assuming the known matrices above.
The latter is easy to compute because:
</p>
$$
X^{(t + 1)T}Y^{(t+1)} = \sum_{i = 1}^{t} x_{i}y_{i} + x_{t + 1}y_{t + 1} = X^{(t)}Y^{(t)} + x_{t + 1}y_{t + 1}
$$
<p>
Now let&rsquo;s try to decompose the other product
</p>
$$
X^{(t + 1)T} X^{(t+ 1)} = \begin{pmatrix}
x_{1} , \dots x_{t}
\end{pmatrix} \cdot \begin{pmatrix}
x_{1}  \\
\vdots \\
x_{t}
\end{pmatrix} = \sum_{i = 1}^{t} x_{i}x_{i}^{T} + x_{t + 1}x_{t + 1}^{T} = X^{(t)T}X^{(t)} + x_{t+1}x_{t + 1}^{T}
$$
<p>
This allows to compute the parameters in a way that the memory requirement is constant and does not grow as $\mathcal{O(t)}$.</p>
<p>Another problem that we have is computing the inverse of $X^{T}X + \sigma_{n}^{2}I$ is very expensive, usually takes $\mathcal{O}(d^{3})$ but we can do it in $\mathcal{O}(d^{2})$ with the use of <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury Identities</a>:
If $A \in \mathbb{R}^{n \times n}$ and $u, v \in \mathbb{R}^{n \times 1}$ then the following is true:
</p>
$$
(A + uv^{T})^{-1} = A^{-1} - \frac{A^{-1}uv^{T}A^{-1}}{1 + v^{T}A^{-1}u}
$$
<p>And we can interpret the value $X^{(t)T}X^{(t)}  + \sigma_{n}^{2}I$ as our $A$ and the $x_{t+1}x_{t + 1}^{T}$ as our two vectors. In this manner we just need to compute matrix multiplications which bring down the complexity to $\mathcal{O}(d^{2})$.</p>
<p>This is all for analysis in the online bayesian setting.</p>
<h4 id="logistic-regression-case-">Logistic regression case ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#logistic-regression-case-">#</a></h4>
<p>With this we just have the likelihood from a Bernoulli distribution (could also be extended for many dimensions, not just 0 or 1 labels, but now we have just the binary case). So we write
</p>
$$
P(y \mid x, w) = Ber(y; \sigma(w^{T}x))
$$
<p>
Where $\sigma$ is the Sigmoid function which gives an idea of confidence of a prediction.</p>
<h3 id="relation-with-dimensionality-">Relation with dimensionality ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#relation-with-dimensionality-">#</a></h3>
<p>We can still apply linear methods on <strong>non-linearly transformed data</strong>, we just do something like
</p>
$$
f(x) = \sum_{i = 1}^{N} w_{i} \phi_{i}(x)
$$
<p>
The problem is that the number of new features grows exponentially with the number of the dimensions, so it&rsquo;s not usually feasible using this trick.
For example if we consider polynomials of degree $m$ of $d$ dimensions, then we will have the following variables:
</p>
$$
\phi = [1, x_{1}, \dots, x_{d}, x_{1}^{2}, .. x_{2}^{2}, x_{1}x_{2}, \dots, x_{1}x_{2}\dots x_{d}]
$$
<p>
which has dimension </p>
$$ \lvert \phi \rvert =  \begin{pmatrix}
d + m \\
d
\end{pmatrix}$$
<h4 id="the-kernel-trick-">The kernel trick ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#the-kernel-trick-">#</a></h4>
<p>We first want to replace the problem using inner products then we want to <strong>replace inner products</strong> using <em>kernels</em>. If we are able to do this, then we can compute inner products in feature space quite efficiently.
For example if $\phi(x) = [\text{ all monomials up to } m \text{ variables} ]$ then we can compute $\phi(x)^{T}\phi(x)$ by just computing $(1 + x^{T}x)^{m}$. This is quite important to understand well. See <a href="/notes/kernel-methods/">Kernel Methods</a> for more.
This leads us in an attempt to kernelize the bayesian linear regression.</p>
<h3 id="function-view">Function view<a hidden class="anchor" aria-hidden="true" href="#function-view">#</a></h3>
<p>In the previous sections we have derived the Bayesian Linear Regression based on priors on the weights and likelihoods. It is possible to derive something very similar by just observing inputs and outputs, so having a distribution on the <em>space of the functions</em>.
This part builds the fundamental ideas that will be later needed for <a href="/notes/gaussian-processes/">Gaussian Processes</a> so it is quite important to understand this quite well.</p>
<p>Assuming this structure $f = \Phi w$ we can assume a structure on the space of functions:
</p>
$$
f \mid X \sim \mathcal{N}(\Phi \mathbb{E}[w], \Phi\mathbb{E}[w]\Phi^{T}) = \mathcal{N}(0, \sigma^{2}_{p}\Phi\Phi^{T})
$$
<p>With this formulation the feature map is implicit in the choice of the Kernel.</p>
<h4 id="predictions-in-function-view">Predictions in function view<a hidden class="anchor" aria-hidden="true" href="#predictions-in-function-view">#</a></h4>
<p>Let&rsquo;s suppose we have a new point $x^{*}$, we need to show that with the function view we can do inference without too many problems:
Let&rsquo;s define
</p>
$$
\bar{\Phi} = \begin{bmatrix}
\Phi \\
\phi(x^{*})^{T}
\end{bmatrix}, \, \bar{y}  =\begin{bmatrix}
y \\
y^{*}
\end{bmatrix}, \bar{f} = \begin{bmatrix}
f \\
f^{*}
\end{bmatrix}
$$
<p>
At the end the inference process is very similar to the one in the linear regression setting:
$\bar{f} = \bar{\Phi} w$ which is a normal with variance $\bar{K} = \sigma^{2}_{p}\bar{\Phi}\bar{\Phi}^{T}$ and mean 0. Then we just add the aleatoric noise and obtain
</p>
$$
\hat{y}\mid X, x^{*} \sim \mathcal{N}(0, \bar{K} + \sigma^{2}_{n}I)
$$


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">âž•Probabilistic-Artificial-Intelligence</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Linear Regression on x"
            href="https://x.com/intent/tweet/?text=Bayesian%20Linear%20Regression&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-linear-regression%2f&amp;hashtags=%e2%9e%95probabilistic-artificial-intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Linear Regression on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-linear-regression%2f&amp;title=Bayesian%20Linear%20Regression&amp;summary=Bayesian%20Linear%20Regression&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-linear-regression%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Linear Regression on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-linear-regression%2f&title=Bayesian%20Linear%20Regression">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Linear Regression on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-linear-regression%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Linear Regression on whatsapp"
            href="https://api.whatsapp.com/send?text=Bayesian%20Linear%20Regression%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-linear-regression%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Linear Regression on telegram"
            href="https://telegram.me/share/url?text=Bayesian%20Linear%20Regression&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-linear-regression%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Linear Regression on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Bayesian%20Linear%20Regression&u=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-linear-regression%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
