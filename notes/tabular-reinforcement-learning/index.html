<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Tabular Reinforcement Learning | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="âž•probabilistic-artificial-intelligence">
<meta name="description" content="This note extends the content Markov Processes in this specific context.
Standard notions Explore-exploit dilemma ðŸŸ© We have seen something similar also in Active Learning when we tried to model if we wanted to look elsewhere or go for the maximum value we have found. The dilemma under analysis is the explore-exploit dilemma: whether if we should just go for the best solution we have found at the moment, or look for a better one.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/tabular-reinforcement-learning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/tabular-reinforcement-learning/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Tabular Reinforcement Learning" />
<meta property="og:description" content="This note extends the content Markov Processes in this specific context.
Standard notions Explore-exploit dilemma ðŸŸ© We have seen something similar also in Active Learning when we tried to model if we wanted to look elsewhere or go for the maximum value we have found. The dilemma under analysis is the explore-exploit dilemma: whether if we should just go for the best solution we have found at the moment, or look for a better one." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/tabular-reinforcement-learning/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Tabular Reinforcement Learning"/>
<meta name="twitter:description" content="This note extends the content Markov Processes in this specific context.
Standard notions Explore-exploit dilemma ðŸŸ© We have seen something similar also in Active Learning when we tried to model if we wanted to look elsewhere or go for the maximum value we have found. The dilemma under analysis is the explore-exploit dilemma: whether if we should just go for the best solution we have found at the moment, or look for a better one."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Tabular Reinforcement Learning",
      "item": "https://flecart.github.io/notes/tabular-reinforcement-learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Tabular Reinforcement Learning",
  "name": "Tabular Reinforcement Learning",
  "description": "This note extends the content Markov Processes in this specific context.\nStandard notions Explore-exploit dilemma ðŸŸ© We have seen something similar also in Active Learning when we tried to model if we wanted to look elsewhere or go for the maximum value we have found. The dilemma under analysis is the explore-exploit dilemma: whether if we should just go for the best solution we have found at the moment, or look for a better one.",
  "keywords": [
    "âž•probabilistic-artificial-intelligence"
  ],
  "articleBody": "This note extends the content Markov Processes in this specific context.\nStandard notions Explore-exploit dilemma ðŸŸ© We have seen something similar also in Active Learning when we tried to model if we wanted to look elsewhere or go for the maximum value we have found. The dilemma under analysis is the explore-exploit dilemma: whether if we should just go for the best solution we have found at the moment, or look for a better one. This also has implications in many other fields, also in normal human life there are a lot of balances in these terms.\nIn the context of RL, if we exploit too much, we would get a sub-optimal solution. Instead, if we explore too much, we would get a low reward.\nTrajectories ðŸŸ© Trajectories can be considered our data in RL settings. Intuitively, trajectories are just sequences of states, action and rewards. So: $\\tau_{i} = \\left\\{ x_{i}, a_{i}, r_{i} , x_{i + 1}\\right\\}$ and a trajectory is $\\tau = \\left\\{ \\tau_{0}, \\dots, \\tau_{n} \\right\\}$ (the trajectory should be correctly linked, the end state of a single item $\\tau_{i}$ should be the start state of the other, but this is just a formalism).\nLearning the World Model ðŸŸ¨++ Given a trajectory, one simple way to estimate transitions and rewards is just the empirical mean:\n$$ P(x_{t + 1}\\mid x , a) = \\frac{\\text{Count}(x_{t + 1}, x_{t}, a)}{\\text{Count}(x_{t}, a)} $$ Which how often we see a sequence in the form $x_{t}, a, r, x_{t + 1}$ where $r$ is any reward, in the trajectories, and the$x, a$ are fixed values of interest. Also in this setting, we are using the Markovian assumption (See Markov Chains.\nWe would also like to learn the rewards $$ r(x, a) = \\frac{1}{N_{x, a}} \\sum_{t: x_{t} = x, a_{t}=a} r_{t} $$ Where $N_{x, a}$ are the total numbers of states in the trajectory where the start state is $x$ and the action is $a$. Simple as that.\nModel-based algorithms There is something more in the case of simple n-bandit, see N-Bandit Problem. Usually, these kinds of models are more sample efficient. (The model of the world acts acts as some sort of prior). Another positive thing is the online fashion: we can interact with the environment and learn. Model free methods are more suitable to cases where you just need to train and then deploy.\nGreedy Approaches ðŸŸ© This is a very simple algorithm, it has something in common with the Metropolis Hasting things we have studied in Monte Carlo Methods. We just choose a new action with probability $\\varepsilon$, else, we go for the best solution we have.\nGreedy solution ðŸŸ©â€“ La parte greedy prende solamente lâ€™azione che massimizza il valore atteso per lâ€™azione. Quindi $$ A_{t} = \\arg\\max_{a} Q_{t}(a) $$ Il problema Ã¨ che questo non esplora. Potrebbe trovare il minimo e restare in quello perchÃ© la sua stima Ã¨ quella.\nEpsilon greedy solution ðŸŸ© La epsilon greedy prova a risolvere questo, tenendo una probabilitÃ  di esplorare. Uniform $\\varepsilon$, e con questa probabilitÃ  si fa qualcosa random, e in questo modo si aggiorna il resto. $$ \\pi_{t}(a) = \\begin{cases} (1 - e) + \\frac{e}{|A|} \u0026 \\text{if } Q(a) = \\max_b Q(b) \\\\ \\\\ \\frac{\\varepsilon}{\\lvert \\mathcal{A} \\rvert } \u0026 \\text{otherwise} \\end{cases} $$ Questo dovrebbe essere lâ€™algoritmo utilizzato per Atari. Problema Ã¨ che continua ad esplorare anche se ha raggiunto convergenza buona della stima\nThe key problem of Ïµ-greedy is that it explores the state space in an uninformed manner. In other words, it explores ignoring all past experience. It thus does not eliminate clearly suboptimal actions. From the lecture notes\nConvergence of Greedy ðŸŸ¨â€“ If instead of keeping the sequence of $\\varepsilon$ fixed, we make it dependent on the timestep, and assume it satisfies the Robbins-Moro conditions (e.g. with $\\varepsilon_{t} = \\frac{\\lambda}{t}$, then it converges to the best solution. This is also why it does very well in practice.\nWe call Greedy in the limit with infinite exploration, meaning we are assuming we visit a state infinitely often.\nFormally this condition is valid if:\nWe explore every action pair infinitely often: $\\lim_{ n \\to \\infty } N_{t}(x, a) = +\\infty$ The policy converges to the greedy policy: $\\lim_{ n \\to \\infty } \\pi_{t}(a \\mid x) = \\mathbb{1}(a = \\arg\\max_{b} Q_{t}(b \\mid x))$ Softmax exploration ðŸŸ¨++ This is a generalization of the above greedy exploration. We just add a softmax and a temperature parameter that tells how to trade-off between exploration and exploitation: $$ \\pi_{t}(a \\mid x) = \\frac{e^{Q_{t}(x, a) / \\tau}}{\\sum_{b} e^{Q_{t}(x, b) / \\tau}} $$ $R_{\\max}$ Algorithm This is based on (Brafman \u0026 Tennenholz â€˜02).\nThis captures the gist of many techniques, but it is more of a theoretical algorithm. We are optimistic at the beginning, then we become more realistic (meaning our assumptions will coincide more with reality after some time).\nWe have these assumptions: if we donâ€™t know $r(x, a)$ we set it to the upper bound $R_{\\max}$ set at the beginning. If we donâ€™t know $P(x' \\mid x, a)$, we add another state that gets all the mass: $P(x^{*} \\mid x, a) = 1$. This is like a black hole state, you will stay there forever, and get a negative or null reward of $r(x^{*}, a) - R_{\\max}$.\nThe nice observation is that we donâ€™t need to distinguish between exploration and exploitation\nThe algorithm ðŸŸ¥+ Initially we assume that every transition leads to the fairy-tale state, and every reward is the best possible. We just use this temporarily as we know that in the end reality will be different, meaning the actual rewards of the MDP will be lower than the fairy-tale ones.\nFrom the lecture notes: We estimate the reward and transition probabilities as before, using the empirical observations. We need to precisely define the enough, which we will do in the next section.\nBounds on $R_{\\max}$ ðŸŸ¨- We use Hoeffdingâ€™s Inequality, presented in Central Limit Theorem and Law of Large Numbers. And we have: $$ \\begin{align} P(\\lvert R_{n} - \\mathbf{E}[R_{n}] \\rvert \\geq \\varepsilon) \u0026\\leq 2e^{-2N(a \\mid x)\\varepsilon^{2}/R_{max}^{2} } \\ \u0026\\implies \\delta \\leq 2e^{-2N(a \\mid x)\\varepsilon^{2}/R_{max}^{2} } \\ \u0026\\implies N(a \\mid x) \\geq \\frac{R_{\\max}^{2}}{2\\varepsilon^{2}} \\log \\frac{2}{\\delta}\n\\end{align} $$\nSo we need that amount of actions per state $x$ in order to be PAC sure of the reward we have there.\nTo get an estimate of the value of each state with $1 - \\delta$ confidence within an error of $\\varepsilon$. We assume $r(x, a) \\in \\left[ 0, R_{\\max} \\right]$\nThen we have another result that tells you that with probability $1 - \\delta$ $R_{\\max}$ will reach an $\\varepsilon$-optimal policy in a number of steps polynomial to the number of actions, states, times, and then free variables of the above bound. This bounds has some similarities with Provably Approximately Correct Learning.\nIn the paper, the authors have proved that an $\\varepsilon$ optimal policy is reached in polynomial time dependent on the number of states $\\lvert X \\rvert$, actions $\\lvert A \\rvert$, time, $\\varepsilon$, $:\\delta$, and $R_{max}$, similar to optimistic Q-learning that we will explore in model-free settings.\nAnalysis of the Computational Requirements Memory ðŸŸ© We need to keep each state and action in memory. For each state we need to store tuples $x_{t}, a, x_{t + 1}$ so the transition probabilities alone are in the order of $\\mathcal{O}(\\lvert S \\rvert^{2} \\lvert A \\rvert)$\nComputation Time ðŸŸ¨ We need to apply policy iteration and value iteration each time we have found a new transition probability, which is often costly. The trade-off is the sample-efficiency: these model based models are more efficient on the number of data needed to achieve a good policy, but infer in high computation costs. Furthermore, the update on R max is done at least for each state action pair, so we have at least $\\mathcal{O}(\\lvert S \\rvert \\lvert A \\rvert)$ solutions of the value or policy iteration, which is often quite costly.\nModel-free Algorithms With model free algorithms we donâ€™t have access to the underlying state-action-state distributions, thus we cannot do policy evaluation using the Bellman backup as in Markov Processes\nTemporal Difference Learning A Monte Carlo estimate ðŸŸ¨â€“ The idea is to run a trajectory starting from $x$ many times and obtaining a Monte Carlo estimate\n$$ V^{\\pi}(x) = \\lim_{ N \\to \\infty } \\frac{1}{N} \\sum_{i = 1}^{N} [r^{(i)} + \\gamma V^{\\pi}_{\\text{old}}(x^{(i)})] $$ Where $(r^{(i)}, x^{(i)}) \\sim P(x', R \\mid x, \\pi(x))$. But the problem in reality is that after have sampled a $x'$ we cannot go back. So the crude approximation is using a single observation as our approximation, which is $r + \\gamma V^{\\pi}_{\\text{old}}(x^{(i)})$. But the variance of the single estimate would be probably quite big, we would like to tamper the problem of this. So we can be more conservative and keep part of the previous estimate. This yields the rule of Temporal Difference Learning for model free value estimation:\nIn theory we can use a full estimate to approximate the value, but in practice is not possible due to the length of simulation that you would need to do. The expected reward $G_{0}$ is not available.\nBootstrapping the Value ðŸŸ©â€“ $$ V^{\\pi}(x) = (1 - \\alpha_{t}) V_{\\text{old}}^{\\pi}(x) + \\alpha_{t}(r + \\gamma V^{\\pi}_{\\text{old}}(x^{(i)}) $$ This is sort of a bootstrap estimate, the idea of using running averages for estimating the value of something. This probably introduces a bias, but I need to investigate this a little bit more.\nThis Algorithm is called temporal difference as we can write the above value equivalently as: $$ V^{\\pi}(x) = V_{\\text{old}}^{\\pi}(x) + \\alpha_{t}(r + \\gamma V^{\\pi}_{\\text{old}}(x^{(i)}) - V_{\\text{old}}^{\\pi}(x)) $$ Where now $\\alpha_{t}$ is now interpretable as a learning rate, and the inside can be seen as a TD error. We can also have here the same convergence bounds that we have used for the epsilon greedy case. This converges to the correct bounds.\nThis is called On-policy as it can continuously estimate the value function.\nBias Analysis of TD See https://claude.ai/chat/512eec65-2c84-4871-a973-de90b5757fab. TODO: write it in your notes. The gist is: if you start with an unbiased estimate, the monte carlo update will keep it unbiased, bu the Bootstrapped value using TD will not.\nConvergence of TD ðŸŸ¥ Jaakkola 1993. A quite similar convergence argument could be put forward for SARSA.\nGradient Update Interpretation See RL Function Approximation.\nSARSA On-policy version ðŸŸ© We can use TD-learning to do policy evaluation under model-free setting, we now need a way to estimate the best policy without knowing the transitions. Now SARSA comes into play. (now we use $s$ instead of $x$ for the state because it is more intuitive with the name).\nThe idea is exactly the same as before, we use bootstrapped samples to evaluate the $Q$ value: $$ Q^{\\pi}(s, a) = (1 - \\alpha_{t})Q^{\\pi}(s, a) + \\alpha_{t}(r + \\gamma Q^{\\pi}(s', a') ) $$ We now have observed two consecutive couples of states in the trajectory: $(x, a, r, x') \\to (x', a', r', x'')$ We can use the same convergence guarantee that we used for TD.\nFinding the optimal Policy After we have estimated these values, we can use the greedy policy $$ \\pi(x) = \\max_{a \\in \\mathcal{A}} Q^{\\pi_{\\text{old}}}(s, a) $$ But in practice this method does not explore enough and one needs to do some random exploration, as for the $\\varepsilon$ greedy method. In principle, on-policy is more general. This is exactly the bellman optimality condition presented in Markov Processes.\nDrawbacks of SARSA On-policy nature prevents the reuse of the sampled trajectories. (So one could argue that is not data efficient). In practice, the policy does not explore enough. This is partially offset by using epsilon exploration. Off-policy version ðŸŸ¨â€“ If instead of picking the policy via $\\pi$, we can run exactly the same algorithm and get the off-policy version. But now we need to assess the probability having any action. This allows us to estimate it from just a single state, instead of relying on the observed $a$\n$$ Q^{\\pi}(s, a) = (1 - \\alpha_{t})Q^{\\pi}(s, a) + \\alpha_{t}(r + \\gamma \\mathbb{E}_{a' \\sim \\pi(\\cdot \\mid x)}[Q^{\\pi}(x', a')] ) $$ But this is exactly the same.\nQ Learning The Q-Learning Algorithm ðŸŸ¨++ We can interpret the Q-learning algorithm as the analogous of Value Iteration in the model-free case. The convergence proof is always the same, with Robbins-Moro. Optimistic Q-Learning ðŸŸ¥ We can use the same idea of $R_{\\max}$ in the model-free case. We are optimistic and then, with some time, find better estimates of the values.\nWhere $V_{\\max} = R_{\\max} / (1 - \\gamma)$ as we would like to get an upper bound on the best value, not just maximum Reward.\nFor large Action Spaces Q learning is quite expensive (difficult to find the maximum of all possible actions). This is a common drawback for every tabular-based reinforcement-learning method.\nChoosing the bounds ðŸŸ¥ This section briefly argues how the optimistic bounds where chosen: Itâ€™s easy to see that $V_{max}$ chosen in this manner effectively upper bounds all possible values: $$ V_{max} = R_{max} + \\gamma R_{max} + \\dots = R_{max} \\sum_{i = 0}^{\\infty}\\gamma^{i} = R_{max} / (1 - \\gamma) \\geq \\max_{x, a} q(x, a) $$ Complexity Analysis ðŸŸ¥ If $n$ is the number of states $s$ and $m$ the number of actions We need to store states and actions so the space complexity is $\\mathcal{O}(n \\cdot m)$, while the computation time is just finding the maximum so the complexity is $\\mathcal{O}(m)$.\nIn the next section we will try to approximate the state value function, see RL Function Approximation.\n",
  "wordCount" : "2232",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/tabular-reinforcement-learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;Â»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Tabular Reinforcement Learning
    </h1>
    <div class="post-meta">11 min&nbsp;Â·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#standard-notions" aria-label="Standard notions">Standard notions</a><ul>
                        
                <li>
                    <a href="#explore-exploit-dilemma-" aria-label="Explore-exploit dilemma ðŸŸ©">Explore-exploit dilemma ðŸŸ©</a></li>
                <li>
                    <a href="#trajectories-" aria-label="Trajectories ðŸŸ©">Trajectories ðŸŸ©</a></li>
                <li>
                    <a href="#learning-the-world-model-" aria-label="Learning the World Model ðŸŸ¨&#43;&#43;">Learning the World Model ðŸŸ¨++</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#model-based-algorithms" aria-label="Model-based algorithms">Model-based algorithms</a><ul>
                        
                <li>
                    <a href="#greedy-approaches-" aria-label="Greedy Approaches ðŸŸ©">Greedy Approaches ðŸŸ©</a><ul>
                        
                <li>
                    <a href="#greedy-solution---" aria-label="Greedy solution ðŸŸ©&ndash;">Greedy solution ðŸŸ©&ndash;</a></li>
                <li>
                    <a href="#epsilon-greedy-solution-" aria-label="Epsilon greedy solution ðŸŸ©">Epsilon greedy solution ðŸŸ©</a></li>
                <li>
                    <a href="#convergence-of-greedy---" aria-label="Convergence of Greedy ðŸŸ¨&ndash;">Convergence of Greedy ðŸŸ¨&ndash;</a></li>
                <li>
                    <a href="#softmax-exploration-" aria-label="Softmax exploration ðŸŸ¨&#43;&#43;">Softmax exploration ðŸŸ¨++</a></li></ul>
                </li>
                <li>
                    <a href="#r_max-algorithm" aria-label="$R_{\max}$ Algorithm">$R_{\max}$ Algorithm</a><ul>
                        
                <li>
                    <a href="#the-algorithm-" aria-label="The algorithm ðŸŸ¥&#43;">The algorithm ðŸŸ¥+</a></li>
                <li>
                    <a href="#bounds-on-r_max--" aria-label="Bounds on $R_{\max}$ ðŸŸ¨-">Bounds on $R_{\max}$ ðŸŸ¨-</a></li></ul>
                </li>
                <li>
                    <a href="#analysis-of-the-computational-requirements" aria-label="Analysis of the Computational Requirements">Analysis of the Computational Requirements</a><ul>
                        
                <li>
                    <a href="#memory-" aria-label="Memory ðŸŸ©">Memory ðŸŸ©</a></li>
                <li>
                    <a href="#computation-time-" aria-label="Computation Time ðŸŸ¨">Computation Time ðŸŸ¨</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#model-free-algorithms" aria-label="Model-free Algorithms">Model-free Algorithms</a><ul>
                        
                <li>
                    <a href="#temporal-difference-learning" aria-label="Temporal Difference Learning">Temporal Difference Learning</a><ul>
                        
                <li>
                    <a href="#a-monte-carlo-estimate---" aria-label="A Monte Carlo estimate ðŸŸ¨&ndash;">A Monte Carlo estimate ðŸŸ¨&ndash;</a></li>
                <li>
                    <a href="#bootstrapping-the-value---" aria-label="Bootstrapping the Value ðŸŸ©&ndash;">Bootstrapping the Value ðŸŸ©&ndash;</a></li>
                <li>
                    <a href="#bias-analysis-of-td" aria-label="Bias Analysis of TD">Bias Analysis of TD</a></li>
                <li>
                    <a href="#convergence-of-td-" aria-label="Convergence of TD ðŸŸ¥">Convergence of TD ðŸŸ¥</a></li>
                <li>
                    <a href="#gradient-update-interpretation" aria-label="Gradient Update Interpretation">Gradient Update Interpretation</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#sarsa" aria-label="SARSA">SARSA</a><ul>
                        
                <li>
                    <a href="#on-policy-version-" aria-label="On-policy version ðŸŸ©">On-policy version ðŸŸ©</a><ul>
                        
                <li>
                    <a href="#finding-the-optimal-policy" aria-label="Finding the optimal Policy">Finding the optimal Policy</a></li>
                <li>
                    <a href="#drawbacks-of-sarsa" aria-label="Drawbacks of SARSA">Drawbacks of SARSA</a></li>
                <li>
                    <a href="#off-policy-version---" aria-label="Off-policy version ðŸŸ¨&ndash;">Off-policy version ðŸŸ¨&ndash;</a></li></ul>
                </li>
                <li>
                    <a href="#q-learning" aria-label="Q Learning">Q Learning</a><ul>
                        
                <li>
                    <a href="#the-q-learning-algorithm-" aria-label="The Q-Learning Algorithm ðŸŸ¨&#43;&#43;">The Q-Learning Algorithm ðŸŸ¨++</a></li>
                <li>
                    <a href="#optimistic-q-learning-" aria-label="Optimistic Q-Learning ðŸŸ¥">Optimistic Q-Learning ðŸŸ¥</a></li>
                <li>
                    <a href="#choosing-the-bounds-" aria-label="Choosing the bounds ðŸŸ¥">Choosing the bounds ðŸŸ¥</a></li>
                <li>
                    <a href="#complexity-analysis-" aria-label="Complexity Analysis ðŸŸ¥">Complexity Analysis ðŸŸ¥</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>This note extends the content <a href="/notes/markov-processes/">Markov Processes</a> in this specific context.</p>
<h3 id="standard-notions">Standard notions<a hidden class="anchor" aria-hidden="true" href="#standard-notions">#</a></h3>
<h4 id="explore-exploit-dilemma-">Explore-exploit dilemma ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#explore-exploit-dilemma-">#</a></h4>
<p>We have seen something similar also in <a href="/notes/active-learning/">Active Learning</a> when we tried to model if we wanted to look elsewhere or go for the maximum value we have found.
The dilemma under analysis is the <strong>explore-exploit</strong> dilemma: whether if we should just go for the best solution we have found at the moment, or look for a better one.
This also has implications in many other fields, also in normal human life there are a lot of balances in these terms.</p>
<p>In the context of RL, if we exploit too much, we would get a sub-optimal solution. Instead, if we explore too much, we would get a low reward.</p>
<h4 id="trajectories-">Trajectories ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#trajectories-">#</a></h4>
<p>Trajectories can be considered our <strong>data</strong> in RL settings.
Intuitively, trajectories are just sequences of states, action and rewards.
So: $\tau_{i} = \left\{ x_{i}, a_{i}, r_{i} , x_{i + 1}\right\}$ and a trajectory is $\tau = \left\{ \tau_{0}, \dots, \tau_{n} \right\}$ (the trajectory should be correctly linked, the end state of a single item $\tau_{i}$ should be the start state of the other, but this is just a formalism).</p>
<h4 id="learning-the-world-model-">Learning the World Model ðŸŸ¨++<a hidden class="anchor" aria-hidden="true" href="#learning-the-world-model-">#</a></h4>
<p>Given a trajectory, one simple way to estimate transitions and rewards is just the empirical mean:</p>
$$
P(x_{t + 1}\mid x , a) = \frac{\text{Count}(x_{t + 1}, x_{t}, a)}{\text{Count}(x_{t}, a)}
$$
<p>
Which how often we see a sequence in the form $x_{t}, a, r, x_{t + 1}$ where $r$ is any reward, in the trajectories, and the$x, a$ are fixed values of interest. Also in this setting, we are using the <em>Markovian assumption</em> (See <a href="/notes/markov-chains/">Markov Chains</a>.</p>
<p>We would also like to learn the rewards
</p>
$$
r(x, a) = \frac{1}{N_{x, a}} \sum_{t: x_{t} = x, a_{t}=a} r_{t}
$$
<p>
Where $N_{x, a}$ are the total numbers of states in the trajectory where the start state is $x$ and the action is $a$.
Simple as that.</p>
<h2 id="model-based-algorithms">Model-based algorithms<a hidden class="anchor" aria-hidden="true" href="#model-based-algorithms">#</a></h2>
<p>There is something more in the case of simple n-bandit, see N-Bandit Problem.
Usually, these kinds of models are more sample efficient. (The model of the world acts acts as some sort of prior). Another positive thing is the online fashion: we can interact with the environment and learn.
Model free methods are more suitable to cases where you just need to train and then deploy.</p>
<h3 id="greedy-approaches-">Greedy Approaches ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#greedy-approaches-">#</a></h3>
<p>This is a very simple algorithm, it has something in common with the Metropolis Hasting things we have studied in <a href="/notes/monte-carlo-methods/">Monte Carlo Methods</a>.
We just choose a new action with probability $\varepsilon$, else, we go for the best solution we have.</p>
<h4 id="greedy-solution---">Greedy solution ðŸŸ©&ndash;<a hidden class="anchor" aria-hidden="true" href="#greedy-solution---">#</a></h4>
<p>La parte greedy prende solamente l&rsquo;azione che massimizza il valore atteso per l&rsquo;azione.
Quindi
</p>
$$
A_{t} = \arg\max_{a} Q_{t}(a)
$$
<p>
Il problema Ã¨ che questo non esplora. Potrebbe trovare il minimo e restare in quello perchÃ© la sua stima Ã¨ quella.</p>
<h4 id="epsilon-greedy-solution-">Epsilon greedy solution ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#epsilon-greedy-solution-">#</a></h4>
<p>La epsilon greedy prova a risolvere questo, tenendo una probabilitÃ  di esplorare.
Uniform $\varepsilon$, e con questa probabilitÃ  si fa qualcosa random, e in questo modo si aggiorna il resto.
</p>
$$
\pi_{t}(a) = 
\begin{cases} (1 - e) + \frac{e}{|A|} & \text{if } Q(a) = \max_b Q(b)  \\
\\ \frac{\varepsilon}{\lvert \mathcal{A} \rvert } & \text{otherwise} \end{cases}
$$
<p>
Questo dovrebbe essere l&rsquo;algoritmo utilizzato per Atari.
Problema Ã¨ che <strong>continua ad esplorare anche se ha raggiunto convergenza buona della stima</strong></p>
<blockquote>
<p>The key problem of Ïµ-greedy is that it
explores the state space in an uninformed manner. In other words, it
explores ignoring all past experience. It thus does not eliminate clearly
suboptimal actions. <em>From the lecture notes</em></p>
</blockquote>
<h4 id="convergence-of-greedy---">Convergence of Greedy ðŸŸ¨&ndash;<a hidden class="anchor" aria-hidden="true" href="#convergence-of-greedy---">#</a></h4>
<p>If instead of keeping the sequence of $\varepsilon$ fixed, we make it dependent on the timestep, and assume it satisfies the Robbins-Moro conditions (e.g. with $\varepsilon_{t} = \frac{\lambda}{t}$, then it converges to the best solution. This is also why it does very well in practice.</p>
<p>We call <em>Greedy in the limit with infinite exploration</em>, meaning we are assuming we visit a state infinitely often.</p>
<p>Formally this condition is valid if:</p>
<ol>
<li>We explore every action pair infinitely often: $\lim_{ n \to \infty } N_{t}(x, a) = +\infty$</li>
<li>The policy converges to the greedy policy: $\lim_{ n \to \infty } \pi_{t}(a \mid x) = \mathbb{1}(a = \arg\max_{b} Q_{t}(b \mid x))$</li>
</ol>
<h4 id="softmax-exploration-">Softmax exploration ðŸŸ¨++<a hidden class="anchor" aria-hidden="true" href="#softmax-exploration-">#</a></h4>
<p>This is a generalization of the above greedy exploration. We just add a softmax and a temperature parameter that tells how to trade-off between exploration and exploitation:
</p>
$$
\pi_{t}(a \mid x) = \frac{e^{Q_{t}(x, a) / \tau}}{\sum_{b} e^{Q_{t}(x, b) / \tau}}
$$
<h3 id="r_max-algorithm">$R_{\max}$ Algorithm<a hidden class="anchor" aria-hidden="true" href="#r_max-algorithm">#</a></h3>
<p>This is based on <a href="https://www.jmlr.org/papers/volume3/brafman02a/brafman02a.pdf">(Brafman &amp; Tennenholz â€˜02)</a>.</p>
<p>This captures the gist of many techniques, but it is more of a theoretical algorithm.
We are <strong>optimistic</strong> at the beginning, then we become more realistic (meaning our assumptions will coincide more with reality after some time).</p>
<p>We have these assumptions:
if we don&rsquo;t know $r(x, a)$ we set it to the upper bound $R_{\max}$ set at the beginning.
If we don&rsquo;t know $P(x' \mid x, a)$, we add another state that gets all the mass: $P(x^{*} \mid x, a) = 1$. This is like a black hole state, you will stay there forever, and get a negative or null reward of $r(x^{*}, a) - R_{\max}$.</p>
<p>The nice observation is that we <strong>don&rsquo;t need to distinguish between exploration and exploitation</strong></p>
<h4 id="the-algorithm-">The algorithm ðŸŸ¥+<a hidden class="anchor" aria-hidden="true" href="#the-algorithm-">#</a></h4>
<p>Initially we assume that every transition leads to the <em>fairy-tale</em> state, and every reward is the best possible. We just use this temporarily as we know that in the end reality will be different, meaning the actual rewards of the MDP will be lower than the fairy-tale ones.</p>
<p>From the lecture notes:
<img src="/images/notes/Tabular Reinforcement Learning-20241122105624625.webp" alt="Tabular Reinforcement Learning-20241122105624625"></p>
<p>We estimate the reward and transition probabilities as before, using the empirical observations.
We need to precisely define the <em>enough</em>, which we will do in the next section.</p>
<h4 id="bounds-on-r_max--">Bounds on $R_{\max}$ ðŸŸ¨-<a hidden class="anchor" aria-hidden="true" href="#bounds-on-r_max--">#</a></h4>
<p>We use Hoeffding&rsquo;s Inequality, presented in <a href="/notes/central-limit-theorem-and-law-of-large-numbers/">Central Limit Theorem and Law of Large Numbers</a>.
And we have:
$$
\begin{align}
P(\lvert R_{n} - \mathbf{E}[R_{n}] \rvert  \geq \varepsilon) &amp;\leq 2e^{-2N(a \mid x)\varepsilon^{2}/R_{max}^{2} }
\
&amp;\implies \delta \leq 2e^{-2N(a \mid x)\varepsilon^{2}/R_{max}^{2} }  \
&amp;\implies N(a \mid x) \geq \frac{R_{\max}^{2}}{2\varepsilon^{2}} \log \frac{2}{\delta}</p>
<p>\end{align}
$$</p>
<p>So we need that amount of actions per state $x$ in order to be PAC sure of the reward we have there.</p>
<p>To get an estimate of the value of each state with $1 - \delta$ confidence within an error of $\varepsilon$.
We assume $r(x, a) \in \left[ 0, R_{\max} \right]$</p>
<p>Then we have another result that tells you that with probability $1 - \delta$ $R_{\max}$ will reach an $\varepsilon$-optimal policy in a number of steps polynomial to the number of actions, states, times, and then free variables of the above bound.
This bounds has some similarities with <a href="/notes/provably-approximately-correct-learning/">Provably Approximately Correct Learning</a>.</p>
<p>In the paper, the authors have proved that an $\varepsilon$ optimal policy is reached in polynomial time dependent on the number of states $\lvert X \rvert$, actions $\lvert A \rvert$, time, $\varepsilon$, $:\delta$, and $R_{max}$, similar to optimistic Q-learning that we will explore in model-free settings.</p>
<h3 id="analysis-of-the-computational-requirements">Analysis of the Computational Requirements<a hidden class="anchor" aria-hidden="true" href="#analysis-of-the-computational-requirements">#</a></h3>
<h4 id="memory-">Memory ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#memory-">#</a></h4>
<p>We need to keep each state and action in memory. For each state we need to store tuples $x_{t}, a, x_{t + 1}$ so the transition probabilities alone are in the order of $\mathcal{O}(\lvert S \rvert^{2} \lvert A \rvert)$</p>
<h4 id="computation-time-">Computation Time ðŸŸ¨<a hidden class="anchor" aria-hidden="true" href="#computation-time-">#</a></h4>
<p>We need to apply policy iteration and value iteration each time we have found a new transition probability, which is often costly.
The trade-off is the <strong>sample-efficiency</strong>: these model based models are more efficient on the number of data needed to achieve a good policy, but infer in high computation costs.
Furthermore, the update on R max is done at least for each state action pair, so we have at least $\mathcal{O}(\lvert S \rvert \lvert A \rvert)$ solutions of the value or policy iteration, which is often quite costly.</p>
<h2 id="model-free-algorithms">Model-free Algorithms<a hidden class="anchor" aria-hidden="true" href="#model-free-algorithms">#</a></h2>
<p>With model free algorithms we don&rsquo;t have access to the underlying state-action-state distributions, thus we cannot do policy evaluation using the Bellman backup as in <a href="/notes/markov-processes/">Markov Processes</a></p>
<h3 id="temporal-difference-learning">Temporal Difference Learning<a hidden class="anchor" aria-hidden="true" href="#temporal-difference-learning">#</a></h3>
<h4 id="a-monte-carlo-estimate---">A Monte Carlo estimate ðŸŸ¨&ndash;<a hidden class="anchor" aria-hidden="true" href="#a-monte-carlo-estimate---">#</a></h4>
<p>The idea is to run a trajectory starting from $x$ many times and obtaining a Monte Carlo estimate</p>
$$
V^{\pi}(x) = \lim_{ N \to \infty } \frac{1}{N} \sum_{i = 1}^{N} [r^{(i)} + \gamma V^{\pi}_{\text{old}}(x^{(i)})]
$$
<p>
Where $(r^{(i)}, x^{(i)}) \sim P(x', R \mid x, \pi(x))$. But the problem in reality is that after have sampled a $x'$ we cannot go back. So the crude approximation is using a single observation as our approximation, which is $r + \gamma V^{\pi}_{\text{old}}(x^{(i)})$.
But the variance of the single estimate would be probably quite big, we would like to tamper the problem of this. So we can be more conservative and keep part of the previous estimate. This yields the rule of Temporal Difference Learning for model free value estimation:</p>
<p>In theory we can use a full estimate to approximate the value, but in practice is not possible due to the length of simulation that you would need to do. The expected reward $G_{0}$ is not available.</p>
<h4 id="bootstrapping-the-value---">Bootstrapping the Value ðŸŸ©&ndash;<a hidden class="anchor" aria-hidden="true" href="#bootstrapping-the-value---">#</a></h4>
$$
V^{\pi}(x) = (1 - \alpha_{t}) V_{\text{old}}^{\pi}(x) + \alpha_{t}(r + \gamma V^{\pi}_{\text{old}}(x^{(i)})
$$
<p>
This is sort of a <strong>bootstrap</strong> estimate, the idea of using running averages for estimating the value of something. This probably introduces a bias, but I need to investigate this a little bit more.</p>
<p>This Algorithm is called <strong>temporal difference</strong> as we can write the above value equivalently as:
</p>
$$
V^{\pi}(x) =  V_{\text{old}}^{\pi}(x) + \alpha_{t}(r + \gamma V^{\pi}_{\text{old}}(x^{(i)}) -  V_{\text{old}}^{\pi}(x))
$$
<p>
Where now $\alpha_{t}$ is now interpretable as a <em>learning rate</em>, and the inside can be seen as a <strong>TD error</strong>. We can also have here the same convergence bounds that we have used for the epsilon greedy case. This converges to the correct bounds.</p>
<p>This is called <strong>On-policy</strong> as it can continuously estimate the value function.</p>
<h4 id="bias-analysis-of-td">Bias Analysis of TD<a hidden class="anchor" aria-hidden="true" href="#bias-analysis-of-td">#</a></h4>
<p>See <a href="https://claude.ai/chat/512eec65-2c84-4871-a973-de90b5757fab.">https://claude.ai/chat/512eec65-2c84-4871-a973-de90b5757fab.</a>
TODO: write it in your notes.
The gist is: if you start with an unbiased estimate, the monte carlo update will keep it unbiased, bu the Bootstrapped value using TD will not.</p>
<h4 id="convergence-of-td-">Convergence of TD ðŸŸ¥<a hidden class="anchor" aria-hidden="true" href="#convergence-of-td-">#</a></h4>
<p>Jaakkola 1993.
A quite similar convergence argument could be put forward for SARSA.</p>
<h4 id="gradient-update-interpretation">Gradient Update Interpretation<a hidden class="anchor" aria-hidden="true" href="#gradient-update-interpretation">#</a></h4>
<p>See <a href="/notes/rl-function-approximation/">RL Function Approximation</a>.</p>
<h2 id="sarsa">SARSA<a hidden class="anchor" aria-hidden="true" href="#sarsa">#</a></h2>
<h3 id="on-policy-version-">On-policy version ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#on-policy-version-">#</a></h3>
<p>We can use TD-learning to do policy evaluation under model-free setting, we now need a way to estimate the best policy without knowing the transitions.
Now SARSA comes into play. (now we use $s$ instead of $x$ for the state because it is more intuitive with the name).</p>
<p>The idea is exactly the same as before, we use bootstrapped samples to evaluate the $Q$ value:
</p>
$$
Q^{\pi}(s, a) = (1 - \alpha_{t})Q^{\pi}(s, a) + \alpha_{t}(r + \gamma Q^{\pi}(s', a') )
$$
<p>
We now have observed two consecutive couples of states in the trajectory: $(x, a, r, x') \to (x', a', r', x'')$
We can use the same convergence guarantee that we used for TD.</p>
<h4 id="finding-the-optimal-policy">Finding the optimal Policy<a hidden class="anchor" aria-hidden="true" href="#finding-the-optimal-policy">#</a></h4>
<p>After we have estimated these values, we can use the greedy policy
</p>
$$
\pi(x) = \max_{a \in \mathcal{A}} Q^{\pi_{\text{old}}}(s, a)
$$
<p>
But in practice this method does not explore enough and one needs to do some random exploration, as for the $\varepsilon$ greedy method.
In principle, on-policy is more general. This is exactly the bellman optimality condition presented in <a href="/notes/markov-processes/">Markov Processes</a>.</p>
<h4 id="drawbacks-of-sarsa">Drawbacks of SARSA<a hidden class="anchor" aria-hidden="true" href="#drawbacks-of-sarsa">#</a></h4>
<ul>
<li>On-policy nature prevents the reuse of the sampled trajectories. (So one could argue that is not data efficient).</li>
<li>In practice, the policy does not explore enough.
<ul>
<li>This is partially offset by using epsilon exploration.</li>
</ul>
</li>
</ul>
<h4 id="off-policy-version---">Off-policy version ðŸŸ¨&ndash;<a hidden class="anchor" aria-hidden="true" href="#off-policy-version---">#</a></h4>
<p>If instead of picking the policy via $\pi$, we can run exactly the same algorithm and get the off-policy version.
But now we need to assess the probability having any action.
This allows us to estimate it from just a single state, instead of relying on the observed $a$</p>
$$
Q^{\pi}(s, a) = (1 - \alpha_{t})Q^{\pi}(s, a) + \alpha_{t}(r + \gamma \mathbb{E}_{a' \sim \pi(\cdot \mid x)}[Q^{\pi}(x', a')] )
$$
<p>But this is exactly the same.</p>
<h3 id="q-learning">Q Learning<a hidden class="anchor" aria-hidden="true" href="#q-learning">#</a></h3>
<h4 id="the-q-learning-algorithm-">The Q-Learning Algorithm ðŸŸ¨++<a hidden class="anchor" aria-hidden="true" href="#the-q-learning-algorithm-">#</a></h4>
<img src="/images/notes/Tabular Reinforcement Learning-20241122133442934.webp" alt="Tabular Reinforcement Learning-20241122133442934">
We can interpret the Q-learning algorithm as the analogous of Value Iteration in the model-free case.
The convergence proof is always the same, with Robbins-Moro.
<h4 id="optimistic-q-learning-">Optimistic Q-Learning ðŸŸ¥<a hidden class="anchor" aria-hidden="true" href="#optimistic-q-learning-">#</a></h4>
<p>We can use the same idea of $R_{\max}$ in the model-free case. We are optimistic and then, with some time, find better estimates of the values.</p>
<p>Where $V_{\max} = R_{\max} / (1 - \gamma)$ as we would like to get an upper bound on the best value, not just maximum Reward.</p>
<img src="/images/notes/Tabular Reinforcement Learning-20241122133919124.webp" alt="Tabular Reinforcement Learning-20241122133919124">
<p>For large Action Spaces Q learning is quite expensive (difficult to find the maximum of all possible actions).
This is a common drawback for every tabular-based reinforcement-learning method.</p>
<h4 id="choosing-the-bounds-">Choosing the bounds ðŸŸ¥<a hidden class="anchor" aria-hidden="true" href="#choosing-the-bounds-">#</a></h4>
<p>This section briefly argues how the optimistic bounds where chosen:
It&rsquo;s easy to see that $V_{max}$ chosen in this manner effectively upper bounds all possible values:
</p>
$$
V_{max} = R_{max} + \gamma R_{max} + \dots = R_{max} \sum_{i = 0}^{\infty}\gamma^{i} = R_{max} / (1 - \gamma) \geq \max_{x, a} q(x, a)
$$
<h4 id="complexity-analysis-">Complexity Analysis ðŸŸ¥<a hidden class="anchor" aria-hidden="true" href="#complexity-analysis-">#</a></h4>
<p>If $n$ is the number of states $s$ and $m$ the number of actions
We need to store states and actions so the space complexity is $\mathcal{O}(n \cdot m)$, while the computation time is just finding the maximum so the complexity is $\mathcal{O}(m)$.</p>
<p>In the next section we will try to approximate the state value function, see <a href="/notes/rl-function-approximation/">RL Function Approximation</a>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">âž•Probabilistic-Artificial-Intelligence</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tabular Reinforcement Learning on x"
            href="https://x.com/intent/tweet/?text=Tabular%20Reinforcement%20Learning&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2ftabular-reinforcement-learning%2f&amp;hashtags=%e2%9e%95probabilistic-artificial-intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tabular Reinforcement Learning on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2ftabular-reinforcement-learning%2f&amp;title=Tabular%20Reinforcement%20Learning&amp;summary=Tabular%20Reinforcement%20Learning&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2ftabular-reinforcement-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tabular Reinforcement Learning on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2ftabular-reinforcement-learning%2f&title=Tabular%20Reinforcement%20Learning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tabular Reinforcement Learning on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2ftabular-reinforcement-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tabular Reinforcement Learning on whatsapp"
            href="https://api.whatsapp.com/send?text=Tabular%20Reinforcement%20Learning%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2ftabular-reinforcement-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tabular Reinforcement Learning on telegram"
            href="https://telegram.me/share/url?text=Tabular%20Reinforcement%20Learning&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2ftabular-reinforcement-learning%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tabular Reinforcement Learning on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Tabular%20Reinforcement%20Learning&u=https%3a%2f%2fflecart.github.io%2fnotes%2ftabular-reinforcement-learning%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
