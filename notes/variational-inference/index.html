<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Variational Inference | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="➕probabilistic-artificial-intelligence, machinelearning">
<meta name="description" content="$$
p(\theta \mid x_{1:n}, y_{1:n}) = \frac{1}{z} p(y_{1:n} \mid \theta, x_{1:n}) p(\theta \mid x_{1:n}) \approx q(\theta \mid \lambda)
$$For Bayesian Linear Regression we had high dimensional Gaussians which made the inference closed form, in general this is not true, so we need some kinds of approximation.
Laplace approximation
Introduction to the Idea
$$
\psi(\theta) \approx \hat{\psi}(\theta) = \psi(\hat{\theta}) &#43; (\theta-\hat{\theta} ) ^{T} \nabla \psi(\hat{\theta}) &#43; \frac{1}{2} (\theta-\hat{\theta} ) ^{T} H_{\psi}(\hat{\theta})(\theta-\hat{\theta} ) = \psi(\hat{\theta}) &#43; \frac{1}{2} (\theta-\hat{\theta} ) ^{T} H_{\psi}(\hat{\theta})(\theta-\hat{\theta} ) 
$$
We simplified the term on the first order because we are considering the mode, so the gradient should be zero for the stationary point.">
<meta name="author" content="
By Xuanqiang Angelo Huang">
<link rel="canonical" href="https://flecart.github.io/notes/variational-inference/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f790d9af969c56c079c1ce2d5972a04486bf3d6144295d5fba319830e1e55a7a.css" integrity="sha256-95DZr5acVsB5wc4tWXKgRIa/PWFEKV1fujGYMOHlWno=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/variational-inference/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/variational-inference/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Variational Inference">
  <meta property="og:description" content="$$ p(\theta \mid x_{1:n}, y_{1:n}) = \frac{1}{z} p(y_{1:n} \mid \theta, x_{1:n}) p(\theta \mid x_{1:n}) \approx q(\theta \mid \lambda) $$For Bayesian Linear Regression we had high dimensional Gaussians which made the inference closed form, in general this is not true, so we need some kinds of approximation.
Laplace approximation Introduction to the Idea $$ \psi(\theta) \approx \hat{\psi}(\theta) = \psi(\hat{\theta}) &#43; (\theta-\hat{\theta} ) ^{T} \nabla \psi(\hat{\theta}) &#43; \frac{1}{2} (\theta-\hat{\theta} ) ^{T} H_{\psi}(\hat{\theta})(\theta-\hat{\theta} ) = \psi(\hat{\theta}) &#43; \frac{1}{2} (\theta-\hat{\theta} ) ^{T} H_{\psi}(\hat{\theta})(\theta-\hat{\theta} ) $$ We simplified the term on the first order because we are considering the mode, so the gradient should be zero for the stationary point.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2025-01-15T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-15T00:00:00+00:00">
    <meta property="article:tag" content="➕Probabilistic-Artificial-Intelligence">
    <meta property="article:tag" content="Machinelearning">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Variational Inference">
<meta name="twitter:description" content="$$
p(\theta \mid x_{1:n}, y_{1:n}) = \frac{1}{z} p(y_{1:n} \mid \theta, x_{1:n}) p(\theta \mid x_{1:n}) \approx q(\theta \mid \lambda)
$$For Bayesian Linear Regression we had high dimensional Gaussians which made the inference closed form, in general this is not true, so we need some kinds of approximation.
Laplace approximation
Introduction to the Idea
$$
\psi(\theta) \approx \hat{\psi}(\theta) = \psi(\hat{\theta}) &#43; (\theta-\hat{\theta} ) ^{T} \nabla \psi(\hat{\theta}) &#43; \frac{1}{2} (\theta-\hat{\theta} ) ^{T} H_{\psi}(\hat{\theta})(\theta-\hat{\theta} ) = \psi(\hat{\theta}) &#43; \frac{1}{2} (\theta-\hat{\theta} ) ^{T} H_{\psi}(\hat{\theta})(\theta-\hat{\theta} ) 
$$
We simplified the term on the first order because we are considering the mode, so the gradient should be zero for the stationary point.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Variational Inference",
      "item": "https://flecart.github.io/notes/variational-inference/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Variational Inference",
  "name": "Variational Inference",
  "description": "$$ p(\\theta \\mid x_{1:n}, y_{1:n}) = \\frac{1}{z} p(y_{1:n} \\mid \\theta, x_{1:n}) p(\\theta \\mid x_{1:n}) \\approx q(\\theta \\mid \\lambda) $$For Bayesian Linear Regression we had high dimensional Gaussians which made the inference closed form, in general this is not true, so we need some kinds of approximation.\nLaplace approximation Introduction to the Idea $$ \\psi(\\theta) \\approx \\hat{\\psi}(\\theta) = \\psi(\\hat{\\theta}) + (\\theta-\\hat{\\theta} ) ^{T} \\nabla \\psi(\\hat{\\theta}) + \\frac{1}{2} (\\theta-\\hat{\\theta} ) ^{T} H_{\\psi}(\\hat{\\theta})(\\theta-\\hat{\\theta} ) = \\psi(\\hat{\\theta}) + \\frac{1}{2} (\\theta-\\hat{\\theta} ) ^{T} H_{\\psi}(\\hat{\\theta})(\\theta-\\hat{\\theta} ) $$ We simplified the term on the first order because we are considering the mode, so the gradient should be zero for the stationary point.\n",
  "keywords": [
    "➕probabilistic-artificial-intelligence", "machinelearning"
  ],
  "articleBody": "$$ p(\\theta \\mid x_{1:n}, y_{1:n}) = \\frac{1}{z} p(y_{1:n} \\mid \\theta, x_{1:n}) p(\\theta \\mid x_{1:n}) \\approx q(\\theta \\mid \\lambda) $$For Bayesian Linear Regression we had high dimensional Gaussians which made the inference closed form, in general this is not true, so we need some kinds of approximation.\nLaplace approximation Introduction to the Idea $$ \\psi(\\theta) \\approx \\hat{\\psi}(\\theta) = \\psi(\\hat{\\theta}) + (\\theta-\\hat{\\theta} ) ^{T} \\nabla \\psi(\\hat{\\theta}) + \\frac{1}{2} (\\theta-\\hat{\\theta} ) ^{T} H_{\\psi}(\\hat{\\theta})(\\theta-\\hat{\\theta} ) = \\psi(\\hat{\\theta}) + \\frac{1}{2} (\\theta-\\hat{\\theta} ) ^{T} H_{\\psi}(\\hat{\\theta})(\\theta-\\hat{\\theta} ) $$ We simplified the term on the first order because we are considering the mode, so the gradient should be zero for the stationary point.\n$$ \\hat{\\psi}(\\theta) =\\log \\mathcal{N}(\\theta; \\hat{\\theta}, -H_{\\psi}^{-1}) + const $$ Then we choose $q(\\theta)= \\mathcal{N}(\\theta; \\hat{\\theta}, -H_{\\psi}^{-1}) \\propto \\exp(\\hat{\\psi}(\\theta))$ One can verify that the hessian in the covariance matrix is indeed symmetric semidefinite positive. (the positive definiteveness comes from the minus of a negative semidefinite matrix of the maximum point). Normally, the covariance matrix of the Laplace approximation is indicated as $\\Lambda = - H^{-1}_{\\psi}$.\nLaplace approximation for a Gaussian We can easily see that the Laplace approximation for a Gaussian is the same as the Gaussian itself, because the Hessian is the inverse of the covariance matrix.\n$$ D_{\\theta}D_{\\theta}\\log p(\\theta) = (D_{\\theta}(\\Sigma^{-1}\\mu - \\Sigma^{-1} \\theta))^{T} = -\\Sigma^{-1} $$An analysis of Bayesian Logistic regression Setting $$ p(y_{1:n} \\mid w, x_{1:n}) = \\prod_{i = 1}^{N} \\sigma(y_{i}w^{T}x_{i}) $$ And that $p(w) = \\mathcal{N}(w; 0, \\sigma^{2}_{p}I)$, where $\\sigma$ is the Sigmoid function $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$.\n$$ p(y_{i} \\mid w, x_{1:n}) = \\begin{cases} \\sigma(w^{T}x) \u0026 \\text{ if } y = 1 \\\\ 1 - \\sigma(w^{T}x) \u0026 \\text{ if } y = 0 \\end{cases} \\implies \\sigma(y_{i}w^{T}x) $$ Thanks to the properties of the Sigmoid function $1 - \\sigma(x) = \\sigma(-x)$\nMAP Weight update We then see that in order to have the proxy distribution for $w$ we need to find the Hessian of that We have that (we omit $x$ for brevity). $$ \\hat{w} = \\arg \\max_{w} p(w \\mid y_{1:n}) = \\arg \\max_{w} \\log p(w) + \\log p(y_{1:n} \\mid w) = \\arg \\max_{w} \\frac{1}{2\\sigma^{2}{p}} w^{T}w + \\sum{i = 1}^{N} \\log \\sigma(y_{i}w^{T}x_{i})\n$$ Continuing:\n$$ = \\arg \\min_{w} - \\frac{1}{2\\sigma^{2}_{p}}w^{T}w + \\sum_{i = 1}^{N} \\underbrace{ \\log(1 + \\exp(- y_{i}w^{T}x_{i})) }_{\\log p(y_{i} \\mid w)} \\implies \\frac{ \\partial \\log p(y_{1:n} \\mid w) }{ \\partial w } = -\\sum_{i = 1}^{N}y_{i}x_{i} \\sigma( - y_{i}w^{T}x_{i}) $$$$ w \\leftarrow w(1 - 2c\\eta) + \\eta yx\\sigma(-y_{i}w^{T}x_{i}) $$Laplace approximation for Bayesian logistic regression $$ \\Lambda = -\\nabla_{w} \\nabla_{w} \\log p(w \\mid y_{1:n} x_{1:n}) = X^{T}\\text{diag}_{i \\in [n]} \\left\\{ \\pi_{i} (1 - \\pi_{i}) \\right\\} X + \\sigma^{-2}_{p} I $$ With $\\pi_{i} = \\sigma(w^{T}x_{i})$. There is a nice interpretation of this covariance matrix, because if the data point is certain then the diagonal value is small, else it is a little bit larger.\nDownsides of Laplace approximation Laplace could be overconfident, in the same way linear regression is for some samples. This is because it greedily approximates basing on the mean of it. So perhaps one direction is ok, other directions can be quite bad.\nFigure from Krause Book\nPrediction with Variational Posterior $$ p(y^{*} \\mid x^{*}, x_{1:n}, y_{1:n}) = \\int p(y^{*} \\mid x^{*}, \\theta) p(\\theta \\mid x_{1:n} y_{1:n}) \\, d\\theta \\approx \\int p(y^{*} \\mid x^{*}, \\theta) q_{\\lambda}(\\theta) \\, d\\theta = \\mathbb{E}_{\\theta \\sim q_{\\lambda}} [p(y^{*} \\mid x^{*}, \\theta)] $$$$ \\mathbb{E}_{\\theta \\sim q} [p(y^{*} \\mid x^{*}, \\theta)] \\approx \\frac{1}{m} \\sum_{j = 1}^{m} p(y^{*} \\mid x^{*}, \\theta) $$$$ \\int p(y^{*} \\mid x^{*}, \\theta) q(\\theta) \\, d\\theta= \\int \\sigma (y^{*} \\cdot \\theta^{T}x)q(\\theta) \\, d\\theta = \\int \\sigma (y^{*} \\cdot f)(x^{*}q(\\theta)) \\, df = \\int \\sigma(y^{*} f) \\mathcal{N}(f; x^{*T}\\mu, x^{*T}\\Sigma x^{*}) \\, df $$ As this is just a single dimensional integral, it could be well approximated with numerical methods like Gauss-Legendre quadrature.\nThe variational approach With this approach we define a family of distributions $Q$ called variational family and the try to find the best member within this family to approximate our $P$.\nFor example we can define the variational family of Gaussians as\n$$ \\mathcal{Q} = \\left\\{ q(\\theta \\mid \\lambda) = \\mathcal{N}(\\theta; \\mu, \\Sigma \\right\\} $$ Where the parameters $\\lambda$ are the mean and the covariance matrix of the Gaussian.\nReverse and Forward KL So we optimize for this: Reverse KL:\n$$ q^{*} \\in \\arg \\min_{q \\in Q} KL(q \\mid \\mid p) $$ $$ q^{*} \\in \\arg \\min_{q \\in Q} KL(p \\mid \\mid q) $$See Kullback-Leibler divergence in Entropy.\nKL of Gaussians Understanding the properties of the KL divergence between Gaussian distributions is crucial for grasping variational inference. I strongly encourage you to review Gaussians#Information Theoretic Properties closely, probably we will use that result in the next observations\nMinimizing Forward KL Divergence We can provide two interpretations of the forward KL divergence. One is a MLE estimation of the dataset in the variational family, the other is a moment matching.\nMLE Estimation We can prove the following: $$ \\begin{align} \\arg\\min_{\\lambda} KL(p \\mid \\mid q) \u0026 = \\arg\\min_{\\lambda} H[p \\mid \\mid q_{\\lambda}] - H[p]\\ \u0026 = \\arg\\min_{\\lambda} \\mathbb{E}{\\theta \\sim p} [-\\log q{\\lambda}(y)] -\\text{ const} \\ \u0026 \\approx \\arg\\min_{\\lambda}- \\frac{1}{m} \\sum_{j = 1}^{m} \\log q_{\\lambda}(y^{(i)})\\ \\end{align}\n$$\nThe problem is that it is often unfeasible.\nThis tells us that any maximum likelihood estimate $q_{\\lambda}$ minimizes the forward KL-divergence to the empirical data distribution.\nThe first interpretation is usually not used as we cannot draw samples using the true distribution.\nMoment Matching We can prove that if we find such $q$ that minimizes the forward KL divergence, then the first two moments of the distribution are the same.\n$$ q(\\theta) = \\exp(\\lambda^{T}T(\\theta) - A(\\lambda)) $$ Where $T(\\theta)$ are the sufficient statistics of the distribution and $A(\\lambda)$ is the log partition function.\n$$ \\begin{align} q^{*} = \\arg\\max_{\\lambda}KL(p \\mid \\mid q) \u0026= \\arg\\max_{\\lambda}\\mathbb{E}_{\\theta \\sim p} \\left[ \\log \\frac{p(\\theta)}{q(\\theta)} \\right] \\\\ \u0026= \\arg\\min_{\\lambda}\\mathbb{E}_{\\theta \\sim p} \\left[\\lambda^{T}T(\\theta) - A(\\lambda) \\right] \\\\ \\end{align} $$ Recall that $A(\\lambda) = \\log \\int \\exp(\\lambda^{T}T(\\theta)) \\, d\\theta$.\n$$ \\begin{align} 0 \u0026= \\nabla_{\\lambda} \\mathbb{E}_{\\theta \\sim p} \\left[\\lambda^{T}T(\\theta) - A(\\lambda) \\right] \\\\ \u0026= \\mathbb{E}_{\\theta \\sim p} \\left[T(\\theta) \\right] - \\nabla_{\\lambda} A(\\lambda) \\\\ \u0026= \\mathbb{E}_{\\theta \\sim p} \\left[T(\\theta) \\right] - \\nabla_{\\lambda} \\log \\int \\exp(\\lambda^{T}T(\\theta)) \\, d\\theta \\\\ \u0026= \\mathbb{E}_{\\theta \\sim p} \\left[T(\\theta) \\right] - \\mathbb{E}_{\\theta \\sim q} \\left[T(\\theta) \\right] \\\\ \u0026\\implies \\mathbb{E}_{\\theta \\sim p} \\left[T(\\theta) \\right] = \\mathbb{E}_{\\theta \\sim q} \\left[T(\\theta) \\right] \\end{align} $$ Which means the expected value of the sufficient statistics of the true distribution are the same as the variational one. For Gaussians, we have that the mean and the covariance matrix are the same as those are the sufficient statistics.\nMinimizing Reverse KL Divergence We need to compute the value\n$$ \\begin{align}\nq^{}{\\lambda} \u0026= \\arg\\min{q \\in Q} KL(q \\mid \\mid p(\\cdot \\mid y)) \\ \u0026= \\arg\\min_{q \\in Q}\\mathbb{E}{\\theta \\sim q} \\left[ \\frac{\\log q(\\theta)}{\\log p(\\theta \\mid y)} \\right] \\ \u0026= \\arg\\min{q \\in Q} - H(q) - \\mathbb{E}{\\theta \\sim q}[\\log p(\\theta \\mid y)] \\ \u0026= \\arg\\max{q \\in Q} H(q) + \\mathbb{E}{\\theta \\sim q}[\\log p(\\theta \\mid y)] \\ \u0026= \\arg\\max{q \\in Q} H(q) + \\mathbb{E}{\\theta \\sim q}[\\log p(\\theta)] + \\mathbb{E}{\\theta \\sim q} [\\log p (y \\mid \\theta)]+const \\ \u0026\\implies q^{}{\\lambda }= \\arg\\max{q \\in Q} \\mathbb{E}_{\\theta \\sim q} [\\log p(y \\mid \\theta)] - KL(q \\mid \\mid p(\\theta)) = ELBO \\end{align} $$ Where we have now the prior what we were trying to optimize. We can interpret the first part as sort of likelihood while the second part is a proximity measure that acts as a regularizer. This cost function is known as the variational free energy principle (highly put forward by Hinton). This optimizer has also some links with the minimum description length principle, cited in Randomness, Model of Analogies, Clustering.\nEvidence Lower Bound $$ L(q, p; D_{n}) = \\log p(y_{1:n} \\mid x_{1:n}) - KL(q \\mid \\mid p(\\cdot \\mid x_{1:n}, y_{1:n})) \\implies \\log p(y_{1:n}) \\geq L(q, p; D_{n}) $$The evidence is defines a $\\log p(y_{1:n})$. This can be played with to have a lower bound on this value with the above knowledge. We can prove the lower bound in another way, noticing that: $$ \\begin{align} \\log p(y_{1:n}) \u0026= \\log \\int p(y_{1:n} \\mid \\theta) p(\\theta) , d\\theta \\ \u0026= \\log \\int p(y_{1:n} \\mid \\theta) p(\\theta) \\frac{q(\\theta)}{q(\\theta)} , d\\theta \\ \\text{using Jensen}\u0026\\geq{E}{\\theta \\sim q} \\left[ \\log p(y{1:n} \\mid \\theta) \\frac{p(\\theta)}{q(\\theta)} \\right] \\ \u0026 ={E}{\\theta \\sim q} \\left[ \\log p(y{1:n} \\mid \\theta) \\right] - KL(q \\mid \\mid p) = ELBO \\\n\\end{align} $$ We observe that this is the same thing that we have seen before while trying to minimize the reverse KL divergence. Thus, finding the best approximation is equivalent to identifying the maximum lower bound on the evidence. Essentially, it involves determining the posterior that is most likely to explain the evidence.\nThis indicates that maximizing the evidence lower bound is an adequate method of model selection which can be used instead of maximizing the evidence (marginal likelihood) directly.\nELBO of logistic regression Suppose we have a classic logistic regression with prior on the weights to be $p(w) = \\mathcal{N}(w; 0, I)$ and the likelihood to be $p(y \\mid w, x) = \\prod_{i = 1}^{N} \\sigma(y_{i}w^{T}x_{i})$. Let’s take the parameters from a Gaussian distribution $\\mathcal{N}(\\mu, \\text{diag}_{i \\in [d]}\\left\\{ \\sigma_{i}^{2} \\right\\})$, our variational family. We want now to find the ELBO for this model.\n$$ KL(q \\mid \\mid p) = \\frac{1}{2} \\sum_{i = 1}^{d} \\left( \\sigma_{i}^{2} + \\mu_{i}^{2} - \\log \\sigma_{i}^{2} - 1 \\right) $$$$ \\mathbb{E}_{w \\sim q} \\left[ \\log p(y \\mid w, x) \\right] = \\sum_{i = 1}^{N} \\mathbb{E}_{w \\sim q} \\left[ \\log \\sigma(y_{i}w^{T}x_{i}) \\right] $$ And we can use everything now.\nGradient of ELBO One possible approach is using the score function trick, explored in RL Function Approximation. Also called score gradients or monte-carlo gradients. Another is using the so called reparametrization trick (also used for VAE in Autoencoders). We can compute the gradient of the ELBO with respect to the variational parameters $\\lambda$. But the first parameter is difficult to compute.\n$$ \\mathbb{E}_{\\theta \\sim q} [f(\\theta)] = \\mathbb{E}_{\\varepsilon \\sim \\phi} [f(g(\\varepsilon, \\lambda))] $$ So now we can use a known distribution to sample from and then compute the gradient of the expectation, this allows to compute stochastic gradients. If you know the ideas of Volume, then the formula of change of variables becomes very clean.\nLet’s work out an example: Let’s suppose $q$ follows a Gaussian distribution $\\mathcal{N}(\\mu, \\Sigma)$ then we can reparametrize it with $\\theta = \\mu + \\Sigma^{1/2} \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, I)$ that we can sample from.\n$$ \\begin{align} \\\\ \\nabla_{\\lambda} L(\\lambda) = \\nabla_{\\lambda} \\mathbb{E}_{\\theta \\sim q} [ \\log p(y \\mid x, \\theta) ] - KL(q \\mid \\mid p) = \\\\ \\nabla_{\\lambda} \\mathbb{E}_{\\varepsilon \\sim \\phi} [ \\log p(y \\mid x, \\Sigma^{1/2}\\varepsilon + \\mu ) ] - KL(q \\mid \\mid p) = \\\\ \\end{align} $$$$ \\begin{align} \\nabla_{\\lambda} \\mathbb{E}_{\\theta \\sim q} [ \\log p(y_{1:n} \\mid x_{1:n}, \\theta) ] \u0026= \\nabla_{\\lambda} \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, 1)} [ \\log p(y_{1:n} \\mid x_{1:n}, \\Sigma^{1/2}\\varepsilon + \\mu ) ] \\\\ \u0026 = n \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, 1)} \\mathbb{E}_{i \\sim \\text{unif}[0, n]}[\\nabla_{\\lambda} \\log p(y_{i} \\mid x_{i}, \\Sigma^{1/2}\\varepsilon + \\mu ) ]\\\\ \u0026 \\approx \\frac{n}{m} \\sum_{j = 1}^{m} \\nabla_{C, \\mu} \\log p(y_{i_{j}} \\mid x_{i_{j}}, C\\varepsilon_{j} + \\mu) \\\\ \\end{align} $$ Which means: We can approximate the derivate of the likelihood with respect of the variational parameters by sampling from a normal and uniformly from the dataset and only then computing an average gradient of the likelihood with respect to the parameters.\nVariational inference enables us to find approximations of distributions using highly optimized stochastic optimization techniques. However, a significant drawback is the difficulty in assessing the quality of these approximations.\n",
  "wordCount" : "1900",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "2025-01-15T00:00:00Z",
  "dateModified": "2025-01-15T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang Angelo Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/variational-inference/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Variational Inference
    </h1>
    <div class="post-meta"><span title='2025-01-15 00:00:00 +0000 UTC'>January 15, 2025</span>&nbsp;·&nbsp;Reading Time: 9 minutes&nbsp;·&nbsp;
By Xuanqiang Angelo Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#laplace-approximation" aria-label="Laplace approximation">Laplace approximation</a><ul>
                        <ul>
                        
                <li>
                    <a href="#introduction-to-the-idea" aria-label="Introduction to the Idea">Introduction to the Idea</a></li>
                <li>
                    <a href="#laplace-approximation-for-a-gaussian" aria-label="Laplace approximation for a Gaussian">Laplace approximation for a Gaussian</a></li></ul>
                    
                <li>
                    <a href="#an-analysis-of-bayesian-logistic-regression" aria-label="An analysis of Bayesian Logistic regression">An analysis of Bayesian Logistic regression</a><ul>
                        
                <li>
                    <a href="#setting" aria-label="Setting">Setting</a></li>
                <li>
                    <a href="#map-weight-update" aria-label="MAP Weight update">MAP Weight update</a></li>
                <li>
                    <a href="#laplace-approximation-for-bayesian-logistic-regression" aria-label="Laplace approximation for Bayesian logistic regression">Laplace approximation for Bayesian logistic regression</a></li></ul>
                </li>
                <li>
                    <a href="#downsides-of-laplace-approximation" aria-label="Downsides of Laplace approximation">Downsides of Laplace approximation</a></li>
                <li>
                    <a href="#prediction-with-variational-posterior" aria-label="Prediction with Variational Posterior">Prediction with Variational Posterior</a></li></ul>
                </li>
                <li>
                    <a href="#the-variational-approach" aria-label="The variational approach">The variational approach</a><ul>
                        <ul>
                        
                <li>
                    <a href="#reverse-and-forward-kl" aria-label="Reverse and Forward KL">Reverse and Forward KL</a></li>
                <li>
                    <a href="#kl-of-gaussians" aria-label="KL of Gaussians">KL of Gaussians</a></li>
                <li>
                    <a href="#minimizing-forward-kl-divergence" aria-label="Minimizing Forward KL Divergence">Minimizing Forward KL Divergence</a><ul>
                        
                <li>
                    <a href="#mle-estimation" aria-label="MLE Estimation">MLE Estimation</a></li>
                <li>
                    <a href="#moment-matching" aria-label="Moment Matching">Moment Matching</a></li></ul>
                </li>
                <li>
                    <a href="#minimizing-reverse-kl-divergence" aria-label="Minimizing Reverse KL Divergence">Minimizing Reverse KL Divergence</a></li>
                <li>
                    <a href="#evidence-lower-bound" aria-label="Evidence Lower Bound">Evidence Lower Bound</a></li>
                <li>
                    <a href="#elbo-of-logistic-regression" aria-label="ELBO of logistic regression">ELBO of logistic regression</a></li>
                <li>
                    <a href="#gradient-of-elbo" aria-label="Gradient of ELBO">Gradient of ELBO</a>
                </li>
            </ul>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content">$$
p(\theta \mid x_{1:n}, y_{1:n}) = \frac{1}{z} p(y_{1:n} \mid \theta, x_{1:n}) p(\theta \mid x_{1:n}) \approx q(\theta \mid \lambda)
$$<p>For <a href="/notes/bayesian-linear-regression">Bayesian Linear Regression</a> we had high dimensional <a href="/notes/gaussians">Gaussians</a> which made the inference <em>closed form</em>, in general this is not true, so we need some kinds of approximation.</p>
<h2 id="laplace-approximation">Laplace approximation<a hidden class="anchor" aria-hidden="true" href="#laplace-approximation">#</a></h2>
<h4 id="introduction-to-the-idea">Introduction to the Idea<a hidden class="anchor" aria-hidden="true" href="#introduction-to-the-idea">#</a></h4>
$$
\psi(\theta) \approx \hat{\psi}(\theta) = \psi(\hat{\theta}) + (\theta-\hat{\theta} ) ^{T} \nabla \psi(\hat{\theta}) + \frac{1}{2} (\theta-\hat{\theta} ) ^{T} H_{\psi}(\hat{\theta})(\theta-\hat{\theta} ) = \psi(\hat{\theta}) + \frac{1}{2} (\theta-\hat{\theta} ) ^{T} H_{\psi}(\hat{\theta})(\theta-\hat{\theta} ) 
$$<p>
We simplified the term on the first order because we are considering the mode, so the gradient should be zero for the stationary point.</p>
$$
\hat{\psi}(\theta) =\log \mathcal{N}(\theta; \hat{\theta}, -H_{\psi}^{-1}) + const
$$<p>
Then we choose $q(\theta)= \mathcal{N}(\theta; \hat{\theta}, -H_{\psi}^{-1}) \propto \exp(\hat{\psi}(\theta))$
One can verify that the hessian in the covariance matrix is indeed symmetric semidefinite positive. (the positive definiteveness comes from the minus of a negative semidefinite matrix of the maximum point).
Normally, the covariance matrix of the Laplace approximation is indicated as $\Lambda = - H^{-1}_{\psi}$.</p>
<h4 id="laplace-approximation-for-a-gaussian">Laplace approximation for a Gaussian<a hidden class="anchor" aria-hidden="true" href="#laplace-approximation-for-a-gaussian">#</a></h4>
<p>We can easily see that the Laplace approximation for a Gaussian is the same as the Gaussian itself, because the Hessian is the inverse of the covariance matrix.</p>
$$
D_{\theta}D_{\theta}\log p(\theta) = (D_{\theta}(\Sigma^{-1}\mu - \Sigma^{-1} \theta))^{T} = -\Sigma^{-1}
$$<h3 id="an-analysis-of-bayesian-logistic-regression">An analysis of Bayesian Logistic regression<a hidden class="anchor" aria-hidden="true" href="#an-analysis-of-bayesian-logistic-regression">#</a></h3>
<h4 id="setting">Setting<a hidden class="anchor" aria-hidden="true" href="#setting">#</a></h4>
$$
p(y_{1:n} \mid w, x_{1:n}) = \prod_{i = 1}^{N} \sigma(y_{i}w^{T}x_{i})
$$<p>
And that $p(w) = \mathcal{N}(w; 0, \sigma^{2}_{p}I)$, where $\sigma$ is the Sigmoid function $\sigma(x) = \frac{1}{1 + \exp(-x)}$.</p>
$$
p(y_{i} \mid w, x_{1:n}) = \begin{cases}
\sigma(w^{T}x)   & \text{ if } y = 1 \\
1 - \sigma(w^{T}x) & \text{ if } y = 0
\end{cases} 
\implies \sigma(y_{i}w^{T}x)
$$<p>
Thanks to the properties of the Sigmoid function $1 - \sigma(x) = \sigma(-x)$</p>
<h4 id="map-weight-update">MAP Weight update<a hidden class="anchor" aria-hidden="true" href="#map-weight-update">#</a></h4>
<p>We then see that in order to have the proxy distribution for $w$  we need to find the Hessian of that
We have that (we omit $x$ for brevity).
$$
\hat{w} = \arg \max_{w} p(w \mid y_{1:n}) = \arg \max_{w} \log p(w) + \log p(y_{1:n} \mid w)
= \arg \max_{w} \frac{1}{2\sigma^{2}<em>{p}} w^{T}w + \sum</em>{i = 1}^{N} \log \sigma(y_{i}w^{T}x_{i})</p>
<p>$$
Continuing:</p>
$$
= \arg \min_{w} - \frac{1}{2\sigma^{2}_{p}}w^{T}w  + \sum_{i = 1}^{N} \underbrace{ \log(1 + \exp(- y_{i}w^{T}x_{i})) }_{\log p(y_{i} \mid w)} \implies
\frac{ \partial \log p(y_{1:n} \mid w) }{ \partial w }  = -\sum_{i = 1}^{N}y_{i}x_{i} \sigma( -  y_{i}w^{T}x_{i})
$$$$
w \leftarrow w(1 - 2c\eta) + \eta yx\sigma(-y_{i}w^{T}x_{i})
$$<h4 id="laplace-approximation-for-bayesian-logistic-regression">Laplace approximation for Bayesian logistic regression<a hidden class="anchor" aria-hidden="true" href="#laplace-approximation-for-bayesian-logistic-regression">#</a></h4>
$$
\Lambda = -\nabla_{w} \nabla_{w} \log p(w \mid y_{1:n} x_{1:n}) = X^{T}\text{diag}_{i \in [n]} \left\{ \pi_{i} (1 - \pi_{i}) \right\} X + \sigma^{-2}_{p} I
$$<p>
With $\pi_{i} = \sigma(w^{T}x_{i})$. There is a nice interpretation of this covariance matrix, because if the data point is certain then the diagonal value is small, else it is a little bit larger.</p>
<h3 id="downsides-of-laplace-approximation">Downsides of Laplace approximation<a hidden class="anchor" aria-hidden="true" href="#downsides-of-laplace-approximation">#</a></h3>
<p>Laplace could be <strong>overconfident</strong>, in the same way <a href="/notes/linear-regression-methods">linear regression</a> is for some samples.
This is because it <em>greedily</em> approximates basing on the mean of it.
So perhaps one direction is ok, other directions can be quite bad.</p>
<figure class="center">
<img src="/images/notes/Variational Inference-20241220105902872.webp" style="width: 100%"   alt="Variational Inference-20241220105902872" title="Variational Inference-20241220105902872"/>
<figcaption><p style="text-align:center;">Figure from Krause Book</p></figcaption>
</figure>
<h3 id="prediction-with-variational-posterior">Prediction with Variational Posterior<a hidden class="anchor" aria-hidden="true" href="#prediction-with-variational-posterior">#</a></h3>
$$
p(y^{*} \mid x^{*}, x_{1:n}, y_{1:n})
= \int p(y^{*} \mid x^{*}, \theta) p(\theta \mid x_{1:n} y_{1:n}) \, d\theta \approx
\int p(y^{*} \mid x^{*}, \theta) q_{\lambda}(\theta) \, d\theta
= \mathbb{E}_{\theta \sim q_{\lambda}} [p(y^{*} \mid x^{*}, \theta)]
$$$$
\mathbb{E}_{\theta \sim q} [p(y^{*} \mid x^{*}, \theta)]
\approx \frac{1}{m} \sum_{j = 1}^{m} p(y^{*} \mid x^{*}, \theta)
$$$$
\int p(y^{*} \mid x^{*}, \theta) q(\theta) \, d\theta=
\int \sigma (y^{*} \cdot \theta^{T}x)q(\theta) \, d\theta
= \int \sigma (y^{*} \cdot f)(x^{*}q(\theta)) \, df
= \int \sigma(y^{*} f) \mathcal{N}(f; x^{*T}\mu, x^{*T}\Sigma x^{*}) \, df
$$<p>
As this is just a single dimensional integral, it could be well approximated with numerical methods like Gauss-Legendre quadrature.</p>
<h2 id="the-variational-approach">The variational approach<a hidden class="anchor" aria-hidden="true" href="#the-variational-approach">#</a></h2>
<p>With this approach we define a family of distributions $Q$ called <strong>variational family</strong> and the try to find the best member within this family to approximate our $P$.</p>
<p>For example we can define the variational family of Gaussians as</p>
$$
\mathcal{Q} = \left\{ q(\theta \mid \lambda) = \mathcal{N}(\theta; \mu, \Sigma
\right\} 
$$<p>
Where the parameters $\lambda$ are the mean and the covariance matrix of the Gaussian.</p>
<h4 id="reverse-and-forward-kl">Reverse and Forward KL<a hidden class="anchor" aria-hidden="true" href="#reverse-and-forward-kl">#</a></h4>
<p>So we optimize for this:
<strong>Reverse KL:</strong></p>
<blockquote>
$$
q^{*} \in \arg \min_{q \in Q} KL(q \mid \mid p)
$$</blockquote>
$$
q^{*} \in \arg \min_{q \in Q} KL(p \mid \mid q)
$$<p>See Kullback-Leibler divergence in <a href="/notes/entropy">Entropy</a>.</p>
<h4 id="kl-of-gaussians">KL of Gaussians<a hidden class="anchor" aria-hidden="true" href="#kl-of-gaussians">#</a></h4>
<p>Understanding the properties of the KL divergence between Gaussian distributions is crucial for grasping variational inference. I strongly encourage you to review <a href="/notes/gaussians#information-theoretic-properties">Gaussians#Information Theoretic Properties</a> closely, probably we will use that result in the next observations</p>
<h4 id="minimizing-forward-kl-divergence">Minimizing Forward KL Divergence<a hidden class="anchor" aria-hidden="true" href="#minimizing-forward-kl-divergence">#</a></h4>
<p>We can provide two interpretations of the forward KL divergence. One is a MLE estimation of the dataset in the variational family, the other is a moment matching.</p>
<h5 id="mle-estimation">MLE Estimation<a hidden class="anchor" aria-hidden="true" href="#mle-estimation">#</a></h5>
<p>We can prove the following:
$$
\begin{align}
\arg\min_{\lambda} KL(p \mid \mid q)  &amp; = \arg\min_{\lambda} H[p \mid \mid q_{\lambda}] - H[p]\
&amp; = \arg\min_{\lambda} \mathbb{E}<em>{\theta \sim p} [-\log q</em>{\lambda}(y)] -\text{ const} \
&amp; \approx \arg\min_{\lambda}- \frac{1}{m} \sum_{j = 1}^{m} \log q_{\lambda}(y^{(i)})\
\end{align}</p>
<p>$$</p>
<p>The problem is that it is often unfeasible.</p>
<blockquote>
<p>This tells us that any maximum likelihood estimate $q_{\lambda}$ minimizes the
forward KL-divergence to the empirical data distribution.</p></blockquote>
<p>The first interpretation is usually not used as we cannot draw samples using the true distribution.</p>
<h5 id="moment-matching">Moment Matching<a hidden class="anchor" aria-hidden="true" href="#moment-matching">#</a></h5>
<p>We can prove that if we find such $q$ that minimizes the forward KL divergence, then the first two moments of the distribution are the same.</p>
$$
q(\theta) = \exp(\lambda^{T}T(\theta) - A(\lambda))
$$<p>
Where $T(\theta)$ are the sufficient statistics of the distribution and $A(\lambda)$ is the log partition function.</p>
$$
\begin{align}
q^{*} = \arg\max_{\lambda}KL(p \mid \mid q) &=  \arg\max_{\lambda}\mathbb{E}_{\theta \sim p} \left[ \log \frac{p(\theta)}{q(\theta)} \right]  \\
&=  \arg\min_{\lambda}\mathbb{E}_{\theta \sim p} \left[\lambda^{T}T(\theta) - A(\lambda) \right] \\
\end{align}
$$<p>
Recall that $A(\lambda) = \log \int \exp(\lambda^{T}T(\theta)) \, d\theta$.</p>
$$
\begin{align}
0 &= \nabla_{\lambda} \mathbb{E}_{\theta \sim p} \left[\lambda^{T}T(\theta) - A(\lambda) \right]  \\
&= \mathbb{E}_{\theta \sim p} \left[T(\theta) \right] - \nabla_{\lambda} A(\lambda)  \\
&= \mathbb{E}_{\theta \sim p} \left[T(\theta) \right] - \nabla_{\lambda} \log \int \exp(\lambda^{T}T(\theta)) \, d\theta  \\
&= \mathbb{E}_{\theta \sim p} \left[T(\theta) \right] - \mathbb{E}_{\theta \sim q} \left[T(\theta) \right]  \\
&\implies \mathbb{E}_{\theta \sim p} \left[T(\theta) \right] = \mathbb{E}_{\theta \sim q} \left[T(\theta) \right]
\end{align}
$$<p>
Which means the expected value of the sufficient statistics of the true distribution are the same as the variational one.
For Gaussians, we have that the mean and the covariance matrix are the same as those are the sufficient statistics.</p>
<h4 id="minimizing-reverse-kl-divergence">Minimizing Reverse KL Divergence<a hidden class="anchor" aria-hidden="true" href="#minimizing-reverse-kl-divergence">#</a></h4>
<p>We need to compute the value</p>
<p>$$
\begin{align}</p>
<p>q^{<em>}<em>{\lambda} &amp;= \arg\min</em>{q \in Q} KL(q \mid \mid p(\cdot \mid y)) \
&amp;= \arg\min_{q \in Q}\mathbb{E}<em>{\theta \sim q} \left[  \frac{\log q(\theta)}{\log p(\theta \mid y)} \right] \
&amp;= \arg\min</em>{q \in Q} - H(q) - \mathbb{E}<em>{\theta \sim q}[\log p(\theta \mid y)] \
&amp;= \arg\max</em>{q \in Q} H(q) +  \mathbb{E}<em>{\theta \sim q}[\log p(\theta \mid y)]  \
&amp;= \arg\max</em>{q \in Q} H(q) +  \mathbb{E}<em>{\theta \sim q}[\log p(\theta)] + \mathbb{E}</em>{\theta \sim q} [\log p (y \mid \theta)]+const \
&amp;\implies  q^{</em>}<em>{\lambda }= \arg\max</em>{q \in Q} \mathbb{E}_{\theta \sim q} [\log p(y \mid  \theta)] - KL(q \mid \mid p(\theta)) = ELBO
\end{align}
$$
Where we have now the prior what we were trying to optimize.
We can interpret the first part as sort of <strong>likelihood</strong> while the second part is a <strong>proximity</strong> measure that acts as a regularizer. This cost function is known as the <strong>variational free energy</strong> principle (highly put forward by Hinton).
This optimizer has also some links with the minimum description length principle, cited in <a href="/notes/randomness">Randomness</a>, <a href="/notes/model-of-analogies">Model of Analogies</a>, <a href="/notes/clustering">Clustering</a>.</p>
<h4 id="evidence-lower-bound">Evidence Lower Bound<a hidden class="anchor" aria-hidden="true" href="#evidence-lower-bound">#</a></h4>
$$
L(q, p; D_{n}) = \log p(y_{1:n} \mid x_{1:n}) - KL(q \mid \mid p(\cdot \mid x_{1:n}, y_{1:n})) \implies \log p(y_{1:n}) \geq L(q, p; D_{n})
$$<p>The evidence is defines a $\log p(y_{1:n})$. This can be played with to have a lower bound on this value with the above knowledge. We can prove the lower bound in another way, noticing that:
$$
\begin{align}
\log p(y_{1:n}) &amp;=  \log \int p(y_{1:n} \mid \theta) p(\theta) , d\theta \
&amp;= \log \int p(y_{1:n} \mid \theta) p(\theta) \frac{q(\theta)}{q(\theta)} , d\theta  \
\text{using Jensen}&amp;\geq{E}<em>{\theta \sim q} \left[ \log p(y</em>{1:n} \mid \theta) \frac{p(\theta)}{q(\theta)} \right] \
&amp; ={E}<em>{\theta \sim q} \left[ \log p(y</em>{1:n} \mid \theta) \right] - KL(q \mid \mid p) = ELBO \</p>
<p>\end{align}
$$
We observe that this is the same thing that we have seen before while trying to minimize the reverse KL divergence. Thus, finding the best approximation is equivalent to identifying the maximum lower bound on the evidence. Essentially, it involves determining the posterior that is most likely to explain the evidence.</p>
<blockquote>
<p>This indicates that maximizing the evidence lower bound is an adequate method of model selection which can be used instead of maximizing the evidence (marginal likelihood) directly.</p></blockquote>
<h4 id="elbo-of-logistic-regression">ELBO of logistic regression<a hidden class="anchor" aria-hidden="true" href="#elbo-of-logistic-regression">#</a></h4>
<p>Suppose we have a classic logistic regression with prior on the weights to be $p(w) = \mathcal{N}(w; 0, I)$ and the likelihood to be $p(y \mid w, x) = \prod_{i = 1}^{N} \sigma(y_{i}w^{T}x_{i})$. Let&rsquo;s take the parameters from a Gaussian distribution $\mathcal{N}(\mu, \text{diag}_{i \in [d]}\left\{ \sigma_{i}^{2} \right\})$, our variational family.
We want now to find the ELBO for this model.</p>
$$
KL(q \mid \mid p) = \frac{1}{2} \sum_{i = 1}^{d} \left( \sigma_{i}^{2} + \mu_{i}^{2} - \log \sigma_{i}^{2} - 1 \right)
$$$$
\mathbb{E}_{w \sim q} \left[ \log p(y \mid w, x) \right] = \sum_{i = 1}^{N} \mathbb{E}_{w \sim q} \left[ \log \sigma(y_{i}w^{T}x_{i}) \right]
$$<p>
And we can use everything now.</p>
<h4 id="gradient-of-elbo">Gradient of ELBO<a hidden class="anchor" aria-hidden="true" href="#gradient-of-elbo">#</a></h4>
<p>One possible approach is using the score function trick, explored in <a href="/notes/rl-function-approximation">RL Function Approximation</a>. Also called score gradients or monte-carlo gradients.
Another is using the so called <strong>reparametrization trick</strong> (also used for VAE in <a href="/notes/autoencoders">Autoencoders</a>).
We can compute the gradient of the ELBO with respect to the variational parameters $\lambda$. But the first parameter is difficult to compute.</p>
$$
\mathbb{E}_{\theta \sim q} [f(\theta)] = \mathbb{E}_{\varepsilon \sim \phi} [f(g(\varepsilon, \lambda))]
$$<p>
So now we can use a known distribution to sample from and then compute the gradient of the expectation, this allows to compute <em>stochastic gradients</em>. If you know the ideas of Volume, then the formula of change of variables becomes very clean.</p>
<p>Let&rsquo;s work out an example:
Let&rsquo;s suppose $q$ follows a Gaussian distribution $\mathcal{N}(\mu, \Sigma)$ then we can reparametrize it with $\theta = \mu + \Sigma^{1/2} \varepsilon$ with $\varepsilon \sim \mathcal{N}(0, I)$ that we can sample from.</p>
$$
\begin{align} \\
\nabla_{\lambda} L(\lambda) = \nabla_{\lambda} \mathbb{E}_{\theta \sim q} [ \log p(y \mid x, \theta) ] - KL(q \mid \mid p) = \\
\nabla_{\lambda} \mathbb{E}_{\varepsilon \sim \phi} [ \log p(y \mid x, \Sigma^{1/2}\varepsilon + \mu ) ] - KL(q \mid \mid p) = \\
\end{align}
$$$$
\begin{align} 
\nabla_{\lambda} \mathbb{E}_{\theta \sim q} [ \log p(y_{1:n} \mid x_{1:n}, \theta) ] &= \nabla_{\lambda} \mathbb{E}_{\varepsilon \sim \mathcal{N}(0, 1)} [ \log p(y_{1:n} \mid x_{1:n}, \Sigma^{1/2}\varepsilon + \mu ) ]   \\
& = n \mathbb{E}_{\varepsilon \sim \mathcal{N}(0, 1)} \mathbb{E}_{i \sim \text{unif}[0, n]}[\nabla_{\lambda}  \log p(y_{i} \mid x_{i}, \Sigma^{1/2}\varepsilon + \mu ) ]\\ 
& \approx  \frac{n}{m} \sum_{j = 1}^{m} \nabla_{C, \mu} \log p(y_{i_{j}} \mid x_{i_{j}}, C\varepsilon_{j} + \mu) \\
\end{align}
$$<p>
Which means:
We can approximate the derivate of the likelihood with respect of the variational parameters by sampling from a normal and uniformly from the dataset and only then computing an average gradient of the likelihood with respect to the parameters.</p>
<p>Variational inference enables us to find <strong>approximations</strong> of distributions using highly optimized stochastic optimization techniques. However, a significant drawback is the difficulty in assessing the quality of these approximations.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">➕Probabilistic-Artificial-Intelligence</a></li>
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Variational Inference on x"
            href="https://x.com/intent/tweet/?text=Variational%20Inference&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fvariational-inference%2f&amp;hashtags=%e2%9e%95probabilistic-artificial-intelligence%2cmachinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Variational Inference on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fvariational-inference%2f&amp;title=Variational%20Inference&amp;summary=Variational%20Inference&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fvariational-inference%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Variational Inference on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fvariational-inference%2f&title=Variational%20Inference">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Variational Inference on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fvariational-inference%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Variational Inference on whatsapp"
            href="https://api.whatsapp.com/send?text=Variational%20Inference%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fvariational-inference%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Variational Inference on telegram"
            href="https://telegram.me/share/url?text=Variational%20Inference&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fvariational-inference%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Variational Inference on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Variational%20Inference&u=https%3a%2f%2fflecart.github.io%2fnotes%2fvariational-inference%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
