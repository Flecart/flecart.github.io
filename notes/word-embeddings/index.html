<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Word Embeddings | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning, üí¨natural-language-processing">
<meta name="description" content="One very insightful idea is unsupervised word representation. That is just say take a lot of text and try to model the word representations statistically.
Salton (1975) was one of the first researchers that tried to use the techniques that later will be common in Google searches.
Theory Johnson-Lindenstrauss Lemma This lemma basically says that semantic embedding is possible, without giving a real algorithm to do so. This seems to be a nice resource about this lemma.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/word-embeddings/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/word-embeddings/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Word Embeddings" />
<meta property="og:description" content="One very insightful idea is unsupervised word representation. That is just say take a lot of text and try to model the word representations statistically.
Salton (1975) was one of the first researchers that tried to use the techniques that later will be common in Google searches.
Theory Johnson-Lindenstrauss Lemma This lemma basically says that semantic embedding is possible, without giving a real algorithm to do so. This seems to be a nice resource about this lemma." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/word-embeddings/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Word Embeddings"/>
<meta name="twitter:description" content="One very insightful idea is unsupervised word representation. That is just say take a lot of text and try to model the word representations statistically.
Salton (1975) was one of the first researchers that tried to use the techniques that later will be common in Google searches.
Theory Johnson-Lindenstrauss Lemma This lemma basically says that semantic embedding is possible, without giving a real algorithm to do so. This seems to be a nice resource about this lemma."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Word Embeddings",
      "item": "https://flecart.github.io/notes/word-embeddings/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Word Embeddings",
  "name": "Word Embeddings",
  "description": "One very insightful idea is unsupervised word representation. That is just say take a lot of text and try to model the word representations statistically.\nSalton (1975) was one of the first researchers that tried to use the techniques that later will be common in Google searches.\nTheory Johnson-Lindenstrauss Lemma This lemma basically says that semantic embedding is possible, without giving a real algorithm to do so. This seems to be a nice resource about this lemma.",
  "keywords": [
    "machinelearning", "üí¨natural-language-processing"
  ],
  "articleBody": "One very insightful idea is unsupervised word representation. That is just say take a lot of text and try to model the word representations statistically.\nSalton (1975) was one of the first researchers that tried to use the techniques that later will be common in Google searches.\nTheory Johnson-Lindenstrauss Lemma This lemma basically says that semantic embedding is possible, without giving a real algorithm to do so. This seems to be a nice resource about this lemma.\nFor any $0 \u003c \\varepsilon \u003c 1$ and any $k$ bigger than $f(\\varepsilon)\\log n$ (see the reference for $f(\\varepsilon)$ its a simple function). The for any $n \\in \\mathbb{R}^{d}$ there is a function $f: \\mathbb{R}^{d} \\to \\mathbb{R}^{k}$ such that $\\forall x_{i} x_{j} \\in A$ we have $$ (1 - \\varepsilon)\\lVert x_{i} - x_{j} \\rVert ^{2} \\leq \\lVert f(x_{i}) - f(x_{j}) \\rVert^{2} \\leq (1 + \\varepsilon) \\lVert x_{i} - x_{j} \\rVert ^{2} $$ It says that the pairwise distances can be preserved (if meaning is relation between different entities, then this is everything that is needed to keep that concept almost unaltered.)\nProbably this is a proof but needs to be checked: https://chatgpt.com/share/826d6525-7d06-4822-b514-3c1f6f006a99.\nVector Embeddings We want to translate entities (words, sentences) into vectors. We can embed many things, for example, images, audios etc. There are also some multi channel embeddings, for example CLIP embeddings.\nStatic word embeddings Word2Vec The reference paper is (Mikolov et al. 2013). This has been one of the first approaches that attempted to imbue a semantic meaning to the embedding, which is given statically by the context. Usually byte level Tokenization are not able to create meanings. A famous hypothesis states that the meaning of a word depends on the context words, as explained in (Firth). This is also called skip-gram model in this cases we accumulate pairs of words with a fixed context length of 3. Then we want to try to predict the context words by using the focal word. This is why we say this model produces contextual word embeddings.\nWe want to model the context as a log-bilinear model: $$ p(c \\mid w) = \\frac{1}{Z(w)} \\exp(e_{wrd}(w) \\cdot e_{ctx}(c)) $$ Similar model to Log Linear Models.\nIf we have $\\lvert V \\rvert$ words, then we have in this case $2 \\lvert V \\rvert d$ parameters. Where $d$ is the dimension of the embedding. Why do we need two embeddings, one for the word and one of the context? It makes it easier to model the unlikeliness of repeating the word in his context. The difficult thing for this method is scaling because the $Z(w)$ is slow to calculate (this is why later we need sampling methods).\nPer questo usiamo i contextualized word embeddings.\nNegative sampling It‚Äôs a way to speed up training for word2vec, doing 600k at the same time was not feasible at the time. Select 2-20 words in the vocabulary that we do not want to predict. 600k because Word2Vec is just a double linear layer, but with 3kk words of input and output.\nThe negative samples are just taking random words from the vocabulary randomly. While the positive samples are the couples of words effectively present.\nI don‚Äôt know if I understood it well but I think it samples a certain number of words at a time and updates only the weights involved (so k targets, and the set of weights involved, only those). I don‚Äôt know why it‚Äôs called negative sampling if this is the idea below.\nNOTE: this is not important, there are many sampling ways that are used to estimate the $Z$. So you can skip this method.\nOther resources Intuitive Youtube Video about Word2Vec Blog pratico\nOther famous embedding models are GloVe and ELMo\nContextualized Embeddings These are state of the art now. We want to have a different word embedding for every context. We usually use Transformers to model these.\nBERT is called ‚ÄúBidirectional Encoder Representations from Transformers‚Äù. 3.3 Billion words (usually used after finetuning), 110 million parameters and 340 million parameters, not so much from today‚Äôs standards.\nReferences [1] Mikolov et al. ‚ÄúEfficient Estimation of Word Representations in Vector Space‚Äù 2013\n[2] Firth ‚ÄúFirth, J. (1957). A Synopsis of Linguistic Theory, 1930-55. In Studies in Linguistic Analysis (Pp. 1-31). Special Volume of the Philological Society. Oxford Blackwell. [Reprinted as Firth (1968)] - References - Scientific Research Publishing‚Äù\n",
  "wordCount" : "720",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/word-embeddings/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Word Embeddings
    </h1>
    <div class="post-meta">4 min&nbsp;¬∑&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#theory" aria-label="Theory">Theory</a><ul>
                        
                <li>
                    <a href="#johnson-lindenstrauss-lemma" aria-label="Johnson-Lindenstrauss Lemma">Johnson-Lindenstrauss Lemma</a></li>
                <li>
                    <a href="#vector-embeddings" aria-label="Vector Embeddings">Vector Embeddings</a></li></ul>
                </li>
                <li>
                    <a href="#static-word-embeddings" aria-label="Static word embeddings">Static word embeddings</a><ul>
                        
                <li>
                    <a href="#word2vec" aria-label="Word2Vec">Word2Vec</a><ul>
                        
                <li>
                    <a href="#negative-sampling" aria-label="Negative sampling">Negative sampling</a></li>
                <li>
                    <a href="#other-resources" aria-label="Other resources">Other resources</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#contextualized-embeddings" aria-label="Contextualized Embeddings">Contextualized Embeddings</a><ul>
                        
                <li>
                    <a href="#bert" aria-label="BERT">BERT</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>One very insightful idea is <em>unsupervised word representation</em>. That is just say take a lot of text and try to model the word representations statistically.</p>
<p>Salton (1975) was one of the first researchers that tried to use the techniques that later will be common in Google searches.</p>
<h2 id="theory">Theory<a hidden class="anchor" aria-hidden="true" href="#theory">#</a></h2>
<h3 id="johnson-lindenstrauss-lemma">Johnson-Lindenstrauss Lemma<a hidden class="anchor" aria-hidden="true" href="#johnson-lindenstrauss-lemma">#</a></h3>
<p>This lemma basically says that semantic embedding is possible, without giving a real algorithm to do so. <a href="https://cs.stanford.edu/people/mmahoney/cs369m/Lectures/lecture1.pdf">This</a> seems to be a nice resource about this lemma.</p>
<p>For any $0 < \varepsilon < 1$ and any $k$ bigger than $f(\varepsilon)\log n$ (see the reference for $f(\varepsilon)$ its a simple function).
The for any $n \in \mathbb{R}^{d}$ there is a function $f: \mathbb{R}^{d} \to \mathbb{R}^{k}$ such that $\forall x_{i} x_{j} \in A$ we have
</p>
$$
 (1 - \varepsilon)\lVert x_{i} - x_{j} \rVert ^{2} \leq \lVert f(x_{i}) - f(x_{j}) \rVert^{2}  \leq (1 + \varepsilon) \lVert x_{i} - x_{j} \rVert ^{2}
$$
<p>
It says that the pairwise distances can be preserved (if meaning is relation between different entities, then this is everything that is needed to keep that concept almost unaltered.)</p>
<p>Probably this is a proof but needs to be checked: <a href="https://chatgpt.com/share/826d6525-7d06-4822-b514-3c1f6f006a99.">https://chatgpt.com/share/826d6525-7d06-4822-b514-3c1f6f006a99.</a></p>
<h3 id="vector-embeddings">Vector Embeddings<a hidden class="anchor" aria-hidden="true" href="#vector-embeddings">#</a></h3>
<p>We want to translate entities (words, sentences) into vectors. We can embed many things, for example, images, audios etc. There are also some multi channel embeddings, for example CLIP embeddings.</p>
<h2 id="static-word-embeddings">Static word embeddings<a hidden class="anchor" aria-hidden="true" href="#static-word-embeddings">#</a></h2>
<h3 id="word2vec">Word2Vec<a hidden class="anchor" aria-hidden="true" href="#word2vec">#</a></h3>
<p>The reference paper is <a href="http://arxiv.org/abs/1301.3781">(Mikolov et al. 2013)</a>. This has been one of the first approaches that attempted to imbue a <strong>semantic meaning</strong> to the embedding, which is given statically by the context. Usually byte level <a href="/notes/tokenization/">Tokenization</a> are not able to create meanings. A famous hypothesis states that the <em>meaning</em> of a word depends on the context words, as explained in <a href="https://www.scirp.org/reference/ReferencesPapers?ReferenceID=1846447">(Firth)</a>.
This is also called <strong>skip-gram</strong> model in this cases we accumulate pairs of words with a fixed context length of 3. Then we want to try to predict the context words by using the focal word. This is why we say this model produces <strong>contextual word embeddings</strong>.</p>
<p>We want to model the context as a <em>log-bilinear</em> model:
</p>
$$
p(c \mid w) = \frac{1}{Z(w)} \exp(e_{wrd}(w) \cdot e_{ctx}(c))
$$
<p>
Similar model to Log Linear Models.</p>
<p>If we have $\lvert V \rvert$ words, then we have in this case $2 \lvert V \rvert d$ parameters. Where $d$ is the dimension of the embedding.
Why do we need two embeddings, one for the word and one of the context? It makes it easier to model the unlikeliness of repeating the word in his context.
The difficult thing for this method is <em>scaling</em> because the $Z(w)$ is slow to calculate (this is why later we need sampling methods).</p>
<p>Per questo usiamo i <strong>contextualized word embeddings</strong>.</p>
<img src="/images/notes/Word Embeddings-20240907192619736.webp" alt="Word Embeddings-20240907192619736">
<h4 id="negative-sampling">Negative sampling<a hidden class="anchor" aria-hidden="true" href="#negative-sampling">#</a></h4>
<p>It&rsquo;s a way to speed up training for word2vec, doing 600k at the same time was not feasible at the time. Select 2-20 words in the vocabulary that we do not want to predict.
600k because Word2Vec is just a double linear layer, but with 3kk words of input and output.</p>
<p>The negative samples are just taking random words from the vocabulary randomly. While the positive samples are the couples of words effectively present.</p>
<p>I don&rsquo;t know if I understood it well but I think it samples a certain number of words at a time and updates only the weights involved (so k targets, and the set of weights involved, only those).
I don&rsquo;t know why it&rsquo;s called negative sampling if this is the idea below.</p>
<p><strong>NOTE:</strong> this is not important, there are many sampling ways that are used to estimate the $Z$. So you can skip this method.</p>
<h4 id="other-resources">Other resources<a hidden class="anchor" aria-hidden="true" href="#other-resources">#</a></h4>
<p><a href="https://www.youtube.com/watch?v=viZrOnJclY0&ab_channel=StatQuestwithJoshStarmer">Intuitive Youtube Video about Word2Vec</a>
<a href="https://towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28">Blog pratico</a></p>
<p>Other famous embedding models are GloVe and ELMo</p>
<h2 id="contextualized-embeddings">Contextualized Embeddings<a hidden class="anchor" aria-hidden="true" href="#contextualized-embeddings">#</a></h2>
<p>These are state of the art now. We want to have a <em>different</em> word embedding for every context. We usually use <a href="/notes/transformers/">Transformers</a> to model these.</p>
<h3 id="bert">BERT<a hidden class="anchor" aria-hidden="true" href="#bert">#</a></h3>
<p>is called &ldquo;Bidirectional Encoder Representations from Transformers&rdquo;.
3.3 Billion words (usually used after finetuning), 110 million parameters and 340 million parameters, not so much from today&rsquo;s standards.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Mikolov et al. <a href="http://arxiv.org/abs/1301.3781">‚ÄúEfficient Estimation of Word Representations in Vector Space‚Äù</a>  2013</p>
<p>[2] Firth <a href="https://www.scirp.org/reference/ReferencesPapers?ReferenceID=1846447">‚ÄúFirth, J. (1957). A Synopsis of Linguistic Theory, 1930-55. In Studies in Linguistic Analysis (Pp. 1-31). Special Volume of the Philological Society. Oxford Blackwell. [Reprinted as Firth (1968)] - References - Scientific Research Publishing‚Äù</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
      <li><a href="https://flecart.github.io/tags/natural-language-processing/">üí¨Natural-Language-Processing</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on x"
            href="https://x.com/intent/tweet/?text=Word%20Embeddings&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f&amp;hashtags=machinelearning%2c%f0%9f%92%acnatural-language-processing">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f&amp;title=Word%20Embeddings&amp;summary=Word%20Embeddings&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f&title=Word%20Embeddings">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on whatsapp"
            href="https://api.whatsapp.com/send?text=Word%20Embeddings%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on telegram"
            href="https://telegram.me/share/url?text=Word%20Embeddings&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Word%20Embeddings&u=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
