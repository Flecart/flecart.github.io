<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Word Embeddings | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning">
<meta name="description" content="Word2Vec Paper di riferimento: (Mikolov et al. 2013).
Video youtube intuitivo Blog pratico
È stato uno dei primi approcci che provano a fare un embedding semantico del significato delle parole. Semplicemente andare a fare Tokenization per andare a encodare le parole non è sufficiente, perché questi non hanno nessun apporto semantico alle parole.
In questo caso vogliamo rappresentare una parola tramite vettori. Il vettore alla fine non sarà altro che il layer lineare iniziale per fare all&rsquo;associazione.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/word-embeddings/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/word-embeddings/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Word Embeddings" />
<meta property="og:description" content="Word2Vec Paper di riferimento: (Mikolov et al. 2013).
Video youtube intuitivo Blog pratico
È stato uno dei primi approcci che provano a fare un embedding semantico del significato delle parole. Semplicemente andare a fare Tokenization per andare a encodare le parole non è sufficiente, perché questi non hanno nessun apporto semantico alle parole.
In questo caso vogliamo rappresentare una parola tramite vettori. Il vettore alla fine non sarà altro che il layer lineare iniziale per fare all&rsquo;associazione." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/word-embeddings/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Word Embeddings"/>
<meta name="twitter:description" content="Word2Vec Paper di riferimento: (Mikolov et al. 2013).
Video youtube intuitivo Blog pratico
È stato uno dei primi approcci che provano a fare un embedding semantico del significato delle parole. Semplicemente andare a fare Tokenization per andare a encodare le parole non è sufficiente, perché questi non hanno nessun apporto semantico alle parole.
In questo caso vogliamo rappresentare una parola tramite vettori. Il vettore alla fine non sarà altro che il layer lineare iniziale per fare all&rsquo;associazione."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Word Embeddings",
      "item": "https://flecart.github.io/notes/word-embeddings/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Word Embeddings",
  "name": "Word Embeddings",
  "description": "Word2Vec Paper di riferimento: (Mikolov et al. 2013).\nVideo youtube intuitivo Blog pratico\nÈ stato uno dei primi approcci che provano a fare un embedding semantico del significato delle parole. Semplicemente andare a fare Tokenization per andare a encodare le parole non è sufficiente, perché questi non hanno nessun apporto semantico alle parole.\nIn questo caso vogliamo rappresentare una parola tramite vettori. Il vettore alla fine non sarà altro che il layer lineare iniziale per fare all\u0026rsquo;associazione.",
  "keywords": [
    "machinelearning"
  ],
  "articleBody": "Word2Vec Paper di riferimento: (Mikolov et al. 2013).\nVideo youtube intuitivo Blog pratico\nÈ stato uno dei primi approcci che provano a fare un embedding semantico del significato delle parole. Semplicemente andare a fare Tokenization per andare a encodare le parole non è sufficiente, perché questi non hanno nessun apporto semantico alle parole.\nIn questo caso vogliamo rappresentare una parola tramite vettori. Il vettore alla fine non sarà altro che il layer lineare iniziale per fare all’associazione. Poi questo viene utilizzato per fare una cosa simile a un autoencoder Autoencoders. La cosa da notare è che il layer iniziale è enorme, è l’intero vocabolario. Già questi\nPoi si utilizza la Crossentropy per fare backprop.\nNegative sampling È un modo per velocizzare il training per word2vec, fare in contemporanea 600k non era feasible al tempo. Seleziona 2-20 parole nel vocabolario che non vogliamo andare a predire. 600k perché Word2Vec è solamente doppio layer lineare, con però 3kk parole di input e output.\nNon so se lo ho capito bene ma credo faccia sampling di un certo numero di parole alla volta e aggiorna solamente i pesi interessati (quindi k target, e l’insieme dei pesi interessati, solamente quelli). Non so perché si chiami negative sampling se è questa l’idea sotto.\nTentativo raw di codice non finito # [https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Word_2_Vec.ipynb](https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Word_2_Vec.ipynb) import re import numpy as np from typing import Literal stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"has\"] def get_file_data(stop_word_removal: bool = False): file_contents = [] with open('jef_archer.txt') as f: file_contents = f.read() text = [] for val in file_contents.split('.'): sent = re.findall(\"[A-Za-z]+\", val) line = '' for words in sent: if stop_word_removal: if len(words) \u003e 1 and words not in stop_words: line = line + ' ' + words else: if len(words) \u003e 1 : line = line + ' ' + words text.append(line) return text def generate_dictinoary_data(text: list[str]): word_to_index= dict() index_to_word = dict() corpus = [] count = 0 vocab_size = 0 for row in text: for word in row.split(): word = word.lower() corpus.append(word) if word_to_index.get(word) == None: word_to_index.update ( {word : count}) index_to_word.update ( {count : word }) count += 1 vocab_size = len(word_to_index) length_of_corpus = len(corpus) return word_to_index,index_to_word,corpus,vocab_size,length_of_corpus def get_one_hot_vectors(target_word: str, context_words: list[str], vocab_size: int, word_to_index: dict[str, int]): #Create an array of size = vocab_size filled with zeros trgt_word_vector = np.zeros(vocab_size) #Get the index of the target_word according to the dictionary word_to_index. #If target_word = best, the index according to the dictionary word_to_index is 0. #So the one hot vector will be [1, 0, 0, 0, 0, 0, 0, 0, 0] index_of_word_dictionary = word_to_index.get(target_word) #Set the index to 1 trgt_word_vector[index_of_word_dictionary] = 1 #Repeat same steps for context_words but in a loop ctxt_word_vector = np.zeros(vocab_size) for word in context_words: index_of_word_dictionary = word_to_index.get(word) ctxt_word_vector[index_of_word_dictionary] = 1 return trgt_word_vector,ctxt_word_vector #Note : Below comments for trgt_word_index, ctxt_word_index are with the above sample text for understanding the code flow def generate_training_data(corpus,window_size,vocab_size,word_to_index,length_of_corpus,sample=None): training_data = [] training_sample_words = [] for i,word in enumerate(corpus): index_target_word = i target_word = word context_words = [] if i == 0: context_words = [corpus[x] for x in range(i + 1 , window_size + 1)] elif i == len(corpus)-1: context_words = [corpus[x] for x in range(length_of_corpus - 2 ,length_of_corpus -2 - window_size , -1 )] else: before_target_word_index = index_target_word - 1 for x in range(before_target_word_index, before_target_word_index - window_size , -1): if x \u003e= 0: context_words.append(corpus[x]) after_target_word_index = index_target_word + 1 for x in range(after_target_word_index, after_target_word_index + window_size): if x \u003c len(corpus): context_words.append(corpus[x]) trgt_word_vector, ctxt_word_vector = get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index) training_data.append([trgt_word_vector, ctxt_word_vector]) if sample is not None: training_sample_words.append([target_word, context_words]) return training_data, training_sample_words def forward_prop(weight_inp_hidden, weight_hidden_output, target_word_vector): hidden_layer = np.dot(weight_inp_hidden.T, target_word_vector) u = np.dot(weight_hidden_output.T, hidden_layer) y_predicted = softmax(u) return y_predicted, hidden_layer, u def softmax(x): e_x = np.exp(x - np.max(x)) return e_x / e_x.sum(axis=0) def calculate_error(y_pred,context_words): total_error = [None] * len(y_pred) index_of_1_in_context_words = {} for index in np.where(context_words == 1)[0]: index_of_1_in_context_words.update ( {index : 'yes'} ) number_of_1_in_context_vector = len(index_of_1_in_context_words) for i,value in enumerate(y_pred): if index_of_1_in_context_words.get(i) != None: total_error[i]= (value-1) + ( (number_of_1_in_context_vector -1) * value) else: total_error[i]= (number_of_1_in_context_vector * value) return np.array(total_error) def backward_prop(weight_inp_hidden,weight_hidden_output,total_error, hidden_layer, target_word_vector,learning_rate): dl_weight_inp_hidden = np.outer(target_word_vector, np.dot(weight_hidden_output, total_error.T)) dl_weight_hidden_output = np.outer(hidden_layer, total_error) # Update weights weight_inp_hidden = weight_inp_hidden - (learning_rate * dl_weight_inp_hidden) weight_hidden_output = weight_hidden_output - (learning_rate * dl_weight_hidden_output) return weight_inp_hidden,weight_hidden_output def calculate_loss(u,ctx): sum_1 = 0 for index in np.where(ctx==1)[0]: sum_1 = sum_1 + u[index] sum_1 = -sum_1 sum_2 = len(np.where(ctx==1)[0]) * np.log(np.sum(np.exp(u))) total_loss = sum_1 + sum_2 return total_loss if __name__ == \"__main__\": # text = get_file_data(stop_word_removal='yes') text = [\"Best way to success is to work hard and never give up\"] word_to_index,index_to_word,corpus,vocab_size,length_of_corpus = generate_dictinoary_data(text) print('word_to_index:',word_to_index) print('index_to_word:',index_to_word) print('corpus:',corpus) print('vocab_size:',vocab_size) print('length_of_corpus:',length_of_corpus) GloVe ELMo References [1] Mikolov et al. “Efficient Estimation of Word Representations in Vector Space” arXiv preprint arXiv:1301.3781 2013\n",
  "wordCount" : "923",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/word-embeddings/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Word Embeddings
    </h1>
    <div class="post-meta">5 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#word2vec" aria-label="Word2Vec">Word2Vec</a><ul>
                        
                <li>
                    <a href="#negative-sampling" aria-label="Negative sampling">Negative sampling</a></li>
                <li>
                    <a href="#tentativo-raw-di-codice-non-finito" aria-label="Tentativo raw di codice non finito">Tentativo raw di codice non finito</a></li></ul>
                </li>
                <li>
                    <a href="#glove" aria-label="GloVe">GloVe</a></li>
                <li>
                    <a href="#elmo" aria-label="ELMo">ELMo</a></li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="word2vec">Word2Vec<a hidden class="anchor" aria-hidden="true" href="#word2vec">#</a></h3>
<p>Paper di riferimento: <a href="http://arxiv.org/abs/1301.3781">(Mikolov et al. 2013)</a>.</p>
<p><a href="https://www.youtube.com/watch?v=viZrOnJclY0&ab_channel=StatQuestwithJoshStarmer">Video youtube intuitivo</a>
<a href="https://towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28">Blog pratico</a></p>
<p>È stato uno dei primi approcci che provano a fare un <strong>embedding semantico</strong> del significato delle parole.
Semplicemente andare a fare <a href="//notes/tokenization">Tokenization</a> per andare a encodare le parole non è sufficiente, perché questi non hanno nessun apporto semantico alle parole.</p>
<p>In questo caso vogliamo <strong>rappresentare una parola tramite vettori</strong>. Il vettore alla fine non sarà altro che il <em>layer lineare iniziale</em> per fare all&rsquo;associazione. Poi questo viene utilizzato per fare una cosa simile a un autoencoder <a href="//notes/autoencoders">Autoencoders</a>.
La cosa da notare è che il layer iniziale è enorme, è <strong>l&rsquo;intero vocabolario.</strong>
Già questi</p>
<p>Poi si utilizza la Crossentropy per fare backprop.</p>
<h4 id="negative-sampling">Negative sampling<a hidden class="anchor" aria-hidden="true" href="#negative-sampling">#</a></h4>
<p>È un modo per velocizzare il training per word2vec, fare in contemporanea 600k non era feasible al tempo. Seleziona 2-20 parole nel vocabolario che non vogliamo andare a predire.
600k perché Word2Vec è solamente doppio layer lineare, con però 3kk parole di input e output.</p>
<p>Non so se lo ho capito bene ma credo faccia sampling di un certo numero di parole alla volta e aggiorna solamente i pesi interessati (quindi k target, e l&rsquo;insieme dei pesi interessati, solamente quelli).
Non so perché si chiami negative sampling se è questa l&rsquo;idea sotto.</p>
<h4 id="tentativo-raw-di-codice-non-finito">Tentativo raw di codice non finito<a hidden class="anchor" aria-hidden="true" href="#tentativo-raw-di-codice-non-finito">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># [https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Word_2_Vec.ipynb](https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Word_2_Vec.ipynb)</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">re</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Literal</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">stop_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="s1">&#39;me&#39;</span><span class="p">,</span> <span class="s1">&#39;my&#39;</span><span class="p">,</span> <span class="s1">&#39;myself&#39;</span><span class="p">,</span> <span class="s1">&#39;we&#39;</span><span class="p">,</span> <span class="s1">&#39;our&#39;</span><span class="p">,</span> <span class="s1">&#39;ours&#39;</span><span class="p">,</span> <span class="s1">&#39;ourselves&#39;</span><span class="p">,</span> <span class="s1">&#39;you&#39;</span><span class="p">,</span> <span class="s2">&#34;you&#39;re&#34;</span><span class="p">,</span> <span class="s2">&#34;you&#39;ve&#34;</span><span class="p">,</span> <span class="s2">&#34;you&#39;ll&#34;</span><span class="p">,</span> <span class="s2">&#34;you&#39;d&#34;</span><span class="p">,</span> <span class="s1">&#39;your&#39;</span><span class="p">,</span> <span class="s1">&#39;yours&#39;</span><span class="p">,</span> <span class="s1">&#39;yourself&#39;</span><span class="p">,</span> <span class="s1">&#39;yourselves&#39;</span><span class="p">,</span> <span class="s1">&#39;he&#39;</span><span class="p">,</span> <span class="s1">&#39;him&#39;</span><span class="p">,</span> <span class="s1">&#39;his&#39;</span><span class="p">,</span> <span class="s1">&#39;himself&#39;</span><span class="p">,</span> <span class="s1">&#39;she&#39;</span><span class="p">,</span> <span class="s2">&#34;she&#39;s&#34;</span><span class="p">,</span> <span class="s1">&#39;her&#39;</span><span class="p">,</span> <span class="s1">&#39;hers&#39;</span><span class="p">,</span> <span class="s1">&#39;herself&#39;</span><span class="p">,</span> <span class="s1">&#39;it&#39;</span><span class="p">,</span> <span class="s2">&#34;it&#39;s&#34;</span><span class="p">,</span> <span class="s1">&#39;its&#39;</span><span class="p">,</span> <span class="s1">&#39;itself&#39;</span><span class="p">,</span> <span class="s1">&#39;they&#39;</span><span class="p">,</span> <span class="s1">&#39;them&#39;</span><span class="p">,</span> <span class="s1">&#39;their&#39;</span><span class="p">,</span> <span class="s1">&#39;theirs&#39;</span><span class="p">,</span> <span class="s1">&#39;themselves&#39;</span><span class="p">,</span> <span class="s1">&#39;what&#39;</span><span class="p">,</span> <span class="s1">&#39;which&#39;</span><span class="p">,</span> <span class="s1">&#39;who&#39;</span><span class="p">,</span> <span class="s1">&#39;whom&#39;</span><span class="p">,</span> <span class="s1">&#39;this&#39;</span><span class="p">,</span> <span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="s2">&#34;that&#39;ll&#34;</span><span class="p">,</span> <span class="s1">&#39;these&#39;</span><span class="p">,</span> <span class="s1">&#39;those&#39;</span><span class="p">,</span> <span class="s1">&#39;am&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;are&#39;</span><span class="p">,</span> <span class="s1">&#39;was&#39;</span><span class="p">,</span> <span class="s1">&#39;were&#39;</span><span class="p">,</span> <span class="s1">&#39;be&#39;</span><span class="p">,</span> <span class="s1">&#39;been&#39;</span><span class="p">,</span> <span class="s1">&#39;being&#39;</span><span class="p">,</span> <span class="s1">&#39;have&#39;</span><span class="p">,</span> <span class="s1">&#39;has&#39;</span><span class="p">,</span> <span class="s1">&#39;had&#39;</span><span class="p">,</span> <span class="s1">&#39;having&#39;</span><span class="p">,</span> <span class="s1">&#39;do&#39;</span><span class="p">,</span> <span class="s1">&#39;does&#39;</span><span class="p">,</span> <span class="s1">&#39;did&#39;</span><span class="p">,</span> <span class="s1">&#39;doing&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;an&#39;</span><span class="p">,</span> <span class="s1">&#39;the&#39;</span><span class="p">,</span> <span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="s1">&#39;but&#39;</span><span class="p">,</span> <span class="s1">&#39;if&#39;</span><span class="p">,</span> <span class="s1">&#39;or&#39;</span><span class="p">,</span> <span class="s1">&#39;because&#39;</span><span class="p">,</span> <span class="s1">&#39;as&#39;</span><span class="p">,</span> <span class="s1">&#39;until&#39;</span><span class="p">,</span> <span class="s1">&#39;while&#39;</span><span class="p">,</span> <span class="s1">&#39;of&#39;</span><span class="p">,</span> <span class="s1">&#39;at&#39;</span><span class="p">,</span> <span class="s1">&#39;by&#39;</span><span class="p">,</span> <span class="s1">&#39;for&#39;</span><span class="p">,</span> <span class="s1">&#39;with&#39;</span><span class="p">,</span> <span class="s1">&#39;about&#39;</span><span class="p">,</span> <span class="s1">&#39;against&#39;</span><span class="p">,</span> <span class="s1">&#39;between&#39;</span><span class="p">,</span> <span class="s1">&#39;into&#39;</span><span class="p">,</span> <span class="s1">&#39;through&#39;</span><span class="p">,</span> <span class="s1">&#39;during&#39;</span><span class="p">,</span> <span class="s1">&#39;before&#39;</span><span class="p">,</span> <span class="s1">&#39;after&#39;</span><span class="p">,</span> <span class="s1">&#39;above&#39;</span><span class="p">,</span> <span class="s1">&#39;below&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;from&#39;</span><span class="p">,</span> <span class="s1">&#39;up&#39;</span><span class="p">,</span> <span class="s1">&#39;down&#39;</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">,</span> <span class="s1">&#39;out&#39;</span><span class="p">,</span> <span class="s1">&#39;on&#39;</span><span class="p">,</span> <span class="s1">&#39;off&#39;</span><span class="p">,</span> <span class="s1">&#39;over&#39;</span><span class="p">,</span> <span class="s1">&#39;under&#39;</span><span class="p">,</span> <span class="s1">&#39;again&#39;</span><span class="p">,</span> <span class="s1">&#39;further&#39;</span><span class="p">,</span> <span class="s1">&#39;then&#39;</span><span class="p">,</span> <span class="s1">&#39;once&#39;</span><span class="p">,</span> <span class="s1">&#39;here&#39;</span><span class="p">,</span> <span class="s1">&#39;there&#39;</span><span class="p">,</span> <span class="s1">&#39;when&#39;</span><span class="p">,</span> <span class="s1">&#39;where&#39;</span><span class="p">,</span> <span class="s1">&#39;why&#39;</span><span class="p">,</span> <span class="s1">&#39;how&#39;</span><span class="p">,</span> <span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;any&#39;</span><span class="p">,</span> <span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="s1">&#39;each&#39;</span><span class="p">,</span> <span class="s1">&#39;few&#39;</span><span class="p">,</span> <span class="s1">&#39;more&#39;</span><span class="p">,</span> <span class="s1">&#39;most&#39;</span><span class="p">,</span> <span class="s1">&#39;other&#39;</span><span class="p">,</span> <span class="s1">&#39;some&#39;</span><span class="p">,</span> <span class="s1">&#39;such&#39;</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span> <span class="s1">&#39;nor&#39;</span><span class="p">,</span> <span class="s1">&#39;not&#39;</span><span class="p">,</span> <span class="s1">&#39;only&#39;</span><span class="p">,</span> <span class="s1">&#39;own&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;so&#39;</span><span class="p">,</span> <span class="s1">&#39;than&#39;</span><span class="p">,</span> <span class="s1">&#39;too&#39;</span><span class="p">,</span> <span class="s1">&#39;very&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">,</span> <span class="s1">&#39;can&#39;</span><span class="p">,</span> <span class="s1">&#39;will&#39;</span><span class="p">,</span> <span class="s1">&#39;just&#39;</span><span class="p">,</span> <span class="s1">&#39;don&#39;</span><span class="p">,</span> <span class="s2">&#34;don&#39;t&#34;</span><span class="p">,</span> <span class="s1">&#39;should&#39;</span><span class="p">,</span> <span class="s2">&#34;should&#39;ve&#34;</span><span class="p">,</span> <span class="s1">&#39;now&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;ll&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;re&#39;</span><span class="p">,</span> <span class="s1">&#39;ve&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;ain&#39;</span><span class="p">,</span> <span class="s1">&#39;aren&#39;</span><span class="p">,</span> <span class="s2">&#34;aren&#39;t&#34;</span><span class="p">,</span> <span class="s1">&#39;couldn&#39;</span><span class="p">,</span> <span class="s2">&#34;couldn&#39;t&#34;</span><span class="p">,</span> <span class="s1">&#39;didn&#39;</span><span class="p">,</span> <span class="s2">&#34;didn&#39;t&#34;</span><span class="p">,</span> <span class="s1">&#39;doesn&#39;</span><span class="p">,</span> <span class="s2">&#34;doesn&#39;t&#34;</span><span class="p">,</span> <span class="s1">&#39;hadn&#39;</span><span class="p">,</span> <span class="s2">&#34;hadn&#39;t&#34;</span><span class="p">,</span> <span class="s1">&#39;hasn&#39;</span><span class="p">,</span> <span class="s2">&#34;has&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_file_data</span><span class="p">(</span><span class="n">stop_word_removal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">file_contents</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;jef_archer.txt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">file_contents</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">text</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">file_contents</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">sent</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s2">&#34;[A-Za-z]+&#34;</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">line</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">stop_word_removal</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">words</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">line</span> <span class="o">=</span> <span class="n">line</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">words</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">line</span> <span class="o">=</span> <span class="n">line</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">words</span>
</span></span><span class="line"><span class="cl">        <span class="n">text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">text</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_dictinoary_data</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">    <span class="n">word_to_index</span><span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">index_to_word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">corpus</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">row</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">word</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">corpus</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">word_to_index</span><span class="o">.</span><span class="n">update</span> <span class="p">(</span> <span class="p">{</span><span class="n">word</span> <span class="p">:</span> <span class="n">count</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">                <span class="n">index_to_word</span><span class="o">.</span><span class="n">update</span> <span class="p">(</span> <span class="p">{</span><span class="n">count</span> <span class="p">:</span> <span class="n">word</span> <span class="p">})</span>
</span></span><span class="line"><span class="cl">                <span class="n">count</span>  <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">length_of_corpus</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">word_to_index</span><span class="p">,</span><span class="n">index_to_word</span><span class="p">,</span><span class="n">corpus</span><span class="p">,</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">length_of_corpus</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_one_hot_vectors</span><span class="p">(</span><span class="n">target_word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">#Create an array of size = vocab_size filled with zeros</span>
</span></span><span class="line"><span class="cl">    <span class="n">trgt_word_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">#Get the index of the target_word according to the dictionary word_to_index. </span>
</span></span><span class="line"><span class="cl">    <span class="c1">#If target_word = best, the index according to the dictionary word_to_index is 0. </span>
</span></span><span class="line"><span class="cl">    <span class="c1">#So the one hot vector will be [1, 0, 0, 0, 0, 0, 0, 0, 0]</span>
</span></span><span class="line"><span class="cl">    <span class="n">index_of_word_dictionary</span> <span class="o">=</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">target_word</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">#Set the index to 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">trgt_word_vector</span><span class="p">[</span><span class="n">index_of_word_dictionary</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">#Repeat same steps for context_words but in a loop</span>
</span></span><span class="line"><span class="cl">    <span class="n">ctxt_word_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">index_of_word_dictionary</span> <span class="o">=</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">        <span class="n">ctxt_word_vector</span><span class="p">[</span><span class="n">index_of_word_dictionary</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">trgt_word_vector</span><span class="p">,</span><span class="n">ctxt_word_vector</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#Note : Below comments for trgt_word_index, ctxt_word_index are with the above sample text for understanding the code flow</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_training_data</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="n">window_size</span><span class="p">,</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">word_to_index</span><span class="p">,</span><span class="n">length_of_corpus</span><span class="p">,</span><span class="n">sample</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">training_data</span> <span class="o">=</span>  <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">training_sample_words</span> <span class="o">=</span>  <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">index_target_word</span> <span class="o">=</span> <span class="n">i</span>
</span></span><span class="line"><span class="cl">        <span class="n">target_word</span> <span class="o">=</span> <span class="n">word</span>
</span></span><span class="line"><span class="cl">        <span class="n">context_words</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  
</span></span><span class="line"><span class="cl">            <span class="n">context_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">corpus</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">,</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">context_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">corpus</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length_of_corpus</span> <span class="o">-</span> <span class="mi">2</span> <span class="p">,</span><span class="n">length_of_corpus</span> <span class="o">-</span><span class="mi">2</span> <span class="o">-</span> <span class="n">window_size</span>  <span class="p">,</span> <span class="o">-</span><span class="mi">1</span> <span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">before_target_word_index</span> <span class="o">=</span> <span class="n">index_target_word</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">before_target_word_index</span><span class="p">,</span> <span class="n">before_target_word_index</span> <span class="o">-</span> <span class="n">window_size</span> <span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">context_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">after_target_word_index</span> <span class="o">=</span> <span class="n">index_target_word</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">after_target_word_index</span><span class="p">,</span> <span class="n">after_target_word_index</span> <span class="o">+</span> <span class="n">window_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">context_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">trgt_word_vector</span><span class="p">,</span> <span class="n">ctxt_word_vector</span> <span class="o">=</span> <span class="n">get_one_hot_vectors</span><span class="p">(</span><span class="n">target_word</span><span class="p">,</span><span class="n">context_words</span><span class="p">,</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">word_to_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_data</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">trgt_word_vector</span><span class="p">,</span> <span class="n">ctxt_word_vector</span><span class="p">])</span>   
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">sample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">training_sample_words</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">target_word</span><span class="p">,</span> <span class="n">context_words</span><span class="p">])</span>   
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">training_sample_words</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward_prop</span><span class="p">(</span><span class="n">weight_inp_hidden</span><span class="p">,</span> <span class="n">weight_hidden_output</span><span class="p">,</span> <span class="n">target_word_vector</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_inp_hidden</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">target_word_vector</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_hidden_output</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">hidden_layer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">y_predicted</span><span class="p">,</span> <span class="n">hidden_layer</span><span class="p">,</span> <span class="n">u</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">e_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">e_x</span> <span class="o">/</span> <span class="n">e_x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">calculate_error</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">context_words</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_error</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">index_of_1_in_context_words</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">context_words</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">index_of_1_in_context_words</span><span class="o">.</span><span class="n">update</span> <span class="p">(</span> <span class="p">{</span><span class="n">index</span> <span class="p">:</span> <span class="s1">&#39;yes&#39;</span><span class="p">}</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="n">number_of_1_in_context_vector</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">index_of_1_in_context_words</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y_pred</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">index_of_1_in_context_words</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">total_error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span> <span class="p">(</span><span class="n">value</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span> <span class="p">(</span><span class="n">number_of_1_in_context_vector</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">total_error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span> <span class="p">(</span><span class="n">number_of_1_in_context_vector</span> <span class="o">*</span> <span class="n">value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">    <span class="k">return</span>  <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">total_error</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">backward_prop</span><span class="p">(</span><span class="n">weight_inp_hidden</span><span class="p">,</span><span class="n">weight_hidden_output</span><span class="p">,</span><span class="n">total_error</span><span class="p">,</span> <span class="n">hidden_layer</span><span class="p">,</span> <span class="n">target_word_vector</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">dl_weight_inp_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">target_word_vector</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_hidden_output</span><span class="p">,</span> <span class="n">total_error</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">dl_weight_hidden_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">hidden_layer</span><span class="p">,</span> <span class="n">total_error</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Update weights</span>
</span></span><span class="line"><span class="cl">    <span class="n">weight_inp_hidden</span> <span class="o">=</span> <span class="n">weight_inp_hidden</span> <span class="o">-</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dl_weight_inp_hidden</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">weight_hidden_output</span> <span class="o">=</span> <span class="n">weight_hidden_output</span> <span class="o">-</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dl_weight_hidden_output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">weight_inp_hidden</span><span class="p">,</span><span class="n">weight_hidden_output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">ctx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">sum_1</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">ctx</span><span class="o">==</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">sum_1</span> <span class="o">=</span> <span class="n">sum_1</span> <span class="o">+</span> <span class="n">u</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">sum_1</span> <span class="o">=</span> <span class="o">-</span><span class="n">sum_1</span>
</span></span><span class="line"><span class="cl">    <span class="n">sum_2</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">ctx</span><span class="o">==</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">u</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">sum_1</span> <span class="o">+</span> <span class="n">sum_2</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">total_loss</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># text = get_file_data(stop_word_removal=&#39;yes&#39;)</span>
</span></span><span class="line"><span class="cl">    <span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Best way to success is to work hard and never give up&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">word_to_index</span><span class="p">,</span><span class="n">index_to_word</span><span class="p">,</span><span class="n">corpus</span><span class="p">,</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">length_of_corpus</span> <span class="o">=</span> <span class="n">generate_dictinoary_data</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;word_to_index:&#39;</span><span class="p">,</span><span class="n">word_to_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;index_to_word:&#39;</span><span class="p">,</span><span class="n">index_to_word</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;corpus:&#39;</span><span class="p">,</span><span class="n">corpus</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;vocab_size:&#39;</span><span class="p">,</span><span class="n">vocab_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;length_of_corpus:&#39;</span><span class="p">,</span><span class="n">length_of_corpus</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="glove">GloVe<a hidden class="anchor" aria-hidden="true" href="#glove">#</a></h3>
<h3 id="elmo">ELMo<a hidden class="anchor" aria-hidden="true" href="#elmo">#</a></h3>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Mikolov et al. <a href="http://arxiv.org/abs/1301.3781">“Efficient Estimation of Word Representations in Vector Space”</a> arXiv preprint arXiv:1301.3781 2013</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on x"
            href="https://x.com/intent/tweet/?text=Word%20Embeddings&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f&amp;hashtags=machinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f&amp;title=Word%20Embeddings&amp;summary=Word%20Embeddings&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f&title=Word%20Embeddings">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on whatsapp"
            href="https://api.whatsapp.com/send?text=Word%20Embeddings%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on telegram"
            href="https://telegram.me/share/url?text=Word%20Embeddings&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Word Embeddings on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Word%20Embeddings&u=https%3a%2f%2fflecart.github.io%2fnotes%2fword-embeddings%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
