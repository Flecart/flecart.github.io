<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Neural Imaging | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="neuroscience">
<meta name="description" content="In general we want to understand how neurons encode the rate and temporal information to build specific features like place cells, grid cells, velocity, head direction, or how it can guide behaviour or coordination. Many neurons encode together some features, it is quite rare that you have the face neuron and similars. Imaging techniques help us to get more information about these parts.
Basics of Microscopy


Image of a classical microscope, from course slides">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/neural-imaging/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/neural-imaging/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/neural-imaging/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Neural Imaging">
  <meta property="og:description" content="In general we want to understand how neurons encode the rate and temporal information to build specific features like place cells, grid cells, velocity, head direction, or how it can guide behaviour or coordination. Many neurons encode together some features, it is quite rare that you have the face neuron and similars. Imaging techniques help us to get more information about these parts.
Basics of Microscopy Image of a classical microscope, from course slides">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="Neuroscience">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Neural Imaging">
<meta name="twitter:description" content="In general we want to understand how neurons encode the rate and temporal information to build specific features like place cells, grid cells, velocity, head direction, or how it can guide behaviour or coordination. Many neurons encode together some features, it is quite rare that you have the face neuron and similars. Imaging techniques help us to get more information about these parts.
Basics of Microscopy


Image of a classical microscope, from course slides">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Neural Imaging",
      "item": "https://flecart.github.io/notes/neural-imaging/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Neural Imaging",
  "name": "Neural Imaging",
  "description": "In general we want to understand how neurons encode the rate and temporal information to build specific features like place cells, grid cells, velocity, head direction, or how it can guide behaviour or coordination. Many neurons encode together some features, it is quite rare that you have the face neuron and similars. Imaging techniques help us to get more information about these parts.\nBasics of Microscopy Image of a classical microscope, from course slides\n",
  "keywords": [
    "neuroscience"
  ],
  "articleBody": "In general we want to understand how neurons encode the rate and temporal information to build specific features like place cells, grid cells, velocity, head direction, or how it can guide behaviour or coordination. Many neurons encode together some features, it is quite rare that you have the face neuron and similars. Imaging techniques help us to get more information about these parts.\nBasics of Microscopy Image of a classical microscope, from course slides\nThe optical Principle The basic idea is to have one first lens that makes an object bigger but inverted, and another lens, called the eyepiece that sees the original part bigger, and in correct shape. With some high school physics is possible to compute how much is the enlargement due to the lens.\nLens Physics $$ \\frac{1}{f} = \\frac{1}{d_o} + \\frac{1}{d_i} $$$$ M = \\frac{d_i}{d_o} $$ Where $d_{o}$ is the distance of the object to the first lens, and $d_{i}$ is the distance of the image to the first lens.\nThe two magnifications compound with each other, giving a final value of $M = m_{1} + m_{2}$.\n#### Rayleigh Criterion This criterion, closely related to image resolution, is defined as: \u003e The limit at which two Airy disks can be resolved into separate entities. $$ \\Delta x = \\frac{1.22 \\cdot \\lambda}{2NA} $$$$ NA = n \\cdot sin(\\theta) $$ Where $n$ is the index of refraction of the medium, and $\\theta$ is the angle of the light cone that enters the lens.\nTwo main things in this formula:\nWavelength Amplitude of the light (low NA means lower resolution). This means we cannot distinguish two objects that are closer than $\\Delta x$, and this is the limit of the resolution of the lens.\nLooking at the aperture you can observe that if you are closer (wider aperture angle), then you resolution is higher, but less field of view. Sometimes is not very easy to bring the object to the tissue (bone, some 3d structure e.g.).\nMeasuring resolution In the lab the resolution of a microscope is measured using small beads and spheres of 100nm, we define the resolution as the width of the blurred dots that are produced by the lens. Types of microscopy Different microscopes Electron Microscopy We have a wavelength of nanometers (meaning we have a nice resolution), which means the resolution is orders of magnitude better than the light microscopy. We use magnets as lenses in this case. Nice thing about scanning electron microscopy is that you can gather 3D information (scattering can be seen at different angles), and the resolution is much better than light microscopy. LM -\u003e 200nm TEM = Think transparency: electrons go through → internal structure, ultra-high resolution. 0.1-1 nm SEM = Think surface: electrons scan across → surface shape \u0026 composition. ~1-10nm Both are limited by sample prep complexity and vacuum requirements. Types of lenses Condenser\nLocation: Just under the specimen stage. Role: Focuses light from the lamp onto the specimen → makes the illumination bright and even. Analogy: Like adjusting a flashlight so the beam spreads evenly over what you’re looking at. Objective Lens\nLocation: Closest to the specimen (the rotating lenses right above the slide). Role: This is the main magnifying lens. It gathers light from the specimen and creates a real, enlarged image inside the microscope. Magnification levels: Often 4×, 10×, 40×, 100×. Analogy: Like a zoom lens on a camera — the real workhorse of magnification. Eyepiece\nLocation: The lens you look through at the top of the microscope.\nRole: Magnifies the real image from the objective lens again, producing the final image that your eye sees.\nMagnification level: Usually 10×.\nAnalogy: Like looking through binoculars to enlarge what the camera (objective) already captured.\nCondenser = lights up the stage\nObjective = magnifies the specimen’s image\nEyepiece = magnifies again for your eye\nComparison EM vs LM One cubic millimeter would take thousands of hours. Now we can mimic human segmentation process, and we can build volumetric segmentation.\nFeature Electron Microscopes Light Microscopes Maximum resolution $0.5 \\text{nm}$ $200 \\text{nm}$ Useful magnification Up to $250,000\\times$ in TEM, $100,000\\times$ in SEM Around $1000\\times$ ($1500\\times$ at best) Wavelength $1.0 \\text{nm}$ Between $400-700 \\text{nm}$ Image Details Highly detailed images, and even 3D surface imaging. See reasonable detail, with true colours. Applications/Specimens Can see organelles of cells, bacteria and even viruses. Good for small organisms, invertebrates and whole cells. Feature Light Microscopy TEM SEM Resolution ~200 nm ~0.1 nm ~1–10 nm Live imaging ✅ Possible ❌ Impossible ❌ Impossible Color ✅ Yes ❌ No (grayscale) ❌ No (grayscale) Surface detail ❌ Limited ❌ Mostly internal ✅ Excellent Internal detail ⚠ Limited ✅ Excellent ❌ Mostly surface Sample prep complexity Low Very high Medium Imaging medium Air or liquid High vacuum High vacuum ! Example of scanning electron microscopy image, you can see the vesicles, the synaptic cleft and similar values. In this setting, there is a big problem segmenting the single neurons and identifying what is valid or not. Now we can use machine learning methods to do the segmentation parts. Fluorescence Microscope We have a dichroic mirror The specimen shines in green light when you send some blue light to the specimen, which is then projected back to the camera lens.\nLower energy light is emitted. or emit more photons (super high density of photons in specific space). Red light is less scattered, due to higher wavelength, so it is a little better. This is why it is called two photon microscope, this is localized exitation, which means you get very crisp images.\n! we need focused beams if we want to excite using lower wavelength sources We have 20$\\mu m$, we don’t have neural overlapping, we can use Deep Learning to detect the neurons. Neuro finder challenge for example.\nTwo-Photon is nicer since the wavelength is smaller it:\nScatters less: ~1.5mm! deep tissue imaging Less energy: so it does not do much photo damage to the tissue (at least less). But it has to be very much focused on the objective to create nice images. GPT NOTES:\nExcitation source Intense light (mercury/xenon arc lamp, LEDs, or lasers) floods the sample at the excitation wavelength. Excitation filter Blocks all but the desired excitation wavelength from reaching the sample. Fluorophore absorption Target molecules in the sample absorb the excitation light and enter an excited electronic state. Emission As the fluorophores return to the ground state, they emit photons at the emission wavelength. Dichroic mirror Reflects the excitation wavelength toward the sample but lets the longer-wavelength emission light pass through to the detector. Emission filter Blocks any residual excitation light so only fluorescence reaches the detector/eyepiece. Genetic Editing for Fluorescence Microscopy A promoter is a region of DNA that initiates transcription of a particular gene (a promoter is a DNA sequence that turns the gene ‘on’). Promoters are located near the transcription start sites of genes, on the same strand and upstream on the DNA.\nA reporter gene (often simply reporter) is a gene that researchers attach to a regulatory sequence of another gene of interest. The reporter is only expressed in those cells that express the gene. Certain genes are chosen as reporters because the characteristics they confer are easily identified and measured, or because they are selectable markers (e.g. such as green fluorescent protein, GFP).\nTwo Photon Microscopy Two photon is very focused, I know excitation is just on that part of the light I am focused on. Lightsheet microscope You substitute first the layers of the sample with transpared molecules.\nThen with this special type of microscope you can have nanometer resolution\nYou illuminate only the plane you’re imaging — not the whole sample — which massively reduces photobleaching and speeds up imaging. The illumination and detection paths are perpendicular: One objective lens sends in a thin laser sheet (illumination axis). A second objective lens collects emitted fluorescence from the side (detection axis). It is some sort of laser slicing thing.\nGeneral Microscopy Types ! Confocal vs Two-photon\n!wide field or focused lights\nSuperresolution Microscopy You excite parts of die molecule (sparse manner), hopefully different, many times, and then combine the signals you get from the different images.\nThis was a light microscopy enhancement, which allows you to get high resolution. They won the nobel prize for this idea. The disadvantage is that it is slow, and cannot be done in vivo, because you need many many images to get the final image.\nThere are two main techniques for superresolution microscopy:\nSTED (Stimulated Emission Depletion) First, excite fluorophores with a laser spot. Then, hit the same spot with a donut-shaped depletion beam that forces fluorophores in the periphery back to the ground state via stimulated emission. Only the tiny center region emits → effective resolution down to ~20 nm. SIM (Structured Illumination Microscopy) Illuminate the sample with a known interference pattern (grids or stripes). The moiré effect between the pattern and fine structures shifts high-frequency information into a range you can detect. Multiple patterned images at different angles are computationally reconstructed into a higher-resolution image (~100 nm lateral). In the class we learned about STORM:\nSpecial fluorophores Uses dyes (often cyanine-based) that can be switched between a fluorescent (“on”) and a dark (“off”) state, either by specific wavelengths of light or by chemical environment. Sparse activation Only a random, sparse subset of fluorophores is switched on at any given moment. Because they’re far apart, their images don’t overlap — each appears as an isolated blurry spot (the point spread function, PSF). Localization For each spot, fit the PSF with a Gaussian to determine the emitter’s center with nanometer precision (often \u003c20 nm accuracy). Switch off and repeat Turn those emitters off, switch on another random subset, and repeat thousands of times. Reconstruction Combine all localized points from all cycles into one composite super-resolution image. The problem is that it is very slow, needs some high intensity lights and can damage cells.\nTetrode Recordings Record spikes on 4 nearby contacts, detect events, extract multi-channel waveform features, cluster in 4-channel space, then validate with refractory/quality metrics—yielding isolated single-unit activity from a local population.\nFunctional Microscopy This entails living cells (Calcium and Voltage images), and functional fluorescence indicators. You need to be fast, not like super-resolution one (slow imaging technique). Calcium is proxy of neural spiking, the nice thing is that we can see this in vivo.\nWe use very tiny microscopes, and get a movie out of that, and use this to detect the blinks. Usually it is terabytes of data.\nAdvantages of in Vivo Voltage imaging -\u003e something in the neurons that can be processed, meaning signal that can be received and made signal for. Another nice thing is that we can extract neuron firing voltage parts from image data. (microscopes create huge datasets).\nChange in amplitude is linear with respect of the number of spikes, so I can kind of count the number of spikes in a zone. One of the downsides is that we are biased towards in vivo neurons. If a neuron never fires during our data gathering, then it is not good. for this. The important thing is that calcium is proxy of neural activity.\nSynthetic Calcium Indicators When calcium binds with BAPTA molecule, this changes shape, and can be detected. Nowadays we use GCamP6 protein (new indicator), which can create some nice images. Chemical dyes (e.g., Fura-2, Fluo-4) can be loaded into cells.\nWe can extract data of the activation of the Neural images using calcium in vivo functional imaging. In calcium imaging we leverage a dye could be synthetic or genetic dye (expressed by the neuron itself), when we have a spike, we have intercellular calcium and can be detected\nGeneral approach in extracting information 1.General Methods for cell extraction from Calcium Imaging Data- The PCA/ICA approach.- The Constrained Nonnegative Metrics Factorisation (CNMF) Approach2. Extracting Neuronal Signals from Multi-electrode recordings.- Filtering and defining signal features.- Clustering of cellular signals.3. Basics of Neuronal Population Analysis- Supervised and unsupervised extraction of population components- Decoding variables from neuronal population signals\nExtracting Neural ROIs At the time this paper was published, around 2010 the neural extraction was manual, so it was very important to find a manner to extract it automatically. One problem is to have overlapping ROIs\nIn miniscope imaging this is a big problem. Since the ROI will detects some signal even if some signal is from the neighbours. They have created **subtractive ROIs**, to get the difference between background and other things (we can catch contamination signals around). Decomposition of the neurons $$ Y = AC + B +E $$We need to find $A$ and $C$ mainly. Quality of extraction depends on the quality of the contraints.\n$A$ are the spatial footprints of our neurons. $C$ are the spatial components. $B$ is the background activity $E$ is the noise.\nPCA/ICA See Principal Component Analysis. Neuron shapes and temporal components are spatially and temporally indipendent. We want to get the principal components right. We want to find new statistically independent parts. The problem is that if regions are close, we get a small contamination of the signals. Sometimes we see negative activation due to removing activity in the zones.\nFilters that induce maximum decorrelation sometimes is not feasible, since some neurons can be correlated. We wanted to have statistical independence in the neurons. ROIs for minimum correlation could be not a wanted notion.\nConstrained Nonnegative Matrix Factorization CNMF is a general framework for simultaneously denoising, deconvolving and demixing calcium imaging data.\nWe assume sparse spiking. Spatial and temporal components are all non negative.\n$$ c_{i}(t) = \\sum_{j = 1}^{p} \\gamma_{j}^{(i)}c_{i}(t - j) + s_{j}(t) $$ Where $s_{i}$ is the spiking signal.\nDefining background in CNMF The background fluctuation at each pixel can be represented as a linear combination of its neighboring pixels’ fluctuations.\nWe need to learn a weight matrix, how much one pixel influences its neighbour. We can recover fluctuations in this manner, but in simulated data.\nThe optimization objective We model it as a linear optimization problem, so you can use Lagrange Multipliers The technique then becomes a pipeline, since we need some $A, C, B$ estimates from the beginning.\nWe don’t see neighbour influence anymore We don't have the problem of the negative values\n### Spike Sorting - **Spike sorting** is the process of: 1. **Detecting spikes** in the raw voltage trace. 2. **Classifying spikes** to their neuron of origin based on features like: - Waveform shape - Amplitude - Timing 3. **Assigning each spike** to a distinct neuron. Sorting pipeline Some spikes have a very very small signal, they are very difficult to classify. Often we don’t have a nice profile, so we just cluster them together. We have thresholding mechanisms to do spike detection.\nFiltering with Features We have standard bandpass filtering 300-400: 4000-7000\nThen we can cut out a window around the spike, and that is our spike function. And we can take out features like:\nPeak amplitude Trough amplitude Crest to trough amplitude Width Rise slope Fall slope One way is to use PCA to reduce it to few features and then use that. And this representation is nice for visual clustering of the type of spikes that we have.\nThen we have many wavelets to chose from to model our neron activation.\nDiscrete Wavelet Transform We use Discrete Wavelet transform with many convolutions. We know how much each wavelet is in the spike in this manner. It learns some coefficients to apply to the recorded data. Perhaps you can view this also as some kind of compression. Then we can sort by choosing the coefficients for higher variance.\nOften this clustering part is done by humans after we have a good compression. We like to group population of neurons for some reason. I can do some sort of threshold analysis with that. This is called population decoder how the activation of a group of neurons move when activated. The decoder can tell you for example the position of a mouse by looking at grid maps of a in vivo recording for mices! You can use this only decoder when the variable of interest is related to your stuff. 89m 05.05.2025 video.\nShuffled decoder comparisons are nice Confounding variables for Neuron Imaging Neuron bursting: Spikes that occur in rapid succession (bursts) often have different waveforms than isolated spikes, potentially confusing spike-sorting algorithms or waveform-based analyses. Waveform overlaps of near-synchronous spikes: When multiple neurons fire nearly simultaneously, their extracellular waveforms can overlap, making it difficult to separate or identify individual neurons. Back-propagation of dendritic action potentials: These are action potentials traveling from the soma back into the dendrites. They can generate electrical signals that may be misinterpreted as separate spike events. Electrode drift over time: Physical movement of the electrode relative to the neuron (or vice versa) can cause changes in recorded waveforms, leading to incorrect clustering or loss of signal continuity. Bio-physiological differences across brain regions (CA1 vs. Cortex): Different brain regions can have different neuron types, morphologies, and firing properties, which may influence waveform shape and other features—limiting generalization across datasets. ",
  "wordCount" : "2843",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/neural-imaging/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Neural Imaging
    </h1>
    <div class="post-meta">14 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#basics-of-microscopy" aria-label="Basics of Microscopy">Basics of Microscopy</a><ul>
                        
                <li>
                    <a href="#the-optical-principle" aria-label="The optical Principle">The optical Principle</a></li>
                <li>
                    <a href="#lens-physics" aria-label="Lens Physics">Lens Physics</a></li>
                <li>
                    <a href="#measuring-resolution" aria-label="Measuring resolution">Measuring resolution</a></li>
                <li>
                    <a href="#types-of-microscopy" aria-label="Types of microscopy">Types of microscopy</a></li></ul>
                </li>
                <li>
                    <a href="#different-microscopes" aria-label="Different microscopes">Different microscopes</a><ul>
                        
                <li>
                    <a href="#electron-microscopy" aria-label="Electron Microscopy">Electron Microscopy</a></li>
                <li>
                    <a href="#types-of-lenses" aria-label="Types of lenses">Types of lenses</a></li>
                <li>
                    <a href="#comparison-em-vs-lm" aria-label="Comparison EM vs LM">Comparison EM vs LM</a></li>
                <li>
                    <a href="#fluorescence-microscope" aria-label="Fluorescence Microscope">Fluorescence Microscope</a></li>
                <li>
                    <a href="#genetic-editing-for-fluorescence-microscopy" aria-label="Genetic Editing for Fluorescence Microscopy">Genetic Editing for Fluorescence Microscopy</a></li>
                <li>
                    <a href="#two-photon-microscopy" aria-label="Two Photon Microscopy">Two Photon Microscopy</a></li>
                <li>
                    <a href="#lightsheet-microscope" aria-label="Lightsheet microscope">Lightsheet microscope</a></li>
                <li>
                    <a href="#general-microscopy-types" aria-label="General Microscopy Types">General Microscopy Types</a></li>
                <li>
                    <a href="#superresolution-microscopy" aria-label="Superresolution Microscopy">Superresolution Microscopy</a></li>
                <li>
                    <a href="#tetrode-recordings" aria-label="Tetrode Recordings">Tetrode Recordings</a></li></ul>
                </li>
                <li>
                    <a href="#functional-microscopy" aria-label="Functional Microscopy">Functional Microscopy</a><ul>
                        
                <li>
                    <a href="#advantages-of-in-vivo" aria-label="Advantages of in Vivo">Advantages of in Vivo</a></li>
                <li>
                    <a href="#synthetic-calcium-indicators" aria-label="Synthetic Calcium Indicators">Synthetic Calcium Indicators</a></li>
                <li>
                    <a href="#general-approach-in-extracting-information" aria-label="General approach in extracting information">General approach in extracting information</a></li>
                <li>
                    <a href="#extracting-neural-rois" aria-label="Extracting Neural ROIs">Extracting Neural ROIs</a></li>
                <li>
                    <a href="#decomposition-of-the-neurons" aria-label="Decomposition of the neurons">Decomposition of the neurons</a></li>
                <li>
                    <a href="#pcaica" aria-label="PCA/ICA">PCA/ICA</a></li>
                <li>
                    <a href="#constrained-nonnegative-matrix-factorization" aria-label="Constrained Nonnegative Matrix Factorization">Constrained Nonnegative Matrix Factorization</a></li>
                <li>
                    <a href="#defining-background-in-cnmf" aria-label="Defining background in CNMF">Defining background in CNMF</a></li>
                <li>
                    <a href="#the-optimization-objective" aria-label="The optimization objective">The optimization objective</a></li>
                <li>
                    <a href="#sorting-pipeline" aria-label="Sorting pipeline">Sorting pipeline</a></li>
                <li>
                    <a href="#filtering-with-features" aria-label="Filtering with Features">Filtering with Features</a></li>
                <li>
                    <a href="#discrete-wavelet-transform" aria-label="Discrete Wavelet Transform">Discrete Wavelet Transform</a></li>
                <li>
                    <a href="#confounding-variables-for-neuron-imaging" aria-label="Confounding variables for Neuron Imaging">Confounding variables for Neuron Imaging</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In general we want to understand how neurons encode the rate and temporal information to build specific features like place cells, grid cells, velocity, head direction, or how it can guide behaviour or coordination. Many neurons encode together some features, it is quite rare that you have the face neuron and similars. Imaging techniques help us to get more information about these parts.</p>
<h3 id="basics-of-microscopy">Basics of Microscopy<a hidden class="anchor" aria-hidden="true" href="#basics-of-microscopy">#</a></h3>
<figure class="center">
<img src="/images/notes/Neural Imaging-20250810121312442.webp" style="width: 100%"   alt="Neural Imaging-20250810121312442" title="Neural Imaging-20250810121312442"/>
<figcaption><p style="text-align:center;">Image of a classical microscope, from course slides</p></figcaption>
</figure>
<h4 id="the-optical-principle">The optical Principle<a hidden class="anchor" aria-hidden="true" href="#the-optical-principle">#</a></h4>
<p>The basic idea is to have one first lens that makes an object bigger but inverted, and another lens, called the <strong>eyepiece</strong> that sees the original part bigger, and in correct shape. With some high school physics is possible to compute how much is the enlargement due to the lens.</p>
<h4 id="lens-physics">Lens Physics<a hidden class="anchor" aria-hidden="true" href="#lens-physics">#</a></h4>
$$
\frac{1}{f} = \frac{1}{d_o} + \frac{1}{d_i}
$$$$
M = \frac{d_i}{d_o}
$$<p>
Where $d_{o}$ is the distance of the object to the first lens, and $d_{i}$ is the distance of the image to the first lens.</p>
<p>The two magnifications compound with each other, giving a final value of $M = m_{1} + m_{2}$.</p>
<img src="/images/notes/Neural Imaging-20250810121012165.webp" style="width: 100%" class="center" alt="Neural Imaging-20250810121012165">
#### Rayleigh Criterion 
This criterion, closely related to image resolution, is defined as:
> The limit at which two Airy disks can be resolved into separate entities.
<img src="/images/notes/Neural Imaging-20250810120720339.webp" width="410" class="center" alt="Neural Imaging-20250810120720339"/>
$$
\Delta x = \frac{1.22 \cdot \lambda}{2NA}
$$$$
NA = n \cdot sin(\theta)
$$<p>
Where $n$ is the index of refraction of the medium, and $\theta$ is the angle of the light cone that enters the lens.</p>
<p>Two main things in this formula:</p>
<ul>
<li>Wavelength</li>
<li>Amplitude of the light (low NA means lower resolution).</li>
</ul>
<p>This means we cannot distinguish two objects that are closer than $\Delta x$, and this is the limit of the resolution of the lens.</p>
<p>Looking at the aperture you can observe that if you are closer (wider aperture angle), then you resolution is higher, but less field of view.
Sometimes is not very easy to bring the object to the tissue (bone, some 3d structure e.g.).</p>
<h4 id="measuring-resolution">Measuring resolution<a hidden class="anchor" aria-hidden="true" href="#measuring-resolution">#</a></h4>
<p>In the lab the resolution of a microscope is measured using small beads and spheres of 100nm, we define the resolution as the width of the blurred dots that are produced by the lens.
<img src="/images/notes/Neural Imaging-20250418154540586.webp" style="width: 100%" class="center" alt="Neural Imaging-20250418154540586"></p>
<h4 id="types-of-microscopy">Types of microscopy<a hidden class="anchor" aria-hidden="true" href="#types-of-microscopy">#</a></h4>
<img src="/images/notes/Neural Imaging-20250418163319038.webp" style="width: 100%" class="center" alt="Neural Imaging-20250418163319038">
<h3 id="different-microscopes">Different microscopes<a hidden class="anchor" aria-hidden="true" href="#different-microscopes">#</a></h3>
<h4 id="electron-microscopy">Electron Microscopy<a hidden class="anchor" aria-hidden="true" href="#electron-microscopy">#</a></h4>
<p>We have a wavelength of <strong>nanometers</strong> (meaning we have a nice resolution), which means the resolution is orders of magnitude better than the light microscopy.
We use magnets as lenses in this case.
Nice thing about scanning electron microscopy is that you can gather 3D information (scattering can be seen at different angles), and the resolution is much better than light microscopy.
<img src="/images/notes/Neural Imaging-20250418155833205.webp" style="width: 100%" class="center" alt="Neural Imaging-20250418155833205"></p>
<ul>
<li>LM -&gt; 200nm</li>
<li><strong>TEM</strong> = Think <em>transparency</em>: electrons go <em>through</em> → internal structure, ultra-high resolution. 0.1-1 nm</li>
<li><strong>SEM</strong> = Think <em>surface</em>: electrons <em>scan</em> across → surface shape &amp; composition. ~1-10nm</li>
<li>Both are limited by sample prep complexity and vacuum requirements.</li>
</ul>
<h4 id="types-of-lenses">Types of lenses<a hidden class="anchor" aria-hidden="true" href="#types-of-lenses">#</a></h4>
<p><strong>Condenser</strong></p>
<ul>
<li><strong>Location:</strong> Just under the specimen stage.</li>
<li><strong>Role:</strong> Focuses light from the lamp onto the specimen → makes the illumination <strong>bright and even</strong>.</li>
<li><strong>Analogy:</strong> Like adjusting a flashlight so the beam spreads evenly over what you’re looking at.</li>
</ul>
<p><strong>Objective Lens</strong></p>
<ul>
<li><strong>Location:</strong> Closest to the specimen (the rotating lenses right above the slide).</li>
<li><strong>Role:</strong> This is the <strong>main magnifying lens</strong>. It gathers light from the specimen and creates a real, enlarged image inside the microscope.</li>
<li><strong>Magnification levels:</strong> Often 4×, 10×, 40×, 100×.</li>
<li><strong>Analogy:</strong> Like a zoom lens on a camera — the real workhorse of magnification.</li>
</ul>
<p><strong>Eyepiece</strong></p>
<ul>
<li>
<p><strong>Location:</strong> The lens you look through at the top of the microscope.</p>
</li>
<li>
<p><strong>Role:</strong> Magnifies the real image from the objective lens again, producing the final image that your eye sees.</p>
</li>
<li>
<p><strong>Magnification level:</strong> Usually 10×.</p>
</li>
<li>
<p><strong>Analogy:</strong> Like looking through binoculars to enlarge what the camera (objective) already captured.</p>
</li>
<li>
<p>Condenser = lights up the stage</p>
</li>
<li>
<p>Objective = magnifies the specimen’s image</p>
</li>
<li>
<p>Eyepiece = magnifies again for your eye</p>
</li>
</ul>
<h4 id="comparison-em-vs-lm">Comparison EM vs LM<a hidden class="anchor" aria-hidden="true" href="#comparison-em-vs-lm">#</a></h4>
<p>One cubic millimeter would take thousands of hours. Now we can mimic human segmentation process, and we can build volumetric segmentation.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Feature</th>
          <th style="text-align: left">Electron Microscopes</th>
          <th style="text-align: left">Light Microscopes</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>Maximum resolution</strong></td>
          <td style="text-align: left">$0.5 \text{nm}$</td>
          <td style="text-align: left">$200 \text{nm}$</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Useful magnification</strong></td>
          <td style="text-align: left">Up to $250,000\times$ in TEM, $100,000\times$ in SEM</td>
          <td style="text-align: left">Around $1000\times$ ($1500\times$ at best)</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Wavelength</strong></td>
          <td style="text-align: left">$1.0 \text{nm}$</td>
          <td style="text-align: left">Between $400-700 \text{nm}$</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Image Details</strong></td>
          <td style="text-align: left">Highly detailed images, and even 3D surface imaging.</td>
          <td style="text-align: left">See reasonable detail, with true colours.</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Applications/Specimens</strong></td>
          <td style="text-align: left">Can see organelles of cells, bacteria and even viruses.</td>
          <td style="text-align: left">Good for small organisms, invertebrates and whole cells.</td>
      </tr>
  </tbody>
</table>
<table>
  <thead>
      <tr>
          <th>Feature</th>
          <th>Light Microscopy</th>
          <th>TEM</th>
          <th>SEM</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Resolution</td>
          <td>~200 nm</td>
          <td>~0.1 nm</td>
          <td>~1–10 nm</td>
      </tr>
      <tr>
          <td>Live imaging</td>
          <td>✅ Possible</td>
          <td>❌ Impossible</td>
          <td>❌ Impossible</td>
      </tr>
      <tr>
          <td>Color</td>
          <td>✅ Yes</td>
          <td>❌ No (grayscale)</td>
          <td>❌ No (grayscale)</td>
      </tr>
      <tr>
          <td>Surface detail</td>
          <td>❌ Limited</td>
          <td>❌ Mostly internal</td>
          <td>✅ Excellent</td>
      </tr>
      <tr>
          <td>Internal detail</td>
          <td>⚠ Limited</td>
          <td>✅ Excellent</td>
          <td>❌ Mostly surface</td>
      </tr>
      <tr>
          <td>Sample prep complexity</td>
          <td>Low</td>
          <td>Very high</td>
          <td>Medium</td>
      </tr>
      <tr>
          <td>Imaging medium</td>
          <td>Air or liquid</td>
          <td>High vacuum</td>
          <td>High vacuum</td>
      </tr>
      <tr>
          <td>!<a href="/notes/neural-imaging-20250810214630734.webp-"> Example of scanning electron microscopy image, you can see the vesicles, the synaptic cleft and similar values.</a></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td>In this setting, there is a big problem segmenting the single neurons and identifying what is valid or not. Now we can use machine learning methods to do the segmentation parts.</td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<h4 id="fluorescence-microscope">Fluorescence Microscope<a hidden class="anchor" aria-hidden="true" href="#fluorescence-microscope">#</a></h4>
<p>We have a <strong>dichroic mirror</strong> The specimen shines in green light when you send some <em>blue</em> light to the specimen, which is then projected back to the camera lens.</p>
<p>Lower energy light is emitted. or emit more photons (super high density of photons in specific space).
Red light is less scattered, due to higher wavelength, so it is a little better. This is why it is called <strong>two photon microscope</strong>, this is localized exitation, which means you get very crisp images.</p>
<p>!<a href="/notes/neural-imaging-20250418162737401.webp-"> we need focused beams if we want to excite using lower wavelength sources</a>
We have 20$\mu m$, we don&rsquo;t have neural overlapping, we can use Deep Learning to detect the neurons.
Neuro finder challenge for example.</p>
<p>Two-Photon is nicer since the wavelength is smaller it:</p>
<ul>
<li><strong>Scatters less</strong>: <strong>~1.5mm!</strong> deep tissue imaging</li>
<li><strong>Less energy</strong>: so it does not do much photo damage to the tissue (at least less).</li>
<li>But it has to be very much focused on the objective to create nice images.</li>
</ul>
<p>GPT NOTES:</p>
<ol>
<li><strong>Excitation source</strong>
<ul>
<li>Intense light (mercury/xenon arc lamp, LEDs, or lasers) floods the sample at the excitation wavelength.</li>
</ul>
</li>
<li><strong>Excitation filter</strong>
<ul>
<li>Blocks all but the desired excitation wavelength from reaching the sample.</li>
</ul>
</li>
<li><strong>Fluorophore absorption</strong>
<ul>
<li>Target molecules in the sample absorb the excitation light and enter an excited electronic state.</li>
</ul>
</li>
<li><strong>Emission</strong>
<ul>
<li>As the fluorophores return to the ground state, they emit photons at the emission wavelength.</li>
</ul>
</li>
<li><strong>Dichroic mirror</strong>
<ul>
<li>Reflects the excitation wavelength toward the sample but lets the longer-wavelength emission light pass through to the detector.</li>
</ul>
</li>
<li><strong>Emission filter</strong>
<ul>
<li>Blocks any residual excitation light so only fluorescence reaches the detector/eyepiece.</li>
</ul>
</li>
</ol>
<h4 id="genetic-editing-for-fluorescence-microscopy">Genetic Editing for Fluorescence Microscopy<a hidden class="anchor" aria-hidden="true" href="#genetic-editing-for-fluorescence-microscopy">#</a></h4>
<p>A <strong>promoter</strong> is a region of DNA that initiates transcription of a particular gene (a promoter is a DNA sequence that turns the gene ‘on’). Promoters are located near the transcription start sites of genes, on the same strand and upstream on the DNA.</p>
<p>A <strong>reporter gene</strong> (often simply reporter) is a gene that researchers attach to a regulatory sequence of another gene of interest. The reporter is only expressed in those cells that express the gene. Certain genes are chosen as reporters because the characteristics they confer are easily identified and measured, or because they are selectable markers (e.g. such as green fluorescent protein, GFP).</p>
<h4 id="two-photon-microscopy">Two Photon Microscopy<a hidden class="anchor" aria-hidden="true" href="#two-photon-microscopy">#</a></h4>
<p>Two photon is very focused, I know excitation is just on that part of the light I am focused on.
<img src="/images/notes/Neural Imaging-20250810215619984.webp" style="width: 100%" class="center" alt="Neural Imaging-20250810215619984"></p>
<h4 id="lightsheet-microscope">Lightsheet microscope<a hidden class="anchor" aria-hidden="true" href="#lightsheet-microscope">#</a></h4>
<p>You substitute first the layers of the sample with transpared molecules.</p>
<p>Then with this special type of microscope you can have nanometer resolution</p>
<ul>
<li>You illuminate <strong>only the plane you’re imaging</strong> — not the whole sample — which massively reduces photobleaching and speeds up imaging.</li>
<li>The illumination and detection paths are <strong>perpendicular</strong>:
<ul>
<li>One objective lens sends in a thin laser sheet (illumination axis).</li>
<li>A second objective lens collects emitted fluorescence from the side (detection axis).</li>
</ul>
</li>
</ul>
<p>It is some sort of <strong>laser slicing</strong> thing.</p>
<h4 id="general-microscopy-types">General Microscopy Types<a hidden class="anchor" aria-hidden="true" href="#general-microscopy-types">#</a></h4>
<p>!<a href="/notes/neural-imaging-20250810215839545.webp-"> Confocal vs Two-photon</a></p>
<p>!<a href="/notes/neural-imaging-20250810215907483.webp-">wide field or focused lights</a></p>
<h4 id="superresolution-microscopy">Superresolution Microscopy<a hidden class="anchor" aria-hidden="true" href="#superresolution-microscopy">#</a></h4>
<p>You excite parts of die molecule (sparse manner), hopefully different, many times, and then combine the signals you get from the different images.</p>
<img src="/images/notes/Neural Imaging-20250418170810861.webp" style="width: 100%" class="center" alt="Neural Imaging-20250418170810861">
<p>This was a light microscopy enhancement, which allows you to get high resolution. They won the nobel prize for this idea.
The disadvantage is that it is slow, and cannot be done in vivo, because you need many many images to get the final image.</p>
<p>There are two main techniques for superresolution microscopy:</p>
<ul>
<li><strong>STED (Stimulated Emission Depletion)</strong>
<ul>
<li>First, excite fluorophores with a laser spot.</li>
<li>Then, hit the same spot with a donut-shaped depletion beam that forces fluorophores in the periphery back to the ground state via stimulated emission.</li>
<li>Only the tiny center region emits → effective resolution down to ~20 nm.</li>
</ul>
</li>
<li><strong>SIM (Structured Illumination Microscopy)</strong>
<ul>
<li>Illuminate the sample with a known interference pattern (grids or stripes).</li>
<li>The moiré effect between the pattern and fine structures shifts high-frequency information into a range you can detect.</li>
<li>Multiple patterned images at different angles are computationally reconstructed into a higher-resolution image (~100 nm lateral).</li>
</ul>
</li>
</ul>
<p>In the class we learned about STORM:</p>
<ul>
<li><strong>Special fluorophores</strong>
<ul>
<li>Uses dyes (often cyanine-based) that can be switched between a fluorescent (“on”) and a dark (“off”) state, either by specific wavelengths of light or by chemical environment.</li>
</ul>
</li>
<li><strong>Sparse activation</strong>
<ul>
<li>Only a random, sparse subset of fluorophores is switched on at any given moment.</li>
<li>Because they’re far apart, their images don’t overlap — each appears as an isolated blurry spot (the point spread function, PSF).</li>
</ul>
</li>
<li><strong>Localization</strong>
<ul>
<li>For each spot, fit the PSF with a Gaussian to determine the emitter’s center with nanometer precision (often &lt;20 nm accuracy).</li>
</ul>
</li>
<li><strong>Switch off and repeat</strong>
<ul>
<li>Turn those emitters off, switch on another random subset, and repeat thousands of times.</li>
</ul>
</li>
<li><strong>Reconstruction</strong>
<ul>
<li>Combine all localized points from all cycles into one composite super-resolution image.</li>
</ul>
</li>
</ul>
<p>The problem is that it is very slow, needs some high intensity lights and can damage cells.</p>
<h4 id="tetrode-recordings">Tetrode Recordings<a hidden class="anchor" aria-hidden="true" href="#tetrode-recordings">#</a></h4>
<p>Record spikes on 4 nearby contacts, detect events, extract multi-channel waveform features, <strong>cluster in 4-channel space</strong>, then <strong>validate with refractory/quality metrics</strong>—yielding isolated single-unit activity from a local population.</p>
<h3 id="functional-microscopy">Functional Microscopy<a hidden class="anchor" aria-hidden="true" href="#functional-microscopy">#</a></h3>
<p>This entails living cells (Calcium and Voltage images), and functional fluorescence indicators.
You need to be <strong>fast</strong>, not like super-resolution one (slow imaging technique). Calcium is proxy of neural spiking, the nice thing is that we can see this in vivo.</p>
<p>We use very tiny microscopes, and get a <strong>movie</strong> out of that, and use this to detect the blinks. Usually it is <strong>terabytes</strong> of data.</p>
<h4 id="advantages-of-in-vivo">Advantages of in Vivo<a hidden class="anchor" aria-hidden="true" href="#advantages-of-in-vivo">#</a></h4>
<p>Voltage imaging -&gt; something in the neurons that can be processed, meaning signal that can be received and made signal for.
Another nice thing is that we can extract neuron firing voltage parts from image data. (microscopes create huge datasets).</p>
<p>Change in amplitude is linear with respect of the number of spikes, so I can kind of count the number of spikes in a zone.
One of the downsides is that we are <strong>biased</strong> towards in vivo neurons. If a neuron never fires during our data gathering, then it is not good. for this.
The important thing is that calcium is <strong>proxy of neural activity</strong>.</p>
<h4 id="synthetic-calcium-indicators">Synthetic Calcium Indicators<a hidden class="anchor" aria-hidden="true" href="#synthetic-calcium-indicators">#</a></h4>
<p>When calcium binds with BAPTA molecule, this changes shape, and can be detected. Nowadays we use <strong>GCamP6</strong> protein (new indicator), which can create some nice images.
<strong>Chemical dyes</strong> (e.g., Fura-2, Fluo-4) can be loaded into cells.</p>
<p>We can extract data of the activation of the Neural images using calcium in vivo functional imaging.
In calcium imaging we leverage a <strong>dye</strong> could be synthetic or <strong>genetic</strong> dye (expressed by the neuron itself), when we have a spike, we have intercellular calcium and can be detected</p>
<h4 id="general-approach-in-extracting-information">General approach in extracting information<a hidden class="anchor" aria-hidden="true" href="#general-approach-in-extracting-information">#</a></h4>
<p>1.General Methods for cell extraction from Calcium Imaging Data- The PCA/ICA approach.- The Constrained Nonnegative Metrics Factorisation (CNMF) Approach2. Extracting Neuronal Signals from Multi-electrode recordings.- Filtering and defining signal features.- Clustering of cellular signals.3. Basics of Neuronal Population Analysis- Supervised and unsupervised extraction of population components- Decoding variables from neuronal population signals</p>
<h4 id="extracting-neural-rois">Extracting Neural ROIs<a hidden class="anchor" aria-hidden="true" href="#extracting-neural-rois">#</a></h4>
<p>At the time this paper was published, around 2010 the neural extraction was manual, so it was very important to find a manner to extract it automatically.
One problem is to have overlapping ROIs</p>
<img src="/images/notes/Neural Imaging-20250805212727899.webp" style="width: 100%" class="center" alt="Neural Imaging-20250805212727899">
In miniscope imaging this is a big problem.
Since the ROI will detects some signal even if some signal is from the neighbours.
They have created **subtractive ROIs**, to get the difference between background and other things (we can catch contamination signals around).
<h4 id="decomposition-of-the-neurons">Decomposition of the neurons<a hidden class="anchor" aria-hidden="true" href="#decomposition-of-the-neurons">#</a></h4>
<img src="/images/notes/Neural Imaging-20250805213801819.webp" style="width: 100%" class="center" alt="Neural Imaging-20250805213801819">
$$
Y = AC + B +E
$$<p>We need to find $A$ and $C$ mainly.
Quality of extraction depends on the quality of the contraints.</p>
<p>$A$ are the <strong>spatial footprints</strong> of our neurons.
$C$ are the spatial components.
$B$ is the background activity
$E$ is the noise.</p>
<h4 id="pcaica">PCA/ICA<a hidden class="anchor" aria-hidden="true" href="#pcaica">#</a></h4>
<p>See Principal Component Analysis.
Neuron shapes and temporal components are <strong>spatially</strong> and <strong>temporally</strong> <strong>indipendent</strong>.
We want to get the principal components right.
<img src="/images/notes/Neural Imaging-20250805214051623.webp" width="602" class="center" alt="Neural Imaging-20250805214051623"/></p>
<p>We want to find new statistically independent parts.
The problem is that if regions are close, we get a small <strong>contamination</strong> of the signals.
Sometimes we see negative activation due to removing activity in the zones.</p>
<p>Filters that induce maximum decorrelation sometimes is not feasible, since some neurons can be correlated. We wanted to have statistical independence in the neurons. ROIs for minimum correlation could be not a wanted notion.</p>
<h4 id="constrained-nonnegative-matrix-factorization">Constrained Nonnegative Matrix Factorization<a hidden class="anchor" aria-hidden="true" href="#constrained-nonnegative-matrix-factorization">#</a></h4>
<blockquote>
<p>CNMF is a general framework for simultaneously denoising, deconvolving and demixing calcium imaging data.</p></blockquote>
<p>We assume sparse spiking. Spatial and temporal components are all non negative.</p>
$$
c_{i}(t) = \sum_{j = 1}^{p} \gamma_{j}^{(i)}c_{i}(t - j) + s_{j}(t)
$$<p>
Where $s_{i}$ is the spiking signal.</p>
<h4 id="defining-background-in-cnmf">Defining background in CNMF<a hidden class="anchor" aria-hidden="true" href="#defining-background-in-cnmf">#</a></h4>
<blockquote>
<p>The background fluctuation at each pixel can be represented as a linear combination of its neighboring pixels’ fluctuations.</p></blockquote>
<p>We need to learn a weight matrix, how much one pixel influences its neighbour.
We can recover fluctuations in this manner, but in simulated data.</p>
<h4 id="the-optimization-objective">The optimization objective<a hidden class="anchor" aria-hidden="true" href="#the-optimization-objective">#</a></h4>
<p>We model it as a linear optimization problem, so you can use <a href="/notes/lagrange-multipliers">Lagrange Multipliers</a>
<img src="/images/notes/Neural Imaging-20250805215034380.webp" style="width: 100%" class="center" alt="Neural Imaging-20250805215034380"></p>
<p>The technique then becomes a pipeline, since we need some $A, C, B$ estimates from the beginning.</p>
<p>We don&rsquo;t see neighbour influence anymore
<img src="/images/notes/Neural Imaging-20250805220954036.webp" style="width: 100%" class="center" alt="Neural Imaging-20250805220954036"></p>
<figure class="center">
<img src="/images/notes/Neural Imaging-20250805221010121.webp" style="width: 100%"   alt="Neural Imaging-20250805221010121" title="Neural Imaging-20250805221010121"/>
<figcaption><p style="text-align:center;">We don't have the problem of the negative values</p></figcaption>
</figure>
### Spike Sorting
- **Spike sorting** is the process of:
    1. **Detecting spikes** in the raw voltage trace.
    2. **Classifying spikes** to their neuron of origin based on features like:
        - Waveform shape
        - Amplitude
        - Timing
    3. **Assigning each spike** to a distinct neuron.
<h4 id="sorting-pipeline">Sorting pipeline<a hidden class="anchor" aria-hidden="true" href="#sorting-pipeline">#</a></h4>
<p>Some spikes have a very very small signal, they are very difficult to classify. Often we don&rsquo;t have a nice profile, so we just cluster them together.
We have thresholding mechanisms to do spike detection.</p>
<img src="/images/notes/Neural Imaging-20250808172945165.webp" width="630" class="center" alt="Neural Imaging-20250808172945165"/>
<h4 id="filtering-with-features">Filtering with Features<a hidden class="anchor" aria-hidden="true" href="#filtering-with-features">#</a></h4>
<p>We have standard bandpass filtering 300-400: 4000-7000</p>
<p>Then we can cut out a window around the spike, and that is our spike function. And we can take out features like:</p>
<ol>
<li>Peak amplitude</li>
<li>Trough amplitude</li>
<li>Crest to trough amplitude</li>
<li>Width</li>
<li>Rise slope</li>
<li>Fall slope</li>
</ol>
<img src="/images/notes/Neural Imaging-20250808173339900.webp" style="width: 100%" class="center" alt="Neural Imaging-20250808173339900">
<p>One way is to use PCA to reduce it to few features and then use that. And this representation is nice for visual clustering of the type of spikes that we have.</p>
<p>Then we have many wavelets to chose from to model our neron activation.</p>
<h4 id="discrete-wavelet-transform">Discrete Wavelet Transform<a hidden class="anchor" aria-hidden="true" href="#discrete-wavelet-transform">#</a></h4>
<p>We use <strong>Discrete Wavelet transform</strong> with many convolutions.
We know how much each wavelet is in the spike in this manner. It learns some coefficients to apply to the recorded data. Perhaps you can view this also as some kind of compression.
Then we can sort by choosing the coefficients for higher variance.</p>
<p>Often this clustering part is done by humans after we have a good compression.
We like to group population of neurons for some reason.
I can do some sort of threshold analysis with that.
This is called <strong>population decoder</strong> how the activation of a group of neurons move when activated. The decoder can tell you for example the position of a mouse by looking at grid maps of a in vivo recording for mices!
You can use this only decoder when the variable of interest is related to your stuff. 89m 05.05.2025 video.</p>
<ul>
<li>Shuffled decoder comparisons are nice</li>
</ul>
<h4 id="confounding-variables-for-neuron-imaging">Confounding variables for Neuron Imaging<a hidden class="anchor" aria-hidden="true" href="#confounding-variables-for-neuron-imaging">#</a></h4>
<ol>
<li><strong>Neuron bursting</strong>: Spikes that occur in rapid succession (bursts) often have different waveforms than isolated spikes, potentially confusing spike-sorting algorithms or waveform-based analyses.</li>
<li><strong>Waveform overlaps of near-synchronous spikes</strong>: When multiple neurons fire nearly simultaneously, their extracellular waveforms can overlap, making it difficult to separate or identify individual neurons.</li>
<li><strong>Back-propagation of dendritic action potentials</strong>: These are action potentials traveling from the soma back into the dendrites. They can generate electrical signals that may be misinterpreted as separate spike events.</li>
<li><strong>Electrode drift over time</strong>: Physical movement of the electrode relative to the neuron (or vice versa) can cause changes in recorded waveforms, leading to incorrect clustering or loss of signal continuity.</li>
<li><strong>Bio-physiological differences across brain regions (CA1 vs. Cortex)</strong>: Different brain regions can have different neuron types, morphologies, and firing properties, which may influence waveform shape and other features—limiting generalization across datasets.</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/neuroscience/">Neuroscience</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Imaging on x"
            href="https://x.com/intent/tweet/?text=Neural%20Imaging&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fneural-imaging%2f&amp;hashtags=neuroscience">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Imaging on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fneural-imaging%2f&amp;title=Neural%20Imaging&amp;summary=Neural%20Imaging&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fneural-imaging%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Imaging on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fneural-imaging%2f&title=Neural%20Imaging">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Imaging on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fneural-imaging%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Imaging on whatsapp"
            href="https://api.whatsapp.com/send?text=Neural%20Imaging%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fneural-imaging%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Imaging on telegram"
            href="https://telegram.me/share/url?text=Neural%20Imaging&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fneural-imaging%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Imaging on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Neural%20Imaging&u=https%3a%2f%2fflecart.github.io%2fnotes%2fneural-imaging%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
