<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Linear Regression methods | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning">
<meta name="description" content="We will present some methods related to regression methods for data analysis. Some of the work here is from (Hastie et al. 2009). This note does not treat the bayesian case, you should see Bayesian Linear Regression for that.
Problem setting In usual regression problems we want to reach the $\arg \min \mathbb{E}_{Y \mid X} \left[ (Y - f(X))^{2} \right]$ and the solution is given by the conditional mean: $f^{*} = \mathbb{E}(Y \mid X = x)$.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/linear-regression-methods/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/linear-regression-methods/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Linear Regression methods" />
<meta property="og:description" content="We will present some methods related to regression methods for data analysis. Some of the work here is from (Hastie et al. 2009). This note does not treat the bayesian case, you should see Bayesian Linear Regression for that.
Problem setting In usual regression problems we want to reach the $\arg \min \mathbb{E}_{Y \mid X} \left[ (Y - f(X))^{2} \right]$ and the solution is given by the conditional mean: $f^{*} = \mathbb{E}(Y \mid X = x)$." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/linear-regression-methods/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Linear Regression methods"/>
<meta name="twitter:description" content="We will present some methods related to regression methods for data analysis. Some of the work here is from (Hastie et al. 2009). This note does not treat the bayesian case, you should see Bayesian Linear Regression for that.
Problem setting In usual regression problems we want to reach the $\arg \min \mathbb{E}_{Y \mid X} \left[ (Y - f(X))^{2} \right]$ and the solution is given by the conditional mean: $f^{*} = \mathbb{E}(Y \mid X = x)$."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Linear Regression methods",
      "item": "https://flecart.github.io/notes/linear-regression-methods/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Linear Regression methods",
  "name": "Linear Regression methods",
  "description": "We will present some methods related to regression methods for data analysis. Some of the work here is from (Hastie et al. 2009). This note does not treat the bayesian case, you should see Bayesian Linear Regression for that.\nProblem setting In usual regression problems we want to reach the $\\arg \\min \\mathbb{E}_{Y \\mid X} \\left[ (Y - f(X))^{2} \\right]$ and the solution is given by the conditional mean: $f^{*} = \\mathbb{E}(Y \\mid X = x)$.",
  "keywords": [
    "machinelearning"
  ],
  "articleBody": "We will present some methods related to regression methods for data analysis. Some of the work here is from (Hastie et al. 2009). This note does not treat the bayesian case, you should see Bayesian Linear Regression for that.\nProblem setting In usual regression problems we want to reach the $\\arg \\min \\mathbb{E}_{Y \\mid X} \\left[ (Y - f(X))^{2} \\right]$ and the solution is given by the conditional mean: $f^{*} = \\mathbb{E}(Y \\mid X = x)$. We have done something similar with Logistic Regression, but that is just for classification analysis. For linear regression models we need to find the parameters $\\beta$ for this function $$ Y = \\beta_{0} + \\sum_{j = 1}^{d} X_{j}\\beta_{j} $$ We usually don‚Äôt know the distribution of $P(X)$ or $P(Y \\mid X)$ so we need to assume something about these distributions.\nOne approach is assuming the distribution $Y \\mid X \\sim \\mathcal{N}(f(X), \\sigma^{2}I)$ and then solve the log likelihood on the probability If we use a statistical learning approach then we know we want to minimize this $\\arg \\min_{f} \\sum_{i = 1}^{n} (y_{i} - f(x_{i}))^{2}$ which is what is often used. Both methods end with the same solution. Usually for these kind of problems we use the Least Squares Method, initially studied in Minimi quadrati. We will describe it again better in this setting. Let‚Äôs consider the linear model\nNormal Equation solution üü© $Y = \\beta_{0} + \\sum_{j = 1}^{d} X_{j}\\beta_{j}$ where $\\beta_{0}$ is the bias or intercept. We can introduce a fictitious variable $X_{1}$ so that we can rewrite the above in Matricial form: $$ Y = X^{T}\\beta $$ And we can attack this problem with the residual squares approach: $$ RSS(\\beta) = \\sum_{i = 1}^{n}(y_{i} - x_{i}^{T}\\beta)^{2} = (y - X\\beta)^{T}(y - X\\beta) $$ This loss can be briefly motivated: we don‚Äôt want to use normal $x - y$ because positive and negative errors can cancel out, then we want to square it to have better differentiability.\nWe can then use this form to minimize using standard multi variable calculus (this is sometimes also called the MLE approach, because we just want to find the maximum for the parameters). And we find that the solution condition is: $$ \\nabla _{\\beta} RSS(\\beta) = 0 \\implies X^{T}(y - X\\beta) = 0 \\implies \\hat{\\beta} = (X^{T}X)^{-1} X^{T}y $$ Which is just slow because of the inverse part, but easily feasible. This is done in the old note section. In order to know that this indeed is the minimum we should also see the hessian, but should be easy to check that. (if not check chapter 3 of (Hastie et al. 2009)).\nThis is the solution: After the $\\hat{\\beta}$ have been computed then a prediction is just the following: $$ y = X\\hat{\\beta} = X(X^{T}X)^{-1}X^{T}y $$ The matrix $X(X^{T}X)^{-1}X^{T}$ is called the hat matrix which has some special properties. One can prove that the estimator is distributed in this way: $$ \\hat{\\beta} \\sim \\mathcal{N} (\\beta, (X^{T}X)^{-1}\\sigma^{2}) $$ And we know that this is usually an unbiased estimator.\nNormal equation estimator is unbiased üü© This is quite easy to show, we need to show that the expected value of our estimator is the same: $$ \\mathbb{E}(a^{T}\\beta) = a^{T} (X^{T}X)^{-1}X^{T}\\mathbb{E}[y] = a^{T} (X^{T}X)^{-1}X^{T}\\mathbb{E}[X\\beta + \\varepsilon] = a^{T}\\beta $$ Also the variance of this has a certain value $$ \\mathbb{V}(a^{T}\\beta) = \\mathbb{E}(a^{T} (X^{T}X)^{-1}X^{T}\\varepsilon\\varepsilon X (X^{T}X)^{-1}a) = \\sigma^{2}a^{T}(X^{T}X)^{-1}a $$ I have no Idea why here we multiply by another vector $a$, probably because we want to generalize to any linear functional over the parameter space. The variance thing is just nice when you want to say that if you have an estimator of the form $Ay$ then it‚Äôs variance is $A^{T}A$.\nGauss Markov Theorem üü® This theorem only talks about unbiased estimators. But we need to remember the Rao-Cramer Bounds explained in Parametric Modeling that some biased methods could do better.\nFrom wiki: Gauss‚ÄìMarkov theorem¬†(or simply¬†Gauss theorem¬†for some authors)states that the¬†ordinary least squares¬†(OLS) estimator has the lowest¬†sampling variance¬†within the¬†class¬†of¬†linear¬†unbiased¬†estimators.\nIf we consider another estimator $\\hat{\\theta} = c^{T}y = a^{T}\\hat{\\beta} + a^{T}Dy$ we cannot use any deviation D, because if it is unbiased we will have $a^{T}DX = 0$\nYou can see the proof in the image: This shows that among all the unbiased models MLE with ordinary least squares produces the best results, but some biased results could do better! Here We have an easier proof based on the difference of the variance of the two estimators.‚Äì\nRegression is biased üü•+ One can show that there are many examples where linear regression is quite biased. This is a problem especially with high dimensional data. There are some numerical instabilities with correlated features, especially when we are trying to invert something small (floating point imprecisions!)\nBias Variance trade-off üü© ‚Äì We want to decompose the value $\\mathbb{E}_{D}\\mathbb{E}_{X, Y}[(\\hat{f}(x) - y)^{2}]$ where $f$ is the parameterized function that we are trying to learn, and $y$ is the true label. $D$ is our dataset is a distribution on all available datasets, and has an influence on $\\hat{f}$ in this case., and $X, Y$ are the true un-accessible distributions of the data. Now let‚Äôs do some calculations. Let‚Äôs consider the value $\\hat{y} = \\mathbb{E}_{Y}[Y \\mid X=x]$. Then let‚Äôs decompose the prediction error at $X = x$\n$$ \\mathbb{E}_{D}\\mathbb{E}_{Y\\mid X=x}[(\\hat{f}(x) - \\hat{y} + \\hat{y} - y)^{2}] = \\mathbb{E}_{D}\\mathbb{E}_{Y\\mid X=x}[(\\hat{f}(x) - \\hat{y})^{2}] + \\mathbb{E}_{D}\\mathbb{E}_{Y\\mid X=x}[(\\hat{y} - y)^{2}] + 0 $$ Now $\\mathbb{E}_{D}\\mathbb{E}_{Y\\mid X=x}[(\\hat{y} - y)^{2}] = \\mathbb{E}_{Y \\mid X=x}[(\\hat{y} - y)^{2}]$ because we are not training anything on $D$, and one can see that this is the noise variance, mean difference between the correct mean and the $y$ itself.\nNow we further decompose the formulas $$ \\mathbb{E}_{D}\\mathbb{E}_{Y\\mid X=x}[(\\hat{f}(x) - \\hat{y})^{2}] = \\mathbb{E}_{D}\\mathbb{E}_{Y\\mid X=x}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)] + \\mathbb{E}[\\hat{f}(x)]- \\hat{y})^{2}] = \\mathbb{E}_{D}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^{2}] + \\mathbb{E}_{D}[( \\mathbb{E}[\\hat{f}(x)] - \\hat{y})^{2}] + 0 $$ The first term - $\\mathbb{E}_{D}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^{2}]$ - is the variance, the second term simplifies to $( \\mathbb{E}[\\hat{f}(x)] - \\hat{y})^{2}$ as everything is constant with respect to $D$ inside that, this is the bias squared (the expected prediction model across all possible datasets minus the expected real value).\nRegularizers A link to statistical learning üü© Quite often it is true that the Statistical learning framework with a cost function and a regularizator is exactly the same as a Maximum a posteriori estimate in the bayesian setting, we can see it quite quickly:\n$$ \\arg \\min_{\\theta} \\sum_{n = 1}^{N} \\mathcal{L}(f(x), y) + R(\\theta) = \\arg \\max_{\\theta} \\prod_{i =1 }^{N} P(y \\mid x, \\theta) P(\\theta) $$ If $R(\\theta) = -\\log(P(\\theta))$ and $\\mathcal{L}(f(x), y) = -\\log(P(y \\mid x, \\theta))$, so we can see the prior as if it was a regularizer!\nFrom a purely theoretical point of view, what we would like to calculate is the following $f^{*}(x) = \\mathbb{E}_{Y \\mid X} [Y \\mid X = x]$, but the number of samples for each $x$ is sparse, quite often we don‚Äôt have a label for a specific $x$. We need a function assumption that uses the nearby samples to make inference of the points close to those. For Buhmann, model free is non sense because its just a hidden model.\nRidge regression üü©‚Äì With linear regression we assume that $y \\approx w^{T}x = f(x;w) = f_{w}(x)$. And usually we have some regularization thing that prevents the overfitting.\nFor example a loss function could be: $$ \\hat{w} = \\arg \\min_{w\\in \\mathbb{R}^{d}} \\sum_{i = 1}^{n} (y_{i} - w^{T}x_{i})^{2} + \\lambda \\lVert w \\rVert ^{2}_{2} $$ This is called ridge regression. The penalty term makes the weights shrunk towards zero. This idea is also used in neural networks, where it is known as weight decay.\nThe ridge solutions are not equivariant under scaling of the inputs, and one should standardize the inputs before solving\nWith this function we have an analytical solution which is $$ \\hat{w} = (X^{T}X + \\lambda I)^{-1} X^{T}y $$ It‚Äôs easy to derive this formula, just take the derivative with respect to $w$, just need to handle the vector calculus well enough. We note that adding the term on the diagonal of $X^{T}X$ makes the matrix non-singular which allows inversion. This was the original reason why it was introduced. Similar thing can be done with standard regression. We will later discover that this is somewhat equivalent to a MAP estimate in the bayesian learning setting, while the MLE estimate is the classical linear regression, see Bayesian Linear Regression.\nFor Ridge to have a sparse solution, the best solution should be on one of the axis, which is rare, not for the Lasso! The lasso sparse solution region scales quadratically with the distance to the rombus, so it‚Äôs far more probable to have sparsity! This is very good for interpretability.\nLasso regression üü© The loss for the lasso is the following: $$ \\hat{w} = \\arg \\min_{w\\in \\mathbb{R}^{d}} \\sum_{i = 1}^{n} (y_{i} - w^{T}x_{i})^{2} + \\lambda \\lVert w \\rVert_{1} $$ Which is without the power of two (just sum of the values norm) $\\lVert w \\rVert_{1} = \\sum_{i =1}^{d} \\lvert w_{i} \\rvert$\nThis is equivalent to the bayesian view with a special prior on the weights $$ w \\sim \\frac{\\lambda}{4\\sigma^{2}}\\exp\\left( -\\lvert w \\rvert \\frac{\\lambda}{-2\\sigma^{2}} \\right) $$ Which is a Laplace distribution.\nThis is an image from the ESLI book. It justifies why with LASSO regularizers its far more easier to get sparse solutions. Diamonds are more likely to hit the error countours. The Kernel Case üü•++ We have $x_{1}, \\dots, x_{d} \\in \\mathbb{R}^{d}$ our feature functions. Let‚Äôs say we are trying to compute this feature function loss $$ \\hat{\\beta} = \\arg \\min_{\\beta \\in \\mathbb{R}^{\\infty}} \\sum_{i} (y_{i} - \\beta^{T}\\varphi(x))^{2} $$ The solution is the following using some calculus (same as normal equation solution). $$ \\hat{\\beta} = (\\Phi^{T}\\Phi)^{-1}\\Phi^{T}y $$ But this is impossible to compute because $\\Phi$ is a matrix in $\\mathbb{R}^{n \\times \\infty}$, but if we sneak in a $\\Phi \\Phi^{T}(\\Phi \\Phi^{T})^{-1}$ between the $y$ and the kernels. Then it simplifies to $$ \\hat{\\beta} = \\Phi^{T}(\\Phi \\Phi^{T})^{-1}y $$ Which simplifies a little bit because that $\\Phi \\Phi^{T}$ is a $\\mathbb{R}^{n\\times n}$ matrix, but we still need to solve the outside $\\Phi^{T}$.\nBut it doesn‚Äôt matter during inference, because for inference we have $$ \\varphi(\\hat{x})^{T}\\hat{\\beta} = \\varphi(\\hat{x})^{T}\\Phi^{T}(\\Phi \\Phi^{T})^{-1}y $$ And the value $\\varphi(\\hat{x})^{T}\\Phi^{T}$ which is just $\\varphi(\\hat{x})^{T} \\varphi(x_{i}) = k_{i}(x)$ We can conclude and say $$ \\hat{\\psi}(x) = k(x)A^{-1}y $$ With $A = \\Phi \\Phi^{T}$ the kernel matrix, where $A_{ij} = \\varphi(x_{i})^{T}\\varphi(x_{j})$.\nBut $A^{-1}$ is highly instable! This is why we add something in the diagonal, something similar to the ridge regression.\nSo we just add the thing on the diagonal: $\\hat{\\psi}(x) = k(x)(A + \\lambda I)^{-1}y$\nThe classification Case There is an analogous problem setting, but we assume that our target, the $y$ is some categorical data. So we still have the training dataset $\\left\\{ x_{i}, y_{i} \\right\\}_{i \\leq n} \\in \\mathbb{R}^{d} \\times \\mathcal{Y}$, we need to find (learn) a function $f: \\mathbb{R}^{d} \\to \\mathcal{Y}$. Usually $\\mathcal{Y} = \\left\\{ 0, 1 \\right\\}$ or $\\left\\{ -1, 1 \\right\\}$. Some common assumptions for this problem settings are:\n$X \\mid Y = y_{i} \\sim \\mathcal{N}(\\mu_{i}, \\Sigma)$ and unknown mean and variance $Y \\sim \\text{ Bern(0.5)}$ which is just uniform over the possible cases I think. With those assumptions and using Bayes rule one can prove that the solution is $$ P(y_{i} = 1 \\mid X) = \\sigma(\\beta^{T}x) $$ Where $\\beta$ is dependent on the mean and variance of the training samples. You should also take a look at Logistic Regression where we have a natural derivation of the Sigmoid function.\nLoss Functions The loss function depends on the set of labels used:\nFor $\\mathcal{Y} = \\left\\{ 0, 1 \\right\\}$ we use the cross entropy loss which is $$ \\mathcal{L}(f(x), y) = -y\\log(f(x)) - (1 - y)\\log(1 - f(x)) $$ For $\\mathcal{Y} = \\left\\{ -1, 1 \\right\\}$ the loss the logistic loss is: $$ \\mathcal{L}(f(x), y) = \\log(1 + \\exp(-yf(x))) $$ Note that the last one is just the negative log likelihood of a probability given by the Sigmoid function! While for the first case, it‚Äôs the negative log likelihood with a probability given by the Bernoulli distribution. Ensemble methods These are better treated in Ensemble Methods. Some mean of methods have a more accurate prediction power. Let‚Äôs consider a set of estimators $f_{1}, \\dots, f_{M}$, and their simple average defined as $$ \\hat{f}(x) = \\frac{1}{M}\\sum_{m = 1}^{M} f_{m}(x) $$ Then the bias is $$ \\text{ bias }[\\hat{f}(x)] = \\mathbb{E}_{D}[\\hat{f}(x)] - \\mathbb{E}[Y\\mid X=x] = \\frac{1}{M}\\sum_{m = 1}^{M} \\mathbb{E}_{D}[f_{m}(x)] - \\mathbb{E}[Y\\mid X=x] = \\frac{1}{M} \\sum_{m=1}^{M}bias(\\hat{f}_{m}(x)) $$ Which implies if we have unbiased estimators, the ensemble will also be unbiased.\nIf we look at the variance we will see that\n$$ \\text{Var}[\\hat{f}(x)] = \\frac{1}{M^{2}}\\sum_{m = 1}^{M} \\text{Var}[f_{m}(x)] + \\frac{1}{M^{2}}\\sum_{m \\neq l} \\text{Cov}[f_{m}(x), f_{l}(x)] $$ And if the covariance is small enough, then the variance of the ensemble is smaller than the variance of the individual estimators, which is why the wisdom of the crowds work. We gain variance reduction of $\\frac{\\sigma^{2}}{M}$\nReferences [1] Hastie et al. ‚ÄúThe Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition‚Äù Springer Science \u0026 Business Media 2009\n",
  "wordCount" : "2178",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/linear-regression-methods/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Linear Regression methods
    </h1>
    <div class="post-meta">11 min&nbsp;¬∑&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#problem-setting" aria-label="Problem setting">Problem setting</a><ul>
                        
                <li>
                    <a href="#normal-equation-solution-" aria-label="Normal Equation solution üü©">Normal Equation solution üü©</a></li>
                <li>
                    <a href="#normal-equation-estimator-is-unbiased-" aria-label="Normal equation estimator is unbiased üü©">Normal equation estimator is unbiased üü©</a></li>
                <li>
                    <a href="#gauss-markov-theorem-" aria-label="Gauss Markov Theorem üü®">Gauss Markov Theorem üü®</a></li>
                <li>
                    <a href="#regression-is-biased-" aria-label="Regression is biased üü•&#43;">Regression is biased üü•+</a></li>
                <li>
                    <a href="#bias-variance-trade-off----" aria-label="Bias Variance trade-off üü© &ndash;">Bias Variance trade-off üü© &ndash;</a></li></ul>
                </li>
                <li>
                    <a href="#regularizers" aria-label="Regularizers">Regularizers</a><ul>
                        
                <li>
                    <a href="#a-link-to-statistical-learning-" aria-label="A link to statistical learning üü©">A link to statistical learning üü©</a></li>
                <li>
                    <a href="#ridge-regression---" aria-label="Ridge regression üü©&ndash;">Ridge regression üü©&ndash;</a></li>
                <li>
                    <a href="#lasso-regression-" aria-label="Lasso regression üü©">Lasso regression üü©</a></li>
                <li>
                    <a href="#the-kernel-case-" aria-label="The Kernel Case üü•&#43;&#43;">The Kernel Case üü•++</a></li></ul>
                </li>
                <li>
                    <a href="#the-classification-case" aria-label="The classification Case">The classification Case</a><ul>
                        
                <li>
                    <a href="#loss-functions" aria-label="Loss Functions">Loss Functions</a></li></ul>
                </li>
                <li>
                    <a href="#ensemble-methods" aria-label="Ensemble methods">Ensemble methods</a></li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>We will present some methods related to regression methods for data analysis.
Some of the work here is from (Hastie et al. 2009). This note does not treat the bayesian case, you should see <a href="/notes/bayesian-linear-regression/">Bayesian Linear Regression</a> for that.</p>
<h3 id="problem-setting">Problem setting<a hidden class="anchor" aria-hidden="true" href="#problem-setting">#</a></h3>
<p>In usual regression problems we want to reach the $\arg \min \mathbb{E}_{Y \mid X} \left[ (Y - f(X))^{2} \right]$ and the solution is given by the <em>conditional mean</em>: $f^{*} = \mathbb{E}(Y \mid X = x)$. We have done something similar with <a href="/notes/logistic-regression/">Logistic Regression</a>, but that is just for classification analysis.
For linear regression models we need to find the parameters $\beta$ for this function
</p>
$$
Y = \beta_{0} + \sum_{j = 1}^{d} X_{j}\beta_{j}
$$
<p>We usually don&rsquo;t know the distribution of $P(X)$ or $P(Y \mid X)$ so we need to assume something about these distributions.</p>
<ul>
<li>One approach is assuming the distribution $Y \mid X \sim \mathcal{N}(f(X), \sigma^{2}I)$ and then solve the log likelihood on the probability</li>
<li>If we use a statistical learning approach then we know we want to minimize this $\arg \min_{f} \sum_{i = 1}^{n} (y_{i} - f(x_{i}))^{2}$ which is what is often used. Both methods end with the same solution.
Usually for these kind of problems we use the Least Squares Method, initially studied in <a href="/notes/minimi-quadrati/">Minimi quadrati</a>.
We will describe it again better in this setting.</li>
</ul>
<p>Let&rsquo;s consider the linear model</p>
<h4 id="normal-equation-solution-">Normal Equation solution üü©<a hidden class="anchor" aria-hidden="true" href="#normal-equation-solution-">#</a></h4>
<p>$Y = \beta_{0} + \sum_{j = 1}^{d} X_{j}\beta_{j}$ where $\beta_{0}$ is the bias or intercept. We can introduce a fictitious variable $X_{1}$ so that we can rewrite the above in Matricial form:
</p>
$$
Y = X^{T}\beta
$$
<p>
And we can attack this problem with the residual squares approach:
</p>
$$
RSS(\beta) = \sum_{i = 1}^{n}(y_{i} - x_{i}^{T}\beta)^{2} = (y - X\beta)^{T}(y - X\beta)
$$
<p>
This loss can be briefly motivated: we don&rsquo;t want to use normal $x - y$ because positive and negative errors can cancel out, then we want to square it to have better differentiability.</p>
<p>We can then use this form to minimize using standard multi variable calculus (this is sometimes also called the MLE approach, because we just want to find the maximum for the parameters). And we find that the solution condition is:
</p>
$$
\nabla _{\beta} RSS(\beta) = 0 \implies X^{T}(y - X\beta) = 0 \implies \hat{\beta} = (X^{T}X)^{-1} X^{T}y
$$
<p>
Which is just slow because of the inverse part, but easily feasible. This is done in the old note section.
In order to know that this indeed is the minimum we should also see the hessian, but should be easy to check that. (if not check chapter 3 of (Hastie et al. 2009)).</p>
<p>This is the solution:
<img src="/images/notes/Regression methods-20240813165201575.webp" width="482" alt="Regression methods-20240813165201575"></p>
<p>After the $\hat{\beta}$ have been computed then a prediction is just the following:
</p>
$$
y = X\hat{\beta} = X(X^{T}X)^{-1}X^{T}y
$$
<p>
The matrix $X(X^{T}X)^{-1}X^{T}$ is called the <strong>hat matrix</strong> which has some special properties.
One can prove that the estimator is distributed in this way:
</p>
$$
\hat{\beta} \sim \mathcal{N} (\beta, (X^{T}X)^{-1}\sigma^{2})
$$
<p>
And we know that this is usually an <em>unbiased</em> estimator.</p>
<h4 id="normal-equation-estimator-is-unbiased-">Normal equation estimator is unbiased üü©<a hidden class="anchor" aria-hidden="true" href="#normal-equation-estimator-is-unbiased-">#</a></h4>
<p>This is quite easy to show, we need to show that the expected value of our estimator is the same:
</p>
$$
\mathbb{E}(a^{T}\beta) = a^{T} (X^{T}X)^{-1}X^{T}\mathbb{E}[y] = a^{T} (X^{T}X)^{-1}X^{T}\mathbb{E}[X\beta + \varepsilon] = a^{T}\beta
$$
<p>Also the variance of this has a certain value
</p>
$$
\mathbb{V}(a^{T}\beta) = \mathbb{E}(a^{T} (X^{T}X)^{-1}X^{T}\varepsilon\varepsilon X (X^{T}X)^{-1}a) = \sigma^{2}a^{T}(X^{T}X)^{-1}a
$$
<p>
I have no Idea why here we multiply by another vector $a$, probably because we want to generalize to any linear functional over the parameter space.
The variance thing is just nice when you want to say that if you have an estimator of the form $Ay$ then it&rsquo;s variance is $A^{T}A$.</p>
<h4 id="gauss-markov-theorem-">Gauss Markov Theorem üü®<a hidden class="anchor" aria-hidden="true" href="#gauss-markov-theorem-">#</a></h4>
<p>This theorem only talks about unbiased estimators. But we need to remember the Rao-Cramer Bounds explained in <a href="/notes/parametric-modeling/">Parametric Modeling</a> that some biased methods could do better.</p>
<p>From <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">wiki</a>:
¬†<strong>Gauss‚ÄìMarkov theorem</strong>¬†(or simply¬†<strong>Gauss theorem</strong>¬†for some authors)states that the¬†<a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares</a>¬†(OLS) estimator has the lowest¬†<a href="https://en.wikipedia.org/wiki/Sampling_variance">sampling variance</a>¬†within the¬†class¬†of¬†<a href="https://en.wikipedia.org/wiki/Linear_combination">linear</a>¬†<a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased</a>¬†<a href="https://en.wikipedia.org/wiki/Estimator">estimators</a>.</p>
<p>If we consider another estimator $\hat{\theta} = c^{T}y = a^{T}\hat{\beta} + a^{T}Dy$ we cannot use any deviation D, because if it is unbiased we will have $a^{T}DX = 0$</p>
<p>You can see the proof in the image:
<img src="/images/notes/Linear Regression methods-20241016104711626.webp" alt="Linear Regression methods-20241016104711626">
This shows that among all the <strong>unbiased models</strong> MLE with ordinary least squares produces the best results, but some biased results could do better!
<a href="https://chatgpt.com/share/67572e5b-215c-8009-9fab-9ad42d05ab50">Here</a> We have an easier proof based on the difference of the variance of the two estimators.&ndash;</p>
<h4 id="regression-is-biased-">Regression is biased üü•+<a hidden class="anchor" aria-hidden="true" href="#regression-is-biased-">#</a></h4>
<p>One can show that there are many examples where linear regression is quite biased. This is a problem especially with <strong>high dimensional</strong> data.
There are some <em>numerical instabilities</em> with correlated features, especially when we are trying to invert something small (floating point imprecisions!)</p>
<h4 id="bias-variance-trade-off----">Bias Variance trade-off üü© &ndash;<a hidden class="anchor" aria-hidden="true" href="#bias-variance-trade-off----">#</a></h4>
<p>We want to decompose the value $\mathbb{E}_{D}\mathbb{E}_{X, Y}[(\hat{f}(x) - y)^{2}]$ where $f$ is the parameterized function that we are trying to learn, and $y$ is the true label. $D$ is our dataset is a distribution on all available datasets, and has an influence on $\hat{f}$ in this case., and $X, Y$ are the true un-accessible distributions of the data. Now let&rsquo;s do some calculations.
Let&rsquo;s consider the value $\hat{y} = \mathbb{E}_{Y}[Y \mid X=x]$. Then let&rsquo;s decompose the prediction error at $X = x$</p>
$$
\mathbb{E}_{D}\mathbb{E}_{Y\mid X=x}[(\hat{f}(x) - \hat{y} + \hat{y} - y)^{2}]
= \mathbb{E}_{D}\mathbb{E}_{Y\mid X=x}[(\hat{f}(x) - \hat{y})^{2}] + \mathbb{E}_{D}\mathbb{E}_{Y\mid X=x}[(\hat{y} - y)^{2}] + 0
$$
<p>
Now $\mathbb{E}_{D}\mathbb{E}_{Y\mid X=x}[(\hat{y} - y)^{2}] = \mathbb{E}_{Y \mid X=x}[(\hat{y} - y)^{2}]$ because we are not training anything on $D$, and one can see that this is the noise variance, mean difference between the correct mean and the $y$ itself.</p>
<p>Now we further decompose the formulas
</p>
$$
\mathbb{E}_{D}\mathbb{E}_{Y\mid X=x}[(\hat{f}(x) - \hat{y})^{2}] = \mathbb{E}_{D}\mathbb{E}_{Y\mid X=x}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)] +  \mathbb{E}[\hat{f}(x)]- \hat{y})^{2}] 
= \mathbb{E}_{D}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^{2}] + \mathbb{E}_{D}[( \mathbb{E}[\hat{f}(x)] - \hat{y})^{2}] + 0
$$
<p>
The first term - $\mathbb{E}_{D}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^{2}]$ - is the variance, the second term simplifies to $( \mathbb{E}[\hat{f}(x)] - \hat{y})^{2}$ as everything is constant with respect to $D$ inside that, this is the bias squared (the expected prediction model across all possible datasets minus the expected real value).</p>
<h3 id="regularizers">Regularizers<a hidden class="anchor" aria-hidden="true" href="#regularizers">#</a></h3>
<h4 id="a-link-to-statistical-learning-">A link to statistical learning üü©<a hidden class="anchor" aria-hidden="true" href="#a-link-to-statistical-learning-">#</a></h4>
<p>Quite often it is true that the Statistical learning framework with a cost function and a regularizator is exactly the same as a Maximum a posteriori estimate in the bayesian setting, we can see it quite quickly:</p>
$$
\arg \min_{\theta} \sum_{n = 1}^{N} \mathcal{L}(f(x), y) + R(\theta) = \arg \max_{\theta} \prod_{i =1 }^{N} P(y \mid x, \theta) P(\theta)
$$
<p>
If $R(\theta) = -\log(P(\theta))$ and $\mathcal{L}(f(x), y) = -\log(P(y \mid x, \theta))$, so we can see the prior as if it was a regularizer!</p>
<p>From a purely theoretical point of view, what we would like to calculate is the following $f^{*}(x) = \mathbb{E}_{Y \mid X} [Y \mid X = x]$, but the number of samples for each $x$ is sparse, quite often we don&rsquo;t have a label for a specific $x$. We need a <em>function assumption</em> that uses the nearby samples to make inference of the points close to those.
For Buhmann, <em>model free</em> is non sense because its just a hidden model.</p>
<h4 id="ridge-regression---">Ridge regression üü©&ndash;<a hidden class="anchor" aria-hidden="true" href="#ridge-regression---">#</a></h4>
<p>With linear regression we assume that $y \approx w^{T}x = f(x;w) = f_{w}(x)$. And usually we have some regularization thing that prevents the overfitting.</p>
<p>For example a loss function could be:
</p>
$$
\hat{w} = \arg \min_{w\in \mathbb{R}^{d}} \sum_{i = 1}^{n} (y_{i} - w^{T}x_{i})^{2} + \lambda \lVert w \rVert ^{2}_{2}
$$
<p>
This is called <strong>ridge regression</strong>. The penalty term makes the weights shrunk towards zero. This idea is also used in neural networks, where it is known as <em>weight decay</em>.</p>
<p>The ridge solutions are not equivariant under scaling of the inputs, and one should standardize the inputs before solving</p>
<p>With this function we have an <em>analytical solution</em> which is
</p>
$$
\hat{w} = (X^{T}X + \lambda I)^{-1} X^{T}y
$$
<p>
It&rsquo;s easy to derive this formula, just take the derivative with respect to $w$, just need to handle the vector calculus well enough. We note that adding the term on the diagonal of $X^{T}X$ makes the matrix <em>non-singular</em> which allows inversion. This was the original reason why it was introduced.
Similar thing can be done with standard regression.
We will later discover that this is somewhat equivalent to a MAP estimate in the bayesian learning setting, while the MLE estimate is the classical linear regression, see <a href="/notes/bayesian-linear-regression/">Bayesian Linear Regression</a>.</p>
<p>For Ridge to have a sparse solution, the best solution should be on one of the axis, which is rare, not for the Lasso! The lasso sparse solution region scales quadratically with the distance to the rombus, so it&rsquo;s far more probable to have sparsity! This is very good for interpretability.</p>
<h4 id="lasso-regression-">Lasso regression üü©<a hidden class="anchor" aria-hidden="true" href="#lasso-regression-">#</a></h4>
<p>The loss for the lasso is the following:
</p>
$$
\hat{w} = \arg \min_{w\in \mathbb{R}^{d}} \sum_{i = 1}^{n} (y_{i} - w^{T}x_{i})^{2} + \lambda \lVert w \rVert_{1}
$$
<p>
Which is without the power of two (just sum of the values norm) $\lVert w \rVert_{1} = \sum_{i =1}^{d} \lvert w_{i} \rvert$</p>
<p>This is equivalent to the bayesian view with a special prior on the weights
</p>
$$
w \sim \frac{\lambda}{4\sigma^{2}}\exp\left( -\lvert w \rvert \frac{\lambda}{-2\sigma^{2}} \right)
$$
<p>
Which is a Laplace distribution.</p>
<p>This is an image from the ESLI book. It justifies why with LASSO regularizers its far more easier to get sparse solutions. Diamonds are more likely to hit the error countours.
<img src="/images/notes/Linear Regression methods-20241016111920295.webp" width="568" alt="Linear Regression methods-20241016111920295"></p>
<h4 id="the-kernel-case-">The Kernel Case üü•++<a hidden class="anchor" aria-hidden="true" href="#the-kernel-case-">#</a></h4>
<p>We have $x_{1}, \dots, x_{d} \in \mathbb{R}^{d}$ our feature functions.
Let&rsquo;s say we are trying to compute this feature function loss
</p>
$$
\hat{\beta} = \arg \min_{\beta \in \mathbb{R}^{\infty}} \sum_{i} (y_{i} - \beta^{T}\varphi(x))^{2}
$$
<p>
The solution is the following using some calculus (same as normal equation solution).
</p>
$$
\hat{\beta} = (\Phi^{T}\Phi)^{-1}\Phi^{T}y
$$
<p>
But this is impossible to compute because $\Phi$ is a matrix in $\mathbb{R}^{n \times \infty}$, but if we sneak in a  $\Phi \Phi^{T}(\Phi \Phi^{T})^{-1}$ between the $y$ and the kernels. Then it simplifies to
</p>
$$
\hat{\beta} = \Phi^{T}(\Phi \Phi^{T})^{-1}y
$$
<p>
Which simplifies a little bit because that $\Phi \Phi^{T}$ is a $\mathbb{R}^{n\times n}$ matrix, but we still need to solve the outside $\Phi^{T}$.</p>
<p>But it doesn&rsquo;t matter during inference, because for inference we have
</p>
$$
\varphi(\hat{x})^{T}\hat{\beta} = \varphi(\hat{x})^{T}\Phi^{T}(\Phi \Phi^{T})^{-1}y
$$
<p>And the value $\varphi(\hat{x})^{T}\Phi^{T}$ which is just $\varphi(\hat{x})^{T} \varphi(x_{i}) = k_{i}(x)$
We can conclude and say
</p>
$$
\hat{\psi}(x) = k(x)A^{-1}y
$$
<p>
With $A = \Phi \Phi^{T}$ the kernel matrix, where $A_{ij} = \varphi(x_{i})^{T}\varphi(x_{j})$.</p>
<p>But $A^{-1}$ is highly instable! This is why we add something in the diagonal, something similar to the ridge regression.</p>
<p>So we just add the thing on the diagonal: $\hat{\psi}(x) = k(x)(A + \lambda I)^{-1}y$</p>
<h3 id="the-classification-case">The classification Case<a hidden class="anchor" aria-hidden="true" href="#the-classification-case">#</a></h3>
<p>There is an analogous problem setting, but we assume that our target, the $y$ is some categorical data.
So we still have the training dataset $\left\{ x_{i}, y_{i} \right\}_{i \leq n} \in \mathbb{R}^{d} \times \mathcal{Y}$, we need to find (learn) a function $f: \mathbb{R}^{d} \to \mathcal{Y}$.
Usually $\mathcal{Y} = \left\{ 0, 1 \right\}$ or $\left\{ -1, 1 \right\}$.
Some common assumptions for this problem settings are:</p>
<ul>
<li>$X \mid Y = y_{i} \sim \mathcal{N}(\mu_{i}, \Sigma)$ and unknown mean and variance</li>
<li>$Y \sim \text{ Bern(0.5)}$ which is just uniform over the possible cases I think.</li>
</ul>
<p>With those assumptions and using Bayes rule one can prove that the solution is
</p>
$$
P(y_{i} = 1 \mid X) = \sigma(\beta^{T}x)
$$
<p>
Where $\beta$ is dependent on the mean and variance of the training samples.
You should also take a look at <a href="/notes/logistic-regression/">Logistic Regression</a> where we have a natural derivation of the Sigmoid function.</p>
<h4 id="loss-functions">Loss Functions<a hidden class="anchor" aria-hidden="true" href="#loss-functions">#</a></h4>
<p>The loss function depends on the set of labels used:</p>
<ul>
<li>For $\mathcal{Y} = \left\{ 0, 1 \right\}$ we use the <em>cross entropy loss</em> which is
$$
\mathcal{L}(f(x), y) = -y\log(f(x)) - (1 - y)\log(1 - f(x))
$$</li>
<li>For $\mathcal{Y} = \left\{ -1, 1 \right\}$ the loss the <em>logistic loss</em> is:
$$
\mathcal{L}(f(x), y) = \log(1 + \exp(-yf(x)))
$$
Note that the last one is just the negative log likelihood of a probability given by the Sigmoid function!
While for the first case, it&rsquo;s the negative log likelihood with a probability given by the Bernoulli distribution.</li>
</ul>
<h3 id="ensemble-methods">Ensemble methods<a hidden class="anchor" aria-hidden="true" href="#ensemble-methods">#</a></h3>
<p>These are better treated in <a href="/notes/ensemble-methods/">Ensemble Methods</a>.
Some mean of methods have a more accurate prediction power.
Let&rsquo;s consider a set of estimators $f_{1}, \dots, f_{M}$, and their simple average defined as
</p>
$$
\hat{f}(x) = \frac{1}{M}\sum_{m = 1}^{M} f_{m}(x)
$$
<p>Then the bias is
</p>
$$
\text{ bias }[\hat{f}(x)] = \mathbb{E}_{D}[\hat{f}(x)] - \mathbb{E}[Y\mid X=x]  = \frac{1}{M}\sum_{m = 1}^{M} \mathbb{E}_{D}[f_{m}(x)] - \mathbb{E}[Y\mid X=x] = \frac{1}{M} \sum_{m=1}^{M}bias(\hat{f}_{m}(x))
$$
<p>Which implies if we have unbiased estimators, the ensemble will also be unbiased.</p>
<p>If we look at the variance we will see that</p>
$$
\text{Var}[\hat{f}(x)] = \frac{1}{M^{2}}\sum_{m = 1}^{M} \text{Var}[f_{m}(x)] + \frac{1}{M^{2}}\sum_{m \neq l} \text{Cov}[f_{m}(x), f_{l}(x)]
$$
<p>
And if the covariance is small enough, then the variance of the ensemble is smaller than the variance of the individual estimators, which is why the wisdom of the crowds work.
We gain variance reduction of $\frac{\sigma^{2}}{M}$</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Hastie et al. ‚ÄúThe Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition‚Äù Springer Science &amp; Business Media 2009</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression methods on x"
            href="https://x.com/intent/tweet/?text=Linear%20Regression%20methods&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2flinear-regression-methods%2f&amp;hashtags=machinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression methods on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2flinear-regression-methods%2f&amp;title=Linear%20Regression%20methods&amp;summary=Linear%20Regression%20methods&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2flinear-regression-methods%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression methods on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2flinear-regression-methods%2f&title=Linear%20Regression%20methods">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression methods on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2flinear-regression-methods%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression methods on whatsapp"
            href="https://api.whatsapp.com/send?text=Linear%20Regression%20methods%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2flinear-regression-methods%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression methods on telegram"
            href="https://telegram.me/share/url?text=Linear%20Regression%20methods&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2flinear-regression-methods%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression methods on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Linear%20Regression%20methods&u=https%3a%2f%2fflecart.github.io%2fnotes%2flinear-regression-methods%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
