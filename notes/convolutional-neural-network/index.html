<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Convolutional Neural Network | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machine-perception">
<meta name="description" content="Introduction to Convolutional NN
Design Goals
We want to be invariant to some transformations but also at the same time to be specific to some thing.
Convolutional Neural Networks (CNNs) are a class of deep neural networks that are particularly effective for image processing tasks. They are designed to automatically and adaptively learn spatial hierarchies of features from images.
Compared to standard Fully connected Neural Networks, they reuse weights, making their number of parameter much fewer.">
<meta name="author" content="
By Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/convolutional-neural-network/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f790d9af969c56c079c1ce2d5972a04486bf3d6144295d5fba319830e1e55a7a.css" integrity="sha256-95DZr5acVsB5wc4tWXKgRIa/PWFEKV1fujGYMOHlWno=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/convolutional-neural-network/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/convolutional-neural-network/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Convolutional Neural Network">
  <meta property="og:description" content="Introduction to Convolutional NN Design Goals We want to be invariant to some transformations but also at the same time to be specific to some thing. Convolutional Neural Networks (CNNs) are a class of deep neural networks that are particularly effective for image processing tasks. They are designed to automatically and adaptively learn spatial hierarchies of features from images. Compared to standard Fully connected Neural Networks, they reuse weights, making their number of parameter much fewer.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2025-05-28T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-28T00:00:00+00:00">
    <meta property="article:tag" content="Machine-Perception">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Convolutional Neural Network">
<meta name="twitter:description" content="Introduction to Convolutional NN
Design Goals
We want to be invariant to some transformations but also at the same time to be specific to some thing.
Convolutional Neural Networks (CNNs) are a class of deep neural networks that are particularly effective for image processing tasks. They are designed to automatically and adaptively learn spatial hierarchies of features from images.
Compared to standard Fully connected Neural Networks, they reuse weights, making their number of parameter much fewer.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Convolutional Neural Network",
      "item": "https://flecart.github.io/notes/convolutional-neural-network/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Convolutional Neural Network",
  "name": "Convolutional Neural Network",
  "description": "Introduction to Convolutional NN Design Goals We want to be invariant to some transformations but also at the same time to be specific to some thing. Convolutional Neural Networks (CNNs) are a class of deep neural networks that are particularly effective for image processing tasks. They are designed to automatically and adaptively learn spatial hierarchies of features from images. Compared to standard Fully connected Neural Networks, they reuse weights, making their number of parameter much fewer.\n",
  "keywords": [
    "machine-perception"
  ],
  "articleBody": "Introduction to Convolutional NN Design Goals We want to be invariant to some transformations but also at the same time to be specific to some thing. Convolutional Neural Networks (CNNs) are a class of deep neural networks that are particularly effective for image processing tasks. They are designed to automatically and adaptively learn spatial hierarchies of features from images. Compared to standard Fully connected Neural Networks, they reuse weights, making their number of parameter much fewer.\nHistorically, they were inspired by the visual cortex of animals, where simple cells respond to specific orientations and complex cells respond to combinations of simple cells. See Hubel and Wiesel experiment, they also saw that there is some hierarchical organization of how the cells communicate with each other.\nThe convolution operator $$ \\begin{align*} I'(i,j) \u0026= \\sum_{m=-k}^{k} \\sum_{n=-k}^{k} I(i - m, j - n) K(m, n) \\\\ \u0026= \\sum_{m=-k}^{k} \\sum_{n=-k}^{k} I(i + m, j + n) K(-m, -n) \\end{align*} $$Il prodotto di convoluzione è matematicamente molto contorto, anche se nella pratica è una cosa molto molto semplice. In pratica voglio calcolare il valore di un pixel in funzione di certi suoi vicini, moltiplicati per un filter che in pratica è una matrice di pesi, che definisce un pattern lineare a cui sarei interessato di cercare nell’immagine.\nShift-Equivariance Property $$ (\\mathcal{T \\circ \\text{ shift }})(g) = (\\text{shift} \\circ \\mathcal{T})(g) $$Correlation operator $$ (f*g)(t) = \\sum_{\\tau = -\\infty}^{\\infty} f(\\tau)g(t-\\tau) $$Pytorch usually learns correlations, but calls it convolution, because the orientation of the filter is learned does not matter much. The two operations are equivalent given symmetric kernels.\n$$ I'(i, j) = \\sum_{m = -k}^{k} \\sum_{n = -k}^{k} I(i + m, j + n) K(m, n) $$ With $k = \\text{floor}\\left( \\frac{M}{2} \\right)$ square kernel of size $M$.\nSlides ed esempi (molto più chiaril) Vedi che per calcolare quell’8 sto facendo cose lineari con tutti pixel intorno ad essa.\nQuesto operatore l’abbiamo già trattato in modo molto breve in Deblur di immagini.\nCounting the parameters Convolutional layer $C_{out} = \\text{number of filters}$ $C_{in} = \\text{number of channels}$ $K = \\text{kernel size}$ The number of parameters is just dependent on the above three values, and the presence of the bias:\n$$ \\text{Num Params} = C_{out} \\cdot (C_{in} \\cdot K^{2} + 1) $$ Where $1$ is from the bias parameter.\nSize of the output The size of the output is given by the formula: $$ \\begin{align*}\n\\text{Output Size} \u0026= \\left\\lfloor \\frac{L_{in} + 2P - D \\cdot (K - 1) - 1}{S} + 1 \\right\\rfloor\\ \\end{align*} $$\nSome properties and uses Sappiamo tutti che le immagini non sono altro che arrai di valori in un certo intervallo, che rappresentano l’intensità dei colori, o solamente del bianco-grigio nel caso delle immagini grigio nere.\nQueste intensità si potrebbero anche rappresentare come superfici 3d in cui la posizione del pixel identifica x e y, mentre l’intensità la z, abbiamo quindi proprio delle superfici!, delle montagne, valli fiumi etc. Le cose molto interessanti sono cambi di intensità improvvisi (con derivata molto alta) ossia i dirupi, le valli, questo cambio improvviso (il cambio di fase come dice Pedro di Master algorithm) è classico anche in nautura, è la parte con qualche informazione di interesse diciamo.\nTHE IDEA OF DERIVATIVE FOR CHANGES\nSlide finite approssimation of derivative Comparison with Fully Connected Parameter sharing: convolutional networks share the parameters while fully connected do not. Local connectivity: in convolution layers neurons are connected only locally while in fully connected layers neurons are connected globally. HMAX model The fundamental principle of HMAX model is the hierarchy of cells that encode deeper and more complex features (higher receptive fields). HMAX is hierarchical and consists of alternating layers of:\nSimple (S) units – increase selectivity (like feature detectors). Complex (C) units – increase invariance (like pooling operations). The $S$ cell is basically a gaussian. $w_{j}$ is some sort of parameter.\n$$ y = \\exp(- \\frac{1}{2\\sigma^{2}} \\sum_{i} (x_{i} - w_{j})^{2}) $$$$ y_{j} = \\max_{j = 1 \\dots N} x_{j} $$ Trained with unsupervised learning.\nSome what close to transfer learning: after you represented the lower features, you can use them for different tasks and just train the higher layers.\nA brief history of CNNs Neurocognitron, mostly historical, but laying the groundwork for CNNs LeNet-5 one of the first dense nets, for handwritten digits Then you have AI winter until 2012 with AlexNet. Image Filtering Image filtering modifies pixels in an image based on the values of neighboring pixels. The convolution operator is used to apply a filter (kernel) to an image, which can enhance or suppress certain features.\nUsually they are linear transforms, see Spazi vettoriali\nVisualization of Learned Features We can recover the image that maximally activates some layer of neurons, from these images we see that convolutional networks are highly sensitive to some kinds of activations, and generally are able to recover complex shapes and patterns starting from those.\nLower level features in layer 2 of AlexNet\nThis actually helped to gain some insight to improve the performance of the network (they reduced the kernel size and went deeper, e.g. VGG model)\nSome architectures Deepwise separable convolution Inception architecture Andiamo a derfinire un modulo di inception (in cui va a fare in un certo senso scrambling, decomporre e recomporre dati, in che modo vanno ad estrarre delle features io non lo so!).\nComunque questa è l’architettura classica, andare ad utilizzare reti convoluzionali e poi operarle con reti deep (alla fine non molto deep) in modo da collegargli insieme.\nEsempio di inception module Un esempio è GoogleLeNet (molte computazioni in parallelo) https://www.youtube.com/watch?v=VxhSouuSZDY\u0026ab_channel=Udacity 1x1 is to reduce the number of channels (way to reduce the parameter count) in the following layers! So that the number of input channels is ok.\nResidual layers Residual learning is the main concept of these networks, it’s when we have a direct link with the beginning! In pratica diamo la possibilità al neurone di scegliere di non modificare o invece sì l’input credo, provo a chiedermi se posso avere un valore migliore di quanto ho attualmente con qualche peso.\nStructure of residual layer (He et al. 2016). Usually these links help the network learn (lesser vanishing gradient.\nShattered Gradients Most gradient descent methods assume to have a smooth gradient. This is not true anymore with very deep layers. Adding residual layers help ease this problem. The residual error signal is actually necessary to train these networks, as the following images show. Smoother Error Surfaces Introducing the residual layer helps in having smoother residual layers: Residual Networks help gradient flow In this section we provide a simple derivation of how residual layers help gradient flow.\n$$ \\begin{align*} h \u0026= \\sigma(Wx + b) \\\\ y \u0026= Vh + c \\end{align*} $$ Where:\n$x$ is the input. $W$ and $V$ are weight matrices. $b$ and $c$ are bias vectors. $\\sigma$ is the activation function (like ReLU). During back-propagation, we calculate the gradient of the loss ($L$) with respect to the input ($x$), denoted as $\\frac{\\partial L}{\\partial x}$. Using the chain rule:\n$$ \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x} $$Expanding this:\n$\\frac{\\partial y}{\\partial h} = V^T$ (the transpose of the output layer weights) $\\frac{\\partial h}{\\partial x} = \\sigma'(Wx + b) \\cdot W^T$ (the derivative of the activation function times the transpose of the input layer weights) So, for the simple feed-forward network:\n$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot V^T \\cdot \\sigma'(Wx + b) \\cdot W^T$\nThe Issue: If $\\sigma'(Wx + b)$ (the derivative of the activation function) becomes very small (e.g., when sigmoid functions saturate or ReLU outputs are negative), this term can shrink the entire gradient. In deep networks, multiplying by such small terms repeatedly can cause the gradient to vanish to zero, stopping learning in early layers.\nNow, let’s add a residual (or skip) connection. For a simple block, the output $y$ includes the original input $x$:\n$h = \\sigma(Wx + b)$ $y = h + x$\nAgain, we calculate $\\frac{\\partial L}{\\partial x}$:\n$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$\nNow, let’s find $\\frac{\\partial y}{\\partial x}$:\n$y = \\sigma(Wx + b) + x$\nTaking the derivative with respect to $x$:\n$\\frac{\\partial y}{\\partial x} = \\frac{\\partial (\\sigma(Wx + b))}{\\partial x} + \\frac{\\partial x}{\\partial x}$ $\\frac{\\partial y}{\\partial x} = \\sigma'(Wx + b) \\cdot W^T + I$ (where $I$ is the identity matrix, representing the derivative of $x$ with respect to $x$)\nSubstituting this back into the $\\frac{\\partial L}{\\partial x}$ equation:\n$$ \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot (\\sigma'(Wx + b) \\cdot W^T + I) $$$$ \\begin{align*} \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\sigma'(Wx + b) \\cdot W^T + \\frac{\\partial L}{\\partial y} \\cdot I \\end{align*} $$The crucial part is the addition of the $\\frac{\\partial L}{\\partial y}$ term in the residual network’s gradient.\nDirect Gradient Path: This $\\frac{\\partial L}{\\partial y}$ term means that a portion of the gradient flows directly from the output $y$ back to the input $x$, unmodified by the weights or activation functions of the internal residual branch. Combating Vanishing Gradients: Even if the $\\sigma'(Wx + b) \\cdot W^T$ part (the “normal” gradient path) becomes very small, the $\\frac{\\partial L}{\\partial y}$ term provides a guaranteed, direct path for the gradient. This ensures that earlier layers still receive substantial gradient updates, making it much easier to train very deep networks without the vanishing gradient problem. In essence, the skip connection acts like a shortcut for gradients, allowing them to bypass layers where they might otherwise diminish, ensuring robust backpropagation through the entire network.\nTransfer learning expected graph with performance with transfer learning !\nComunque l’intuizione principale del transfer learning è l’idea che i primi layers facciano una sorta di estrazione di features più ad alto livello utili poi ai layers di deep NN. Se questa prima parte l’ho trainata su un corpus enorme, allora gli aspetti che è riuscito a generalizzare potrebbero essere utili anche per altro, e quindi utilizzo i pesi trovati in questa rete anche per altro, senza problemi.\nFine tune o finetuning è un pò rischioso, faccio un freeze di una parte del network più larga, potrei andare a overfittare e fare cose simili! Però ha più senso, ci aiuta a rendere l’intera architettura ancora più focussato in quello che vogliamo fare noi (in un certo senso forse dà via alcune generalizzazioni inutili nel nostro dominio)\nTraining of CNN Backpropagation of CNNs We can unroll the input and output layers as a single linear trasformation of a deep network (with weights adjusted accordingly). But how do we unroll?? We can see everything as a matrix with $[input\\_size \\times output\\_size]$ as you can see from the image in the toggle\nAfter we have modeled this matrix, we can learn using standard backpropagation we have talked about in Neural Networks.\nOne problem with this method is that the matrix is sparse if the input is very large and the kernel is small. This would result in a very high number of zeros, making it not very efficient to store in this way.\nAnother aspect of this matrix is the shifted repetition of weights. These weights are the same in each column of the matrix, but simply shifted. This changes how weight updates are performed; a weighted average update is used.\nTransposed Convolutions After too much downsampling with CNNs, I might want to increase the dimension again (for example, if the input is an image). Transposed convolutions allow us to increase the dimension back up. (I believe statistical techniques also work for this).\nThis technique is called transposed convolution because if we transpose the convolution matrix, we see that we are upscaling the input!. Però non ho capito in che modo funziona!\nDilated convolutions Slide intuizione di questo Facciamo una specie di padding interno sul kernel (non vado a contare certe cose, però riesco a ingrandire la receptive field del mio network.\nHa più senso fare sta cosa quando sto analizzando HIGH RESOLUTION IMAGE in cui il valore dei pixel cambia molto poco. Una differenza con le Transposed convolutions (Upconvolution) è il fatto che quelle sono fatte sull’input, questa la facciamo su come viene calcolato il kernel. Sono molto utilizzate in temporal convolution networks, in cui provo a diluire volta per volta lo spazio all’interno del kernel, anche se non so ancora perché va\nThis is somewhat similar to what is done in WaveNets, see Autoregressive Modelling Upsampling Nearest-neighbor and bed of nails NN upsampling just repeats the number in the position, while bed of nails is just putting the number in the position and zeroing everything else.\nMax-unpooling You use the equivalent positions of the max-pooling layers, and then put the number in that position there, circled by zeros. Upconvolution This is also called transposed convolution. Since normal convolutions can be understood as some matrix $y = Mx$ this is just some other matrix $x = M^{T}y$.\nYou have the same thing here:\nStrides, kernels, padding Sum when the things overlap You can also have a learnable kernel (which is an advantage of this method compared to others) One simple example might explain what upconvolution do: Consider for simplicity a 1D case, with kernel size 3 and stride 2. Suppose input are two numbers $x_{0}$ and $x_{1}$, then the output is $\\left[ k_{0}x_{0}, k_{1}x_{0}, k_{2}x_{0} + k_{0}x_{1}, k_{1}x_{1}, k_{2}x_{1} \\right]$\nNormalization layers Why normalization Reasons:\nHave a more stable and faster learning More independence between layers (e.g. Layer Norm) Why is normalization a good idea :D?\nSo the quantitative values are comparable from each other (e.g. ages and income) We want the output of the layers to be comparable from each other, the middle outputs are inputs for other layers! We can better control the activation layers. (non vogliamo che faccia come output NaN 😟) Decoupling of the layers. (non dobbiamo andare ad imparare il range di input aspettato, dato che sarà sempre data di stesso tipo) Batch Normalization This paper introduces the idea of Batch Normalization.\nAs each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same whitening of the inputs of each layer. By whitening the inputs to each layer, we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the internal covariate shift.\nLeCun showed that whitened data speeds up learning, but it was applied only on the input layer. The idea here is to attempt to whiten the input data for every single layer, and this helps. (They call this the internal covariate shift). The thing is that you are learning the normalization parameters.\nProbably the internal covariate shift is not the real reason:\nInstead BatchNorm fundamentally impacts the training process by making the optimization landscape significantly smoother, thus leading to a more predictive and stable behaviour of the gradients\nThis is the most common form of normalization (ma l’idea è sempre la stessa, computare varianza e media, e poi sottrarre media e dividere per varianza). La cosa in più è che vengono aggiunte delle varianze e una media, per denormalizzare l’output, in modo che abbia la forma dei dati migliore possibile.\nIn reality, how it is implemented in pytorch is that $\\mu^{B}$ and $\\sigma^{B}$ are computed for each batch, and then used to normalize the output, they are not learned parameters. But an weighted average is kept for inference.\nBatch normalization makes weights in deeper layers more robust to changes to weights in the shallower layers of the network.\nOther Normalization Potremmo provare a normalizzare per canale\nSlide normalizations !!\nThere are many kinds of normalization, here is a table that summarizes the most famous cases.\nName Normalizes over Parameters Dependency on Batch Size Typical Use Cases BatchNorm (BN) (N,H,W)(N, H, W) — per channel Yes Yes Classification, convnets LayerNorm (LN) (C,H,W)(C, H, W) — per sample Yes No NLP, transformers InstanceNorm (IN) (H,W)(H, W) — per channel per sample Yes No Style transfer, generative models GroupNorm (GN) Subsets of channels Yes No General-purpose, batch-size agnostic PixelNorm Across channels per pixel No No GANs (especially StyleGAN) References [1] He et al. “Deep Residual Learning for Image Recognition” IEEE 2016 ",
  "wordCount" : "2679",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "2025-05-28T00:00:00Z",
  "dateModified": "2025-05-28T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/convolutional-neural-network/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Convolutional Neural Network
    </h1>
    <div class="post-meta"><span title='2025-05-28 00:00:00 +0000 UTC'>May 28, 2025</span>&nbsp;·&nbsp;Reading Time: 13 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#introduction-to-convolutional-nn" aria-label="Introduction to Convolutional NN">Introduction to Convolutional NN</a><ul>
                        <ul>
                        
                <li>
                    <a href="#design-goals" aria-label="Design Goals">Design Goals</a></li></ul>
                    
                <li>
                    <a href="#the-convolution-operator" aria-label="The convolution operator">The convolution operator</a><ul>
                        
                <li>
                    <a href="#shift-equivariance-property" aria-label="Shift-Equivariance Property">Shift-Equivariance Property</a></li>
                <li>
                    <a href="#correlation-operator" aria-label="Correlation operator">Correlation operator</a></li>
                <li>
                    <a href="#counting-the-parameters" aria-label="Counting the parameters">Counting the parameters</a></li>
                <li>
                    <a href="#size-of-the-output" aria-label="Size of the output">Size of the output</a></li></ul>
                </li>
                <li>
                    <a href="#some-properties-and-uses" aria-label="Some properties and uses">Some properties and uses</a><ul>
                        
                <li>
                    <a href="#comparison-with-fully-connected" aria-label="Comparison with Fully Connected">Comparison with Fully Connected</a></li>
                <li>
                    <a href="#hmax-model" aria-label="HMAX model">HMAX model</a></li>
                <li>
                    <a href="#a-brief-history-of-cnns" aria-label="A brief history of CNNs">A brief history of CNNs</a></li>
                <li>
                    <a href="#image-filtering" aria-label="Image Filtering">Image Filtering</a></li>
                <li>
                    <a href="#visualization-of-learned-features" aria-label="Visualization of Learned Features">Visualization of Learned Features</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#some-architectures" aria-label="Some architectures">Some architectures</a><ul>
                        
                <li>
                    <a href="#deepwise-separable-convolution" aria-label="Deepwise separable convolution">Deepwise separable convolution</a></li>
                <li>
                    <a href="#inception-architecture" aria-label="Inception architecture">Inception architecture</a></li>
                <li>
                    <a href="#residual-layers" aria-label="Residual layers">Residual layers</a><ul>
                        
                <li>
                    <a href="#shattered-gradients" aria-label="Shattered Gradients">Shattered Gradients</a></li>
                <li>
                    <a href="#smoother-error-surfaces" aria-label="Smoother Error Surfaces">Smoother Error Surfaces</a></li>
                <li>
                    <a href="#residual-networks-help-gradient-flow" aria-label="Residual Networks help gradient flow">Residual Networks help gradient flow</a></li></ul>
                </li>
                <li>
                    <a href="#transfer-learning" aria-label="Transfer learning">Transfer learning</a></li></ul>
                </li>
                <li>
                    <a href="#training-of-cnn" aria-label="Training of CNN">Training of CNN</a><ul>
                        
                <li>
                    <a href="#backpropagation-of-cnns" aria-label="Backpropagation of CNNs">Backpropagation of CNNs</a></li>
                <li>
                    <a href="#transposed-convolutions" aria-label="Transposed Convolutions">Transposed Convolutions</a></li>
                <li>
                    <a href="#dilated-convolutions" aria-label="Dilated convolutions">Dilated convolutions</a></li>
                <li>
                    <a href="#upsampling" aria-label="Upsampling">Upsampling</a><ul>
                        
                <li>
                    <a href="#nearest-neighbor-and-bed-of-nails" aria-label="Nearest-neighbor and bed of nails">Nearest-neighbor and bed of nails</a></li>
                <li>
                    <a href="#max-unpooling" aria-label="Max-unpooling">Max-unpooling</a></li>
                <li>
                    <a href="#upconvolution" aria-label="Upconvolution">Upconvolution</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#normalization-layers" aria-label="Normalization layers">Normalization layers</a><ul>
                        
                <li>
                    <a href="#why-normalization" aria-label="Why normalization">Why normalization</a></li>
                <li>
                    <a href="#batch-normalization" aria-label="Batch Normalization">Batch Normalization</a></li>
                <li>
                    <a href="#other-normalization" aria-label="Other Normalization">Other Normalization</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction-to-convolutional-nn">Introduction to Convolutional NN<a hidden class="anchor" aria-hidden="true" href="#introduction-to-convolutional-nn">#</a></h2>
<h4 id="design-goals">Design Goals<a hidden class="anchor" aria-hidden="true" href="#design-goals">#</a></h4>
<p>We want to be invariant to some transformations but also at the same time to be specific to some thing.
Convolutional Neural Networks (CNNs) are a class of deep neural networks that are particularly effective for image processing tasks. They are designed to automatically and adaptively learn spatial hierarchies of features from images.
Compared to standard Fully connected <a href="/notes/neural-networks">Neural Networks</a>, they reuse weights, making their number of parameter much fewer.</p>
<p>Historically, they were inspired by the visual cortex of animals, where simple cells respond to specific orientations and complex cells respond to combinations of simple cells. See Hubel and Wiesel experiment, they also saw that there is some hierarchical organization of how the cells communicate with each other.</p>
<h3 id="the-convolution-operator">The convolution operator<a hidden class="anchor" aria-hidden="true" href="#the-convolution-operator">#</a></h3>
$$
\begin{align*}
I'(i,j) &= \sum_{m=-k}^{k} \sum_{n=-k}^{k} I(i - m, j - n) K(m, n) \\
&= \sum_{m=-k}^{k} \sum_{n=-k}^{k} I(i + m, j + n) K(-m, -n)
\end{align*}
$$<p>Il prodotto di convoluzione è matematicamente molto contorto, anche se nella pratica è una cosa molto molto semplice. In pratica voglio calcolare il valore di un pixel in funzione di certi suoi vicini, moltiplicati per un <strong>filter</strong> che in pratica è una matrice di pesi, che definisce un pattern lineare a cui sarei interessato di cercare nell&rsquo;immagine.</p>
<h4 id="shift-equivariance-property">Shift-Equivariance Property<a hidden class="anchor" aria-hidden="true" href="#shift-equivariance-property">#</a></h4>
$$
(\mathcal{T \circ \text{ shift }})(g) = (\text{shift} \circ \mathcal{T})(g)
$$<h4 id="correlation-operator">Correlation operator<a hidden class="anchor" aria-hidden="true" href="#correlation-operator">#</a></h4>
$$
(f*g)(t) = \sum_{\tau = -\infty}^{\infty} f(\tau)g(t-\tau)
$$<p>Pytorch usually learns correlations, but calls it convolution, because the orientation of the filter is learned does not matter much.
The two operations are <strong>equivalent given symmetric</strong> kernels.</p>
$$
I'(i, j) = \sum_{m = -k}^{k} \sum_{n = -k}^{k} I(i + m, j + n) K(m, n)
$$<p>
With $k = \text{floor}\left( \frac{M}{2} \right)$ square kernel of size $M$.</p>
<ul>
<li>
<p>Slides ed esempi (molto più chiaril)
Vedi che per calcolare quell’8 sto facendo cose lineari con tutti pixel intorno ad essa.</p>
  <img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled">
  <img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 1.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 1">
</li>
</ul>
<p>Questo operatore l’abbiamo già trattato in modo molto breve in <a href="/notes/deblur-di-immagini">Deblur di immagini</a>.</p>
<h4 id="counting-the-parameters">Counting the parameters<a hidden class="anchor" aria-hidden="true" href="#counting-the-parameters">#</a></h4>
<ul>
<li>Convolutional layer
<ul>
<li>$C_{out} = \text{number of filters}$</li>
<li>$C_{in} = \text{number of channels}$</li>
<li>$K = \text{kernel size}$</li>
</ul>
</li>
</ul>
<p>The number of parameters is just dependent on the above three values, and the presence of the bias:</p>
$$
\text{Num Params} = C_{out} \cdot (C_{in} \cdot K^{2} + 1)
$$<p>
Where $1$ is from the bias parameter.</p>
<h4 id="size-of-the-output">Size of the output<a hidden class="anchor" aria-hidden="true" href="#size-of-the-output">#</a></h4>
<p>The size of the output is given by the formula:
$$
\begin{align*}</p>
<p>\text{Output Size} &amp;= \left\lfloor  \frac{L_{in} + 2P - D \cdot (K - 1) - 1}{S} + 1  \right\rfloor\
\end{align*}
$$</p>
<h3 id="some-properties-and-uses">Some properties and uses<a hidden class="anchor" aria-hidden="true" href="#some-properties-and-uses">#</a></h3>
<p>Sappiamo tutti che le immagini non sono altro che arrai di valori in un certo intervallo, che rappresentano l’intensità dei colori, o solamente del bianco-grigio nel caso delle immagini grigio nere.</p>
<p>Queste intensità si potrebbero anche rappresentare come superfici 3d in cui la posizione del pixel identifica x e y, mentre l’intensità la z, abbiamo quindi proprio delle superfici!, delle montagne, valli fiumi etc. Le cose molto interessanti sono cambi di intensità improvvisi (con derivata molto alta) ossia i dirupi, le valli, questo cambio improvviso (il cambio di fase come dice Pedro di Master algorithm) è classico anche in nautura, è la parte con qualche informazione di interesse diciamo.</p>
<p><strong>THE IDEA OF DERIVATIVE FOR CHANGES</strong></p>
<ul>
<li>Slide finite approssimation of derivative</li>
</ul>
<h4 id="comparison-with-fully-connected">Comparison with Fully Connected<a hidden class="anchor" aria-hidden="true" href="#comparison-with-fully-connected">#</a></h4>
<ul>
<li><strong>Parameter sharing</strong>: convolutional networks share the parameters while fully connected do not.</li>
<li><strong>Local connectivity:</strong> in convolution layers neurons are connected only locally while in fully connected layers neurons are connected globally.</li>
</ul>
<h4 id="hmax-model">HMAX model<a hidden class="anchor" aria-hidden="true" href="#hmax-model">#</a></h4>
<p>The fundamental principle of HMAX model is the hierarchy of cells that encode deeper and more complex features (higher receptive fields).
HMAX is hierarchical and consists of alternating layers of:</p>
<ol>
<li><strong>Simple (S) units</strong> – increase selectivity (like feature detectors).</li>
<li><strong>Complex (C) units</strong> – increase invariance (like pooling operations).
<img src="/images/notes/Convolutional Neural Network-20250520135334007.webp" style="width: 100%" class="center" alt="Convolutional Neural Network-20250520135334007"></li>
</ol>
<p>The $S$ cell is basically a gaussian. $w_{j}$ is some sort of parameter.</p>
$$
y = \exp(- \frac{1}{2\sigma^{2}} \sum_{i} (x_{i} - w_{j})^{2})
$$$$
y_{j} = \max_{j = 1 \dots N} x_{j}
$$<p>
Trained with unsupervised learning.</p>
<p>Some what close to transfer learning: after you represented the lower features, you can use them for different tasks and just train the higher layers.</p>
<h4 id="a-brief-history-of-cnns">A brief history of CNNs<a hidden class="anchor" aria-hidden="true" href="#a-brief-history-of-cnns">#</a></h4>
<ul>
<li>Neurocognitron, mostly historical, but laying the groundwork for CNNs</li>
<li>LeNet-5 one of the first dense nets, for handwritten digits</li>
<li>Then you have AI winter until 2012 with AlexNet.</li>
</ul>
<h4 id="image-filtering">Image Filtering<a hidden class="anchor" aria-hidden="true" href="#image-filtering">#</a></h4>
<p>Image filtering modifies pixels in an image based on the values of neighboring pixels. The convolution operator is used to apply a filter (kernel) to an image, which can enhance or suppress certain features.</p>
<p>Usually they are <strong>linear transforms</strong>, see <a href="/notes/spazi-vettoriali">Spazi vettoriali</a></p>
<h4 id="visualization-of-learned-features">Visualization of Learned Features<a hidden class="anchor" aria-hidden="true" href="#visualization-of-learned-features">#</a></h4>
<p>We can recover the image that maximally activates some layer of neurons, from these images we see that convolutional networks are highly sensitive to some kinds of activations, and generally are able to recover complex shapes and patterns starting from those.</p>
<figure class="center">
<img src="/images/notes/Convolutional Neural Network-20250520142336325.webp" style="width: 100%"   alt="Convolutional Neural Network-20250520142336325" title="Convolutional Neural Network-20250520142336325"/>
<figcaption><p style="text-align:center;">Lower level features in layer 2 of AlexNet</p></figcaption>
</figure>
<p>This actually helped to gain some insight to improve the performance of the network (they reduced the kernel size and went deeper, e.g. VGG model)</p>
<h2 id="some-architectures">Some architectures<a hidden class="anchor" aria-hidden="true" href="#some-architectures">#</a></h2>
<h3 id="deepwise-separable-convolution">Deepwise separable convolution<a hidden class="anchor" aria-hidden="true" href="#deepwise-separable-convolution">#</a></h3>
<h3 id="inception-architecture">Inception architecture<a hidden class="anchor" aria-hidden="true" href="#inception-architecture">#</a></h3>
<p>Andiamo a derfinire un modulo di inception (in cui va a fare in un certo senso scrambling, decomporre e recomporre dati, in che modo vanno ad estrarre delle features io non lo so!).</p>
<blockquote>
<p>Comunque questa è l’architettura classica, andare ad utilizzare reti convoluzionali e poi operarle con reti deep (alla fine non molto deep) in modo da collegargli insieme.</p></blockquote>
<ul>
<li>Esempio di inception module
Un esempio è GoogleLeNet (molte computazioni in parallelo)</li>
</ul>
<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 3.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 3">
  <a href="https://www.youtube.com/watch?v=VxhSouuSZDY&ab_channel=Udacity">https://www.youtube.com/watch?v=VxhSouuSZDY&ab_channel=Udacity</a>
<p>1x1 is to reduce the number of channels (way to reduce the parameter count) in the following layers! So that the number of input channels is ok.</p>
<h3 id="residual-layers">Residual layers<a hidden class="anchor" aria-hidden="true" href="#residual-layers">#</a></h3>
<p><strong>Residual learning</strong> is the main concept of these networks, it’s when we have a direct link with the beginning! In pratica diamo la possibilità al neurone di scegliere di non modificare o invece sì l’input credo, provo a chiedermi se posso avere un valore migliore di quanto ho attualmente con qualche peso.</p>
<ul>
<li>Structure of residual layer <a href="http://ieeexplore.ieee.org/document/7780459/">(He et al. 2016)</a>.</li>
</ul>
<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 4.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 4">
<p>Usually these links help the network learn (lesser vanishing gradient.</p>
<h4 id="shattered-gradients">Shattered Gradients<a hidden class="anchor" aria-hidden="true" href="#shattered-gradients">#</a></h4>
<p>Most gradient descent methods assume to have a smooth gradient. This is not true anymore with very deep layers. Adding residual layers help ease this problem.
The residual error signal is actually necessary to train these networks, as the following images show.
<img src="/images/notes/Convolutional Neural Network-20250520142932912.webp" style="width: 100%" class="center" alt="Convolutional Neural Network-20250520142932912"></p>
<h4 id="smoother-error-surfaces">Smoother Error Surfaces<a hidden class="anchor" aria-hidden="true" href="#smoother-error-surfaces">#</a></h4>
<p>Introducing the residual layer helps in having smoother residual layers:
<img src="/images/notes/Convolutional NN-20250206160716966.webp" style="width: 100%" class="center" alt="Convolutional NN-20250206160716966"></p>
<h4 id="residual-networks-help-gradient-flow">Residual Networks help gradient flow<a hidden class="anchor" aria-hidden="true" href="#residual-networks-help-gradient-flow">#</a></h4>
<p>In this section we provide a simple derivation of how residual layers help gradient flow.</p>
$$
\begin{align*}
h &= \sigma(Wx + b) \\
y &= Vh + c
\end{align*}
$$<p>
Where:</p>
<ul>
<li>$x$ is the <strong>input</strong>.</li>
<li>$W$ and $V$ are <strong>weight matrices</strong>.</li>
<li>$b$ and $c$ are <strong>bias vectors</strong>.</li>
<li>$\sigma$ is the <strong>activation function</strong> (like ReLU).</li>
</ul>
<p>During back-propagation, we calculate the gradient of the loss ($L$) with respect to the input ($x$), denoted as $\frac{\partial L}{\partial x}$. Using the chain rule:</p>
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial h} \cdot \frac{\partial h}{\partial x}
$$<p>Expanding this:</p>
<ul>
<li>$\frac{\partial y}{\partial h} = V^T$ (the transpose of the output layer weights)</li>
<li>$\frac{\partial h}{\partial x} = \sigma'(Wx + b) \cdot W^T$ (the derivative of the activation function times the transpose of the input layer weights)</li>
</ul>
<p>So, for the simple feed-forward network:</p>
<p>$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot V^T \cdot \sigma'(Wx + b) \cdot W^T$</p>
<p><strong>The Issue:</strong> If $\sigma'(Wx + b)$ (the derivative of the activation function) becomes very small (e.g., when sigmoid functions saturate or ReLU outputs are negative), this term can shrink the entire gradient. In deep networks, multiplying by such small terms repeatedly can cause the gradient to <strong>vanish to zero</strong>, stopping learning in early layers.</p>
<p>Now, let&rsquo;s add a <strong>residual (or skip) connection</strong>. For a simple block, the output $y$ includes the original input $x$:</p>
<p>$h = \sigma(Wx + b)$
$y = h + x$</p>
<p>Again, we calculate $\frac{\partial L}{\partial x}$:</p>
<p>$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$</p>
<p>Now, let&rsquo;s find $\frac{\partial y}{\partial x}$:</p>
<p>$y = \sigma(Wx + b) + x$</p>
<p>Taking the derivative with respect to $x$:</p>
<p>$\frac{\partial y}{\partial x} = \frac{\partial (\sigma(Wx + b))}{\partial x} + \frac{\partial x}{\partial x}$
$\frac{\partial y}{\partial x} = \sigma'(Wx + b) \cdot W^T + I$ (where $I$ is the identity matrix, representing the derivative of $x$ with respect to $x$)</p>
<p>Substituting this back into the $\frac{\partial L}{\partial x}$ equation:</p>
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot (\sigma'(Wx + b) \cdot W^T + I)
$$$$
\begin{align*}
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \sigma'(Wx + b) \cdot W^T + \frac{\partial L}{\partial y} \cdot I
\end{align*}
$$<p>The crucial part is the addition of the $\frac{\partial L}{\partial y}$ term in the residual network&rsquo;s gradient.</p>
<ul>
<li><strong>Direct Gradient Path:</strong> This $\frac{\partial L}{\partial y}$ term means that a portion of the gradient flows directly from the output $y$ back to the input $x$, <strong>unmodified by the weights or activation functions</strong> of the internal residual branch.</li>
<li><strong>Combating Vanishing Gradients:</strong> Even if the $\sigma'(Wx + b) \cdot W^T$ part (the &ldquo;normal&rdquo; gradient path) becomes very small, the $\frac{\partial L}{\partial y}$ term provides a guaranteed, direct path for the gradient. This ensures that earlier layers still receive substantial gradient updates, making it much easier to train very deep networks without the vanishing gradient problem.</li>
</ul>
<p>In essence, the skip connection acts like a <strong>shortcut</strong> for gradients, allowing them to bypass layers where they might otherwise diminish, ensuring robust backpropagation through the entire network.</p>
<h3 id="transfer-learning">Transfer learning<a hidden class="anchor" aria-hidden="true" href="#transfer-learning">#</a></h3>
<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 5.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 5">
<ul>
<li>expected graph with performance with transfer learning</li>
</ul>
<p>!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 6.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 6"></p>
<p>Comunque l’intuizione principale del transfer learning è l’idea che i primi layers facciano una sorta di estrazione di features più ad alto livello utili poi ai layers di deep NN. Se questa prima parte l’ho trainata su un corpus enorme, allora gli aspetti che è riuscito a generalizzare potrebbero essere utili anche per altro, e quindi utilizzo i pesi trovati in questa rete anche per altro, senza problemi.</p>
<p><strong>Fine tune o finetuning</strong> è un pò rischioso, faccio un freeze di una parte del network più larga, potrei andare a overfittare e fare cose simili! Però ha più senso, ci aiuta a rendere l’intera architettura ancora più focussato in quello che vogliamo fare noi (in un certo senso forse dà via alcune generalizzazioni inutili nel nostro dominio)</p>
<h2 id="training-of-cnn">Training of CNN<a hidden class="anchor" aria-hidden="true" href="#training-of-cnn">#</a></h2>
<h3 id="backpropagation-of-cnns">Backpropagation of CNNs<a hidden class="anchor" aria-hidden="true" href="#backpropagation-of-cnns">#</a></h3>
<p>We can unroll the input and output layers as a single linear trasformation of a deep network (with weights adjusted accordingly).
<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 7.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 7"></p>
<p>But how do we unroll?? We can see everything as a matrix with $[input\_size \times output\_size]$ as you can see from the image in the toggle</p>
<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 8.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 8">
<p>After we have modeled this matrix, we can learn using standard backpropagation we have talked about in <a href="/notes/neural-networks">Neural Networks</a>.</p>
<p>One problem with this method is that the matrix is <strong>sparse</strong> if the input is very large and the kernel is small. This would result in a very high number of zeros, making it not very efficient to store in this way.</p>
<p>Another aspect of this matrix is the <strong>shifted repetition</strong> of weights. These weights are the same in each column of the matrix, but simply shifted. This changes how weight updates are performed; a weighted average update is used.</p>
<h3 id="transposed-convolutions">Transposed Convolutions<a hidden class="anchor" aria-hidden="true" href="#transposed-convolutions">#</a></h3>
<p>After too much downsampling with CNNs, I might want to increase the dimension again (for example, if the input is an image). <strong>Transposed convolutions</strong> allow us to increase the dimension back up. (I believe statistical techniques also work for this).</p>
<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 9.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 9">
<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 10.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 10">
<p>This technique is called transposed convolution because if we transpose the convolution matrix, we see that we are upscaling the input!. Però non ho capito in che modo funziona!</p>
<h3 id="dilated-convolutions">Dilated convolutions<a hidden class="anchor" aria-hidden="true" href="#dilated-convolutions">#</a></h3>
<ul>
<li>Slide intuizione di questo
<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 11.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 11"></li>
</ul>
<p>Facciamo una specie di padding interno sul kernel (non vado a contare certe cose, però riesco a ingrandire la receptive field del mio network.</p>
<p>Ha più senso fare sta cosa quando sto analizzando HIGH RESOLUTION IMAGE in cui il valore dei pixel cambia molto poco.
Una differenza con le Transposed convolutions (Upconvolution) è il fatto che quelle sono fatte sull’input, questa la facciamo su come viene calcolato il kernel.
Sono molto utilizzate in <strong>temporal convolution networks</strong>, in cui provo a diluire volta per volta lo spazio all’interno del kernel, anche se non so ancora perché va</p>
<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 12.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 12">
This is somewhat similar to what is done in WaveNets, see <a href="/notes/autoregressive-modelling">Autoregressive Modelling</a>
<h3 id="upsampling">Upsampling<a hidden class="anchor" aria-hidden="true" href="#upsampling">#</a></h3>
<h4 id="nearest-neighbor-and-bed-of-nails">Nearest-neighbor and bed of nails<a hidden class="anchor" aria-hidden="true" href="#nearest-neighbor-and-bed-of-nails">#</a></h4>
<p>NN upsampling just repeats the number in the position, while bed of nails is just putting the number in the position and zeroing everything else.</p>
<img src="/images/notes/Convolutional Neural Network-20250526111928389.webp" style="width: 100%" class="center" alt="Convolutional Neural Network-20250526111928389">
<h4 id="max-unpooling">Max-unpooling<a hidden class="anchor" aria-hidden="true" href="#max-unpooling">#</a></h4>
<p>You use the equivalent positions of the max-pooling layers, and then put the number in that position there, circled by zeros.
<img src="/images/notes/Convolutional Neural Network-20250520141951550.webp" style="width: 100%" class="center" alt="Convolutional Neural Network-20250520141951550"></p>
<h4 id="upconvolution">Upconvolution<a hidden class="anchor" aria-hidden="true" href="#upconvolution">#</a></h4>
<p>This is also called <strong>transposed convolution</strong>. Since normal convolutions can be understood as some matrix $y = Mx$ this is just some other matrix $x = M^{T}y$.</p>
<p>You have the same thing here:</p>
<ul>
<li>Strides, kernels, padding</li>
<li>Sum when the things overlap</li>
<li>You can also have a learnable kernel (which is an advantage of this method compared to others)</li>
</ul>
<p>One simple example might explain what upconvolution do:
Consider for simplicity a 1D case, with kernel size 3 and stride 2. Suppose input are two numbers $x_{0}$ and $x_{1}$, then the output is $\left[ k_{0}x_{0}, k_{1}x_{0}, k_{2}x_{0} + k_{0}x_{1}, k_{1}x_{1}, k_{2}x_{1} \right]$</p>
<h2 id="normalization-layers">Normalization layers<a hidden class="anchor" aria-hidden="true" href="#normalization-layers">#</a></h2>
<h3 id="why-normalization">Why normalization<a hidden class="anchor" aria-hidden="true" href="#why-normalization">#</a></h3>
<p>Reasons:</p>
<ul>
<li>Have a more stable and faster learning</li>
<li>More independence between layers (e.g. Layer Norm)</li>
</ul>
<p>Why is normalization a good idea :D?</p>
<ul>
<li>So the quantitative values are comparable from each other (e.g. ages and income)</li>
<li>We want the output of the layers to be comparable from each other, the middle outputs are inputs for other layers!</li>
<li>We can better control the activation layers. (non vogliamo che faccia come output NaN 😟)</li>
<li>Decoupling of the layers. (non dobbiamo andare ad imparare il range di input aspettato, dato che sarà sempre data di stesso tipo)</li>
</ul>
<h3 id="batch-normalization">Batch Normalization<a hidden class="anchor" aria-hidden="true" href="#batch-normalization">#</a></h3>
<p><a href="https://arxiv.org/pdf/1502.03167">This paper</a> introduces the idea of Batch Normalization.</p>
<blockquote>
<p>As each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same whitening of the inputs of each layer. By whitening the inputs to each layer, we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the internal covariate shift.</p></blockquote>
<p>LeCun showed that whitened data speeds up learning, but it was applied only on the input layer. The idea here is to attempt to whiten the input data for every single layer, and this helps. (They call this the internal covariate shift).
The thing is that you are learning the normalization parameters.</p>
<p>Probably the internal covariate shift is not the real reason:</p>
<blockquote>
<p>Instead BatchNorm fundamentally impacts the training process by making the <strong>optimization landscape significantly smoother</strong>, thus leading to a more predictive and stable behaviour of the gradients</p></blockquote>
<p>This is the most common form of normalization (ma l’idea è sempre la stessa, computare varianza e media, e poi sottrarre media e dividere per varianza). La cosa in più è che vengono aggiunte delle varianze e una media, per denormalizzare l’output, in modo che abbia la forma dei dati migliore possibile.</p>
<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 14.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 14">
<p>In reality, how it is implemented in pytorch is that $\mu^{B}$ and $\sigma^{B}$ are computed for each batch, and then used to normalize the output, they are not learned parameters. But an <strong>weighted average</strong> is kept for inference.</p>
<blockquote>
<p>Batch normalization makes weights in deeper layers more robust to changes to weights in the shallower layers of the network.</p></blockquote>
<h3 id="other-normalization">Other Normalization<a hidden class="anchor" aria-hidden="true" href="#other-normalization">#</a></h3>
<p>Potremmo provare a <strong>normalizzare per canale</strong></p>
<ul>
<li>Slide normalizations</li>
</ul>
<p>!!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 15.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Convolutional NN/Untitled 15"></p>
<p>There are many kinds of normalization, here is a table that summarizes the most famous cases.</p>
<table>
  <thead>
      <tr>
          <th>Name</th>
          <th>Normalizes over</th>
          <th>Parameters</th>
          <th>Dependency on Batch Size</th>
          <th>Typical Use Cases</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>BatchNorm (BN)</strong></td>
          <td>(N,H,W)(N, H, W) — per channel</td>
          <td>Yes</td>
          <td><strong>Yes</strong></td>
          <td>Classification, convnets</td>
      </tr>
      <tr>
          <td><strong>LayerNorm (LN)</strong></td>
          <td>(C,H,W)(C, H, W) — per sample</td>
          <td>Yes</td>
          <td>No</td>
          <td>NLP, transformers</td>
      </tr>
      <tr>
          <td><strong>InstanceNorm (IN)</strong></td>
          <td>(H,W)(H, W) — per channel per sample</td>
          <td>Yes</td>
          <td>No</td>
          <td>Style transfer, generative models</td>
      </tr>
      <tr>
          <td><strong>GroupNorm (GN)</strong></td>
          <td>Subsets of channels</td>
          <td>Yes</td>
          <td>No</td>
          <td>General-purpose, batch-size agnostic</td>
      </tr>
      <tr>
          <td><strong>PixelNorm</strong></td>
          <td>Across channels per pixel</td>
          <td>No</td>
          <td>No</td>
          <td>GANs (especially StyleGAN)</td>
      </tr>
  </tbody>
</table>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=heDeepResidualLearning2016>[1] He et al. <a href="http://ieeexplore.ieee.org/document/7780459/">“Deep Residual Learning for Image Recognition”</a> IEEE  2016
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machine-perception/">Machine-Perception</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Convolutional Neural Network on x"
            href="https://x.com/intent/tweet/?text=Convolutional%20Neural%20Network&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fconvolutional-neural-network%2f&amp;hashtags=machine-perception">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Convolutional Neural Network on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fconvolutional-neural-network%2f&amp;title=Convolutional%20Neural%20Network&amp;summary=Convolutional%20Neural%20Network&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fconvolutional-neural-network%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Convolutional Neural Network on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fconvolutional-neural-network%2f&title=Convolutional%20Neural%20Network">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Convolutional Neural Network on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fconvolutional-neural-network%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Convolutional Neural Network on whatsapp"
            href="https://api.whatsapp.com/send?text=Convolutional%20Neural%20Network%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fconvolutional-neural-network%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Convolutional Neural Network on telegram"
            href="https://telegram.me/share/url?text=Convolutional%20Neural%20Network&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fconvolutional-neural-network%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Convolutional Neural Network on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Convolutional%20Neural%20Network&u=https%3a%2f%2fflecart.github.io%2fnotes%2fconvolutional-neural-network%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
