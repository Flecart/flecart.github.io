<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Counterfactual Invariance | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning">
<meta name="description" content="Machine learning cannot distinguish between causal and environment features.
Shortcut learning Often we observe shortcut learning: the model learns some dataset dependent shortcuts (e.g. the machine that was used to take the X-ray) to make inference, but this is very brittle, and is not usually able to generalize.
Shortcut learning happens when there are correlations in the test set between causal and non-causal features. Our object of interest should be the main focus, not the environment around, in most of the cases.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/counterfactual-invariance/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/counterfactual-invariance/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Counterfactual Invariance" />
<meta property="og:description" content="Machine learning cannot distinguish between causal and environment features.
Shortcut learning Often we observe shortcut learning: the model learns some dataset dependent shortcuts (e.g. the machine that was used to take the X-ray) to make inference, but this is very brittle, and is not usually able to generalize.
Shortcut learning happens when there are correlations in the test set between causal and non-causal features. Our object of interest should be the main focus, not the environment around, in most of the cases." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/counterfactual-invariance/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Counterfactual Invariance"/>
<meta name="twitter:description" content="Machine learning cannot distinguish between causal and environment features.
Shortcut learning Often we observe shortcut learning: the model learns some dataset dependent shortcuts (e.g. the machine that was used to take the X-ray) to make inference, but this is very brittle, and is not usually able to generalize.
Shortcut learning happens when there are correlations in the test set between causal and non-causal features. Our object of interest should be the main focus, not the environment around, in most of the cases."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Counterfactual Invariance",
      "item": "https://flecart.github.io/notes/counterfactual-invariance/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Counterfactual Invariance",
  "name": "Counterfactual Invariance",
  "description": "Machine learning cannot distinguish between causal and environment features.\nShortcut learning Often we observe shortcut learning: the model learns some dataset dependent shortcuts (e.g. the machine that was used to take the X-ray) to make inference, but this is very brittle, and is not usually able to generalize.\nShortcut learning happens when there are correlations in the test set between causal and non-causal features. Our object of interest should be the main focus, not the environment around, in most of the cases.",
  "keywords": [
    "machinelearning"
  ],
  "articleBody": " Machine learning cannot distinguish between causal and environment features.\nShortcut learning Often we observe shortcut learning: the model learns some dataset dependent shortcuts (e.g. the machine that was used to take the X-ray) to make inference, but this is very brittle, and is not usually able to generalize.\nShortcut learning happens when there are correlations in the test set between causal and non-causal features. Our object of interest should be the main focus, not the environment around, in most of the cases. For example, a camel in a grass land should still be recognized as a camel, not a cow. One solution could be engineering invariant representations which are independent of the environment. So having a kind of encoder that creates these representations.\nCounterfactual Invariance Counterfactual invariance is a formal framework to define the variables that influence and do not influence the output of a model under certain contexts (i.e. downstream tasks that you could have). It has been introduced in (Veitch et al. 2021) for text perturbations originally.\nA first notion of counterfactual invariance 🟨 Suppose we have a function $f : \\mathcal{X} \\to \\mathcal{Y}$. Let’s define a counterfactual for a random variable $X$. Let’s say $W$ is a random variable that represents our non-causal features, e.g. our background. We say $X(\\omega)$ is the result of $X$ when $W=\\omega$, so we force the background to be some specific thing. We would like to formalize the following idea: the outcome of $X$ should be only dependent on $X$, not on $\\omega$.\nWe say $f$ is counterfactually invariant if the following holds: $f(X(\\omega)) = f(X(\\omega'))$ for any $\\omega, \\omega' \\in W$. How does this happen in practice? Ideally we would like to train any counterfactual, but this is practically impossible (too many resources to get camels into Himalaya to create this counterfactual! Additionally, we have too many possible background environments!)\nCausal Graphs The main advantage of using causal graphs is the intuitive understanding of the relations between the variables. Furthermore, these graph relations could be used to define algorithms for inference that exploit their structure. So we say: causal graphs are both interpretable and useful inference models. We now explore some desiderata that is clearly understood in terms of causal graphs: to correctly formalize the notion of counterfactual invariance.\nCausal Graphs 🟩 In causal scenarios our input features $X$ indeed have a causal relation with $Y$\nSuppose we want to classify cancer and we have three features, $X = (location, CO_{2}, smoke)$, where location is $\\mathbb{R}^{2}$, $CO_{2}\\in \\mathbb{R}$, and $smoke$ is a boolean is our $Y$, or categorical, $W = city$. We can build a causal graph of possible relations between the variables. In the above image $X^{\\perp}_{Y}$ is the set of variables that do not influence $Y$, and $X_{W \\\u0026 Y}$ is the set of variables that influence both $W$ and $Y$. And $X_{W}^{\\perp}$ is the set of variables that are not influenced by $W$ but do influence $Y$.\nAnti-Causal 🟩 Let’s consider another scenario anti-causal scenario, where we want to predict celiac disease, we have stomach ache, fatigue and income as features. Our background variable would be the Job. In these kinds of scenarios, our output random variables $Y$ have a causal relationship with the features in $X$. Whys of non-causal relations 🟥 We can say that two are the main causes of non-causal associations in causal graphs:\nConfounding variables: existence of another random variable U that could affect both of the variables of our interest. Selection bias: We have a variable S that filters the dataset based on the features that we want. Where $X$ are the input features , $Y$ are the output, and $W$ is the environment. In this whole set of note we will keep this nomenclature. An example of selection bias is studying the success of jobs, but you just sample from people on LinkedIn. Formally, we say we have a selection bias if all our samples have a selection criteria $S = 1$. If we want to account both for confounding variables and selection bias, we say our samples satisfy\n$$ P(X, Y, Z) = \\int P(X, Y, Z, u \\mid S = 1) \\, du $$ We say that a relationship is purely spurious if $$ Y \\perp X \\mid W, X_{W}^{\\perp} $$ That is: we can predict $Y$ by only using features that do not depend on $W$. This is a easy way to define spuriousness.\nSimpson’s Paradox 🟨 We give the intuition on this paradox with a simple example. Let’s say we have treatment A and B, we would like to know which treatment is better. We sent 350 to A and 350 to B. Let’s say we observe that with treatment A $78\\%$ recovered, with B $83\\%$ recovered. Seems treatment B is better. But in the case we have another variable, e.g. the severity of the illness, the view could be far more different! These are called confounding variables. When designing an experiment we should keep also this in mind.\nSimpson’s paradox occurs due to confounding variables that influence both the group formation and the variables being studied. The decision whether to use treatment $A$ or $B$ should be based on causal considerations for Judea Pearl: different causal structures could arise for the same data, see (Pearl 2009) Chapter 6.1.3.\nThis phenomenon can be described in terms of events in probability. Consider $E$ to be the variable of effect, $C$ a cause (for example the drug trial) and $F$ an indicator variable describing a sub-population (i.e. Male or Female). We have: $$ \\begin{align} P(E \\mid C) \u003e P(E \\mid C^{c}) \\\\ P(E \\mid C, F) \u003c P(E \\mid C^{c}, F) \\\\ P(E \\mid C, F^{c}) \u003c P(E \\mid C^{c}, F^{c}) \\end{align} $$ A formal definition for counterfactual invariance 🟨+ Intuitively a model $f$ is counterfactually invariant if it only depends on $X_{W}^{\\perp}$ which are the features independent on the background (the cow in the example before). The following has been proven by Veitch in (Veitch et al. 2021), this should be still an active area of research.\nFor an estimator $f$ to be counterfactually invariant we need:\nAnti-causal scenario $f(X) \\perp W \\mid Y$ Causal scenario without selection (possibly confounded) $f(X) \\perp W$ Causal scenario with selection we need $f(X) \\perp W \\mid Y$ as long as $(X_{W \\\u0026 Y}, X_{Y}^{\\perp})$ do not influence $Y$, i.e. we have $Y \\perp X \\mid X_{W}^{\\perp}, W$ . Therefore, connecting to the intuitive notion of counterfactual invariance we would like to have $f(X) \\mid W = w, Y=y$ to have the same distribution as $f(X) \\mid W = w', Y= y$ for any $w, w'$. It is\nV-structures 🟩 This is called d-separation. Let’s take for example they are 3 random variables $A, B, C$ in this order. They are all Markov Chains. The nice thing that we have is that $$ p(A, C \\mid B) = P(A \\mid B)p(C \\mid B) $$ A V structure is a markov chain in this form $A \\to B \\leftarrow C$: if i know B, then $A, C$ are related to each other.\nTODO: make the reasoning for a $A \\to B \\to C$ Markov chain, and conclude that knowing $B$, makes A and C conditionally independent.\nOne thing that has not been said about collisions, is that we need everychild of it to be not observed (this is what the pyramid below that means).\nSimilarity Metrics If we have two $X$ that represent the same idea but in different backgrounds, we would like their two representations to be somewhat similar. This brings the need to create some sort of a metric to measure their similarity. This section attempts to build upon this idea. This seems to be one of the seminal papers on the idea.\nChecking the difference 🟨+ We would like a way to compute a metric that tells us how different two probability distributions are (this is called two sample or homogeneity problem) . Given a probability space $(\\mathcal{X}, \\mathcal{\\Sigma}, p^{*})$ and another $(\\mathcal{X}, \\mathcal{\\Sigma}, q^{*})$, and $\\mathcal{X}$ is compact. Given some realizations: $$ \\begin{align} \\{ x_{1}, \\dots, x_{n} \\} \\sim p^{*} \\\\ \\{ y_{1}', \\dots, y_{n}' \\} \\sim q^{*} \\end{align} $$ We would like to quantify the sameness between these two distributions. Note that they share the sample space and the Sigma algebra on that.\nThe idea is that if $p \\neq q$ then there exists a set in the sigma algebra that has different measure (else, it would be exactly the same). We can write the same thing with the use of expectation as: $$ p^{*}(x) \\neq q^{*}(x) \\implies\\mathbb{E}_{p^{*}}[\\mathbb{1}_{A}(x)]]\\neq \\mathbb{E}_{q^{*}}[\\mathbb{1}_{A}(x)]] $$ The indicator can be approximated by a continuous trapezoidal function with very high slope. $$ \\exists f \\in C(x) : \\mathbb{E}_{p^{*}}[f(x)] \\neq \\mathbb{E}_{q^{*}}[f(x)] $$ So checking the difference is the same as computing the expectation of the approximation of the indicator function. This has been formally proven by Dudley (2002) lemma 9.3.2 (they have proved something stronger).\nComparison with KL Divergence This section was generated by GPT.\nFeature MMD KL Divergence Requires explicit densities No Yes Symmetry Symmetric Asymmetric Support mismatch Well-defined Can be infinite Computational feasibility Efficient for empirical samples Challenging for high-dimensional data Robustness to noise Robust Sensitive MMD is ideal in situations where:\nYou only have empirical samples of the distributions. The distributions are high-dimensional and nonparametric. A symmetric or support-insensitive measure is needed. Maximum Mean Discrepancy 🟨+ The Maximum Mean Discrepancy is a way to measure the difference between two distributions that builds upon the previous idea. It is defined as: $$ MMD(\\mathcal{F}, \\mathcal{X}, \\mathcal{Y}) = \\sup_{f \\in \\mathcal{F}} \\left| \\mathop{\\mathbb{E}}_{x \\sim p}[f(x)] - \\mathop{\\mathbb{E}}_{y \\sim q}[f(y)] \\right| $$ Where $\\mathcal{F}$ is a set of functions that are bounded and continuous. The MMD is a metric that measures the difference between two distributions (Müller 1997). The bad thing is that it is difficult to compute: the space of the functions is quite large. In this discussion we will restrict ourselves to the unit sphere of universal RKHS, see Kernel Methods for that. One interpretation of this set is the polynomials whose coefficients squared is 1.\nRiesz Representation Space 🟨 Applying a bounded linear operator in a Hilbert’s Space, then the operator can be represented as a inner product with a function in the space. This is the Riesz Representation Theorem in short! The main usage is moving from the functional realm to an algebraic realm. So we have a strong connection between functional analysis and algebra!\nFormally, it states that for every linear functional $L$ on $H$, a Hilbert’s Space, there exists a unique vector $v$ in $H$ such that $$ L(u) = \\langle u, v \\rangle $$ And the norm of the functional is the norm of the vector. In this context, we use it to say that we can write $$ \\mathbb{E}_{\\mathcal{X}}[f(x)] = \\beta_{f}^{T}\\mu_{\\mathcal{X}} $$ Algebraic Maximum Mean Discrepancy 🟨 We will use Riesz representation theorem and the above MMD to come up with an algebraic version of it that should be easier to compute. By Riesz theorem, computing $f(x)$ is the same as computing the inner product with $\\phi(x) \\in RKHS$ where $\\phi$ is from a family of functions in the RKHS (See Kernel Methods). We express this version of MMD in the following way (Lemma by Borgwardt et al. 2006): $$ \\begin{align} MMD^{2}(\\mathcal{F}, \\mathcal{X}, \\mathcal{Y}) \u0026= \\left[ \\sup_{\\lVert f \\rVert {\\mathcal{H}} \\leq 1} (\\mathop{\\mathbb{E}}{p}[f(x)] - \\mathop{\\mathbb{E}}{q}[f(y)] \\right]^{2} \\ \\text{Using Riesz Th. }\u0026= \\left[ \\sup{\\lVert f \\rVert {\\mathcal{H}} \\leq 1} (\\mathop{\\mathbb{E}}{p}[\\langle \\phi(x), f \\rangle_{\\mathcal{H}}] - \\mathop{\\mathbb{E}}{q}[\\langle \\phi(y), f \\rangle{\\mathcal{H}}] \\right]^{2} \\ \\text{Using linearify of expectation} \u0026= \\left[ \\sup_{\\lVert f \\rVert {\\mathcal{H}} \\leq 1} \\langle \\mu{\\mathcal{X}} - \\mu_{\\mathcal{Y}}, f \\rangle_{\\mathcal{H}} \\right]^{2} \\ \\text{Using Chauchy Schwarz} \u0026= \\lVert \\mu_{\\mathcal{X}} - \\mu_{\\mathcal{Y}} \\rVert^{2}{\\mathcal{H}} \\ \u0026= \\langle \\mu{p}, \\mu_{p} \\rangle_{\\mathcal{H}} + \\langle \\mu_{q}, \\mu_{q} \\rangle_{\\mathcal{H}} - 2\\langle \\mu_{p}, \\mu_{q} \\rangle_{\\mathcal{H}} \\ \u0026= \\mathop{\\mathbb{E}}{p}[k(x, x’)] + \\mathop{\\mathbb{E}}{q}[k(y, y’)] - 2\\mathop{\\mathbb{E}}_{p, q}[k(x, y)] \\end{align}\n$$\nThe nice thing is that the latter form can be empirically approximated: $$ \\mathop{\\mathbb{E}}_{x, x' \\sim p}[k(x, x')] \\approx \\frac{1}{n^{2}} \\sum_{i, j} k(x_{i}, x_{j}) $$ And the last two are also compute accordingly.\nReferences [1] Pearl “Causality” Cambridge University Press 2009\n[2] Veitch et al. “Counterfactual Invariance to Spurious Correlations in Text Classification” Curran Associates, Inc. 2021\n",
  "wordCount" : "2034",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/counterfactual-invariance/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Counterfactual Invariance
    </h1>
    <div class="post-meta">10 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul><ul>
                <li>
                    <a href="#shortcut-learning" aria-label="Shortcut learning">Shortcut learning</a></li></ul>
                    </ul>
                    
                <li>
                    <a href="#counterfactual-invariance" aria-label="Counterfactual Invariance">Counterfactual Invariance</a><ul>
                        <ul>
                        
                <li>
                    <a href="#a-first-notion-of-counterfactual-invariance-" aria-label="A first notion of counterfactual invariance 🟨">A first notion of counterfactual invariance 🟨</a></li></ul>
                    
                <li>
                    <a href="#causal-graphs" aria-label="Causal Graphs">Causal Graphs</a><ul>
                        
                <li>
                    <a href="#causal-graphs-" aria-label="Causal Graphs 🟩">Causal Graphs 🟩</a></li>
                <li>
                    <a href="#anti-causal-" aria-label="Anti-Causal 🟩">Anti-Causal 🟩</a></li>
                <li>
                    <a href="#whys-of-non-causal-relations-" aria-label="Whys of non-causal relations 🟥">Whys of non-causal relations 🟥</a></li>
                <li>
                    <a href="#simpsons-paradox-" aria-label="Simpson&rsquo;s Paradox 🟨">Simpson&rsquo;s Paradox 🟨</a></li>
                <li>
                    <a href="#a-formal-definition-for-counterfactual-invariance-" aria-label="A formal definition for counterfactual invariance 🟨&#43;">A formal definition for counterfactual invariance 🟨+</a></li>
                <li>
                    <a href="#v-structures-" aria-label="V-structures 🟩">V-structures 🟩</a></li></ul>
                </li>
                <li>
                    <a href="#similarity-metrics" aria-label="Similarity Metrics">Similarity Metrics</a><ul>
                        
                <li>
                    <a href="#checking-the-difference-" aria-label="Checking the difference 🟨&#43;">Checking the difference 🟨+</a></li>
                <li>
                    <a href="#comparison-with-kl-divergence" aria-label="Comparison with KL Divergence">Comparison with KL Divergence</a></li>
                <li>
                    <a href="#maximum-mean-discrepancy-" aria-label="Maximum Mean Discrepancy 🟨&#43;">Maximum Mean Discrepancy 🟨+</a></li>
                <li>
                    <a href="#riesz-representation-space-" aria-label="Riesz Representation Space 🟨">Riesz Representation Space 🟨</a></li>
                <li>
                    <a href="#algebraic-maximum-mean-discrepancy-" aria-label="Algebraic Maximum Mean Discrepancy 🟨">Algebraic Maximum Mean Discrepancy 🟨</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Machine learning cannot distinguish between causal and environment features.</p>
</blockquote>
<h4 id="shortcut-learning">Shortcut learning<a hidden class="anchor" aria-hidden="true" href="#shortcut-learning">#</a></h4>
<p>Often we observe <strong>shortcut learning</strong>: the model learns some dataset dependent shortcuts (e.g. the machine that was used to take the X-ray) to make inference, but this is very brittle, and is not usually able to generalize.</p>
<p>Shortcut learning happens when there are correlations in the test set between causal and non-causal features. Our object of interest should be the main focus, not the environment around, in most of the cases. For example, a camel in a grass land should still be recognized as a camel, not a cow.
One solution could be engineering <strong>invariant representations</strong> which are independent of the environment. So having a kind of encoder that creates these representations.</p>
<h2 id="counterfactual-invariance">Counterfactual Invariance<a hidden class="anchor" aria-hidden="true" href="#counterfactual-invariance">#</a></h2>
<p>Counterfactual invariance is a formal framework to define the variables that influence and do not influence the output of a model under certain contexts (i.e. downstream tasks that you could have).
It has been introduced in <a href="https://proceedings.neurips.cc/paper/2021/hash/8710ef761bbb29a6f9d12e4ef8e4379c-Abstract.html">(Veitch et al. 2021)</a> for text perturbations originally.</p>
<h4 id="a-first-notion-of-counterfactual-invariance-">A first notion of counterfactual invariance 🟨<a hidden class="anchor" aria-hidden="true" href="#a-first-notion-of-counterfactual-invariance-">#</a></h4>
<p>Suppose we have a function $f : \mathcal{X} \to \mathcal{Y}$. Let&rsquo;s define a <strong>counterfactual</strong> for a random variable $X$. Let&rsquo;s say $W$ is a random variable that represents our non-causal features, e.g. our background. We say $X(\omega)$ is the result of $X$ when $W=\omega$, so we force the background to be some specific thing.
We would like to formalize the following idea: the outcome of $X$ should be <em>only</em> dependent on $X$, not on $\omega$.</p>
<p>We say $f$ is <strong>counterfactually invariant</strong> if the following holds: $f(X(\omega)) = f(X(\omega'))$ for any $\omega, \omega' \in W$.
How does this happen in practice? Ideally we would like to train any counterfactual, but this is practically impossible (too many resources to get camels into Himalaya to create this counterfactual! Additionally, we have too many possible background environments!)</p>
<h3 id="causal-graphs">Causal Graphs<a hidden class="anchor" aria-hidden="true" href="#causal-graphs">#</a></h3>
<p>The main advantage of using causal graphs is the intuitive understanding of the relations between the variables. Furthermore, these graph relations could be used to define algorithms for inference that exploit their structure.
So we say: causal graphs are both interpretable and useful inference models. We now explore some desiderata that is clearly understood in terms of causal graphs: to correctly formalize the notion of counterfactual invariance.</p>
<h4 id="causal-graphs-">Causal Graphs 🟩<a hidden class="anchor" aria-hidden="true" href="#causal-graphs-">#</a></h4>
<p>In causal scenarios our input features $X$ indeed have a causal relation with $Y$</p>
<p>Suppose we want to classify cancer and we have three features, $X = (location, CO_{2}, smoke)$, where location is $\mathbb{R}^{2}$, $CO_{2}\in \mathbb{R}$, and $smoke$ is a boolean is our $Y$, or categorical, $W = city$.
We can build a <strong>causal graph</strong> of possible relations between the variables.
<img src="/images/notes/Counterfactual Invariance-20241208145112544.webp" alt="Counterfactual Invariance-20241208145112544"></p>
<p>In the above image $X^{\perp}_{Y}$ is the set of variables that do not influence $Y$, and $X_{W \& Y}$ is the set of variables that influence both $W$ and $Y$. And $X_{W}^{\perp}$ is the set of variables that are not influenced by $W$ but do influence $Y$.</p>
<h4 id="anti-causal-">Anti-Causal 🟩<a hidden class="anchor" aria-hidden="true" href="#anti-causal-">#</a></h4>
<p>Let&rsquo;s consider another scenario <em>anti-causal</em> scenario, where we want to predict celiac disease, we have stomach ache, fatigue and income as features. Our background variable would be the Job.
In these kinds of scenarios, our output random variables $Y$ have a causal relationship with the features in $X$.
<img src="/images/notes/Counterfactual Invariance-20241208145402080.webp" alt="Counterfactual Invariance-20241208145402080"></p>
<h4 id="whys-of-non-causal-relations-">Whys of non-causal relations 🟥<a hidden class="anchor" aria-hidden="true" href="#whys-of-non-causal-relations-">#</a></h4>
<p>We can say that two are the main causes of non-causal associations in causal graphs:</p>
<ol>
<li>Confounding variables: existence of another random variable U that could affect both of the variables of our interest.</li>
<li>Selection bias: We have a variable S that filters the dataset based on the features that we want.</li>
</ol>
<p>Where $X$ are the input features , $Y$ are the output, and $W$ is the environment. In this whole set of note we will keep this nomenclature.
An example of selection bias is studying the success of jobs, but you just sample from people on LinkedIn. Formally, we say we have a selection bias if all our samples have a selection criteria $S = 1$.
If we want to account both for confounding variables and selection bias, we say our samples satisfy</p>
$$
P(X, Y, Z) = \int P(X, Y, Z, u \mid S = 1) \, du
$$
<p>We say that a relationship is <em>purely spurious</em> if
</p>
$$
Y \perp X \mid W, X_{W}^{\perp}
$$
<p>
That is: we can predict $Y$ by only using features that do not depend on $W$. This is a easy way to define spuriousness.</p>
<h4 id="simpsons-paradox-">Simpson&rsquo;s Paradox 🟨<a hidden class="anchor" aria-hidden="true" href="#simpsons-paradox-">#</a></h4>
<p>We give the intuition on this paradox with a simple example.
Let&rsquo;s say we have treatment A and B, we would like to know which treatment is better. We sent 350 to A and 350 to B.
Let&rsquo;s say we observe that with treatment A $78\%$ recovered, with B $83\%$ recovered. Seems treatment B is better. But in the case we have another variable, e.g. the severity of the illness, the view could be far more different! These are called <strong>confounding variables</strong>. When designing an experiment we should keep also this in mind.</p>
<img src="/images/notes/Causality-20241013221959683.webp" alt="Causality-20241013221959683">
<p>Simpson&rsquo;s paradox occurs due to <strong>confounding variables</strong> that influence both the group formation and the variables being studied. The decision whether to use treatment $A$ or $B$  should be based on <em>causal considerations</em> for Judea Pearl: different causal structures could arise for the same data, see <a href="https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B">(Pearl 2009)</a> Chapter 6.1.3.</p>
<p>This phenomenon can be described in terms of events in probability. Consider $E$ to be the variable of effect, $C$ a cause (for example the drug trial) and $F$ an indicator variable describing a sub-population (i.e. Male or Female).
We have:
</p>
$$
\begin{align}
P(E \mid C) > P(E \mid C^{c}) \\
P(E \mid C, F) < P(E \mid C^{c}, F)  \\
P(E \mid C, F^{c}) < P(E \mid C^{c}, F^{c})
\end{align}
$$
<h4 id="a-formal-definition-for-counterfactual-invariance-">A formal definition for counterfactual invariance 🟨+<a hidden class="anchor" aria-hidden="true" href="#a-formal-definition-for-counterfactual-invariance-">#</a></h4>
<p>Intuitively a model $f$ is counterfactually invariant if it only depends on $X_{W}^{\perp}$ which are the features independent on the background (the cow in the example before).
The following has been proven by Veitch in <a href="https://proceedings.neurips.cc/paper/2021/hash/8710ef761bbb29a6f9d12e4ef8e4379c-Abstract.html">(Veitch et al. 2021)</a>, this should be still an active area of research.</p>
<p>For an estimator $f$ to be counterfactually invariant we need:</p>
<ul>
<li>Anti-causal scenario $f(X) \perp W \mid Y$</li>
<li>Causal scenario without selection (possibly confounded) $f(X) \perp W$</li>
<li>Causal scenario with selection we need $f(X) \perp W \mid Y$ as long as $(X_{W \& Y}, X_{Y}^{\perp})$ do not influence $Y$, i.e.  we have $Y \perp X \mid X_{W}^{\perp}, W$ .</li>
</ul>
<p>Therefore, connecting to the intuitive notion of counterfactual invariance we would like to have
$f(X) \mid W = w, Y=y$ to have the same distribution as $f(X) \mid W = w', Y= y$ for any $w, w'$.
It is</p>
<h4 id="v-structures-">V-structures 🟩<a hidden class="anchor" aria-hidden="true" href="#v-structures-">#</a></h4>
<p>This is called <em>d-separation</em>.
<img src="/images/notes/Causality-20241017153344164.webp" width="382" alt="Causality-20241017153344164"></p>
<p>Let&rsquo;s take for example they are 3 random variables $A, B, C$ in this order. They are all <a href="/notes/markov-chains/">Markov Chains</a>.
The nice thing that we have is that
</p>
$$
p(A, C \mid B) = P(A \mid B)p(C \mid B)
$$
<p>A V structure is a markov chain in this form $A \to B \leftarrow C$: if i know B, then $A, C$ are related to each other.</p>
<p>TODO: make the reasoning for a $A \to B \to C$ Markov chain, and conclude that knowing $B$, makes A and C conditionally independent.</p>
<p>One thing that has not been said about collisions, is that we need everychild of it to be not observed (this is what the pyramid below that means).</p>
<h3 id="similarity-metrics">Similarity Metrics<a hidden class="anchor" aria-hidden="true" href="#similarity-metrics">#</a></h3>
<p>If we have two $X$ that represent the same idea but in different backgrounds, we would like their two representations to be somewhat similar. This brings the need to create some sort of a metric to measure their similarity. This section attempts to build upon this idea.
<a href="https://arxiv.org/pdf/0805.2368">This</a> seems to be one of the seminal papers on the idea.</p>
<h4 id="checking-the-difference-">Checking the difference 🟨+<a hidden class="anchor" aria-hidden="true" href="#checking-the-difference-">#</a></h4>
<p>We would like a way to compute a metric that tells us how different two probability distributions are (this is called <em>two sample</em> or <em>homogeneity problem</em>) .
Given a probability space $(\mathcal{X}, \mathcal{\Sigma}, p^{*})$ and another $(\mathcal{X}, \mathcal{\Sigma}, q^{*})$, and $\mathcal{X}$ is compact.
Given some <em>realizations</em>:
</p>
$$
\begin{align}
 \{ x_{1}, \dots, x_{n} \} \sim  p^{*}  \\
  \{ y_{1}', \dots, y_{n}' \} \sim  q^{*}
\end{align}
$$
<p>
We would like to quantify the <em>sameness</em> between these two distributions. Note that they share the sample space and the Sigma algebra on that.</p>
<p>The idea is that if $p \neq q$ then there exists a set in the sigma algebra that has different measure (else, it would be exactly the same).
We can write the same thing with the use of expectation as:
</p>
$$
p^{*}(x) \neq q^{*}(x) \implies\mathbb{E}_{p^{*}}[\mathbb{1}_{A}(x)]]\neq \mathbb{E}_{q^{*}}[\mathbb{1}_{A}(x)]]
$$
<p>
The indicator can be approximated by a continuous trapezoidal function with very high slope.
</p>
$$
\exists f \in C(x) : \mathbb{E}_{p^{*}}[f(x)] \neq \mathbb{E}_{q^{*}}[f(x)]
$$
<p>
So checking the difference is the same as computing the expectation of the approximation of the indicator function. This has been formally proven by <a href="https://www.cambridge.org/core/books/real-analysis-and-probability/26DDF2D09E526185F2347AA5658B96F6">Dudley (2002)</a> lemma 9.3.2 (they have proved something stronger).</p>
<h4 id="comparison-with-kl-divergence">Comparison with KL Divergence<a hidden class="anchor" aria-hidden="true" href="#comparison-with-kl-divergence">#</a></h4>
<p>This section was generated by GPT.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>MMD</th>
<th>KL Divergence</th>
</tr>
</thead>
<tbody>
<tr>
<td>Requires explicit densities</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Symmetry</td>
<td>Symmetric</td>
<td>Asymmetric</td>
</tr>
<tr>
<td>Support mismatch</td>
<td>Well-defined</td>
<td>Can be infinite</td>
</tr>
<tr>
<td>Computational feasibility</td>
<td>Efficient for empirical samples</td>
<td>Challenging for high-dimensional data</td>
</tr>
<tr>
<td>Robustness to noise</td>
<td>Robust</td>
<td>Sensitive</td>
</tr>
</tbody>
</table>
<p>MMD is ideal in situations where:</p>
<ol>
<li>You only have empirical samples of the distributions.</li>
<li>The distributions are high-dimensional and nonparametric.</li>
<li>A symmetric or support-insensitive measure is needed.</li>
</ol>
<h4 id="maximum-mean-discrepancy-">Maximum Mean Discrepancy 🟨+<a hidden class="anchor" aria-hidden="true" href="#maximum-mean-discrepancy-">#</a></h4>
<p>The Maximum Mean Discrepancy is a way to measure the difference between two distributions that builds upon the previous idea. It is defined as:
</p>
$$
MMD(\mathcal{F}, \mathcal{X}, \mathcal{Y}) = \sup_{f \in \mathcal{F}} \left| \mathop{\mathbb{E}}_{x \sim p}[f(x)] - \mathop{\mathbb{E}}_{y \sim q}[f(y)] \right|
$$
<p>
Where $\mathcal{F}$ is a set of functions that are bounded and continuous. The MMD is a metric that measures the difference between two distributions (Müller 1997).
The bad thing  is that it is difficult to compute: the space of the functions is quite large. In this discussion we will restrict ourselves to the unit sphere of universal RKHS, see <a href="/notes/kernel-methods/">Kernel Methods</a> for that.
One interpretation of this set is the polynomials whose coefficients squared is 1.</p>
<h4 id="riesz-representation-space-">Riesz Representation Space 🟨<a hidden class="anchor" aria-hidden="true" href="#riesz-representation-space-">#</a></h4>
<p>Applying a bounded linear operator in a Hilbert&rsquo;s Space, then the operator can be represented as a inner product with a function in the space. This is the Riesz Representation Theorem in short!
The main usage is moving from the functional realm to an algebraic realm. So we have a strong connection between functional analysis and algebra!</p>
<p>Formally, it states that for every linear functional $L$ on $H$, a Hilbert&rsquo;s Space, there exists a <em>unique</em> vector $v$ in $H$ such that
</p>
$$
L(u) = \langle u, v \rangle 
$$
<p>
And the norm of the functional is the norm of the vector.
In this context, we use it to say that we can write
</p>
$$
\mathbb{E}_{\mathcal{X}}[f(x)]  = \beta_{f}^{T}\mu_{\mathcal{X}}
$$
<h4 id="algebraic-maximum-mean-discrepancy-">Algebraic Maximum Mean Discrepancy 🟨<a hidden class="anchor" aria-hidden="true" href="#algebraic-maximum-mean-discrepancy-">#</a></h4>
<p>We will use Riesz representation theorem and the above MMD to come up with an algebraic version of it that should be easier to compute. By Riesz theorem, computing $f(x)$ is the same as computing the inner product with $\phi(x) \in RKHS$ where $\phi$ is from a family of functions in the RKHS (See <a href="/notes/kernel-methods/">Kernel Methods</a>).
We express this version of MMD in the following way (Lemma by Borgwardt et al. 2006):
$$
\begin{align}
MMD^{2}(\mathcal{F}, \mathcal{X}, \mathcal{Y}) &amp;= \left[ \sup_{\lVert f \rVert <em>{\mathcal{H}} \leq 1} (\mathop{\mathbb{E}}</em>{p}[f(x)] - \mathop{\mathbb{E}}<em>{q}[f(y)] \right]^{2}   \
\text{Using Riesz Th. }&amp;= \left[ \sup</em>{\lVert f \rVert <em>{\mathcal{H}} \leq 1} (\mathop{\mathbb{E}}</em>{p}[\langle \phi(x), f \rangle_{\mathcal{H}}] - \mathop{\mathbb{E}}<em>{q}[\langle \phi(y), f \rangle</em>{\mathcal{H}}] \right]^{2}  \
\text{Using linearify of expectation} &amp;= \left[ \sup_{\lVert f \rVert <em>{\mathcal{H}} \leq 1} \langle \mu</em>{\mathcal{X}} - \mu_{\mathcal{Y}}, f \rangle_{\mathcal{H}} \right]^{2}   \
\text{Using Chauchy Schwarz} &amp;= \lVert \mu_{\mathcal{X}} - \mu_{\mathcal{Y}} \rVert^{2}<em>{\mathcal{H}} \
&amp;= \langle \mu</em>{p}, \mu_{p} \rangle_{\mathcal{H}} + \langle \mu_{q}, \mu_{q} \rangle_{\mathcal{H}} - 2\langle \mu_{p}, \mu_{q} \rangle_{\mathcal{H}}  \
&amp;= \mathop{\mathbb{E}}<em>{p}[k(x, x&rsquo;)] + \mathop{\mathbb{E}}</em>{q}[k(y, y&rsquo;)] - 2\mathop{\mathbb{E}}_{p, q}[k(x, y)]
\end{align}</p>
<p>$$</p>
<p>The nice thing is that the latter form can be empirically approximated:
</p>
$$
\mathop{\mathbb{E}}_{x, x' \sim p}[k(x, x')] \approx \frac{1}{n^{2}} \sum_{i, j} k(x_{i}, x_{j})
$$
<p>
And the last two are also compute accordingly.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Pearl <a href="https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B">“Causality”</a> Cambridge University Press 2009</p>
<p>[2] Veitch et al. <a href="https://proceedings.neurips.cc/paper/2021/hash/8710ef761bbb29a6f9d12e4ef8e4379c-Abstract.html">“Counterfactual Invariance to Spurious Correlations in Text Classification”</a> Curran Associates, Inc.  2021</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Counterfactual Invariance on x"
            href="https://x.com/intent/tweet/?text=Counterfactual%20Invariance&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fcounterfactual-invariance%2f&amp;hashtags=machinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Counterfactual Invariance on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fcounterfactual-invariance%2f&amp;title=Counterfactual%20Invariance&amp;summary=Counterfactual%20Invariance&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fcounterfactual-invariance%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Counterfactual Invariance on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fcounterfactual-invariance%2f&title=Counterfactual%20Invariance">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Counterfactual Invariance on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fcounterfactual-invariance%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Counterfactual Invariance on whatsapp"
            href="https://api.whatsapp.com/send?text=Counterfactual%20Invariance%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fcounterfactual-invariance%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Counterfactual Invariance on telegram"
            href="https://telegram.me/share/url?text=Counterfactual%20Invariance&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fcounterfactual-invariance%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Counterfactual Invariance on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Counterfactual%20Invariance&u=https%3a%2f%2fflecart.github.io%2fnotes%2fcounterfactual-invariance%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
