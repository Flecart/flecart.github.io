<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Kernel Methods | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="➕probabilistic-artificial-intelligence, machinelearning">
<meta name="description" content="As we will briefly see, Kernels will have an important role in many machine learning applications. In this note we will get to know what are Kernels and why are they useful. Intuitively they measure the similarity between two input points. So if they are close the kernel should be big, else it should be small.
We briefly state the requirements of a Kernel, then we will argue with a simple example why they are useful.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/kernel-methods/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/kernel-methods/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/kernel-methods/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Kernel Methods">
  <meta property="og:description" content="As we will briefly see, Kernels will have an important role in many machine learning applications. In this note we will get to know what are Kernels and why are they useful. Intuitively they measure the similarity between two input points. So if they are close the kernel should be big, else it should be small.
We briefly state the requirements of a Kernel, then we will argue with a simple example why they are useful.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="➕Probabilistic-Artificial-Intelligence">
    <meta property="article:tag" content="Machinelearning">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Kernel Methods">
<meta name="twitter:description" content="As we will briefly see, Kernels will have an important role in many machine learning applications. In this note we will get to know what are Kernels and why are they useful. Intuitively they measure the similarity between two input points. So if they are close the kernel should be big, else it should be small.
We briefly state the requirements of a Kernel, then we will argue with a simple example why they are useful.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Kernel Methods",
      "item": "https://flecart.github.io/notes/kernel-methods/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Kernel Methods",
  "name": "Kernel Methods",
  "description": "As we will briefly see, Kernels will have an important role in many machine learning applications. In this note we will get to know what are Kernels and why are they useful. Intuitively they measure the similarity between two input points. So if they are close the kernel should be big, else it should be small.\nWe briefly state the requirements of a Kernel, then we will argue with a simple example why they are useful.\n",
  "keywords": [
    "➕probabilistic-artificial-intelligence", "machinelearning"
  ],
  "articleBody": "As we will briefly see, Kernels will have an important role in many machine learning applications. In this note we will get to know what are Kernels and why are they useful. Intuitively they measure the similarity between two input points. So if they are close the kernel should be big, else it should be small.\nWe briefly state the requirements of a Kernel, then we will argue with a simple example why they are useful.\nKernel Function requirements Every function $k$ must be\nSymmetric: $\\forall x, x' \\in \\mathbb{X}$ we have $k(x, x') = k(x', x)$ Positive definiteness For all $A$ we have $K_{A A}$ is positive definite. (it’s not clear the interpretation of the negative similarity) This function encodes information about the correlation. If a kernel satisfies those, it is called a Mercer kernel. Mercer has proven that these two conditions are necessary and sufficient for a function to be considered a kernel. If we have linear kernel, then we have bayesian linear regression. They are exactly the same thing. Another nice thing about kernels is that we can create new kernels by linearly combining them. Choosing kernels (or building them) is somewhat interpretable as choosing the correct prior for our functions.\nA motivating example Recall, we can use linear regression to learn non linear features, we just need some feature map.\nFeature maps -\u003e Curse of dimensionality It is possible to include in linear regression settings, also monomials of greater order, and then use the same old algorithms to derive their value. But in doing so, one quickly gets into dimensionality problems: if we have $d$ variables, then the order of the feature map to all polynomials of order $m$ will be $\\mathcal{O}(d^{m})$, which is often quite prohibitive. We need a simple method not to store everything.\nSimple linear regression $$ \\theta_{\\text{new}} = \\theta - \\alpha \\sum_{i = 1}^{N} (y^{(i)} - \\theta^{T}\\phi(x^{(i)}))\\phi( x^{(i)}) $$ Normally we would need to explicitly compute the product every single time, but we know that if the feature map is too large this would be infeasible.\nBut if we note that $\\theta$ could be written by a linear combination of the features, then this problem will solve itself! Let’s say there are some $\\beta_{1}, \\dots, \\beta_{n}$ such that $\\theta_{0} = \\sum_{i = 1}^{N} \\beta_{i}\\phi(x^{(i)})$. It can be shown by induction that the new $\\theta$ updated using gradient descent will always be a linear combination. With this in mind, we just need to store a number that grows with $N$, not with the number of features $n$. But then we can rewrite the update rule in the following way:\n$$ \\theta_{new} = \\sum_{i = 1}^{N} \\beta_{i}\\phi(x^{(i)}) - \\alpha \\sum_{i = 1}^{N} (y^{(i)} - ( \\sum_{j = 1}^{N} \\beta_{j}\\phi(x^{(j)}))\\phi(x^{(i)}))\\phi( x^{(i)}) = \\sum_{i = 1}^{N} \\left( \\beta_{i} - \\alpha (y^{(i)} - ( \\sum_{j = 1}^{N} \\beta_{j}\\phi(x^{(j)})^{T})\\phi(x^{(i)}) \\right) \\phi(x^{(i)}) $$$$ \\beta_{i} := \\beta_{i} - \\alpha (y^{(i)} - \\sum_{j = 1}^{N} \\beta_{j}\\phi(x^{(j)})^{T}\\phi(x^{(i)}) $$$$ \\langle \\phi(x), \\phi(z) \\rangle = 1 + \\langle x, z \\rangle + \\langle x, z \\rangle ^{2} $$$$ \\beta := \\beta - \\alpha (y - K\\beta) $$ which $K$ our $n \\times n$ matrix of the features such that $K_{ij} = K(\\phi(x^{(i)}), \\phi(x^{(j)}))$.\nSome takeaways From the above example, one can observe the kernel trick doing its job and bringing down the computational cost related to our optimization problem. This can be done in many other problems, and its quite a general idea. We need to highlight two main observations making this framework tractable:\nThe kernel trick allows to compute optimizations with feature functions without explicitly computing them. If the feature space had dimension $e$ which could be infinitely dimensional, exponential on the number of terms $d$ et cetera, the optimization still takes take $N$, the number of samples. It is efficient to compute a single entry of the kernel matrix, often the complexity is just $d$, instead of being $e$. (this is often because we just need to compute the product $x^{T}z$ and then apply this value, the best way to understand this is to work out some examples). Building new Kernels 🟨 After we have a valid kernel matrix, we can compose them in some ways, while retaining the properties that suffice to make them kernels. We report the methods listed in (Bishop 2006) Chapter 6: Logarithms would be nice, but we lose the positive definiteness because small eigenvalues are mapped to negative values.\nChoosing the best kernel parameters 🟩 $$ \\hat{f}_{i} = \\arg \\max_{f} p(y_{\\text{train}}\\mid f, \\theta_{i}, x_{\\text{train}}) p(f \\mid \\theta_{i}) $$ Then pick $\\hat{\\theta} =\\arg \\max_{\\theta_{i}} p(y_{\\text{vol} }\\mid \\hat{f}_{i}, \\theta_{i}, x_{\\text{val}})$. This method still collapses the uncertainty in f into a point estimate. But we want to keep the uncertainty.\n$$ \\hat{\\theta_{MLE}} = \\arg \\max_{\\theta} p(y_{\\text{train} }\\mid x_{\\text{train}}, \\theta) = \\int p(y_{\\text{train}} \\mid f, x_{\\text{train}}, \\theta)p(f \\mid \\theta) \\, df $$ The $p(f \\mid \\theta)$ is the prior, the other is the likelihood. The delighful observation on this MLE observation is that it naturally prevents underfitting or overfitting the model. This is often called Bayesian model selection.\nThis table attempts to give an intuitive understanding of what is explained here.\nLikelihood Prior $H$ is simple, underfitting Small for most $f$ Large for some $f$ (concentrated), small for most $f$. $H$ is complex, overfitting Large for some $f$, small for most. Small for most $f$, more distributed. Just right ok ok So in the overfitting case we are spreading the values too much, in the underfitting case, the likelihood is always small so the summation should be small too, this is why usually it is just right.\nIntuitively: We are attempting to characterize the space of the functions given certain parameters. We are asking: what is the priori probability of having that function if we are given some parameters? If we overfit, we say that is unlikely that the function is random, so the prior is small (there is a high number of functions to choose from). If we underfit, then it is the inverse.\nLittle notes on Kernel Engineering Kernels should be designed for some specific characteristics of the data. So if you know a dataset has a certain invariance, then you should use kernels that have that invariance. For example if you have periodic data, you should use periodic kernels. Else: that periodicity aspect would be hardly obtainable.\nSome famous Kernels Sometimes is important to recognize famous kernel families because then you can compose them together, and easily recognize if some other function is a kernel or not.\nLinear kernel 🟩 This is one of the simplest kernels, which is just $k(x, y) = x^{T}y$, with $x, y \\in\\mathbb{R}^{d}$. A simple example of linear kernel, is the view of Gaussian Processes as a generalization of Bayesian Linear Regression with linear kernel.\nRadial Basis Kernel $$ k(x_{i}, x_{j}; l, \\sigma) = \\sigma\\exp\\left( -\\frac{1}{2l} \\lVert x_{i} - x_{j} \\rVert_{2}^{2} \\right) $$ They are similar to the Gaussians probability distribution function, but we don’t need to normalize in this case. This has one of the most natural interpretations of kernels as a distance similarity functions.\nThis family is both stationary and isotropic kernels, because they are translation invariant and depend only on the second norm. The $l$ parameter is called length scale and determines how much the function can vary. Usually high value means the function is more sensitive to old data, which implies it varies less.\n$$ p(\\omega) = (2\\pi)^{-d/2} \\exp\\left( - \\frac{\\lVert \\omega \\rVert_{2}^{2}}{2} \\right) $$ Which is just the standard Gaussian distribution in d dimensions, this is why sometimes the radial basis kernel is called Gaussian Kernel.\nRBF as infinite polinomials (Hard version) $$ \\psi(x) = \\sum_{j_{1}, \\dots, j_{d} \\in \\mathbb{N}} \\beta_{j_{1}, \\dots j_{d}} x_{1}^{j_{1}} \\cdot \\dots \\cdot x^{j_{d}}_{d} = \\beta^{T}\\varphi(x) $$$$ \\varphi_{j_{1},\\dots j_{d}}(x) = \\exp\\left( -\\frac{1}{2} \\lVert x \\rVert_{2}^{2} \\right) \\frac{x_{1}^{j_{1}} \\cdot \\dots \\cdot x^{j_{d}}_{d}}{\\sqrt{ j_{1}! \\dots j_{d}! }} \\implies \\varphi(x)^{T}\\varphi(x') = \\exp\\left( - \\frac{\\lVert x - x'\\rVert_{2}^{2}}{2} \\right) $$$$ \\sum_{\\alpha \\in \\mathbb{N}^{d}} \\frac{x^{\\alpha}(x')^\\alpha}{\\alpha!} = \\exp(x^{T}x') $$$$ \\exp(x) = \\sum_{i = 1}^{\\infty} \\frac{x^{d}}{d!} $$ And it’s also a nice exercise to prove that.\nRBF as infinite polinomials 🟨++ $$ \\begin{align} \\phi(x) = \\begin{bmatrix} \\phi_{0}(x) \\\\ \\phi_{1}(x) \\\\ \\vdots \\end{bmatrix} \u0026\u0026 \\phi_{j}(x) = \\exp\\left( -\\frac{1}{2}x^{2} \\right) \\frac{x^{j}}{\\sqrt{j!}} \\\\ \\end{align} $$$$ \\begin{align} \\phi(x)^{T}\\phi(x') \u0026= \\sum_{j = 0}^{\\infty} \\exp\\left( -\\frac{1}{2}x^{2} \\right) \\frac{x^{j}}{\\sqrt{j!}} \\exp\\left( -\\frac{1}{2}x'^{2} \\right) \\frac{x'^{j}}{\\sqrt{j!}} \\\\ \u0026= \\sum_{j = 0}^{\\infty} \\exp\\left( -\\frac{1}{2}x^{2} -\\frac{1}{2}x'^{2} \\right) \\frac{x^{j}x'^{j}}{j!} \\\\ \u0026= \\exp\\left( -\\frac{1}{2}x^{2} -\\frac{1}{2}x'^{2} \\right) \\sum_{j = 0}^{\\infty} \\frac{(xx')^{j}}{j!} \\\\ \u0026= \\exp\\left( -\\frac{1}{2}x^{2} -\\frac{1}{2}x'^{2} \\right) \\exp(xx') \u0026\\text{Taylor expansion of} \\exp \\\\ \u0026= \\exp\\left( -\\frac{1}{2}x^{2} -\\frac{1}{2}x'^{2} + xx' \\right) \\\\ \u0026= \\exp\\left( -\\frac{1}{2}(x - x')^{2} \\right) \\\\ \\end{align} $$ Which ends the proof $\\square$.\nSpectral Density In this section we compute the spectral density for the RBF kernel, this is useful for the Fourier features in Gaussian Processes.\n$$ \\begin{align} p(\\omega) \u0026= \\int k(x, x') \\cdot \\exp(-i \\omega^{T}(x - x'))\\, d(x - x') \\\\ \u0026=\\int \\exp\\left( -\\frac{1}{2l}(x - x')^{2} \\right) \\exp(-i \\omega^{T}(x - x'))\\, d(x - x') \\\\ \u0026= \\int \\exp\\left( -\\frac{\\lVert x \\rVert_{2} ^{2}}{2l} -i \\omega^{T}x \\right)\\, dx \\\\ \u0026= (2l^{2}\\pi)^{d/2} \\exp\\left( -\\frac{l^{2}\\lVert \\omega \\rVert_{2}^{2}}{2} \\right) \\end{align} $$ In the last part we completed the square by adding and removing $(ilw^{T})^{2} / 2$\nExponential Kernel $$ k(x_{i}, x_{j} ; l) = \\exp\\left( - \\frac{1}{l} \\lvert x_{i} - x_{j} \\rvert \\right) $$ They are also called Ornstein-Uhlenbeck Kernels or Laplace Kernels.\nMatérn Kernels These kernels have a quite weird form:\n$$ k(x_{i}, x_{j}; v, \\ell) = \\frac{2^{1 - v}}{\\Gamma(v)} \\left( \\frac{\\sqrt{ 2v }}{\\ell} \\lVert x_{i} - x_{j} \\rVert_{2} \\right)^{v} K_{v} \\left( \\frac{\\sqrt{ 2v }}{\\ell} \\lVert x_{i} - x_{j} \\rVert_{2} \\right) $$ I currently do not know what are these used for. But it is $\\lceil v \\rceil - 1$ times differentiable.\nPeriodic Kernels $$ k(x, x') = \\sigma^{2} \\exp \\left( - \\frac{\\left( 2\\sin ^{2}\\left( \\frac{\\pi \\lVert x' - x \\rVert }{p} \\right) \\right)}{l^{2}} \\right) $$Reproducing Kernel Hilbert space Definition of RKHS 🟩 $$ \\mathcal{H}_{k}(\\mathcal{X}) = \\left\\{ f(x) = \\sum_{i = 1}^{n} \\alpha_{i}k(x, x_{i}) :n \\in \\mathbb{N}, x_{i} \\in \\mathcal{X} , \\alpha_{i} \\in \\mathbb{R}\\right\\} $$$$ \\langle f, g \\rangle_{k} = \\sum_{i = 1}^{n}\\sum_{j = 1}^{n '}\\alpha_{i}\\alpha'_{j} k(x_{i}, x'_{j}) $$$$ \\lVert f \\rVert _{k} = \\sqrt{ \\langle f, f \\rangle _{k} } $$The Reducing Property 🟩– $$ f(x) = \\langle f(\\cdot), k(x, \\cdot) \\rangle _{k} $$ Meaning a simple evaluation of the function can be understood as a inner product between itself and a kernel.\n$$ f(x) = \\sum_{i = 1}^{n} \\alpha_{1} k(x, x') = \\sum_{i = 1}^{n} \\alpha_{1} \\langle k(\\cdot, x), k(\\cdot, x') \\rangle_{k} = \\langle \\sum_{i = 1}^{n} \\alpha_{1}k(\\cdot, x), k(\\cdot, x') \\rangle_{k} = \\langle f(\\cdot), k(x, \\cdot) \\rangle _{k} $$ $\\square$\nNorm as a measure of smoothness 🟨 $$ \\lvert f(x) - f(y) \\rvert \\leq \\lVert f \\rVert _{k} \\lVert k(x, \\cdot) - k(y, \\cdot) \\rVert _{k} $$$$ \\lvert\\langle f, k(x, \\cdot) - k(y, \\cdot) \\rangle \\rvert^{2} \\leq \\langle f, f \\rangle \\cdot \\langle k(x, \\cdot) - k(y, \\cdot), k(x, \\cdot) - k(y, \\cdot) \\rangle $$$$ \\langle f, k(x, \\cdot) - k(y, \\cdot) \\rangle = \\sum_{i = 1}^{n} \\alpha_{i} (k(x, x_{i}) - k(y, x_{i})) = f(x) - f(y) $$ Then taking the square root has the desired output.\nThe Representer Theorem $$ \\hat{f} \\in \\arg \\min_{f} l(f, D) + \\lambda \\lVert f \\rVert ^{2}_{k} $$$$ \\hat{f}(x) = \\alpha^{T} k_{x, A} $$ Where $A$ are all points in the Dataset without the labels. Something similar to this, the Riesz representation theorem, will be used in Counterfactual Invariance for the MMD computation.\nReferences [1] Bishop “Pattern Recognition and Machine Learning” Springer 2006\n",
  "wordCount" : "1906",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/kernel-methods/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Kernel Methods
    </h1>
    <div class="post-meta">9 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul><ul>
                <li>
                    <a href="#kernel-function-requirements" aria-label="Kernel Function requirements">Kernel Function requirements</a></li></ul>
                    
                <li>
                    <a href="#a-motivating-example" aria-label="A motivating example">A motivating example</a><ul>
                        
                <li>
                    <a href="#feature-maps---curse-of-dimensionality" aria-label="Feature maps -&gt; Curse of dimensionality">Feature maps -&gt; Curse of dimensionality</a></li>
                <li>
                    <a href="#simple-linear-regression" aria-label="Simple linear regression">Simple linear regression</a></li>
                <li>
                    <a href="#some-takeaways" aria-label="Some takeaways">Some takeaways</a></li>
                <li>
                    <a href="#building-new-kernels-" aria-label="Building new Kernels 🟨">Building new Kernels 🟨</a></li>
                <li>
                    <a href="#choosing-the-best-kernel-parameters-" aria-label="Choosing the best kernel parameters 🟩">Choosing the best kernel parameters 🟩</a></li>
                <li>
                    <a href="#little-notes-on-kernel-engineering" aria-label="Little notes on Kernel Engineering">Little notes on Kernel Engineering</a></li></ul>
                </li>
                <li>
                    <a href="#some-famous-kernels" aria-label="Some famous Kernels">Some famous Kernels</a><ul>
                        
                <li>
                    <a href="#linear-kernel-" aria-label="Linear kernel 🟩">Linear kernel 🟩</a></li>
                <li>
                    <a href="#radial-basis-kernel" aria-label="Radial Basis Kernel">Radial Basis Kernel</a><ul>
                        
                <li>
                    <a href="#rbf-as-infinite-polinomials-hard-version" aria-label="RBF as infinite polinomials (Hard version)">RBF as infinite polinomials (Hard version)</a></li>
                <li>
                    <a href="#rbf-as-infinite-polinomials-" aria-label="RBF as infinite polinomials 🟨&#43;&#43;">RBF as infinite polinomials 🟨++</a></li>
                <li>
                    <a href="#spectral-density" aria-label="Spectral Density">Spectral Density</a></li></ul>
                </li>
                <li>
                    <a href="#exponential-kernel" aria-label="Exponential Kernel">Exponential Kernel</a></li>
                <li>
                    <a href="#mat%c3%a9rn-kernels" aria-label="Matérn Kernels">Matérn Kernels</a></li>
                <li>
                    <a href="#periodic-kernels" aria-label="Periodic Kernels">Periodic Kernels</a></li></ul>
                </li>
                <li>
                    <a href="#reproducing-kernel-hilbert-space" aria-label="Reproducing Kernel Hilbert space">Reproducing Kernel Hilbert space</a><ul>
                        
                <li>
                    <a href="#definition-of-rkhs-" aria-label="Definition of RKHS 🟩">Definition of RKHS 🟩</a></li>
                <li>
                    <a href="#the-reducing-property---" aria-label="The Reducing Property 🟩&ndash;">The Reducing Property 🟩&ndash;</a></li>
                <li>
                    <a href="#norm-as-a-measure-of-smoothness-" aria-label="Norm as a measure of smoothness 🟨">Norm as a measure of smoothness 🟨</a></li>
                <li>
                    <a href="#the-representer-theorem" aria-label="The Representer Theorem">The Representer Theorem</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>As we will briefly see, Kernels will have an important role in many machine learning applications. In this note we will get to know what are Kernels and why are they useful. Intuitively they measure the <strong>similarity</strong> between two input points. So if they are close the kernel should be big, else it should be small.</p>
<p>We briefly state the requirements of a Kernel, then we will argue with a simple example why they are useful.</p>
<h4 id="kernel-function-requirements">Kernel Function requirements<a hidden class="anchor" aria-hidden="true" href="#kernel-function-requirements">#</a></h4>
<p>Every function $k$ must be</p>
<ol>
<li><strong>Symmetric:</strong> $\forall x, x' \in \mathbb{X}$ we have $k(x, x') = k(x', x)$</li>
<li><strong>Positive definiteness</strong> For all $A$ we have $K_{A A}$ is positive definite. (it&rsquo;s not clear the interpretation of the negative similarity)
This function encodes information about the <strong>correlation</strong>. If a kernel satisfies those, it is called a <strong>Mercer</strong> kernel. Mercer has proven that these two conditions are necessary and sufficient for a function to be considered a kernel.
If we have linear kernel, then we have <a href="/notes/bayesian-linear-regression">bayesian linear regression</a>. They are exactly the same thing.</li>
</ol>
<p>Another nice thing about kernels is that we can create new kernels by linearly combining them. Choosing kernels (or building them) is somewhat interpretable as choosing the correct prior for our functions.</p>
<h3 id="a-motivating-example">A motivating example<a hidden class="anchor" aria-hidden="true" href="#a-motivating-example">#</a></h3>
<p>Recall, we can use linear regression to learn non linear features, we just need some feature map.</p>
<h4 id="feature-maps---curse-of-dimensionality">Feature maps -&gt; Curse of dimensionality<a hidden class="anchor" aria-hidden="true" href="#feature-maps---curse-of-dimensionality">#</a></h4>
<p>It is possible to include in linear regression settings, also monomials of greater order, and then use the same old algorithms to derive their value. But in doing so, one quickly gets into dimensionality problems: if we have $d$ variables, then the order of the feature map to all polynomials of order $m$ will be $\mathcal{O}(d^{m})$, which is often quite prohibitive. We need a simple method not to store everything.</p>
<h4 id="simple-linear-regression">Simple linear regression<a hidden class="anchor" aria-hidden="true" href="#simple-linear-regression">#</a></h4>
$$
\theta_{\text{new}} = \theta - \alpha \sum_{i = 1}^{N} (y^{(i)} - \theta^{T}\phi(x^{(i)}))\phi( x^{(i)}) 
$$<p>
Normally we would need to explicitly compute the product every single time, but we know that if the feature map is too large this would be infeasible.</p>
<p>But if we note that $\theta$ could be written by a linear combination of the features, then this problem will solve itself!
Let&rsquo;s say there are some $\beta_{1}, \dots, \beta_{n}$ such that $\theta_{0} = \sum_{i = 1}^{N} \beta_{i}\phi(x^{(i)})$.
It can be shown by induction that the new $\theta$ updated using gradient descent will always be a linear combination. With this in mind, we just need to store a number that grows with $N$, not with the number of features $n$.
But then we can rewrite the update rule in the following way:</p>
$$
\theta_{new} = \sum_{i = 1}^{N} \beta_{i}\phi(x^{(i)}) - \alpha \sum_{i = 1}^{N} (y^{(i)} - ( \sum_{j = 1}^{N} \beta_{j}\phi(x^{(j)}))\phi(x^{(i)}))\phi( x^{(i)}) 
=  \sum_{i = 1}^{N} \left( \beta_{i} - \alpha (y^{(i)} - ( \sum_{j = 1}^{N} \beta_{j}\phi(x^{(j)})^{T})\phi(x^{(i)}) \right) \phi(x^{(i)})
$$$$
\beta_{i} :=  \beta_{i} - \alpha (y^{(i)} - \sum_{j = 1}^{N} \beta_{j}\phi(x^{(j)})^{T}\phi(x^{(i)})
$$$$
\langle \phi(x), \phi(z) \rangle  = 1 + \langle x, z \rangle + \langle x, z \rangle ^{2}
$$$$
\beta := \beta - \alpha (y - K\beta)
$$<p>
which $K$ our $n \times n$ matrix of the features such that $K_{ij} = K(\phi(x^{(i)}), \phi(x^{(j)}))$.</p>
<h4 id="some-takeaways">Some takeaways<a hidden class="anchor" aria-hidden="true" href="#some-takeaways">#</a></h4>
<p>From the above example, one can observe the kernel trick doing its job and bringing down the computational cost related to our optimization problem. This can be done in many other problems, and its quite a general idea.
We need to highlight two main observations making this framework tractable:</p>
<ol>
<li>The kernel trick allows to compute optimizations with feature functions <strong>without explicitly computing them</strong>. If the feature space had dimension $e$ which could be infinitely dimensional, exponential on the number of terms $d$ et cetera, the optimization still takes take $N$, the number of samples.</li>
<li>It is efficient to compute a single entry of the kernel matrix, often the complexity is just $d$, instead of being $e$. (this is often because we just need to compute the product $x^{T}z$ and then apply this value, the best way to understand this is to work out some examples).</li>
</ol>
<h4 id="building-new-kernels-">Building new Kernels 🟨<a hidden class="anchor" aria-hidden="true" href="#building-new-kernels-">#</a></h4>
<p>After we have a valid kernel matrix, we can compose them in some ways, while retaining the properties that suffice to make them kernels.
We report the methods listed in (Bishop 2006) Chapter 6:
<img src="/images/notes/The Kernel Trick-20241010153911349.webp" style="width: 100%" class="center" alt="The Kernel Trick-20241010153911349"></p>
<p>Logarithms would be nice, but we lose the positive definiteness because small eigenvalues are mapped to negative values.</p>
<h4 id="choosing-the-best-kernel-parameters-">Choosing the best kernel parameters 🟩<a hidden class="anchor" aria-hidden="true" href="#choosing-the-best-kernel-parameters-">#</a></h4>
$$
\hat{f}_{i} = \arg \max_{f} p(y_{\text{train}}\mid f, \theta_{i}, x_{\text{train}}) p(f \mid \theta_{i})
$$<p>
Then pick $\hat{\theta} =\arg \max_{\theta_{i}} p(y_{\text{vol} }\mid \hat{f}_{i}, \theta_{i}, x_{\text{val}})$.
This method still collapses the uncertainty in f into a point estimate. But we want to keep the uncertainty.</p>
$$
\hat{\theta_{MLE}} = \arg \max_{\theta} p(y_{\text{train} }\mid  x_{\text{train}}, \theta)
= \int p(y_{\text{train}} \mid f, x_{\text{train}}, \theta)p(f \mid \theta) \, df
$$<p>
The $p(f \mid \theta)$ is the prior, the other is the likelihood. The delighful observation on this MLE observation is that it naturally prevents underfitting or overfitting the model. This is often called <strong>Bayesian model selection</strong>.</p>
<p>This table attempts to give an intuitive understanding of what is explained here.</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Likelihood</th>
          <th>Prior</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>$H$ is simple, <em>underfitting</em></strong></td>
          <td>Small for most $f$</td>
          <td>Large for some $f$ (concentrated), small for most $f$.</td>
      </tr>
      <tr>
          <td><strong>$H$ is complex, <em>overfitting</em></strong></td>
          <td>Large for some $f$, small for most.</td>
          <td>Small for most $f$, more distributed.</td>
      </tr>
      <tr>
          <td><strong>Just right</strong></td>
          <td>ok</td>
          <td>ok</td>
      </tr>
  </tbody>
</table>
<p>So in the overfitting case we are spreading the values too much, in the underfitting case, the likelihood is always small so the summation should be small too, this is why usually it is just right.</p>
<p><strong>Intuitively</strong>:
We are attempting to characterize the space of the functions given certain parameters. We are asking: what is the priori probability of having that function if we are given some parameters? If we overfit, we say that is unlikely that the function is random, so the prior is small (there is a high number of functions to choose from). If we underfit, then it is the inverse.</p>
<h4 id="little-notes-on-kernel-engineering">Little notes on Kernel Engineering<a hidden class="anchor" aria-hidden="true" href="#little-notes-on-kernel-engineering">#</a></h4>
<p>Kernels should be designed for some <em>specific characteristics</em> of the data. So if you know a dataset has a certain invariance, then you should use kernels that have that invariance.
For example if you have periodic data, you should use periodic kernels. Else: that periodicity aspect would be hardly obtainable.</p>
<h3 id="some-famous-kernels">Some famous Kernels<a hidden class="anchor" aria-hidden="true" href="#some-famous-kernels">#</a></h3>
<p>Sometimes is important to recognize famous kernel families because then you can compose them together, and easily recognize if some other function is a kernel or not.</p>
<h4 id="linear-kernel-">Linear kernel 🟩<a hidden class="anchor" aria-hidden="true" href="#linear-kernel-">#</a></h4>
<p>This is one of the simplest kernels, which is just $k(x, y) = x^{T}y$, with $x, y \in\mathbb{R}^{d}$.
A simple example of linear kernel, is the view of <a href="/notes/gaussian-processes">Gaussian Processes</a> as a generalization of <a href="/notes/bayesian-linear-regression">Bayesian Linear Regression</a> with linear kernel.</p>
<h4 id="radial-basis-kernel">Radial Basis Kernel<a hidden class="anchor" aria-hidden="true" href="#radial-basis-kernel">#</a></h4>
$$
k(x_{i}, x_{j}; l, \sigma) = \sigma\exp\left( -\frac{1}{2l}  \lVert x_{i} - x_{j}  \rVert_{2}^{2}  \right)
$$<p>
They are similar to the <a href="/notes/gaussians">Gaussians</a> probability distribution function, but we don&rsquo;t need to normalize in this case. This has one of the most natural interpretations of kernels as a distance similarity functions.</p>
<p>This family is both <em>stationary and isotropic</em> kernels, because they are translation invariant and depend only on the second norm.
The $l$ parameter is called <em>length scale</em> and determines how much the function can vary. Usually high value means the function is more sensitive to old data, which implies it varies less.</p>
$$
p(\omega) = (2\pi)^{-d/2} \exp\left( - \frac{\lVert \omega \rVert_{2}^{2}}{2} \right)
$$<p>
Which is just the standard Gaussian distribution in d dimensions, this is why sometimes the radial basis kernel is called Gaussian Kernel.</p>
<h5 id="rbf-as-infinite-polinomials-hard-version">RBF as infinite polinomials (Hard version)<a hidden class="anchor" aria-hidden="true" href="#rbf-as-infinite-polinomials-hard-version">#</a></h5>
$$
\psi(x) = \sum_{j_{1}, \dots, j_{d} \in \mathbb{N}} \beta_{j_{1}, \dots j_{d}} x_{1}^{j_{1}} \cdot \dots \cdot x^{j_{d}}_{d} = \beta^{T}\varphi(x)
$$$$
\varphi_{j_{1},\dots j_{d}}(x) = \exp\left( -\frac{1}{2} \lVert x \rVert_{2}^{2} \right) \frac{x_{1}^{j_{1}} \cdot \dots \cdot x^{j_{d}}_{d}}{\sqrt{ j_{1}! \dots j_{d}! }} \implies \varphi(x)^{T}\varphi(x') = \exp\left( -  \frac{\lVert x - x'\rVert_{2}^{2}}{2} \right)
$$$$
\sum_{\alpha \in \mathbb{N}^{d}} \frac{x^{\alpha}(x')^\alpha}{\alpha!} = \exp(x^{T}x')
$$$$
\exp(x) = \sum_{i = 1}^{\infty} \frac{x^{d}}{d!}
$$<p>
And it&rsquo;s also a nice exercise to prove that.</p>
<h5 id="rbf-as-infinite-polinomials-">RBF as infinite polinomials 🟨++<a hidden class="anchor" aria-hidden="true" href="#rbf-as-infinite-polinomials-">#</a></h5>
$$
\begin{align}
\phi(x) = \begin{bmatrix}
\phi_{0}(x) \\
\phi_{1}(x) \\
\vdots
\end{bmatrix} && \phi_{j}(x) = \exp\left( -\frac{1}{2}x^{2} \right) \frac{x^{j}}{\sqrt{j!}} \\
\end{align}
$$$$
\begin{align}
\phi(x)^{T}\phi(x') &= \sum_{j = 0}^{\infty} \exp\left( -\frac{1}{2}x^{2} \right) \frac{x^{j}}{\sqrt{j!}} \exp\left( -\frac{1}{2}x'^{2} \right) \frac{x'^{j}}{\sqrt{j!}}  \\
&= \sum_{j = 0}^{\infty} \exp\left( -\frac{1}{2}x^{2} -\frac{1}{2}x'^{2} \right) \frac{x^{j}x'^{j}}{j!}  \\
&= \exp\left( -\frac{1}{2}x^{2} -\frac{1}{2}x'^{2} \right) \sum_{j = 0}^{\infty} \frac{(xx')^{j}}{j!}   \\
&= \exp\left( -\frac{1}{2}x^{2} -\frac{1}{2}x'^{2} \right) \exp(xx') &\text{Taylor expansion of} \exp \\
&= \exp\left( -\frac{1}{2}x^{2} -\frac{1}{2}x'^{2} + xx' \right)   \\
&= \exp\left( -\frac{1}{2}(x - x')^{2} \right)   \\
\end{align}
$$<p>
Which ends the proof $\square$.</p>
<h5 id="spectral-density">Spectral Density<a hidden class="anchor" aria-hidden="true" href="#spectral-density">#</a></h5>
<p>In this section we compute the spectral density for the RBF kernel, this is useful for the Fourier features in <a href="/notes/gaussian-processes">Gaussian Processes</a>.</p>
$$
\begin{align}
 p(\omega) &= \int k(x, x') \cdot \exp(-i \omega^{T}(x - x'))\, d(x - x')  \\
&=\int \exp\left( -\frac{1}{2l}(x - x')^{2} \right)  \exp(-i \omega^{T}(x - x'))\, d(x - x') \\
&= \int \exp\left( -\frac{\lVert x \rVert_{2} ^{2}}{2l} -i \omega^{T}x \right)\, dx  \\
&= (2l^{2}\pi)^{d/2} \exp\left( -\frac{l^{2}\lVert \omega \rVert_{2}^{2}}{2} \right)
\end{align}
$$<p>
In the last part we completed the square by adding and removing $(ilw^{T})^{2} / 2$</p>
<h4 id="exponential-kernel">Exponential Kernel<a hidden class="anchor" aria-hidden="true" href="#exponential-kernel">#</a></h4>
$$
k(x_{i}, x_{j} ; l) = \exp\left( - \frac{1}{l} \lvert  x_{i} - x_{j} \rvert  \right)
$$<p>
They are also called <em>Ornstein-Uhlenbeck Kernels</em> or <em>Laplace Kernels</em>.</p>
<h4 id="matérn-kernels">Matérn Kernels<a hidden class="anchor" aria-hidden="true" href="#matérn-kernels">#</a></h4>
<p>These kernels have a quite weird form:</p>
$$
k(x_{i}, x_{j}; v, \ell) = \frac{2^{1 - v}}{\Gamma(v)} \left( \frac{\sqrt{ 2v }}{\ell} \lVert x_{i} - x_{j} \rVert_{2} \right)^{v} K_{v} \left( \frac{\sqrt{ 2v }}{\ell} \lVert x_{i} - x_{j} \rVert_{2} \right)
$$<p>
I currently do not know what are these used for. But it is $\lceil v \rceil - 1$ times differentiable.</p>
<h4 id="periodic-kernels">Periodic Kernels<a hidden class="anchor" aria-hidden="true" href="#periodic-kernels">#</a></h4>
$$
k(x, x') = \sigma^{2} \exp \left( - \frac{\left( 2\sin ^{2}\left(  \frac{\pi \lVert x' - x \rVert }{p} \right) \right)}{l^{2}} \right) 
$$<h3 id="reproducing-kernel-hilbert-space">Reproducing Kernel Hilbert space<a hidden class="anchor" aria-hidden="true" href="#reproducing-kernel-hilbert-space">#</a></h3>
<h4 id="definition-of-rkhs-">Definition of RKHS 🟩<a hidden class="anchor" aria-hidden="true" href="#definition-of-rkhs-">#</a></h4>
$$
\mathcal{H}_{k}(\mathcal{X}) = \left\{ f(x) = \sum_{i = 1}^{n} \alpha_{i}k(x, x_{i}) :n \in \mathbb{N}, x_{i} \in \mathcal{X} , \alpha_{i} \in \mathbb{R}\right\} 
$$$$
\langle f, g \rangle_{k} = \sum_{i = 1}^{n}\sum_{j = 1}^{n '}\alpha_{i}\alpha'_{j} k(x_{i}, x'_{j})
$$$$
\lVert f \rVert _{k} = \sqrt{ \langle f, f \rangle _{k} }
$$<h4 id="the-reducing-property---">The Reducing Property 🟩&ndash;<a hidden class="anchor" aria-hidden="true" href="#the-reducing-property---">#</a></h4>
$$
f(x) = \langle f(\cdot), k(x, \cdot) \rangle _{k}
$$<p>
Meaning a simple evaluation of the function can be understood as a inner product between itself and a kernel.</p>
$$
f(x) = \sum_{i = 1}^{n} \alpha_{1} k(x, x') = \sum_{i = 1}^{n} \alpha_{1} \langle k(\cdot, x), k(\cdot, x') \rangle_{k} =  \langle \sum_{i = 1}^{n} \alpha_{1}k(\cdot, x), k(\cdot, x') \rangle_{k} = \langle f(\cdot), k(x, \cdot) \rangle _{k}
$$<p>
$\square$</p>
<h4 id="norm-as-a-measure-of-smoothness-">Norm as a measure of smoothness 🟨<a hidden class="anchor" aria-hidden="true" href="#norm-as-a-measure-of-smoothness-">#</a></h4>
$$
\lvert f(x) - f(y) \rvert \leq \lVert f \rVert _{k} \lVert k(x, \cdot) - k(y, \cdot) \rVert _{k}
$$$$
\lvert\langle f, k(x, \cdot) - k(y, \cdot) \rangle   \rvert^{2} \leq \langle f, f \rangle \cdot \langle k(x, \cdot) - k(y, \cdot), k(x, \cdot) - k(y, \cdot) \rangle 
$$$$
\langle f, k(x, \cdot) - k(y, \cdot) \rangle = \sum_{i = 1}^{n} \alpha_{i} (k(x, x_{i}) - k(y, x_{i})) = f(x) - f(y)
$$<p>
Then taking the square root has the desired output.</p>
<h4 id="the-representer-theorem">The Representer Theorem<a hidden class="anchor" aria-hidden="true" href="#the-representer-theorem">#</a></h4>
$$
\hat{f} \in \arg \min_{f} l(f, D) + \lambda \lVert f \rVert ^{2}_{k}
$$$$
\hat{f}(x) = \alpha^{T} k_{x, A}
$$<p>
Where $A$ are all points in the Dataset without the labels.
Something similar to this, the Riesz representation theorem, will be used in <a href="/notes/counterfactual-invariance">Counterfactual Invariance</a> for the MMD computation.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Bishop “Pattern Recognition and Machine Learning” Springer 2006</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">➕Probabilistic-Artificial-Intelligence</a></li>
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kernel Methods on x"
            href="https://x.com/intent/tweet/?text=Kernel%20Methods&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fkernel-methods%2f&amp;hashtags=%e2%9e%95probabilistic-artificial-intelligence%2cmachinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kernel Methods on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fkernel-methods%2f&amp;title=Kernel%20Methods&amp;summary=Kernel%20Methods&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fkernel-methods%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kernel Methods on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fkernel-methods%2f&title=Kernel%20Methods">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kernel Methods on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fkernel-methods%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kernel Methods on whatsapp"
            href="https://api.whatsapp.com/send?text=Kernel%20Methods%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fkernel-methods%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kernel Methods on telegram"
            href="https://telegram.me/share/url?text=Kernel%20Methods&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fkernel-methods%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kernel Methods on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Kernel%20Methods&u=https%3a%2f%2fflecart.github.io%2fnotes%2fkernel-methods%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
