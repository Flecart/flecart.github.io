<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RL Function Approximation | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="➕probabilistic-artificial-intelligence">
<meta name="description" content="These algorithms are good for scaling state spaces, but not actions spaces.
The Gradient Idea Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in Tabular Reinforcement Learning.
A simple parametrization The idea here is to parametrize the value estimation function so that similar inputs gets similar values akin to Parametric Modeling estimation we have done in the other courses. In this manner, we don&rsquo;t need to explicitly explore every single state in the state space.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/rl-function-approximation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/rl-function-approximation/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="RL Function Approximation" />
<meta property="og:description" content="These algorithms are good for scaling state spaces, but not actions spaces.
The Gradient Idea Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in Tabular Reinforcement Learning.
A simple parametrization The idea here is to parametrize the value estimation function so that similar inputs gets similar values akin to Parametric Modeling estimation we have done in the other courses. In this manner, we don&rsquo;t need to explicitly explore every single state in the state space." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/rl-function-approximation/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="RL Function Approximation"/>
<meta name="twitter:description" content="These algorithms are good for scaling state spaces, but not actions spaces.
The Gradient Idea Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in Tabular Reinforcement Learning.
A simple parametrization The idea here is to parametrize the value estimation function so that similar inputs gets similar values akin to Parametric Modeling estimation we have done in the other courses. In this manner, we don&rsquo;t need to explicitly explore every single state in the state space."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RL Function Approximation",
      "item": "https://flecart.github.io/notes/rl-function-approximation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RL Function Approximation",
  "name": "RL Function Approximation",
  "description": "These algorithms are good for scaling state spaces, but not actions spaces.\nThe Gradient Idea Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in Tabular Reinforcement Learning.\nA simple parametrization The idea here is to parametrize the value estimation function so that similar inputs gets similar values akin to Parametric Modeling estimation we have done in the other courses. In this manner, we don\u0026rsquo;t need to explicitly explore every single state in the state space.",
  "keywords": [
    "➕probabilistic-artificial-intelligence"
  ],
  "articleBody": "These algorithms are good for scaling state spaces, but not actions spaces.\nThe Gradient Idea Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in Tabular Reinforcement Learning.\nA simple parametrization The idea here is to parametrize the value estimation function so that similar inputs gets similar values akin to Parametric Modeling estimation we have done in the other courses. In this manner, we don’t need to explicitly explore every single state in the state space.\nFor example, a single linear parametrization for the value function gives a quite nice interpretation of why we are introducing loss functions in this case:\nLet’s say $V^{\\pi}(x; \\theta) = \\theta_{x}$ and $\\theta$ is a vector in the size of the state space. Then the bellman update rule, which is $$ V^{\\pi}(x; \\theta_{\\text{new}}) = r(x, \\pi(x)) + \\gamma \\sum_{x'}p(x'\\mid x, \\pi(x))V^{\\pi}(x^{'}; \\theta_{\\text{old}}) $$ Can bee seen as a minimization problem for the following loss: $$ \\forall x, \\theta_{new, x} = \\arg\\min_{\\theta_{x} \\in \\mathbb{R}} (\\theta_{x} - (r(x, \\pi(x)) + \\gamma \\sum_{x'}p(x'\\mid x, \\pi(x))V^{\\pi}(x^{'}; \\theta_{\\text{old}}))))^{2} $$ Which can be written for every single state: $$ \\theta_{new} = \\arg\\min_{\\theta \\in \\mathbb{R}^{n}} \\mathbb{E}_{x}(V^{\\pi}(x; \\theta)\n(r(x, \\pi(x)) + \\gamma \\sum_{x’}p(x’\\mid x, \\pi(x))V^{\\pi}(x^{’}; \\theta_{\\text{old}})))^{2} $$ Where $x$ is drawn from some distribution that has non zero mass for every state (so that it updates every state indefinitely). This simple motivation example opens the door for gradient descent methods for parameter estimation! The only drawback that we will see using these methods is the enormous number of samples that we need to get a good estimate of the value function.\nTD-Gradient View Recall the update for the temporal difference was a Bootstrapped montecarlo estimate, which gives a biased result, but nonetheless should converge to the correct result (I think, this should be validated): $$ V^{\\pi}(x) = V_{\\text{old}}^{\\pi}(x) + \\alpha_{t}(r + \\gamma V^{\\pi}_{\\text{old}}(x^{(i)}) - V_{\\text{old}}^{\\pi}(x)) $$ We can observe that the part that multiplicates $\\alpha_{t}$ can bee seen as the gradient for the following loss $$ l(\\theta; x, x') = -\\frac{1}{2}(r + \\gamma V^{\\pi}_{\\text{old}}(x^{'};\\theta) - V_{\\text{old}}^{\\pi}(x;\\theta))^{2} $$ Where the value is parameterized by theta. It’s derivative, is called the TD error, indicated with $\\delta(x)$. TODO: fix the mistake for the expectation of the estimate, also expand on the fact that the gradient is 1 for the linear case, so classic TD and Q-Learning are just doing gradient descent on the linear feature vector.\nQ-Learning View We can have exactly the same loss for the Q-learning update rule. $$ Q^{\\pi}(x, a) = Q_{\\text{old}}^{\\pi}(x, a) + \\alpha_{t}(r + \\gamma \\max_{a} Q^{\\pi}_{\\text{old}}(x^{'}, a) - Q_{\\text{old}}^{\\pi}(x, a)) $$ And getting the loss, given the trajectory $(x, a, r, x')$ $$ l(\\theta; x, a, r, x') = -\\frac{1}{2}(r + \\gamma \\max_{a} Q^{\\pi}_{\\text{old}}(x^{'}, a;\\theta) - Q_{\\text{old}}^{\\pi}(x, a;\\theta))^{2} $$ Deep Q-networks DQN updates the neural network used for the approximate bootstrapping estimate infrequently to maintain a constant optimization target across multiple episodes.\nThe problem we are trying to solve is the moving optimization target property of the above q-learning optimization, which leads to instabilities (it’s somewhat similar to changing the array you are iterating in, which gives impredictable results, sometimes, or more difficult to analyze or debug).\nThe idea here is use one network for the $old$ values, and the other used for the optimization objective. This idea is quite simple. This technique is known in the literature as Polyak averaging, or experience replay. here $\\theta_{old}$ is not update every step, but only after $D$ iterations, which attempts to give some stability to the optimization.\n$$ \\max_{a} q(x, a) \\approx \\max_{a} Q(x, a; \\theta_{\\text{old}}) $$ The problem with this technique is introducing the q-value estimation, this leads to a maximization bias: This leads to Double Q-Learning which leads to more accurate estimation of the real $q$ (Van Hasselt et al., 2016) Which is just taking the maximum with respect of the new network, and not the old. Also during gradient estimation, we are always selecting with the new network, not the old, we just use the old network to get the reward. (Meaning the $a$ inside the $Q(x, a; \\theta_{\\text{old}})$) is now taken from the new policy induced by the network parameterized with $\\theta$ that changes often. It works quite well, but the reason why it works is not well explained.\nPolicy Approximation Until now, we have always assumed to have discrete action spaces, but in other domains, we could be interested in continuous action spaces, which leads to a parameterized form of actions. So we write: $$ \\pi^{*}(x) = \\pi(x; \\varphi) $$ The methods that attempt to estimate this are called policy search or policy gradient methods.\nPolicy Parameterization examples For continuous spaces we might want to say that $$ \\pi(a \\mid x, \\theta) \\sim \\mathcal{N}(a ; \\mu(x, \\theta), \\Sigma(x, \\theta)) $$ Where the $\\mu$ and $\\Sigma$ are parameterized by a Neural Network. For discrete parameterizations we might choose a classical linear network based on some features: $$ \\pi(a \\mid x, \\theta) = Cat(a; \\sigma(f(x, \\theta))) $$ We can also decompose this using the Markov property if $m$ is large.\nThe important thing we need is:\nBe able to use Backpropagation Easy to sample from, so that we can use Monte Carlo Methods. Function Gradient Methods Assume we have a policy, then we can have a rollouts for a given specific policy, which we can consider to be a sample from $\\Pi_{\\theta}(\\tau)$. Then we define the function that we would like to maximize, which is $$ J_{T}(\\theta) = \\mathbb{E}_{\\tau \\sim \\Pi_{\\theta}}[G_{0}] \\approx \\frac{1}{N}\\sum_{i=1}^{N}G_{0}^{(i)} $$ Taking trajectories following the current policy.\nScore Trick We have encountered the score function before in Parametric Modeling when estimating the Rao-Cramer Bound. We will use it here to get an unbiased estimate of the gradient of the policy, so that we can use Backpropagation to estimate the gradient of the policy.\nThe main theorem here is that the gradient we want to estimate can be rewritten as: $$ \\nabla_{\\varphi}\\mathbb{E}_{\\tau \\sim \\prod_{\\varphi}}[G_{0}] = \\mathbb{E}_{\\tau \\sim \\prod_{\\varphi}}\\left[ G_{0}\\nabla_{\\varphi}\\log \\Pi_{\\varphi}(x) \\right] $$ Then we can use this estimate for the update of the gradient. Then you can also prove that in the context of the optimization of $\\varphi$ we can write the score as $$ \\nabla_{\\varphi}\\log \\Pi_{\\varphi}(x) = \\sum_{t = 0}^{T - 1}\\nabla _{\\varphi}\\log \\pi(a_{t}\\mid x_{t}) $$ The latter is often called the eligibility vector. With also this trick in place, then we can take the gradient without any problems (using Monte Carlo estimation) we will then have the following: $$ \\nabla_{\\varphi}J_{T}(\\theta) \\approx \\frac{1}{m} \\sum_{i = 1}^{m}g^{(i)}_{0:T} \\sum_{t = 0}^{T - 1}\\nabla _{\\varphi}\\log \\pi(a_{t}\\mid x_{t}) $$ But this estimate has usually high variance.\nAdding Baselines Adding a baseline is simply choosing a constant $b$ and subtracting it to the reward estimates: $$ \\mathbb{E}_{\\tau \\sim \\prod_{\\varphi}}\\left[ (G_{0} - b)\\nabla_{\\varphi}\\log \\Pi_{\\varphi}(x) \\right] $$ Adding this constant still keeps the expectation unvaried, as the derivative of the score is 0, so it doesn’t affect the estimate, but it affects the variance by reducing it if we choose $b$ correctly, which helps in the stability of the estimate. The condition we need to satisfy is $b^{2} \\leq 2 b \\cdot r(x, a)$ for each state and action. (The proof is along variance and covariance of the two functions).\nFor instance, if we choose the baseline to be $$ b_{0:t - 1} = \\sum_{i = 0}^{t - 1}\\gamma^{i}r_{i} $$ Then, the adjusted gradient of the loss comes to be: $$ \\nabla_{\\varphi}J(\\theta) \\approx \\mathbb{E}_{\\tau \\sim \\Pi} \\left[ \\sum_{t = 0}^{T - 1} \\gamma^{t}G_{t:T} \\nabla_{\\varphi} \\log \\pi_{\\varphi}(a_{t }\\mid x _{t}) \\right] $$ Which should be better in terms of its variance.\nREINFORCE The Reinforce algorithm allows optimization for the policy in continuous spaces but does not guarantee the convergence to the best policy possible.\nOne can also add the baseline as before, and it should reduce the variance somehow.\nDrawbacks These methods are On-policy because we need to simulate many many roll-outs High variance in the gradient estimates The last point implied slow convergence of the method (related to the sample efficiency) Not guaranteed convergence Another drawback is the difficulty of handling the exploration-exploitation tradeoff. These algorithms, along with the AC methods in the next section, are fundamentally exploitative. We usually employ random exploration or epsilon-greedy methods to explore, but usually these still might converge without actually having explored the state space enough.\nPolicy Gradient Method Often, the above policy gradient we derived for the REINFORCE algorithm is written in terms of discounted rate occupancy measure. TODO: write the equality.\nActor Critic Methods Actor critic methods describe a different family of approaches that explicitly attempt to jointly improve an approximator network for the value, called critic, and a network for the policy, called actor. These methods are, as usual, divided into online and offline actor critic methods. We will analyze and present a few different methods and their properties.\nOnline Actor Critic The main idea here is to use an approximated $Q-$value to get an approximate policy update. The main drawback is that we are doing an approximation of an approximation, so the variance of this method should be quite high. From other point of view, this is somewhat similar to the SARSA update explained in Tabular Reinforcement Learning Usually the updates here are bootstrapped too!\nThe notion of Advantage This will be the base for the so called A2C algorithm, where we use the notion of Advantage to bound the variance, in a manner similar to what we have done with the baselines.\nAdvantage is defined as: $$ A^{\\pi}(x, a) = Q^{\\pi}(x, a) - V^{\\pi}(x) $$ Which describes the gain in taking a specific action in a specific state compared to the average value of the state. We can prove that $$ \\forall \\pi, x \\max_{a} A^{\\pi}(x, a) \\geq 0 $$ This is easy to see for deterministic policies, if we choose an $a$ following the policy, then it is 0, but if it could be improved then it has positive value.\nUsing the notion of advantage instead of the classical policy network, we get the update rule to be: $$ \\varphi = \\varphi + \\alpha_{\\varphi}\\nabla_{\\varphi}\\log \\pi(a_{t}\\mid x_{t})A^{\\pi}(x_{t}, a_{t}) $$ Which is similar to the above actor-critic model, but with lesser variance if we choose the baseline equivalent correctly.\nAdvantage Actor Critic This method is very similar to the original Actor Critic method: instead of using the $Q$ estimate to update the critic we use the advantage\n",
  "wordCount" : "1719",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/rl-function-approximation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      RL Function Approximation
    </h1>
    <div class="post-meta">9 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#the-gradient-idea" aria-label="The Gradient Idea">The Gradient Idea</a><ul>
                        
                <li>
                    <a href="#a-simple-parametrization" aria-label="A simple parametrization">A simple parametrization</a></li>
                <li>
                    <a href="#td-gradient-view" aria-label="TD-Gradient View">TD-Gradient View</a></li>
                <li>
                    <a href="#q-learning-view" aria-label="Q-Learning View">Q-Learning View</a></li>
                <li>
                    <a href="#deep-q-networks" aria-label="Deep Q-networks">Deep Q-networks</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#policy-approximation" aria-label="Policy Approximation">Policy Approximation</a><ul>
                        <ul>
                        
                <li>
                    <a href="#policy-parameterization-examples" aria-label="Policy Parameterization examples">Policy Parameterization examples</a></li></ul>
                    
                <li>
                    <a href="#function-gradient-methods" aria-label="Function Gradient Methods">Function Gradient Methods</a><ul>
                        
                <li>
                    <a href="#score-trick" aria-label="Score Trick">Score Trick</a></li>
                <li>
                    <a href="#adding-baselines" aria-label="Adding Baselines">Adding Baselines</a></li>
                <li>
                    <a href="#reinforce" aria-label="REINFORCE">REINFORCE</a></li>
                <li>
                    <a href="#drawbacks" aria-label="Drawbacks">Drawbacks</a></li>
                <li>
                    <a href="#policy-gradient-method" aria-label="Policy Gradient Method">Policy Gradient Method</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#actor-critic-methods" aria-label="Actor Critic Methods">Actor Critic Methods</a><ul>
                        
                <li>
                    <a href="#online-actor-critic" aria-label="Online Actor Critic">Online Actor Critic</a><ul>
                        
                <li>
                    <a href="#the-notion-of-advantage" aria-label="The notion of Advantage">The notion of Advantage</a></li>
                <li>
                    <a href="#advantage-actor-critic" aria-label="Advantage Actor Critic">Advantage Actor Critic</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>These algorithms are good for scaling state spaces, but not actions spaces.</p>
<h3 id="the-gradient-idea">The Gradient Idea<a hidden class="anchor" aria-hidden="true" href="#the-gradient-idea">#</a></h3>
<p>Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in <a href="/notes/tabular-reinforcement-learning/">Tabular Reinforcement Learning</a>.</p>
<h4 id="a-simple-parametrization">A simple parametrization<a hidden class="anchor" aria-hidden="true" href="#a-simple-parametrization">#</a></h4>
<p>The idea here is to parametrize the value estimation function so that <em>similar inputs</em> gets <em>similar values</em> akin to <a href="/notes/parametric-modeling/">Parametric Modeling</a> estimation we have done in the other courses. In this manner, we don&rsquo;t need to explicitly explore every single state in the state space.</p>
<p>For example, a single linear parametrization for the value function gives a quite nice interpretation of why we are introducing loss functions in this case:</p>
<p>Let&rsquo;s say $V^{\pi}(x; \theta) = \theta_{x}$ and $\theta$ is a vector in the size of the state space.
Then the bellman update rule, which is
</p>
$$
V^{\pi}(x; \theta_{\text{new}}) = r(x, \pi(x)) + \gamma \sum_{x'}p(x'\mid x, \pi(x))V^{\pi}(x^{'}; \theta_{\text{old}})
$$
<p>
Can bee seen as a minimization problem for the following loss:
</p>
$$
\forall x, \theta_{new, x} = \arg\min_{\theta_{x} \in \mathbb{R}} (\theta_{x} - (r(x, \pi(x)) + \gamma  \sum_{x'}p(x'\mid x, \pi(x))V^{\pi}(x^{'}; \theta_{\text{old}}))))^{2}
$$
<p>
Which can be written for every single state:
$$
\theta_{new} = \arg\min_{\theta \in \mathbb{R}^{n}} \mathbb{E}_{x}(V^{\pi}(x; \theta)</p>
<ul>
<li>(r(x, \pi(x)) + \gamma  \sum_{x&rsquo;}p(x&rsquo;\mid x, \pi(x))V^{\pi}(x^{&rsquo;}; \theta_{\text{old}})))^{2}
$$
Where $x$ is drawn from some distribution that has non zero mass for every state (so that it updates every state indefinitely).</li>
</ul>
<p>This simple motivation example opens the door for gradient descent methods for parameter estimation!
The only drawback that we will see using these methods is the enormous number of samples that we need to get a good estimate of the value function.</p>
<h4 id="td-gradient-view">TD-Gradient View<a hidden class="anchor" aria-hidden="true" href="#td-gradient-view">#</a></h4>
<p>Recall the update for the temporal difference was a Bootstrapped montecarlo estimate, which gives a biased result, but nonetheless should converge to the correct result (I think, this should be validated):
</p>
$$
V^{\pi}(x) =  V_{\text{old}}^{\pi}(x) + \alpha_{t}(r + \gamma V^{\pi}_{\text{old}}(x^{(i)}) -  V_{\text{old}}^{\pi}(x))
$$
<p>
We can observe that the part that multiplicates $\alpha_{t}$ can bee seen as the gradient for the following loss
</p>
$$
l(\theta; x, x') = -\frac{1}{2}(r + \gamma V^{\pi}_{\text{old}}(x^{'};\theta) -  V_{\text{old}}^{\pi}(x;\theta))^{2}
$$
<p>
Where the value is parameterized by theta.
It&rsquo;s derivative, is called the TD error, indicated with $\delta(x)$.
TODO: fix the mistake for the expectation of the estimate, also expand on the fact that the gradient is 1 for the linear case, so classic TD and Q-Learning are just doing gradient descent on the linear feature vector.</p>
<h4 id="q-learning-view">Q-Learning View<a hidden class="anchor" aria-hidden="true" href="#q-learning-view">#</a></h4>
<p>We can have exactly the same loss for the Q-learning update rule.
</p>
$$
Q^{\pi}(x, a) =  Q_{\text{old}}^{\pi}(x, a) + \alpha_{t}(r + \gamma \max_{a} Q^{\pi}_{\text{old}}(x^{'}, a) -  Q_{\text{old}}^{\pi}(x, a))
$$
<p>
And getting the loss, given the trajectory $(x, a, r, x')$
</p>
$$
l(\theta; x, a, r, x') = -\frac{1}{2}(r + \gamma \max_{a} Q^{\pi}_{\text{old}}(x^{'}, a;\theta) -  Q_{\text{old}}^{\pi}(x, a;\theta))^{2}
$$
<h4 id="deep-q-networks">Deep Q-networks<a hidden class="anchor" aria-hidden="true" href="#deep-q-networks">#</a></h4>
<blockquote>
<p>DQN updates the neural network used for the approximate bootstrapping estimate infrequently to maintain a constant optimization target across multiple episodes.</p>
</blockquote>
<p>The problem we are trying to solve is the moving optimization target property of the above q-learning optimization, which leads to instabilities (it&rsquo;s somewhat similar to changing the array you are iterating in, which gives impredictable results, sometimes, or more difficult to analyze or debug).</p>
<p>The idea here is use one network for the $old$ values, and the other used for the optimization objective. This idea is quite simple.
This technique is known in the literature as <em>Polyak averaging</em>, or <em>experience replay</em>.
here $\theta_{old}$ is not update every step, but only after $D$ iterations, which attempts to give some stability to the optimization.</p>
$$
\max_{a} q(x, a) \approx \max_{a} Q(x, a; \theta_{\text{old}})
$$
<p>
The problem with this technique is introducing the q-value estimation, this leads to a <strong>maximization bias</strong>:
<img loading="lazy" src="/notes/rl-function-approximation-20241129105307115.webp" alt="image from the book"  />
</p>
<p>This leads to Double Q-Learning which leads to more accurate estimation of the real $q$ (Van Hasselt et al., 2016)
Which is just taking the maximum with respect of the new network, and not the old.
Also during gradient estimation, we are always selecting with the new network, not the old, we just use the old network to get the reward. (Meaning the $a$ inside the $Q(x, a; \theta_{\text{old}})$) is now taken from the new policy induced by the network parameterized with $\theta$ that changes often.
It works quite well, but the reason why it works is not well explained.</p>
<h2 id="policy-approximation">Policy Approximation<a hidden class="anchor" aria-hidden="true" href="#policy-approximation">#</a></h2>
<p>Until now, we have always assumed to have discrete action spaces, but in other domains, we could be interested in continuous action spaces, which leads to a parameterized form of actions.
So we write:
</p>
$$
\pi^{*}(x) = \pi(x; \varphi)
$$
<p>
The methods that attempt to estimate this are called <em>policy search</em> or <em>policy gradient</em> methods.</p>
<h4 id="policy-parameterization-examples">Policy Parameterization examples<a hidden class="anchor" aria-hidden="true" href="#policy-parameterization-examples">#</a></h4>
<p>For continuous spaces we might want to say that
</p>
$$
\pi(a \mid x, \theta) \sim \mathcal{N}(a ; \mu(x, \theta), \Sigma(x, \theta))
$$
<p>
Where the $\mu$ and $\Sigma$ are parameterized by a Neural Network.
For discrete parameterizations we might choose a classical linear network based on some features:
</p>
$$
\pi(a \mid x, \theta) = Cat(a; \sigma(f(x, \theta)))
$$
<p>
We can also decompose this using the Markov property if $m$ is large.</p>
<p>The important thing we need is:</p>
<ol>
<li>Be able to use <a href="/notes/backpropagation/">Backpropagation</a></li>
<li>Easy to sample from, so that we can use <a href="/notes/monte-carlo-methods/">Monte Carlo Methods</a>.</li>
</ol>
<h3 id="function-gradient-methods">Function Gradient Methods<a hidden class="anchor" aria-hidden="true" href="#function-gradient-methods">#</a></h3>
<p>Assume we have a policy, then we can have a rollouts for a given specific policy, which we can consider to be a sample from $\Pi_{\theta}(\tau)$.
Then we define the function that we would like to maximize, which is
</p>
$$
J_{T}(\theta) = \mathbb{E}_{\tau \sim \Pi_{\theta}}[G_{0}] \approx \frac{1}{N}\sum_{i=1}^{N}G_{0}^{(i)}
$$
<p>
Taking trajectories following the current policy.</p>
<h4 id="score-trick">Score Trick<a hidden class="anchor" aria-hidden="true" href="#score-trick">#</a></h4>
<p>We have encountered the score function before in <a href="/notes/parametric-modeling/">Parametric Modeling</a> when estimating the Rao-Cramer Bound.
We will use it here to get an unbiased estimate of the gradient of the policy, so that we can use <a href="/notes/backpropagation/">Backpropagation</a> to estimate the gradient of the policy.</p>
<p>The main theorem here is that the gradient we want to estimate can be rewritten as:
</p>
$$
\nabla_{\varphi}\mathbb{E}_{\tau \sim \prod_{\varphi}}[G_{0}] 
= \mathbb{E}_{\tau \sim \prod_{\varphi}}\left[ G_{0}\nabla_{\varphi}\log \Pi_{\varphi}(x) \right]
$$
<p>
Then we can use this estimate for the update of the gradient.
Then you can also prove that in the context of the optimization of $\varphi$ we can write the score as
</p>
$$
\nabla_{\varphi}\log \Pi_{\varphi}(x)
= \sum_{t = 0}^{T - 1}\nabla _{\varphi}\log \pi(a_{t}\mid x_{t})
$$
<p>
The latter is often called the <em>eligibility vector</em>.
With also this trick in place, then we can take the gradient without any problems (using Monte Carlo estimation) we will then have the following:
</p>
$$
\nabla_{\varphi}J_{T}(\theta) \approx \frac{1}{m} \sum_{i = 1}^{m}g^{(i)}_{0:T}  \sum_{t = 0}^{T - 1}\nabla _{\varphi}\log \pi(a_{t}\mid x_{t})
$$
<p>But this estimate has usually <strong>high variance</strong>.</p>
<h4 id="adding-baselines">Adding Baselines<a hidden class="anchor" aria-hidden="true" href="#adding-baselines">#</a></h4>
<p>Adding a baseline is simply choosing a constant $b$ and subtracting it to the reward estimates:
</p>
$$
 \mathbb{E}_{\tau \sim \prod_{\varphi}}\left[ (G_{0} - b)\nabla_{\varphi}\log \Pi_{\varphi}(x) \right]
$$
<p>
Adding this constant still keeps the expectation unvaried, as the derivative of the score is 0, so it doesn&rsquo;t affect the estimate, but it affects the variance by reducing it if we choose $b$ correctly, which helps in the stability of the estimate.
The condition we need to satisfy is $b^{2} \leq 2 b  \cdot r(x, a)$ for each state and action. (The proof is along variance and covariance of the two functions).</p>
<p>For instance, if we choose the baseline to be
</p>
$$
b_{0:t - 1} = \sum_{i = 0}^{t - 1}\gamma^{i}r_{i}
$$
<p>
Then, the adjusted gradient of the loss comes to be:
</p>
$$
\nabla_{\varphi}J(\theta) \approx \mathbb{E}_{\tau \sim \Pi} \left[ \sum_{t = 0}^{T - 1} \gamma^{t}G_{t:T} \nabla_{\varphi} \log \pi_{\varphi}(a_{t  }\mid x _{t}) \right] 
$$
<p>
Which should be better in terms of its variance.</p>
<h4 id="reinforce">REINFORCE<a hidden class="anchor" aria-hidden="true" href="#reinforce">#</a></h4>
<p>The Reinforce algorithm allows optimization for the policy in <em>continuous spaces</em> but does not guarantee the convergence to the best policy possible.</p>
<p><img loading="lazy" src="/notes/rl-function-approximation-20241129132442764.webp" alt="REINFORCE Algorithm from the Textbook"  />
</p>
<p>One can also add the baseline as before, and it should reduce the variance somehow.</p>
<h4 id="drawbacks">Drawbacks<a hidden class="anchor" aria-hidden="true" href="#drawbacks">#</a></h4>
<ul>
<li>These methods are <strong>On-policy</strong> because we need to simulate many many roll-outs</li>
<li>High variance in the gradient estimates</li>
<li>The last point implied <strong>slow convergence</strong> of the method (related to the <em>sample efficiency</em>)</li>
<li>Not guaranteed convergence</li>
</ul>
<p>Another drawback is the difficulty of handling the exploration-exploitation tradeoff. These algorithms, along with the AC methods in the next section, are fundamentally exploitative. We usually employ random exploration or epsilon-greedy methods to explore, but usually these still might converge without actually having explored the state space enough.</p>
<h4 id="policy-gradient-method">Policy Gradient Method<a hidden class="anchor" aria-hidden="true" href="#policy-gradient-method">#</a></h4>
<p>Often, the above policy gradient we derived for the REINFORCE algorithm is written in terms of <em>discounted rate occupancy measure</em>.
TODO: write the equality.</p>
<h2 id="actor-critic-methods">Actor Critic Methods<a hidden class="anchor" aria-hidden="true" href="#actor-critic-methods">#</a></h2>
<p>Actor critic methods describe a different family of approaches that explicitly attempt to jointly improve an approximator network for the value, called critic, and a network for the policy, called actor.
These methods are, as usual, divided into online and offline actor critic methods. We will analyze and present a few different methods and their properties.</p>
<h3 id="online-actor-critic">Online Actor Critic<a hidden class="anchor" aria-hidden="true" href="#online-actor-critic">#</a></h3>
<p><img loading="lazy" src="/notes/rl-function-approximation-20241129145305087.webp" alt="Image from the Textbook"  />
</p>
<p>The main idea here is to use an approximated $Q-$value to get an approximate policy update.
The main drawback is that we are doing an approximation of an approximation, so the variance of this method should be quite high.
From other point of view, this is somewhat similar to the SARSA update explained in <a href="/notes/tabular-reinforcement-learning/">Tabular Reinforcement Learning</a>
Usually the updates here are bootstrapped too!</p>
<h4 id="the-notion-of-advantage">The notion of Advantage<a hidden class="anchor" aria-hidden="true" href="#the-notion-of-advantage">#</a></h4>
<p>This will be the base for the so called A2C algorithm, where we use the notion of Advantage to bound the variance, in a manner similar to what we have done with the baselines.</p>
<p>Advantage is defined as:
</p>
$$
A^{\pi}(x, a) = Q^{\pi}(x, a) - V^{\pi}(x)
$$
<p>
Which describes the gain in taking a specific action in a specific state compared to the average value of the state.
We can prove that
</p>
$$
\forall \pi, x \max_{a} A^{\pi}(x, a) \geq 0
$$
<p>
This is easy to see for deterministic policies, if we choose an $a$ following the policy, then it is 0, but if it could be improved then it has positive value.</p>
<p>Using the notion of advantage instead of the classical policy network, we get the update rule to be:
</p>
$$
\varphi  = \varphi + \alpha_{\varphi}\nabla_{\varphi}\log \pi(a_{t}\mid x_{t})A^{\pi}(x_{t}, a_{t})
$$
<p>
Which is similar to the above actor-critic model, but with lesser variance if we choose the baseline equivalent correctly.</p>
<h4 id="advantage-actor-critic">Advantage Actor Critic<a hidden class="anchor" aria-hidden="true" href="#advantage-actor-critic">#</a></h4>
<p>This method is very similar to the original Actor Critic method: instead of using the $Q$ estimate to update the critic we use the advantage</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">➕Probabilistic-Artificial-Intelligence</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on x"
            href="https://x.com/intent/tweet/?text=RL%20Function%20Approximation&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f&amp;hashtags=%e2%9e%95probabilistic-artificial-intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f&amp;title=RL%20Function%20Approximation&amp;summary=RL%20Function%20Approximation&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f&title=RL%20Function%20Approximation">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on whatsapp"
            href="https://api.whatsapp.com/send?text=RL%20Function%20Approximation%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on telegram"
            href="https://telegram.me/share/url?text=RL%20Function%20Approximation&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=RL%20Function%20Approximation&u=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
