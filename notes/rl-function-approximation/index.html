<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RL Function Approximation | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="‚ûïprobabilistic-artificial-intelligence">
<meta name="description" content="These algorithms are good for scaling state spaces, but not actions spaces.
The Gradient Idea Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in Tabular Reinforcement Learning.
A simple parametrization üü© The idea here is to parametrize the value estimation function so that similar inputs gets similar values akin to Parametric Modeling estimation we have done in the other courses. In this manner, we don&rsquo;t need to explicitly explore every single state in the state space.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/rl-function-approximation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/rl-function-approximation/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="RL Function Approximation" />
<meta property="og:description" content="These algorithms are good for scaling state spaces, but not actions spaces.
The Gradient Idea Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in Tabular Reinforcement Learning.
A simple parametrization üü© The idea here is to parametrize the value estimation function so that similar inputs gets similar values akin to Parametric Modeling estimation we have done in the other courses. In this manner, we don&rsquo;t need to explicitly explore every single state in the state space." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/rl-function-approximation/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="RL Function Approximation"/>
<meta name="twitter:description" content="These algorithms are good for scaling state spaces, but not actions spaces.
The Gradient Idea Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in Tabular Reinforcement Learning.
A simple parametrization üü© The idea here is to parametrize the value estimation function so that similar inputs gets similar values akin to Parametric Modeling estimation we have done in the other courses. In this manner, we don&rsquo;t need to explicitly explore every single state in the state space."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RL Function Approximation",
      "item": "https://flecart.github.io/notes/rl-function-approximation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RL Function Approximation",
  "name": "RL Function Approximation",
  "description": "These algorithms are good for scaling state spaces, but not actions spaces.\nThe Gradient Idea Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in Tabular Reinforcement Learning.\nA simple parametrization üü© The idea here is to parametrize the value estimation function so that similar inputs gets similar values akin to Parametric Modeling estimation we have done in the other courses. In this manner, we don\u0026rsquo;t need to explicitly explore every single state in the state space.",
  "keywords": [
    "‚ûïprobabilistic-artificial-intelligence"
  ],
  "articleBody": "These algorithms are good for scaling state spaces, but not actions spaces.\nThe Gradient Idea Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in Tabular Reinforcement Learning.\nA simple parametrization üü© The idea here is to parametrize the value estimation function so that similar inputs gets similar values akin to Parametric Modeling estimation we have done in the other courses. In this manner, we don‚Äôt need to explicitly explore every single state in the state space.\nFor example, a single linear parametrization for the value function gives a quite nice interpretation of why we are introducing loss functions in this case:\nLet‚Äôs say $V^{\\pi}(x; \\theta) = \\theta_{x}$ and $\\theta$ is a vector in the size of the state space. Then the bellman update rule, which is $$ V^{\\pi}(x; \\theta_{\\text{new}}) = r(x, \\pi(x)) + \\gamma \\sum_{x'}p(x'\\mid x, \\pi(x))V^{\\pi}(x^{'}; \\theta_{\\text{old}}) $$ Can bee seen as a minimization problem for the following loss: $$ \\forall x, \\theta_{new, x} = \\arg\\min_{\\theta_{x} \\in \\mathbb{R}} (\\theta_{x} - (r(x, \\pi(x)) + \\gamma \\sum_{x'}p(x'\\mid x, \\pi(x))V^{\\pi}(x^{'}; \\theta_{\\text{old}}))))^{2} $$ Which can be written for every single state: $$ \\theta_{new} = \\arg\\min_{\\theta \\in \\mathbb{R}^{n}} \\mathbb{E}_{x}(V^{\\pi}(x; \\theta)\n(r(x, \\pi(x)) + \\gamma \\sum_{x‚Äô}p(x‚Äô\\mid x, \\pi(x))V^{\\pi}(x^{‚Äô}; \\theta_{\\text{old}})))^{2} $$ Where $x$ is drawn from some distribution that has non zero mass for every state (so that it updates every state indefinitely). This simple motivation example opens the door for gradient descent methods for parameter estimation! The only drawback that we will see using these methods is the enormous number of samples that we need to get a good estimate of the value function.\nTD-Gradient View üü© Recall the update for the temporal difference was a Bootstrapped Monte Carlo estimate, which gives a biased result, but nonetheless should converge to the correct result (I think, this should be validated): $$ V^{\\pi}(x) = V_{\\text{old}}^{\\pi}(x) + \\alpha_{t}(r + \\gamma V^{\\pi}_{\\text{old}}(x^{(i)}) - V_{\\text{old}}^{\\pi}(x)) $$ We can observe that the part that multiplies $\\alpha_{t}$ can bee seen as the gradient for the following loss $$ l(\\theta; x, x') = -\\frac{1}{2}(r + \\gamma V^{\\pi}_{\\text{old}}(x^{'};\\theta) - V_{\\text{old}}^{\\pi}(x;\\theta))^{2} $$ Where the value is parameterized by theta. It‚Äôs derivative, is called the TD error, indicated with $\\delta(x)$. TODO: fix the mistake for the expectation of the estimate, also expand on the fact that the gradient is 1 for the linear case, so classic TD and Q-Learning are just doing gradient descent on the linear feature vector.\nOne nice thing about this view, is that the gradient with respect of this loss is unbiased, due to the law of large numbers, see Central Limit Theorem and Law of Large Numbers. Sometimes gradient descent with a bootstrap estimate is called stochastic semi-gradient descent.\nQ-Learning View üü© We can have exactly the same loss for the Q-learning update rule. $$ Q^{\\pi}(x, a) = Q_{\\text{old}}^{\\pi}(x, a) + \\alpha_{t}(r + \\gamma \\max_{a} Q^{\\pi}_{\\text{old}}(x^{'}, a) - Q_{\\text{old}}^{\\pi}(x, a)) $$ And getting the loss, given the trajectory $(x, a, r, x')$ $$ l(\\theta; x, a, r, x') = -\\frac{1}{2}(r + \\gamma \\max_{a} Q^{\\pi}_{\\text{old}}(x^{'}, a;\\theta) - Q_{\\text{old}}^{\\pi}(x, a;\\theta))^{2} $$ Deep Q-networks üü© DQN updates the neural network used for the approximate bootstrapping estimate infrequently to maintain a constant optimization target across multiple episodes.\nThe problem we are trying to solve is the moving optimization target property of the above q-learning optimization, which leads to instabilities (it‚Äôs somewhat similar to changing the array you are iterating in, which gives unpredictable results, sometimes, or more difficult to analyze or debug).\nWe assume to have a dataset $\\mathcal{D}$ called experience buffer. The idea here is use one network for the $old$ values, and the other used for the optimization objective. This idea is quite simple. This technique is known in the literature as Polyak averaging, or experience replay. here $\\theta_{old}$ is not update every step, but only after $D$ iterations, which attempts to give some stability to the optimization.\n$$ \\max_{a} q(x, a) \\approx \\max_{a} Q(x, a; \\theta_{\\text{old}}) $$ The problem with this technique is introducing the q-value estimation, this leads to a maximization bias: Double Q-learning üü© This leads to Double Q-Learning which leads to more accurate estimation of the real $q$ (Van Hasselt et al., 2016) Which is just taking the maximum with respect of the new network, and not the old. Also during gradient estimation, we are always selecting with the new network, not the old, we just use the old network to get the reward. (Meaning the $a$ inside the $Q(x, a; \\theta_{\\text{old}})$) is now taken from the new policy induced by the network parameterized with $\\theta$ that changes often.\nLet‚Äôs quickly formalize this by viewing the loss. For standard Deep Q-learning algorithm, we have that the loss is: $$ \\mathcal{L}(\\theta) = \\frac{1}{2}\\mathbb{E}_{s, a, r, s'}\\left[ (r + \\gamma \\max_{a'}Q(s', a'; \\theta_{\\text{old}}) - Q(s, a; \\theta))^{2} \\right] $$ For this modified q-learning, we use the action that maximizes the new network! Let $a^{*} = \\max_{a} Q(s, a; \\theta)$, then the loss for double q-learning is: $$ \\mathcal{L}(\\theta) = \\frac{1}{2}\\mathbb{E}_{s, a, r, s'}\\left[ (r + \\gamma Q(s', a^{*}; \\theta_{\\text{old}}) - Q(s, a; \\theta))^{2} \\right] $$ It works quite well, but the reason why it works is not well explained mathematically. Intuitively, the old network is quite biased toward overly high values of $Q$. With Double Q-learning, we are shifting this bias towards a more accurate version, the current $Q_{\\theta}$, but still evaluating using the bootstrapped old estimate.\nPolicy Approximation Until now, we have always assumed to have discrete action spaces, but in other domains, we could be interested in continuous action spaces, which leads to a parameterized form of actions. So we write: $$ \\pi^{*}(x) = \\pi(x; \\varphi) $$ The methods that attempt to estimate this are called policy search or policy gradient methods.\nPolicy Parametrization examples For continuous spaces we might want to say that $$ \\pi(a \\mid x, \\theta) \\sim \\mathcal{N}(a ; \\mu(x, \\theta), \\Sigma(x, \\theta)) $$ Where the $\\mu$ and $\\Sigma$ are parameterized by a Neural Network. For discrete parametrizations we might choose a classical linear network based on some features: $$ \\pi(a \\mid x, \\theta) = Cat(a; \\sigma(f(x, \\theta))) $$ We can also decompose this using the Markov property if $m$ is large.\nThe important thing we need is:\nBe able to use Backpropagation Easy to sample from, so that we can use Monte Carlo Methods. Function Gradient Methods The Objective üü©‚Äì Assume we have a policy, then we can have a rollouts for a given specific policy, which we can consider to be a sample from $\\Pi_{\\theta}(\\tau)$. Then we define the function that we would like to maximize, which is $$ J_{T}(\\theta) = \\mathbb{E}_{\\tau \\sim \\Pi_{\\theta}}[G_{0}] \\approx \\frac{1}{N}\\sum_{i=1}^{N}G_{0}^{(i)} $$ Taking trajectories following the current policy.\nScore Trick üü© We have encountered the score function before in Parametric Modeling when estimating the Rao-Cramer Bound. In this context, the score is defined as follows:\n$$ \\nabla_{\\varphi} \\log \\Pi_{\\varphi} = \\frac{\\nabla_{\\varphi}\\Pi_{\\varphi}}{\\Pi\\varphi} $$ We will use it here to get an unbiased estimate of the gradient of the policy, so that we can use Backpropagation to estimate the gradient of the policy.\nThe main theorem here is that the gradient we want to estimate can be rewritten as: $$ \\nabla_{\\varphi}\\mathbb{E}_{\\tau \\sim \\prod_{\\varphi}}[G_{0}] = \\mathbb{E}_{\\tau \\sim \\prod_{\\varphi}}\\left[ G_{0}\\nabla_{\\varphi}\\log \\Pi_{\\varphi}(x) \\right] $$ Then we can use this estimate for the update of the gradient. Then you can also prove that in the context of the optimization of $\\varphi$ we can write the score as $$ \\begin{align} \\nabla_{\\varphi}\\log \\Pi_{\\varphi}(x) \u0026= \\nabla_{\\varphi} \\left[ \\log p(x_{0}) + \\sum_{t = 0}^{T - 1} \\log \\pi(a_{t} \\mid x_{t}) + \\sum_{t = 0}^{T - 1} \\log p(x_{t + 1} \\mid x_{t}, a_{t}) \\right] \\ \u0026=\\sum_{t = 0}^{T - 1}\\nabla {\\varphi}\\log \\pi(a{t}\\mid x_{t})\n\\end{align} $$ The latter is often called the *eligibility vector*. A common parametrization akin to [Log Linear Models](/notes/log-linear-models) is the following: $$ \\pi_{\\varphi}(a \\mid x) = \\frac{\\exp(h(x, a, \\varphi))}{\\sum_{a‚Äô \\in A} \\exp(h(x, a‚Äô, \\varphi))} $$\nAnd $h(x, a, \\varphi) = \\varphi^{T}\\phi(x, a)$ given some feature vector.\nWith also this trick in place, then we can take the gradient without any problems (using Monte Carlo estimation) we will then have the following: $$ \\nabla_{\\varphi}J_{T}(\\theta) \\approx \\frac{1}{m} \\sum_{i = 1}^{m}g^{(i)}_{0:T} \\sum_{t = 0}^{T - 1}\\nabla _{\\varphi}\\log \\pi(a_{t}\\mid x_{t}) $$ But this estimate has usually high variance.\nAdding Baselines üü©‚Äì Adding a baseline is simply choosing a constant $b$ and subtracting it to the reward estimates: $$ \\mathbb{E}_{\\tau \\sim \\prod_{\\varphi}}\\left[ (G_{0} - b)\\nabla_{\\varphi}\\log \\Pi_{\\varphi}(x) \\right] $$ Adding this constant still keeps the expectation unvaried, as the derivative of the score is 0, so it doesn‚Äôt affect the estimate, but it affects the variance by reducing it if we choose $b$ correctly, which helps in the stability of the estimate.\nFor instance, if we choose the baseline to be $$ b_{0:t - 1} = \\sum_{i = 0}^{t - 1}\\gamma^{i}r_{i} $$ Then, the adjusted gradient of the loss comes to be: $$ \\nabla_{\\varphi}J(\\theta) \\approx \\mathbb{E}_{\\tau \\sim \\Pi} \\left[ \\sum_{t = 0}^{T - 1} \\gamma^{t}G_{t:T} \\nabla_{\\varphi} \\log \\pi_{\\varphi}(a_{t }\\mid x _{t}) \\right] $$ Which should be better in terms of its variance.\nBaselines reduce variance üü®+ The condition we need to satisfy is $b^{2} \\leq 2 b \\cdot r(x, a)$ for each state and action. (The proof is along variance and covariance of the two functions).\nWe know that the expected value of the baseline is exactly $0$, this is due to the expected value of the score function being $0$, see Parametric Modeling. We then observe this: $$ \\mathop{\\mathbb{E}}[f(x)]= \\mathop{\\mathbb{E}}[f(x) - b] + \\mathop{\\mathbb{E}}[b] = \\mathop{\\mathbb{E}}[f(x) - b] $$ Furthermore, under certain conditions we have that:\nRecall that $\\text{Var[X + Y]} = \\text{Var[X]} + \\text{Var[Y]} + 2\\text{Cov[X, Y]}$.\nSo we have that: $$ \\text{Var}[f(x) - b] = \\text{Var}[f(x)] + \\text{Var}[b] - 2\\text{Cov}[f(x), b] $$ Which means that if $\\text{Var}[b] \\leq 2\\text{Cov}[f(x), b]$ then the variance of the function will be reduced.\nIn the case of the baseline, the variance of $b$ is exactly $b^{2}$\nOne can prove that given a sample trajectory, the above state dependent baseline is a valid baseline.\nREINFORCE üü© The Reinforce algorithm allows optimization for the policy in continuous spaces but does not guarantee the convergence to the best policy possible.\nOne can also add the baseline as before, and it should reduce the variance, under certain cases. (for the classical state dependent baseline is easy to see that it indeed does it).\nA common technique is to further reduce the variance, and only consider step-1 updates. This is somewhat akin to Stochastic Gradient Descent on policy. One can also subtract the baseline from t onwards.\nDrawbacks of REINFORCE üü®‚Äì These methods are On-policy because we need to simulate many many roll-outs High variance in the gradient estimates The last point implied slow convergence of the method (related to the sample efficiency) Not guaranteed convergence Another drawback is the difficulty of handling the exploration-exploitation tradeoff. These algorithms, along with the AC methods in the next section, are fundamentally exploitative. We usually employ random exploration or epsilon-greedy methods to explore, but usually these still might converge without actually having explored the state space enough.\nPolicy Gradient Method üü© Given a trajectory ($\\tau_{t: \\infty} = (x_{t}, a_{t}, r_{t}, x_{t + 1}, \\ldots)$) we can write the policy gradient as: $$ \\begin{align} \\nabla_{\\varphi} J_{T}(\\varphi) \u0026= \\sum_{t = 0}^{\\infty}\\mathbb{E}{t: \\infty \\sim \\Pi{\\varphi}}\\left[ \\gamma^{t} G_{t} \\nabla_{\\varphi}\\log \\pi(a_{t}\\mid x_{t})\\right] \\ \u0026= \\sum_{t = 0}^{\\infty} \\mathbb{E}{x{t}, a_{t}} \\left[ \\gamma^{t}Q^{\\pi}(x_{t}, a_{t})\\nabla_{\\varphi}\\log \\pi(a_{t}\\mid x_{t}) \\right]\n\\end{align} $$ Assuming we are using a baseline that starts from the $t$ time step.\nOften, the above policy gradient we derived for the REINFORCE algorithm is written in terms of discounted rate occupancy measure.\nThe discounted rate occupancy measure tells us how probable that after a certain number of passes we will be in a certain state. $$ d_{\\pi}(x) = (1-\\gamma)\\sum_{t = 0}^{\\infty}\\gamma^{t}p(x_{t} = x) $$ The $1 - \\gamma$ factor is a normalization constant.\nWe can write the policy gradient as: $$ \\nabla_{\\varphi}J_{T}(\\varphi) = \\frac{1}{(1 - \\gamma)} \\cdot \\mathbb{E}_{x\\sim d_{\\pi}}\\left[ \\mathbb{E}_{a\\sim \\pi(\\cdot \\mid x)}[Q^{\\pi}(x, a)\\nabla_{\\varphi}\\log \\pi(a\\mid x)] \\right] $$ This form is known as the policy gradient theorem, it will be useful to characterize the actor critic methods.\nWe will see that for offline methods, explored in #Offline Actor Critic, we wont need the $\\log \\pi$ part, as we cannot actively explore the next state from the current state.\nPolicy Gradient and The Exponential Family üü® If the policy is characterized by a distribution in the exponential family, it is easy to derive a closed form solution for the policy gradient as above. In this section we briefly discuss some gradients that are part of this family. See The Exponential Family for a discussion on distributions of this family.\nRecall that if the policy is part of the exponential family we can write it as: $$ \\pi(a \\mid x ) = h(a) \\exp(a f_{\\varphi}(x) - A(f_\\varphi(x))) $$ And written in this manner the un-baselined policy gradient is:\n$$ \\begin{align} \\nabla_{\\varphi}J_{T}(\\varphi) \u0026= \\mathbb{E}_{x\\sim d_{\\pi}}\\left[ \\mathbb{E}_{a\\sim \\pi(\\cdot \\mid x)}[Q^{\\pi}(x, a)\\nabla_{\\varphi}\\log \\pi(a\\mid x)] \\right] \\\\ \u0026= \\mathbb{E}_{x\\sim d_{\\pi}}\\left[ \\mathbb{E}_{a\\sim \\pi(\\cdot \\mid x)}[Q^{\\pi}(x, a)\\nabla_{\\varphi}(a f_{\\varphi}(x) - A(f_\\varphi(x)))] \\right] \\\\ \u0026= \\mathbb{E}_{x\\sim d_{\\pi}}\\left[ \\mathbb{E}_{a\\sim \\pi(\\cdot \\mid x)}[Q^{\\pi}(x, a)(f_{\\varphi}(x) - \\nabla_{\\varphi}A(f_\\varphi(x))\\nabla_{\\varphi}f_{\\varphi}(x))] \\right] \\\\ \\end{align} $$ We just need to plug the correct values of the exponential family in.\nActor Critic Methods Actor critic methods describe a different family of approaches that explicitly attempt to jointly improve an approximator network for the value, called critic, and a network for the policy, called actor. These methods are, as usual, divided into online and offline actor critic methods. We will analyze and present a few different methods and their properties.\nOnline Actor Critic With online methods, we\nA first algorithm üü© The main idea here is to use an approximated $Q-$value to get an approximate policy update. The main drawback is that we are doing an approximation of an approximation, so the variance of this method should be quite high. From other point of view, this is somewhat similar to the SARSA update explained in Tabular Reinforcement Learning Usually the updates here are bootstrapped too!\nThe notion of Advantage üü© This will be the base for the so called A2C algorithm, where we use the notion of Advantage to bound the variance, in a manner similar to what we have done with the baselines.\nAdvantage is defined as: $$ A^{\\pi}(x, a) = Q^{\\pi}(x, a) - V^{\\pi}(x) $$ Which describes the gain in taking a specific action in a specific state compared to the average value of the state. We can prove that $$ \\forall \\pi, x \\max_{a} A^{\\pi}(x, a) \\geq 0 $$ This is easy to see for deterministic policies, if we choose an $a$ following the policy, then it is 0, but if it could be improved then it has positive value.\nAdvantage Actor Critic üü© This method is very similar to the original Actor Critic method: instead of using the $Q$ estimate to update the critic we use the advantage This can be view as the standard Policy Gradient method with some special baseline.\nUsing the notion of advantage instead of the classical policy network, we get the update rule to be: $$ \\varphi = \\varphi + \\alpha_{\\varphi}\\nabla_{\\varphi}\\log \\pi(a_{t}\\mid x_{t})A^{\\pi}(x_{t}, a_{t}) $$ Which is similar to the above actor-critic model, but with lesser variance if we choose the baseline equivalent correctly.\nRecall that $V(x) = \\mathop{\\mathbb{E}}_{a \\sim \\pi(x)} Q(x, a)$, so this is the baseline that has been chosen for this kind of Actor-Critic network. This is probably difficult to compute, and probably not stable to estimate using MonteCarlo Networks.\nOne thing that is usually done, is adding the parametrized value network $V(x ; \\theta)$ and use that one.\nTrusted Region Policy Optimization üü©‚Äì This method is a variant of the above method, but it introduces a constraint on the policy update, so that we don‚Äôt update the policy too much, which can lead to instability in the optimization. For a similar reason, we use a fixed critic for multiple iterations.\nThe update of the policy becomes: $$ \\varphi_{t + 1} = \\arg\\max_{\\varphi} J(\\varphi) \\text{ s.t. } KL(\\pi_{\\varphi_{t}} \\parallel \\pi_{\\varphi}) \\leq \\delta $$ For a fixed $\\delta$ value, which defines the trust region, which is also important for the importance weights to be stable. In this case, the cost function is slightly changed, using importance sampling: $$ J(\\varphi) = \\mathbb{E}_{x, a\\sim \\pi_{\\varphi_{t}}}\\left[ \\frac{\\pi_{\\varphi}(a\\mid x)}{\\pi_{\\varphi_{t}}(a\\mid x)}A^{\\pi_{\\varphi_{t}}}(x, a) \\right] $$ One nice thing about TRPO is that it is possible to use this algorithm in an offline fashion as long as the policy can still be trusted (i.e. it satisfies the delta constraint).\nProximal Policy Optimization üü©‚Äì This method is a variant of the above method, but instead of using the KL divergence, we use a penalty term in the loss function to keep the policy update close to the old policy. This algorithm has also had great influence during training of language models.\nOne common variant uses the following objective function: $$ \\varphi_{t + 1} = \\arg\\max_{\\varphi} J(\\varphi) - \\beta \\cdot KL(\\pi_{\\varphi_{t}} \\parallel \\pi_{\\varphi}) $$ With $\\beta \u003e 0$. Other variants might work on the importance sampling.\nOffline Actor Critic One clear advantage of offline learning algorithms is the possibility of re-using past data.\nThe Main Idea üü®++ Recall the DQN loss function: $$ \\mathcal{L}(\\theta) = \\mathbb{E}_{s, a, r, s'}\\left[ (r + \\gamma \\max_{a'}Q(s', a'; \\theta_{\\text{old}}) - Q(s, a; \\theta))^{2} \\right] $$ The main problem with the standard setting is having to maximize over the set of actions, which could be very large, or even continuous.\nWhat we do now is assume we have a ‚Äúrich enough‚Äù parametrization of $Q$ and just assume that our network is learning the maximum by itself. In this way, our loss function becomes: $$ \\mathcal{L}(\\theta) = \\mathbb{E}_{s, a, r, s'}\\left[ (r + \\gamma Q(s', \\pi_{\\varphi}(s'); \\theta_{\\text{old}}) - Q(s, a; \\theta))^{2} \\right] $$ Where $\\pi_{\\varphi}$ is the policy network. If the policy is good enough, by Bellman Optimality condition (see Markov Processes), it will naturally converge to the maximum of the $Q$ function, one problem could be the instability of this double optimization, as we are optimizing for $Q$ and $\\pi$ at the same time.\nIn the policy update phase, instead of taking the maximum over all possible actions, we take the policy that is yielding highest returns following the rule: $$ \\varphi_{t + 1} = \\arg\\max_{\\varphi} \\mathbb{E}_{s, a\\sim \\pi_{\\varphi_{t}}}\\left[ Q(s, a; \\theta_{t}) \\right] = \\arg\\max_{\\varphi} J(\\varphi) $$ Usually, instead of sampling from $\\pi$, we have an offline buffer of trajectories and sample uniformly from that buffer, but nothing prevents us to sample in an online fashion. So we write: $$ \\nabla_{\\varphi}J(\\varphi) = \\mathbb{E}_{s \\sim \\mu}\\left[ Q(s, \\pi_{\\varphi}(s); \\theta_{t}) \\right] $$ And we can get an estimate in a similar manner for the $Q$ network: $$ \\nabla_{\\varphi}Q(s, a; \\theta) = D\\varphi \\pi_{\\varphi}(s) \\cdot \\nabla_{a}Q(s, a; \\theta) \\mid_{a = \\pi_{\\varphi}(s)} $$ Note that here we are not considering the $\\log \\pi(a \\mid x)$ to compute the gradient for the policy. My personal take is just that we don‚Äôt know how to compute this value (we don‚Äôt have a forward step to compute it), so we just ignore it. Here we can see a discussion on the use of $\\log \\pi(a \\mid x)$.\nDeep Deterministic Policy Gradient üü®++ Putting everything together from the section before we get the DDPG algorithm: It is possible to extend this algorithm with TD3, which is a variant of the above algorithm that uses a double critic to estimate the Q-value, which helps in reducing the overestimation bias.\nOn Exploration With the offline methods we have just presented, the usual solution for exploration is what is called Gaussian Noise Dithering: this method just adds some noise to the output of the policy $\\pi_{\\varphi}$ in continuous settings. However, these methods often suffer from not exploring enough. The following section presents a method that attempts to ease this problem. Some methods of exploration as are also cited in Planning.\nMaximum Entropy Reinforcement Learning üü® Maximum Entropy Reinforcement Learning (MERL) adds a regularizer that prevents the policy to become too narrow, or deterministic, for a single action. This is easily considered during the evaluation of the cost: $$ J(\\varphi) = \\mathbb{E}_{s, a\\sim \\pi_{\\varphi}}\\left[ Q(s, a; \\theta) + \\alpha \\mathcal{H}(\\pi_{\\varphi}(\\cdot \\mid s)) \\right] $$ Where the parameter $\\alpha$ is a hyper-parameter that controls how much the entropy of the policy weights.\nTrust Region Policy Optimization (TRPO) To address the instability of optimizing both $Q$ and $\\pi$ simultaneously, Trust Region Policy Optimization (TRPO) introduces a constrained optimization framework. The primary idea behind TRPO is to update the policy in such a way that it stays within a trust region, ensuring that each update does not deviate excessively from the current policy. This prevents performance degradation due to overly large updates.\nObjective Function: TRPO optimizes the following constrained objective:\n$$ \\max_{\\varphi} \\mathbb{E}_{s \\sim \\mu, a \\sim \\pi_{\\varphi_t}}\\left[ \\frac{\\pi_{\\varphi}(a \\mid s)}{\\pi_{\\varphi_t}(a \\mid s)} A_{\\pi_{\\varphi_t}}(s, a) \\right] $$ subject to:\n$$ \\mathbb{E}_{s \\sim \\mu}\\left[ D_{\\text{KL}}(\\pi_{\\varphi_t}(\\cdot \\mid s) \\parallel \\pi_{\\varphi}(\\cdot \\mid s)) \\right] \\leq \\delta, $$ where $A_{\\pi_{\\varphi_t}}(s, a)$ is the advantage function, $D_{\\text{KL}}$ is the Kullback-Leibler divergence, and $\\delta$ is a small positive constant controlling the size of the update.\nProximal Policy Optimization (PPO) Proximal Policy Optimization (PPO) simplifies TRPO by avoiding the computational complexity of solving a constrained optimization problem. PPO achieves similar benefits by clipping the policy update directly within the objective function.\nObjective Function PPO modifies the policy update by introducing a clipped surrogate objective:\n$$ \\mathcal{L}^{\\text{PPO}}(\\varphi) = \\mathbb{E}_{s, a}\\left[ \\min\\left( r_t(\\varphi) A_{\\pi_{\\varphi_t}}(s, a), \\text{clip}(r_t(\\varphi), 1 - \\epsilon, 1 + \\epsilon) A_{\\pi_{\\varphi_t}}(s, a) \\right) \\right], $$ where:\n$$ r_t(\\varphi) = \\frac{\\pi_{\\varphi}(a \\mid s)}{\\pi_{\\varphi_t}(a \\mid s)}, $$ and $\\epsilon$ is a small hyperparameter (e.g., $\\epsilon = 0.2$) controlling the extent of clipping.\nAlgorithm Advantages Challenges TRPO Stable updates, trust region enforcement Computationally expensive, complex PPO Simplicity, robust performance Sensitive to hyperparameters ($\\epsilon$) ",
  "wordCount" : "3612",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/rl-function-approximation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      RL Function Approximation
    </h1>
    <div class="post-meta">17 min&nbsp;¬∑&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#the-gradient-idea" aria-label="The Gradient Idea">The Gradient Idea</a><ul>
                        
                <li>
                    <a href="#a-simple-parametrization-" aria-label="A simple parametrization üü©">A simple parametrization üü©</a></li>
                <li>
                    <a href="#td-gradient-view-" aria-label="TD-Gradient View üü©">TD-Gradient View üü©</a></li>
                <li>
                    <a href="#q-learning-view-" aria-label="Q-Learning View üü©">Q-Learning View üü©</a></li>
                <li>
                    <a href="#deep-q-networks-" aria-label="Deep Q-networks üü©">Deep Q-networks üü©</a></li>
                <li>
                    <a href="#double-q-learning-" aria-label="Double Q-learning üü©">Double Q-learning üü©</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#policy-approximation" aria-label="Policy Approximation">Policy Approximation</a><ul>
                        <ul>
                        
                <li>
                    <a href="#policy-parametrization-examples" aria-label="Policy Parametrization examples">Policy Parametrization examples</a></li></ul>
                    
                <li>
                    <a href="#function-gradient-methods" aria-label="Function Gradient Methods">Function Gradient Methods</a><ul>
                        
                <li>
                    <a href="#the-objective---" aria-label="The Objective üü©&ndash;">The Objective üü©&ndash;</a></li>
                <li>
                    <a href="#score-trick-" aria-label="Score Trick üü©">Score Trick üü©</a></li>
                <li>
                    <a href="#adding-baselines---" aria-label="Adding Baselines üü©&ndash;">Adding Baselines üü©&ndash;</a></li>
                <li>
                    <a href="#baselines-reduce-variance-" aria-label="Baselines reduce variance üü®&#43;">Baselines reduce variance üü®+</a></li>
                <li>
                    <a href="#reinforce-" aria-label="REINFORCE üü©">REINFORCE üü©</a></li>
                <li>
                    <a href="#drawbacks-of-reinforce---" aria-label="Drawbacks of REINFORCE üü®&ndash;">Drawbacks of REINFORCE üü®&ndash;</a></li>
                <li>
                    <a href="#policy-gradient-method-" aria-label="Policy Gradient Method üü©">Policy Gradient Method üü©</a></li>
                <li>
                    <a href="#policy-gradient-and-the-exponential-family-" aria-label="Policy Gradient and The Exponential Family üü®">Policy Gradient and The Exponential Family üü®</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#actor-critic-methods" aria-label="Actor Critic Methods">Actor Critic Methods</a><ul>
                        
                <li>
                    <a href="#online-actor-critic" aria-label="Online Actor Critic">Online Actor Critic</a><ul>
                        
                <li>
                    <a href="#a-first-algorithm-" aria-label="A first algorithm üü©">A first algorithm üü©</a></li>
                <li>
                    <a href="#the-notion-of-advantage-" aria-label="The notion of Advantage üü©">The notion of Advantage üü©</a></li>
                <li>
                    <a href="#advantage-actor-critic-" aria-label="Advantage Actor Critic üü©">Advantage Actor Critic üü©</a></li>
                <li>
                    <a href="#trusted-region-policy-optimization---" aria-label="Trusted Region Policy Optimization üü©&ndash;">Trusted Region Policy Optimization üü©&ndash;</a></li>
                <li>
                    <a href="#proximal-policy-optimization---" aria-label="Proximal Policy Optimization üü©&ndash;">Proximal Policy Optimization üü©&ndash;</a></li></ul>
                </li>
                <li>
                    <a href="#offline-actor-critic" aria-label="Offline Actor Critic">Offline Actor Critic</a><ul>
                        
                <li>
                    <a href="#the-main-idea-" aria-label="The Main Idea üü®&#43;&#43;">The Main Idea üü®++</a></li>
                <li>
                    <a href="#deep-deterministic-policy-gradient-" aria-label="Deep Deterministic Policy Gradient üü®&#43;&#43;">Deep Deterministic Policy Gradient üü®++</a></li></ul>
                </li>
                <li>
                    <a href="#on-exploration" aria-label="On Exploration">On Exploration</a><ul>
                        
                <li>
                    <a href="#maximum-entropy-reinforcement-learning-" aria-label="Maximum Entropy Reinforcement Learning üü®">Maximum Entropy Reinforcement Learning üü®</a></li>
                <li>
                    <a href="#trust-region-policy-optimization-trpo" aria-label="Trust Region Policy Optimization (TRPO)">Trust Region Policy Optimization (TRPO)</a></li>
                <li>
                    <a href="#proximal-policy-optimization-ppo" aria-label="Proximal Policy Optimization (PPO)">Proximal Policy Optimization (PPO)</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>These algorithms are good for scaling state spaces, but not actions spaces.</p>
<h3 id="the-gradient-idea">The Gradient Idea<a hidden class="anchor" aria-hidden="true" href="#the-gradient-idea">#</a></h3>
<p>Recall Temporal difference learning and Q-Learning, two model free policy evaluation techniques explored in <a href="/notes/tabular-reinforcement-learning/">Tabular Reinforcement Learning</a>.</p>
<h4 id="a-simple-parametrization-">A simple parametrization üü©<a hidden class="anchor" aria-hidden="true" href="#a-simple-parametrization-">#</a></h4>
<p>The idea here is to parametrize the value estimation function so that <em>similar inputs</em> gets <em>similar values</em> akin to <a href="/notes/parametric-modeling/">Parametric Modeling</a> estimation we have done in the other courses. In this manner, we don&rsquo;t need to explicitly explore every single state in the state space.</p>
<p>For example, a single linear parametrization for the value function gives a quite nice interpretation of why we are introducing loss functions in this case:</p>
<p>Let&rsquo;s say $V^{\pi}(x; \theta) = \theta_{x}$ and $\theta$ is a vector in the size of the state space.
Then the bellman update rule, which is
</p>
$$
V^{\pi}(x; \theta_{\text{new}}) = r(x, \pi(x)) + \gamma \sum_{x'}p(x'\mid x, \pi(x))V^{\pi}(x^{'}; \theta_{\text{old}})
$$
<p>
Can bee seen as a minimization problem for the following loss:
</p>
$$
\forall x, \theta_{new, x} = \arg\min_{\theta_{x} \in \mathbb{R}} (\theta_{x} - (r(x, \pi(x)) + \gamma  \sum_{x'}p(x'\mid x, \pi(x))V^{\pi}(x^{'}; \theta_{\text{old}}))))^{2}
$$
<p>
Which can be written for every single state:
$$
\theta_{new} = \arg\min_{\theta \in \mathbb{R}^{n}} \mathbb{E}_{x}(V^{\pi}(x; \theta)</p>
<ul>
<li>(r(x, \pi(x)) + \gamma  \sum_{x&rsquo;}p(x&rsquo;\mid x, \pi(x))V^{\pi}(x^{&rsquo;}; \theta_{\text{old}})))^{2}
$$
Where $x$ is drawn from some distribution that has non zero mass for every state (so that it updates every state indefinitely).</li>
</ul>
<p>This simple motivation example opens the door for gradient descent methods for parameter estimation!
The only drawback that we will see using these methods is the enormous number of samples that we need to get a good estimate of the value function.</p>
<h4 id="td-gradient-view-">TD-Gradient View üü©<a hidden class="anchor" aria-hidden="true" href="#td-gradient-view-">#</a></h4>
<p>Recall the update for the temporal difference was a Bootstrapped Monte Carlo estimate, which gives a biased result, but nonetheless should converge to the correct result (I think, this should be validated):
</p>
$$
V^{\pi}(x) =  V_{\text{old}}^{\pi}(x) + \alpha_{t}(r + \gamma V^{\pi}_{\text{old}}(x^{(i)}) -  V_{\text{old}}^{\pi}(x))
$$
<p>
We can observe that the part that multiplies $\alpha_{t}$ can bee seen as the gradient for the following loss
</p>
$$
l(\theta; x, x') = -\frac{1}{2}(r + \gamma V^{\pi}_{\text{old}}(x^{'};\theta) -  V_{\text{old}}^{\pi}(x;\theta))^{2}
$$
<p>
Where the value is parameterized by theta.
It&rsquo;s derivative, is called the TD error, indicated with $\delta(x)$.
TODO: fix the mistake for the expectation of the estimate, also expand on the fact that the gradient is 1 for the linear case, so classic TD and Q-Learning are just doing gradient descent on the linear feature vector.</p>
<p>One nice thing about this view, is that the gradient with respect of this loss is <strong>unbiased</strong>, due to the law of large numbers, see <a href="/notes/central-limit-theorem-and-law-of-large-numbers/">Central Limit Theorem and Law of Large Numbers</a>.
Sometimes gradient descent with a bootstrap estimate is called <em>stochastic semi-gradient descent</em>.</p>
<h4 id="q-learning-view-">Q-Learning View üü©<a hidden class="anchor" aria-hidden="true" href="#q-learning-view-">#</a></h4>
<p>We can have exactly the same loss for the Q-learning update rule.
</p>
$$
Q^{\pi}(x, a) =  Q_{\text{old}}^{\pi}(x, a) + \alpha_{t}(r + \gamma \max_{a} Q^{\pi}_{\text{old}}(x^{'}, a) -  Q_{\text{old}}^{\pi}(x, a))
$$
<p>
And getting the loss, given the trajectory $(x, a, r, x')$
</p>
$$
l(\theta; x, a, r, x') = -\frac{1}{2}(r + \gamma \max_{a} Q^{\pi}_{\text{old}}(x^{'}, a;\theta) -  Q_{\text{old}}^{\pi}(x, a;\theta))^{2}
$$
<h4 id="deep-q-networks-">Deep Q-networks üü©<a hidden class="anchor" aria-hidden="true" href="#deep-q-networks-">#</a></h4>
<blockquote>
<p>DQN updates the neural network used for the approximate bootstrapping estimate infrequently to maintain a constant optimization target across multiple episodes.</p>
</blockquote>
<p>The problem we are trying to solve is the moving optimization target property of the above q-learning optimization, which leads to instabilities (it&rsquo;s somewhat similar to changing the array you are iterating in, which gives unpredictable results, sometimes, or more difficult to analyze or debug).</p>
<p>We assume to have a dataset $\mathcal{D}$ called <em>experience buffer</em>.
The idea here is use one network for the $old$ values, and the other used for the optimization objective. This idea is quite simple.
This technique is known in the literature as <em>Polyak averaging</em>, or <em>experience replay</em>.
here $\theta_{old}$ is not update every step, but only after $D$ <strong>iterations</strong>, which attempts to give some stability to the optimization.</p>
$$
\max_{a} q(x, a) \approx \max_{a} Q(x, a; \theta_{\text{old}})
$$
<p>
The problem with this technique is introducing the q-value estimation, this leads to a <strong>maximization bias</strong>:
<img loading="lazy" src="/notes/rl-function-approximation-20241129105307115.webp" alt="image from the book"  />
</p>
<h4 id="double-q-learning-">Double Q-learning üü©<a hidden class="anchor" aria-hidden="true" href="#double-q-learning-">#</a></h4>
<p>This leads to <strong>Double Q-Learning</strong> which leads to more accurate estimation of the real $q$ (Van Hasselt et
al., 2016)
Which is just taking the maximum with respect of the new network, and not the old.
Also during gradient estimation, we are always selecting with the new network, not the old, we just use the old network to get the reward. (Meaning the $a$ inside the $Q(x, a; \theta_{\text{old}})$) is now taken from the new policy induced by the network parameterized with $\theta$ that changes often.</p>
<p>Let&rsquo;s quickly formalize this by viewing the loss.
For  standard Deep Q-learning algorithm, we have that the loss is:
</p>
$$
\mathcal{L}(\theta) = \frac{1}{2}\mathbb{E}_{s, a, r, s'}\left[ (r + \gamma \max_{a'}Q(s', a'; \theta_{\text{old}}) - Q(s, a; \theta))^{2} \right]
$$
<p>
For this modified q-learning, we use the action that maximizes the new network! Let $a^{*} = \max_{a} Q(s, a; \theta)$, then the loss for double q-learning is:
</p>
$$
\mathcal{L}(\theta) = \frac{1}{2}\mathbb{E}_{s, a, r, s'}\left[ (r + \gamma Q(s', a^{*}; \theta_{\text{old}}) - Q(s, a; \theta))^{2} \right]
$$
<p>It works quite well, but the reason why it works is not well explained mathematically. Intuitively, the old network is quite biased toward overly high values of $Q$. With Double Q-learning, we are shifting this bias towards a more accurate version, the current $Q_{\theta}$, but still evaluating using the bootstrapped old estimate.</p>
<h2 id="policy-approximation">Policy Approximation<a hidden class="anchor" aria-hidden="true" href="#policy-approximation">#</a></h2>
<p>Until now, we have always assumed to have discrete action spaces, but in other domains, we could be interested in continuous action spaces, which leads to a parameterized form of actions.
So we write:
</p>
$$
\pi^{*}(x) = \pi(x; \varphi)
$$
<p>
The methods that attempt to estimate this are called <em>policy search</em> or <em>policy gradient</em> methods.</p>
<h4 id="policy-parametrization-examples">Policy Parametrization examples<a hidden class="anchor" aria-hidden="true" href="#policy-parametrization-examples">#</a></h4>
<p>For continuous spaces we might want to say that
</p>
$$
\pi(a \mid x, \theta) \sim \mathcal{N}(a ; \mu(x, \theta), \Sigma(x, \theta))
$$
<p>
Where the $\mu$ and $\Sigma$ are parameterized by a Neural Network.
For discrete parametrizations we might choose a classical linear network based on some features:
</p>
$$
\pi(a \mid x, \theta) = Cat(a; \sigma(f(x, \theta)))
$$
<p>
We can also decompose this using the Markov property if $m$ is large.</p>
<p>The important thing we need is:</p>
<ol>
<li>Be able to use <a href="/notes/backpropagation/">Backpropagation</a></li>
<li>Easy to sample from, so that we can use <a href="/notes/monte-carlo-methods/">Monte Carlo Methods</a>.</li>
</ol>
<h3 id="function-gradient-methods">Function Gradient Methods<a hidden class="anchor" aria-hidden="true" href="#function-gradient-methods">#</a></h3>
<h4 id="the-objective---">The Objective üü©&ndash;<a hidden class="anchor" aria-hidden="true" href="#the-objective---">#</a></h4>
<p>Assume we have a policy, then we can have a rollouts for a given specific policy, which we can consider to be a sample from $\Pi_{\theta}(\tau)$.
Then we define the function that we would like to maximize, which is
</p>
$$
J_{T}(\theta) = \mathbb{E}_{\tau \sim \Pi_{\theta}}[G_{0}] \approx \frac{1}{N}\sum_{i=1}^{N}G_{0}^{(i)}
$$
<p>
Taking trajectories following the current policy.</p>
<h4 id="score-trick-">Score Trick üü©<a hidden class="anchor" aria-hidden="true" href="#score-trick-">#</a></h4>
<p>We have encountered the score function before in <a href="/notes/parametric-modeling/">Parametric Modeling</a> when estimating the Rao-Cramer Bound.
In this context, the score is defined as follows:</p>
$$
\nabla_{\varphi} \log \Pi_{\varphi} =  \frac{\nabla_{\varphi}\Pi_{\varphi}}{\Pi\varphi}
$$
<p>We will use it here to get an unbiased estimate of the gradient of the policy, so that we can use <a href="/notes/backpropagation/">Backpropagation</a> to estimate the gradient of the policy.</p>
<p>The main theorem here is that the gradient we want to estimate can be rewritten as:
</p>
$$
\nabla_{\varphi}\mathbb{E}_{\tau \sim \prod_{\varphi}}[G_{0}] 
= \mathbb{E}_{\tau \sim \prod_{\varphi}}\left[ G_{0}\nabla_{\varphi}\log \Pi_{\varphi}(x) \right]
$$
<p>
Then we can use this estimate for the update of the gradient.
Then you can also prove that in the context of the optimization of $\varphi$ we can write the score as
$$
\begin{align}
\nabla_{\varphi}\log \Pi_{\varphi}(x) &amp;=  \nabla_{\varphi} \left[  \log p(x_{0}) + \sum_{t = 0}^{T - 1} \log \pi(a_{t} \mid x_{t}) + \sum_{t = 0}^{T - 1} \log p(x_{t + 1} \mid x_{t}, a_{t}) \right] \
&amp;=\sum_{t = 0}^{T - 1}\nabla <em>{\varphi}\log \pi(a</em>{t}\mid x_{t})</p>
<p>\end{align}
</p>
$$
The latter is often called the *eligibility vector*.
A common parametrization akin to [Log Linear Models](/notes/log-linear-models) is the following:
$$
<p>
\pi_{\varphi}(a \mid x)  = \frac{\exp(h(x, a, \varphi))}{\sum_{a&rsquo; \in A} \exp(h(x, a&rsquo;, \varphi))}
$$</p>
<p>And $h(x, a, \varphi) = \varphi^{T}\phi(x, a)$ given some feature vector.</p>
<p>With also this trick in place, then we can take the gradient without any problems (using Monte Carlo estimation) we will then have the following:
</p>
$$
\nabla_{\varphi}J_{T}(\theta) \approx \frac{1}{m} \sum_{i = 1}^{m}g^{(i)}_{0:T}  \sum_{t = 0}^{T - 1}\nabla _{\varphi}\log \pi(a_{t}\mid x_{t})
$$
<p>But this estimate has usually <strong>high variance</strong>.</p>
<h4 id="adding-baselines---">Adding Baselines üü©&ndash;<a hidden class="anchor" aria-hidden="true" href="#adding-baselines---">#</a></h4>
<p>Adding a baseline is simply choosing a constant $b$ and subtracting it to the reward estimates:
</p>
$$
 \mathbb{E}_{\tau \sim \prod_{\varphi}}\left[ (G_{0} - b)\nabla_{\varphi}\log \Pi_{\varphi}(x) \right]
$$
<p>
Adding this constant still keeps the expectation unvaried, as the derivative of the score is 0, so it doesn&rsquo;t affect the estimate, but it affects the variance by reducing it if we choose $b$ correctly, which helps in the stability of the estimate.</p>
<p>For instance, if we choose the baseline to be
</p>
$$
b_{0:t - 1} = \sum_{i = 0}^{t - 1}\gamma^{i}r_{i}
$$
<p>
Then, the adjusted gradient of the loss comes to be:
</p>
$$
\nabla_{\varphi}J(\theta) \approx \mathbb{E}_{\tau \sim \Pi} \left[ \sum_{t = 0}^{T - 1} \gamma^{t}G_{t:T} \nabla_{\varphi} \log \pi_{\varphi}(a_{t  }\mid x _{t}) \right] 
$$
<p>
Which should be better in terms of its variance.</p>
<h4 id="baselines-reduce-variance-">Baselines reduce variance üü®+<a hidden class="anchor" aria-hidden="true" href="#baselines-reduce-variance-">#</a></h4>
<p>The condition we need to satisfy is $b^{2} \leq 2 b  \cdot r(x, a)$ for each state and action. (The proof is along variance and covariance of the two functions).</p>
<p>We know that the expected value of the baseline is exactly $0$, this is due to the expected value of the score function being $0$, see <a href="/notes/parametric-modeling/">Parametric Modeling</a>.
We then observe this:
</p>
$$
\mathop{\mathbb{E}}[f(x)]= \mathop{\mathbb{E}}[f(x) - b] + \mathop{\mathbb{E}}[b]  = \mathop{\mathbb{E}}[f(x) - b] 
$$
<p>
Furthermore, under certain conditions we have that:</p>
<p>Recall that $\text{Var[X + Y]} = \text{Var[X]} + \text{Var[Y]} + 2\text{Cov[X, Y]}$.</p>
<p>So we have that:
</p>
$$
\text{Var}[f(x) - b] = \text{Var}[f(x)] + \text{Var}[b] - 2\text{Cov}[f(x), b]
$$
<p>Which means that if $\text{Var}[b] \leq 2\text{Cov}[f(x), b]$ then the variance of the function will be reduced.</p>
<p>In the case of the baseline, the variance of $b$ is exactly $b^{2}$</p>
<p>One can prove that given a sample trajectory, the above state dependent baseline is a valid baseline.</p>
<h4 id="reinforce-">REINFORCE üü©<a hidden class="anchor" aria-hidden="true" href="#reinforce-">#</a></h4>
<p>The Reinforce algorithm allows optimization for the policy in <em>continuous spaces</em> but does not guarantee the convergence to the best policy possible.</p>
<p><img loading="lazy" src="/notes/rl-function-approximation-20241129132442764.webp" alt="REINFORCE Algorithm from the Textbook"  />
</p>
<p>One can also add the baseline as before, and it should reduce the variance, under certain cases. (for the classical state dependent baseline is easy to see that it indeed does it).</p>
<p>A common technique is to further reduce the variance, and only consider step-1 updates. This is somewhat akin to Stochastic Gradient Descent on policy.
One can also subtract the baseline from t onwards.</p>
<h4 id="drawbacks-of-reinforce---">Drawbacks of REINFORCE üü®&ndash;<a hidden class="anchor" aria-hidden="true" href="#drawbacks-of-reinforce---">#</a></h4>
<ul>
<li>These methods are <strong>On-policy</strong> because we need to simulate many many roll-outs</li>
<li>High variance in the gradient estimates</li>
<li>The last point implied <strong>slow convergence</strong> of the method (related to the <em>sample efficiency</em>)</li>
<li>Not guaranteed convergence</li>
</ul>
<p>Another drawback is the difficulty of handling the exploration-exploitation tradeoff. These algorithms, along with the AC methods in the next section, are fundamentally exploitative. We usually employ random exploration or epsilon-greedy methods to explore, but usually these still might converge without actually having explored the state space enough.</p>
<h4 id="policy-gradient-method-">Policy Gradient Method üü©<a hidden class="anchor" aria-hidden="true" href="#policy-gradient-method-">#</a></h4>
<p>Given a trajectory ($\tau_{t: \infty} = (x_{t}, a_{t}, r_{t}, x_{t + 1}, \ldots)$) we can write the policy gradient as:
$$
\begin{align}
\nabla_{\varphi} J_{T}(\varphi) &amp;= \sum_{t = 0}^{\infty}\mathbb{E}<em>{t: \infty \sim \Pi</em>{\varphi}}\left[ \gamma^{t} G_{t} \nabla_{\varphi}\log \pi(a_{t}\mid x_{t})\right]  \
&amp;= \sum_{t = 0}^{\infty} \mathbb{E}<em>{x</em>{t}, a_{t}} \left[ \gamma^{t}Q^{\pi}(x_{t}, a_{t})\nabla_{\varphi}\log \pi(a_{t}\mid x_{t}) \right]</p>
<p>\end{align}
$$
Assuming we are using a baseline that starts from the $t$  time step.</p>
<p>Often, the above policy gradient we derived for the REINFORCE algorithm is written in terms of <em>discounted rate occupancy measure</em>.</p>
<p>The discounted rate occupancy measure tells us how probable that after a certain  number of passes we will be in a certain state.
</p>
$$
d_{\pi}(x) = (1-\gamma)\sum_{t = 0}^{\infty}\gamma^{t}p(x_{t} = x)
$$
<p>
The $1 - \gamma$ factor is a normalization constant.</p>
<p>We can write the policy gradient as:
</p>
$$
\nabla_{\varphi}J_{T}(\varphi) = \frac{1}{(1 - \gamma)} \cdot \mathbb{E}_{x\sim d_{\pi}}\left[ \mathbb{E}_{a\sim \pi(\cdot \mid x)}[Q^{\pi}(x, a)\nabla_{\varphi}\log \pi(a\mid x)] \right]
$$
<p>
This form is known as the <strong>policy gradient theorem</strong>, it will be useful to characterize the actor critic methods.</p>
<p>We will see that for offline methods, explored in <a href="/notes/rl-function-approximation/#offline-actor-critic">#Offline Actor Critic</a>, we wont need the $\log \pi$ part, as we cannot actively explore the next state from the current state.</p>
<h4 id="policy-gradient-and-the-exponential-family-">Policy Gradient and The Exponential Family üü®<a hidden class="anchor" aria-hidden="true" href="#policy-gradient-and-the-exponential-family-">#</a></h4>
<p>If the policy is characterized by a distribution in the exponential family, it is easy to derive a closed form solution for the policy gradient as above. In this section we briefly discuss some gradients that are part of this family. See <a href="/notes/the-exponential-family/">The Exponential Family</a> for a discussion on distributions of this family.</p>
<p>Recall that if the policy is part of the exponential family we can write it as:
</p>
$$
\pi(a \mid x ) = h(a) \exp(a f_{\varphi}(x) - A(f_\varphi(x)))
$$
<p>And written in this manner the un-baselined policy gradient is:</p>
$$
\begin{align}
\nabla_{\varphi}J_{T}(\varphi) &= \mathbb{E}_{x\sim d_{\pi}}\left[ \mathbb{E}_{a\sim \pi(\cdot \mid x)}[Q^{\pi}(x, a)\nabla_{\varphi}\log \pi(a\mid x)] \right] \\
&= \mathbb{E}_{x\sim d_{\pi}}\left[ \mathbb{E}_{a\sim \pi(\cdot \mid x)}[Q^{\pi}(x, a)\nabla_{\varphi}(a f_{\varphi}(x) - A(f_\varphi(x)))] \right]  \\
&= \mathbb{E}_{x\sim d_{\pi}}\left[ \mathbb{E}_{a\sim \pi(\cdot \mid x)}[Q^{\pi}(x, a)(f_{\varphi}(x) - \nabla_{\varphi}A(f_\varphi(x))\nabla_{\varphi}f_{\varphi}(x))] \right]  \\
\end{align}
$$
<p>We just need to plug the correct values of the exponential family in.</p>
<h2 id="actor-critic-methods">Actor Critic Methods<a hidden class="anchor" aria-hidden="true" href="#actor-critic-methods">#</a></h2>
<p>Actor critic methods describe a different family of approaches that explicitly attempt to jointly improve an approximator network for the value, called critic, and a network for the policy, called actor.
These methods are, as usual, divided into online and offline actor critic methods. We will analyze and present a few different methods and their properties.</p>
<h3 id="online-actor-critic">Online Actor Critic<a hidden class="anchor" aria-hidden="true" href="#online-actor-critic">#</a></h3>
<p>With online methods, we</p>
<h4 id="a-first-algorithm-">A first algorithm üü©<a hidden class="anchor" aria-hidden="true" href="#a-first-algorithm-">#</a></h4>
<p><img loading="lazy" src="/notes/rl-function-approximation-20241129145305087.webp" alt="Image from the Textbook"  />
</p>
<p>The main idea here is to use an approximated $Q-$value to get an approximate policy update.
The main drawback is that we are doing an approximation of an approximation, so the variance of this method should be quite high.
From other point of view, this is somewhat similar to the SARSA update explained in <a href="/notes/tabular-reinforcement-learning/">Tabular Reinforcement Learning</a>
Usually the updates here are bootstrapped too!</p>
<h4 id="the-notion-of-advantage-">The notion of Advantage üü©<a hidden class="anchor" aria-hidden="true" href="#the-notion-of-advantage-">#</a></h4>
<p>This will be the base for the so called A2C algorithm, where we use the notion of Advantage to bound the variance, in a manner similar to what we have done with the baselines.</p>
<p>Advantage is defined as:
</p>
$$
A^{\pi}(x, a) = Q^{\pi}(x, a) - V^{\pi}(x)
$$
<p>
Which describes the gain in taking a specific action in a specific state compared to the average value of the state.
We can prove that
</p>
$$
\forall \pi, x \max_{a} A^{\pi}(x, a) \geq 0
$$
<p>
This is easy to see for deterministic policies, if we choose an $a$ following the policy, then it is 0, but if it could be improved then it has positive value.</p>
<h4 id="advantage-actor-critic-">Advantage Actor Critic üü©<a hidden class="anchor" aria-hidden="true" href="#advantage-actor-critic-">#</a></h4>
<p>This method is very similar to the original Actor Critic method: instead of using the $Q$ estimate to update the critic we use the advantage
This can be view as the standard Policy Gradient method with some special baseline.</p>
<p>Using the notion of advantage instead of the classical policy network, we get the update rule to be:
</p>
$$
\varphi  = \varphi + \alpha_{\varphi}\nabla_{\varphi}\log \pi(a_{t}\mid x_{t})A^{\pi}(x_{t}, a_{t})
$$
<p>
Which is similar to the above actor-critic model, but with lesser variance if we choose the baseline equivalent correctly.</p>
<p>Recall that $V(x) = \mathop{\mathbb{E}}_{a \sim \pi(x)} Q(x, a)$, so this is the baseline that has been chosen for this kind of Actor-Critic network. This is probably difficult to compute, and probably not stable to estimate using MonteCarlo Networks.</p>
<p>One thing that is usually done, is adding the parametrized value network $V(x ; \theta)$ and use that one.</p>
<h4 id="trusted-region-policy-optimization---">Trusted Region Policy Optimization üü©&ndash;<a hidden class="anchor" aria-hidden="true" href="#trusted-region-policy-optimization---">#</a></h4>
<p>This method is a variant of the above method, but it introduces a constraint on the policy update, so that we don&rsquo;t update the policy too much, which can lead to instability in the optimization. For a similar reason, we use a fixed critic for multiple iterations.</p>
<p>The update of the policy becomes:
</p>
$$
\varphi_{t + 1} = \arg\max_{\varphi} J(\varphi) \text{ s.t. } KL(\pi_{\varphi_{t}} \parallel \pi_{\varphi}) \leq \delta
$$
<p>
For a fixed $\delta$ value, which defines the <strong>trust region</strong>, which is also important for the importance weights to be stable.
In this case, the cost function is slightly changed, using importance sampling:
</p>
$$
J(\varphi) = \mathbb{E}_{x, a\sim \pi_{\varphi_{t}}}\left[ \frac{\pi_{\varphi}(a\mid x)}{\pi_{\varphi_{t}}(a\mid x)}A^{\pi_{\varphi_{t}}}(x, a) \right]
$$
<p>One nice thing about TRPO is that it is possible to use this algorithm in an <em>offline</em> fashion as long as the policy can still be trusted (i.e. it satisfies the delta constraint).</p>
<h4 id="proximal-policy-optimization---">Proximal Policy Optimization üü©&ndash;<a hidden class="anchor" aria-hidden="true" href="#proximal-policy-optimization---">#</a></h4>
<p>This method is a variant of the above method, but instead of using the KL divergence, we use a penalty term in the loss function to keep the policy update close to the old policy. This algorithm has also had great influence during training of language models.</p>
<p>One common variant uses the following objective function:
</p>
$$
\varphi_{t + 1} = \arg\max_{\varphi} J(\varphi) - \beta \cdot KL(\pi_{\varphi_{t}} \parallel \pi_{\varphi})
$$
<p>
With $\beta > 0$. Other variants might work on the importance sampling.</p>
<h3 id="offline-actor-critic">Offline Actor Critic<a hidden class="anchor" aria-hidden="true" href="#offline-actor-critic">#</a></h3>
<p>One clear advantage of offline learning algorithms is the possibility of re-using past data.</p>
<h4 id="the-main-idea-">The Main Idea üü®++<a hidden class="anchor" aria-hidden="true" href="#the-main-idea-">#</a></h4>
<p>Recall the DQN loss function:
</p>
$$
\mathcal{L}(\theta) = \mathbb{E}_{s, a, r, s'}\left[ (r + \gamma \max_{a'}Q(s', a'; \theta_{\text{old}}) - Q(s, a; \theta))^{2} \right]
$$
<p>
The main problem with the standard setting is having to maximize over the set of actions, which could be very large, or even continuous.</p>
<p>What we do now is assume we have a &ldquo;rich enough&rdquo; parametrization of $Q$ and just assume that our network is <strong>learning the maximum</strong> by itself. In this way, our loss function becomes:
</p>
$$
\mathcal{L}(\theta) = \mathbb{E}_{s, a, r, s'}\left[ (r + \gamma Q(s', \pi_{\varphi}(s'); \theta_{\text{old}}) - Q(s, a; \theta))^{2} \right]
$$
<p>
Where $\pi_{\varphi}$ is the policy network. If the policy is good enough, by Bellman Optimality condition (see <a href="/notes/markov-processes/">Markov Processes</a>), it will naturally converge to the maximum of the $Q$ function, one problem could be the instability of this double optimization, as we are optimizing for $Q$ and $\pi$ at the same time.</p>
<p>In the policy update phase, instead of taking the maximum over all possible actions, we take the policy that is yielding highest returns following the rule:
</p>
$$
\varphi_{t + 1} = \arg\max_{\varphi} \mathbb{E}_{s, a\sim \pi_{\varphi_{t}}}\left[ Q(s, a; \theta_{t}) \right] = \arg\max_{\varphi} J(\varphi)
$$
<p>
Usually, instead of sampling from $\pi$, we have an offline <em>buffer</em> of trajectories and sample uniformly from that buffer, but nothing prevents us to sample in an online fashion.
So we write:
</p>
$$
\nabla_{\varphi}J(\varphi) = \mathbb{E}_{s \sim \mu}\left[ Q(s, \pi_{\varphi}(s); \theta_{t}) \right]
$$
<p>And we can get an estimate in a similar manner for the $Q$ network:
</p>
$$
\nabla_{\varphi}Q(s, a; \theta) = D\varphi \pi_{\varphi}(s) \cdot  \nabla_{a}Q(s, a; \theta) \mid_{a = \pi_{\varphi}(s)}
$$
<p>Note that here we are not considering the $\log \pi(a \mid x)$ to compute the gradient for the policy. My personal take is just that we don&rsquo;t know how to compute this value (we don&rsquo;t have a forward step to compute it), so we just ignore it.
<a href="https://chatgpt.com/share/6782694b-0f48-8009-aa8b-9942903aa817">Here</a> we can see a discussion on the use of $\log \pi(a \mid x)$.</p>
<h4 id="deep-deterministic-policy-gradient-">Deep Deterministic Policy Gradient üü®++<a hidden class="anchor" aria-hidden="true" href="#deep-deterministic-policy-gradient-">#</a></h4>
<p>Putting everything together from the section before we get the DDPG algorithm:
<img src="/images/notes/RL Function Approximation-20241220171826356.webp" alt="RL Function Approximation-20241220171826356"></p>
<p>It is possible to extend this algorithm with TD3, which is a variant of the above algorithm that uses a double critic to estimate the Q-value, which helps in reducing the overestimation bias.</p>
<h3 id="on-exploration">On Exploration<a hidden class="anchor" aria-hidden="true" href="#on-exploration">#</a></h3>
<p>With the offline methods we have just presented, the usual solution for exploration is what is called <em>Gaussian Noise Dithering</em>: this method just adds some noise to the output of the policy $\pi_{\varphi}$ in continuous settings.
However, these methods often suffer from not exploring enough. The following section presents a method that attempts to ease this problem.
Some methods of exploration as are also cited in <a href="/notes/planning/">Planning</a>.</p>
<h4 id="maximum-entropy-reinforcement-learning-">Maximum Entropy Reinforcement Learning üü®<a hidden class="anchor" aria-hidden="true" href="#maximum-entropy-reinforcement-learning-">#</a></h4>
<p>Maximum Entropy Reinforcement Learning (MERL) adds a <em>regularizer</em> that prevents the policy to become too narrow, or deterministic, for a single action. This is easily considered during the evaluation of the cost:
</p>
$$
J(\varphi) = \mathbb{E}_{s, a\sim \pi_{\varphi}}\left[ Q(s, a; \theta) + \alpha \mathcal{H}(\pi_{\varphi}(\cdot \mid s)) \right]
$$
<p>
Where the parameter $\alpha$ is a hyper-parameter that controls how much the entropy of the policy weights.</p>
<h4 id="trust-region-policy-optimization-trpo">Trust Region Policy Optimization (TRPO)<a hidden class="anchor" aria-hidden="true" href="#trust-region-policy-optimization-trpo">#</a></h4>
<p>To address the instability of optimizing both $Q$ and $\pi$ simultaneously, Trust Region Policy Optimization (TRPO) introduces a constrained optimization framework. The primary idea behind TRPO is to update the policy in such a way that it stays within a <em>trust region</em>, ensuring that each update does not deviate excessively from the current policy. This prevents performance degradation due to overly large updates.</p>
<p><strong>Objective Function:</strong>
TRPO optimizes the following constrained objective:</p>
$$
\max_{\varphi} \mathbb{E}_{s \sim \mu, a \sim \pi_{\varphi_t}}\left[ \frac{\pi_{\varphi}(a \mid s)}{\pi_{\varphi_t}(a \mid s)} A_{\pi_{\varphi_t}}(s, a) \right]
$$
<p>subject to:</p>
$$
\mathbb{E}_{s \sim \mu}\left[ D_{\text{KL}}(\pi_{\varphi_t}(\cdot \mid s) \parallel \pi_{\varphi}(\cdot \mid s)) \right] \leq \delta,
$$
<p>where $A_{\pi_{\varphi_t}}(s, a)$ is the advantage function, $D_{\text{KL}}$ is the Kullback-Leibler divergence, and $\delta$ is a small positive constant controlling the size of the update.</p>
<h4 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)<a hidden class="anchor" aria-hidden="true" href="#proximal-policy-optimization-ppo">#</a></h4>
<p>Proximal Policy Optimization (PPO) simplifies TRPO by avoiding the computational complexity of solving a constrained optimization problem. PPO achieves similar benefits by clipping the policy update directly within the objective function.</p>
<p><strong>Objective Function</strong>
PPO modifies the policy update by introducing a clipped surrogate objective:</p>
$$
\mathcal{L}^{\text{PPO}}(\varphi) = \mathbb{E}_{s, a}\left[ \min\left( r_t(\varphi) A_{\pi_{\varphi_t}}(s, a), \text{clip}(r_t(\varphi), 1 - \epsilon, 1 + \epsilon) A_{\pi_{\varphi_t}}(s, a) \right) \right],
$$
<p>where:</p>
$$
r_t(\varphi) = \frac{\pi_{\varphi}(a \mid s)}{\pi_{\varphi_t}(a \mid s)},
$$
<p>and $\epsilon$ is a small hyperparameter (e.g., $\epsilon = 0.2$) controlling the extent of clipping.</p>
<table>
<thead>
<tr>
<th><strong>Algorithm</strong></th>
<th><strong>Advantages</strong></th>
<th><strong>Challenges</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>TRPO</td>
<td>Stable updates, trust region enforcement</td>
<td>Computationally expensive, complex</td>
</tr>
<tr>
<td>PPO</td>
<td>Simplicity, robust performance</td>
<td>Sensitive to hyperparameters ($\epsilon$)</td>
</tr>
</tbody>
</table>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">‚ûïProbabilistic-Artificial-Intelligence</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on x"
            href="https://x.com/intent/tweet/?text=RL%20Function%20Approximation&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f&amp;hashtags=%e2%9e%95probabilistic-artificial-intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f&amp;title=RL%20Function%20Approximation&amp;summary=RL%20Function%20Approximation&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f&title=RL%20Function%20Approximation">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on whatsapp"
            href="https://api.whatsapp.com/send?text=RL%20Function%20Approximation%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on telegram"
            href="https://telegram.me/share/url?text=RL%20Function%20Approximation&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RL Function Approximation on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=RL%20Function%20Approximation&u=https%3a%2f%2fflecart.github.io%2fnotes%2frl-function-approximation%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
