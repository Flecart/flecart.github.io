<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Generative Adversarial Networks | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machine-perception">
<meta name="description" content="Generative Adversarial Network has been introduced in 2014 by Ian Goodfellow (at that time they where still gray and white). Now the images have been improved so much with Diffusion Models. This idea has been considered by Yann LeCun as one of the most important ideas. Nowadays (2025) they are still used for super-resolution and other applications, but it has still some limitations (mainly stability), and now has good competition against other models.
The resolution purported by GAN is much higher than VAE (see Autoencoders#Variational Autoencoders). This is a easy plugin to improve the results of other models (VAE, flow, Diffusion). Also ChatGPT has some sort of adversarial learning for example, not explained in the same manner as here.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/generative-adversarial-networks/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/generative-adversarial-networks/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/generative-adversarial-networks/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Generative Adversarial Networks">
  <meta property="og:description" content="Generative Adversarial Network has been introduced in 2014 by Ian Goodfellow (at that time they where still gray and white). Now the images have been improved so much with Diffusion Models. This idea has been considered by Yann LeCun as one of the most important ideas. Nowadays (2025) they are still used for super-resolution and other applications, but it has still some limitations (mainly stability), and now has good competition against other models. The resolution purported by GAN is much higher than VAE (see Autoencoders#Variational Autoencoders). This is a easy plugin to improve the results of other models (VAE, flow, Diffusion). Also ChatGPT has some sort of adversarial learning for example, not explained in the same manner as here.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="Machine-Perception">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Generative Adversarial Networks">
<meta name="twitter:description" content="Generative Adversarial Network has been introduced in 2014 by Ian Goodfellow (at that time they where still gray and white). Now the images have been improved so much with Diffusion Models. This idea has been considered by Yann LeCun as one of the most important ideas. Nowadays (2025) they are still used for super-resolution and other applications, but it has still some limitations (mainly stability), and now has good competition against other models.
The resolution purported by GAN is much higher than VAE (see Autoencoders#Variational Autoencoders). This is a easy plugin to improve the results of other models (VAE, flow, Diffusion). Also ChatGPT has some sort of adversarial learning for example, not explained in the same manner as here.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Generative Adversarial Networks",
      "item": "https://flecart.github.io/notes/generative-adversarial-networks/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Generative Adversarial Networks",
  "name": "Generative Adversarial Networks",
  "description": "Generative Adversarial Network has been introduced in 2014 by Ian Goodfellow (at that time they where still gray and white). Now the images have been improved so much with Diffusion Models. This idea has been considered by Yann LeCun as one of the most important ideas. Nowadays (2025) they are still used for super-resolution and other applications, but it has still some limitations (mainly stability), and now has good competition against other models. The resolution purported by GAN is much higher than VAE (see Autoencoders#Variational Autoencoders). This is a easy plugin to improve the results of other models (VAE, flow, Diffusion). Also ChatGPT has some sort of adversarial learning for example, not explained in the same manner as here.\n",
  "keywords": [
    "machine-perception"
  ],
  "articleBody": "Generative Adversarial Network has been introduced in 2014 by Ian Goodfellow (at that time they where still gray and white). Now the images have been improved so much with Diffusion Models. This idea has been considered by Yann LeCun as one of the most important ideas. Nowadays (2025) they are still used for super-resolution and other applications, but it has still some limitations (mainly stability), and now has good competition against other models. The resolution purported by GAN is much higher than VAE (see Autoencoders#Variational Autoencoders). This is a easy plugin to improve the results of other models (VAE, flow, Diffusion). Also ChatGPT has some sort of adversarial learning for example, not explained in the same manner as here.\nGeneral Idea Here we have two main networks that are jointly trained:\nGenerator: this is a neural network that takes a random vector as input and generates a fake image. The goal of the generator is to produce images that are indistinguishable from real images. Discriminator: this is a neural network that takes an image as input and predicts whether it is real or fake. The goal of the discriminator is to correctly classify images as real or fake. Adversarial Loss: the generator and discriminator are trained in an adversarial manner. The generator tries to fool the discriminator, while the discriminator tries to correctly classify images. This creates a game-like scenario where both networks improve over time. This has some sort of similarity of natural evolution when you have a predator and a prey and they coevolve to surpass each other’s strategy. We can define this more formally:\nGenerator: $G: \\mathbb{R}^{L} \\to \\mathbb{R}^{Q}$ where $Q \\gg L$, and the discriminator is a function $D: \\mathbb{R}^{Q} \\to [0, 1]$.\nTraining a Generative Adversarial Network. Training Process 🟩 $$ \\min_{G} \\max_{D} V(G,D) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log(1 - D(G(z)))] $$This is the original loss function, but it has some problems (like vanishing gradients). The generator tries to minimize this function, while the discriminator tries to maximize it.\nTheory shows that the stable point $p_{\\text{model}} = p_{\\text{data}}$ but only with $G$ and $D$ have infinity capacity (they can match any data distribution, which is a quite strong assumption). In practice optimizing both jointly is quite expensive computationally. Usually you do $k$ iterations for D and $1$ for $G$ because we want to have informative output for $G$. It looks to me that this idea is quite similar to the Actor Critic model see RL Function Approximation.\nProblems with Likelihood Given a generative model, we ask here, is likelihood of a certain sample a good indicator of it?\nSometimes we have poor samples yet good likelihood, this is usually the case when you have a strong spike of likelihood values, irrelevant of the noise (e.g. $\\log(0.01p(x)) = \\log p(x) - \\log 100$, if $p(x)$ is proportional to $d$, for independent dimensions, the first value could be very high, nonetheless having poor sample quality, this is an example from eq 11 of https://arxiv.org/pdf/1511.01844) Sometimes we have low likelihood yet good samples: a simple example is test data evaluation after over-fitting, models would give low likelihood to good samples, since they have not been seen during training. This means that likelihood alone is not a good metric to compare the models. GAN Training Algorithm 🟩 While not converged do:\n$$\\nabla_{\\Theta_D} \\frac{1}{N} \\sum_{i=1}^{N} [\\log(D(x^{(i)})) + \\log(1 - D(G(z^{(i)})))]$$ * $\\Theta_D$ represents the parameters of the discriminator. * $D(x)$ is the discriminator’s output for a real sample $x$ (probability that $x$ is real). * $G(z)$ is the generator’s output for a noise sample $z$ (a generated fake sample). * The goal is to maximize this objective, making the discriminator better at distinguishing real from fake samples.\nFreeze Discriminator (D). Draw $N$ noise samples $\\{z^{(1)}, ..., z^{(N)}\\}$ from $p(z)$.\n$$\\nabla_{\\Theta_G} \\frac{1}{N} \\sum_{i=1}^{N} [\\log(1 - D(G(z^{(i)})))]$$ $\\Theta_G$ represents the parameters of the generator. The goal is to minimize this objective, making the generator better at fooling the discriminator into thinking its generated samples are real. Explanation:\nThe discriminator tries to learn to distinguish between real data and fake data generated by the generator. The generator tries to learn to produce fake data that is indistinguishable from real data, thus fooling the discriminator. The process typically involves alternating between training the discriminator for $k$ steps and then training the generator for one step (although other ratios are possible). This ensures that the discriminator doesn’t become too strong too quickly, preventing the generator from learning useful gradients. The training continues until a convergence criterion is met, ideally when the generator produces samples that the discriminator can no longer reliably classify as fake.\nOptimal behaviour requirements We need to have a specific format for $D$ to be optimal, which is: TODO.\nGAN optimizes for Jensen-Shannon Divergence 🟩 $$ D_{JS}(P \\| Q) = \\frac{1}{2} D_{KL}(P \\| M) + \\frac{1}{2} D_{KL}(Q \\| M)$$ where $M = \\frac{1}{2}(P + Q)$. One can prove that the min max game above is the same as optimizing for the JS divergence.\nThe value function of the optimal discriminator $D^*$ in a Generative Adversarial Network (GAN) is given by:\n$$V(G, D^*) = \\mathbb{E}_{x \\sim p_d} \\left[ \\log \\left( \\frac{p_d(x)}{p_d(x) + p_m(x)} \\right) \\right] + \\mathbb{E}_{x \\sim p_m} \\left[ \\log \\left( \\frac{p_m(x)}{p_d(x) + p_m(x)} \\right) \\right]$$ minimizing this divergence equals maximizing some likelihood.\n$$ \\begin{align*} V(G, D^*) \u0026= \\mathbb{E}_{x \\sim p_d} \\left[ \\log \\left( \\frac{p_d(x)}{(p_d(x) + p_m(x))/2} \\cdot \\frac{1}{2} \\right) \\right] + \\mathbb{E}_{x \\sim p_m} \\left[ \\log \\left( \\frac{p_m(x)}{(p_d(x) + p_m(x))/2} \\cdot \\frac{1}{2} \\right) \\right] \\\\ \u0026= \\mathbb{E}_{x \\sim p_d} \\left[ \\log \\left( \\frac{2 p_d(x)}{p_d(x) + p_m(x)} \\right) - \\log(2) \\right] + \\mathbb{E}_{x \\sim p_m} \\left[ \\log \\left( \\frac{2 p_m(x)}{p_d(x) + p_m(x)} \\right) - \\log(2) \\right] \\\\ \u0026= -\\log(2) + \\mathbb{E}_{x \\sim p_d} \\left[ \\log \\left( \\frac{2 p_d(x)}{p_d(x) + p_m(x)} \\right) \\right] - \\log(2) + \\mathbb{E}_{x \\sim p_m} \\left[ \\log \\left( \\frac{2 p_m(x)}{p_d(x) + p_m(x)} \\right) \\right] \\\\ \u0026= -2\\log(2) + \\int_x p_d(x) \\log \\left( \\frac{2 p_d(x)}{p_d(x) + p_m(x)} \\right) dx + \\int_x p_m(x) \\log \\left( \\frac{2 p_m(x)}{p_d(x) + p_m(x)} \\right) dx \\\\ \u0026= -2\\log(2) + \\int_x p_d(x) \\log \\left( \\frac{p_d(x)}{(p_d(x) + p_m(x))/2} \\right) dx + \\int_x p_m(x) \\log \\left( \\frac{p_m(x)}{(p_d(x) + p_m(x))/2} \\right) dx \\\\ \u0026= -2\\log(2) + D_{KL} \\left( p_d(x) \\middle\\| \\frac{p_d(x) + p_m(x)}{2} \\right) + D_{KL} \\left( p_m(x) \\middle\\| \\frac{p_d(x) + p_m(x)}{2} \\right) \\\\ \u0026= -2\\log(2) + 2 D_{JS}(p_d(x) \\| p_m(x)) \\end{align*} $$Where:\n$p_d(x)$ is the true data distribution. $p_m(x)$ is the distribution of the generated samples (implicitly defined by the generator $G$). $D^*$ is the optimal discriminator. $\\mathbb{E}_{x \\sim p}$ denotes the expectation over the distribution $p$. $D_{KL}(p \\| q)$ is the Kullback-Leibler divergence between distributions $p$ and $q$. $D_{JS}(p \\| q)$ is the Jensen-Shannon divergence between distributions $p$ and $q$. Key Takeaway:\nThe maximum value of the GAN’s discriminator loss is related to the Jensen-Shannon divergence between the real data distribution and the distribution of the generated samples. Minimizing the GAN loss (for the generator) corresponds to minimizing the Jensen-Shannon divergence between these two distributions, ideally leading $p_m(x)$ to become equal to $p_d(x)$.\nTranining Issues 🟩 Vanishing Gradients (the same problem we have seen in Recurrent Neural Networks). Using non-saturation loss. Model Collapse: this is a problem where the generator produces a limited variety of samples, leading to a lack of diversity in the generated images. This can happen when the generator finds a small set of images that fool the discriminator, but doesn’t explore other possibilities. Unrolled GAN: they basically do less updates, more diffused iterations (unrolling parameter with gradient accumulation). It is very easy to see that when the discriminator is too good, then the signal is very low for the generator to learn fast, leading to slow convergence. One idea to circumvent this is to have smoother discriminator lines, so that the signal to the generator is stronger (See Mao et al 2016 or Sønderby) Another is changing the signal to maximize the real image discrimination (changes the error signal by a lot!) Training Instability: GANs can be sensitive to hyperparameters, and small changes in the learning rate or architecture can lead to large changes in performance. This can make training GANs difficult and unpredictable Gradient penalty: basically adding a penalty to the gradient of the discriminator’s output with respect to its input, which helps stabilize the training process. This is also known as Wasserstein GAN, since it is inspired by that distance. GAN applications StyleGAN 🟨– (Karras et al. 2019). They gradually trained stacked model from low resolution to high resolution, both generator and discriminator jointly. downsample the original dataset to train at that resolution.\n$$ c' = \\gamma c + \\beta $$ This is called AdaIN (Instance normalization and feature modulation), different kind of normalization, close to batch normalization.\nImage from StyleGAN paper\n#### Image to Image translation 🟩-- Introduced in Pix2Pix (Isola et al. 2018). With this problem we want to start from one kind of image, like a segmentation, and output the original image that created it, or from real image to a segmentation image, which is a cool idea. We add here a L1 loss to the original loss. One drawback is that we need paired images in this case, cycleGAN solves this problem, see (Zhu et al. 2020) (guarantee that we can map back to the original image, which is another added loss), then extended to bycycleGAN (one to many, instead of one-to-one, others extended to video-to-video and autoregressive modelling).\n$$ \\mathcal{L}(G, F, D_{X}, D_{Y}) = \\mathcal{L}_{GAN}(G, D_{Y}, X, Y) + \\mathcal{L}_{GAN}(F, D_{X}, Y, X) + \\lambda \\mathcal{L}_{cyc}(G, F) $$ Where we have a cycle consistent loss, and $X$ and $Y$ are start and finish images. The idea is that when a starting image is translated to an end image and back, they should be close to each other.\nRegarding CycleGAN: Here we have pairs of generators and discriminators. Ones should generate the first kind of image, others the second kind of image, same for the generators.\nReferences [1] Karras et al. “A Style-Based Generator Architecture for Generative Adversarial Networks” arXiv preprint arXiv:1812.04948 2019\n[2] Zhu et al. “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks” arXiv preprint arXiv:1703.10593 2020\n[3] Isola et al. “Image-to-Image Translation with Conditional Adversarial Networks” arXiv preprint arXiv:1611.07004 2018\n",
  "wordCount" : "1699",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/generative-adversarial-networks/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Generative Adversarial Networks
    </h1>
    <div class="post-meta">8 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul><ul>
                <li>
                    <a href="#general-idea" aria-label="General Idea">General Idea</a></li></ul>
                    
                <li>
                    <a href="#training-a-generative-adversarial-network" aria-label="Training a Generative Adversarial Network.">Training a Generative Adversarial Network.</a><ul>
                        
                <li>
                    <a href="#training-process-" aria-label="Training Process 🟩">Training Process 🟩</a></li>
                <li>
                    <a href="#problems-with-likelihood" aria-label="Problems with Likelihood">Problems with Likelihood</a></li>
                <li>
                    <a href="#gan-training-algorithm-" aria-label="GAN Training Algorithm 🟩">GAN Training Algorithm 🟩</a></li>
                <li>
                    <a href="#optimal-behaviour-requirements" aria-label="Optimal behaviour requirements">Optimal behaviour requirements</a></li>
                <li>
                    <a href="#gan-optimizes-for-jensen-shannon-divergence-" aria-label="GAN optimizes for Jensen-Shannon Divergence 🟩">GAN optimizes for Jensen-Shannon Divergence 🟩</a></li>
                <li>
                    <a href="#tranining-issues-" aria-label="Tranining Issues 🟩">Tranining Issues 🟩</a></li></ul>
                </li>
                <li>
                    <a href="#gan-applications" aria-label="GAN applications">GAN applications</a><ul>
                        
                <li>
                    <a href="#stylegan---" aria-label="StyleGAN 🟨&ndash;">StyleGAN 🟨&ndash;</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Generative Adversarial Network has been introduced in 2014 by Ian Goodfellow (at that time they where still gray and white). Now the images have been improved so much with <a href="/notes/diffusion-models">Diffusion Models</a>. This idea has been considered by Yann LeCun as one of the most important ideas. Nowadays (2025) they are still used for super-resolution and other applications, but it has still some limitations (mainly stability), and now has good competition against other models.
The resolution purported by GAN is much higher than VAE (see <a href="/notes/autoencoders#variational-autoencoders">Autoencoders#Variational Autoencoders</a>). This is a easy plugin to improve the results of other models (VAE, flow, Diffusion). Also ChatGPT has some sort of adversarial learning for example, not explained in the same manner as here.</p>
<h4 id="general-idea">General Idea<a hidden class="anchor" aria-hidden="true" href="#general-idea">#</a></h4>
<p>Here we have two main networks that are jointly trained:</p>
<ul>
<li><strong>Generator</strong>: this is a neural network that takes a random vector as input and generates a fake image. The goal of the generator is to produce images that are indistinguishable from real images.</li>
<li><strong>Discriminator</strong>: this is a neural network that takes an image as input and predicts whether it is real or fake. The goal of the discriminator is to correctly classify images as real or fake.</li>
<li><strong>Adversarial Loss</strong>: the generator and discriminator are trained in an adversarial manner. The generator tries to fool the discriminator, while the discriminator tries to correctly classify images. This creates a game-like scenario where both networks improve over time.
This has some sort of similarity of natural evolution when you have a predator and a prey and they coevolve to surpass each other&rsquo;s strategy.</li>
</ul>
<p>We can define this more formally:</p>
<p>Generator: $G: \mathbb{R}^{L} \to \mathbb{R}^{Q}$ where $Q \gg L$, and the discriminator is a function $D: \mathbb{R}^{Q} \to [0, 1]$.</p>
<h3 id="training-a-generative-adversarial-network">Training a Generative Adversarial Network.<a hidden class="anchor" aria-hidden="true" href="#training-a-generative-adversarial-network">#</a></h3>
<h4 id="training-process-">Training Process 🟩<a hidden class="anchor" aria-hidden="true" href="#training-process-">#</a></h4>
$$
\min_{G} \max_{D} V(G,D) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log(1 - D(G(z)))]
$$<p>This is the original loss function, but it has some problems (like vanishing gradients). The generator tries to minimize this function, while the discriminator tries to maximize it.</p>
<p>Theory shows that the stable point  $p_{\text{model}} = p_{\text{data}}$ but only with $G$ and $D$ have infinity capacity (they can match any data distribution, which is a quite strong assumption). In practice optimizing both jointly is quite expensive computationally.
Usually you do $k$ iterations for D and $1$ for $G$ because we want to have informative output for $G$.
It looks to me that this idea is quite similar to the Actor Critic model see <a href="/notes/rl-function-approximation">RL Function Approximation</a>.</p>
<h4 id="problems-with-likelihood">Problems with Likelihood<a hidden class="anchor" aria-hidden="true" href="#problems-with-likelihood">#</a></h4>
<p>Given a generative model, we ask here, is likelihood of a certain sample a good indicator of it?</p>
<ul>
<li>Sometimes we have <strong>poor samples yet good likelihood</strong>, this is usually the case when you have a strong spike of likelihood values, irrelevant of the noise (e.g. $\log(0.01p(x)) = \log p(x) - \log 100$, if $p(x)$ is proportional to $d$, for independent dimensions, the first value could be very high, nonetheless having poor sample quality, this is an example from eq 11 of <a href="https://arxiv.org/pdf/1511.01844">https://arxiv.org/pdf/1511.01844</a>)</li>
<li>Sometimes we have <strong>low likelihood yet good samples</strong>: a simple example is test data evaluation after over-fitting, models would give low likelihood to good samples, since they have not been seen during training.
This means that likelihood alone is not a good metric to compare the models.</li>
</ul>
<h4 id="gan-training-algorithm-">GAN Training Algorithm 🟩<a hidden class="anchor" aria-hidden="true" href="#gan-training-algorithm-">#</a></h4>
<p><strong>While not converged do:</strong></p>
<ol>
<li>
$$\nabla_{\Theta_D} \frac{1}{N} \sum_{i=1}^{N} [\log(D(x^{(i)})) + \log(1 - D(G(z^{(i)})))]$$<p>
* $\Theta_D$ represents the parameters of the discriminator.
* $D(x)$ is the discriminator&rsquo;s output for a real sample $x$ (probability that $x$ is real).
* $G(z)$ is the generator&rsquo;s output for a noise sample $z$ (a generated fake sample).
* The goal is to maximize this objective, making the discriminator better at distinguishing real from fake samples.</p>
</li>
<li>
<p><strong>Freeze Discriminator (D). Draw $N$ noise samples $\{z^{(1)}, ..., z^{(N)}\}$ from $p(z)$.</strong></p>
</li>
<li>
$$\nabla_{\Theta_G} \frac{1}{N} \sum_{i=1}^{N} [\log(1 - D(G(z^{(i)})))]$$<ul>
<li>$\Theta_G$ represents the parameters of the generator.</li>
<li>The goal is to minimize this objective, making the generator better at fooling the discriminator into thinking its generated samples are real.</li>
</ul>
</li>
</ol>
<p><strong>Explanation:</strong></p>
<ul>
<li>The <strong>discriminator</strong> tries to learn to distinguish between real data and fake data generated by the generator.</li>
<li>The <strong>generator</strong> tries to learn to produce fake data that is indistinguishable from real data, thus fooling the discriminator.</li>
</ul>
<p>The process typically involves alternating between training the discriminator for $k$ steps and then training the generator for one step (although other ratios are possible). This ensures that the discriminator doesn&rsquo;t become too strong too quickly, preventing the generator from learning useful gradients. The training continues until a convergence criterion is met, ideally when the generator produces samples that the discriminator can no longer reliably classify as fake.</p>
<h4 id="optimal-behaviour-requirements">Optimal behaviour requirements<a hidden class="anchor" aria-hidden="true" href="#optimal-behaviour-requirements">#</a></h4>
<p>We need to have a specific format for $D$ to be optimal, which is:
TODO.</p>
<h4 id="gan-optimizes-for-jensen-shannon-divergence-">GAN optimizes for Jensen-Shannon Divergence 🟩<a hidden class="anchor" aria-hidden="true" href="#gan-optimizes-for-jensen-shannon-divergence-">#</a></h4>
$$
D_{JS}(P \| Q) = \frac{1}{2} D_{KL}(P \| M) + \frac{1}{2} D_{KL}(Q \| M)$$<p>
where $M = \frac{1}{2}(P + Q)$.
One can prove that the min max game above is the same as optimizing for the JS divergence.</p>
<p>The value function of the optimal discriminator $D^*$ in a Generative Adversarial Network (GAN) is given by:</p>
$$V(G, D^*) = \mathbb{E}_{x \sim p_d} \left[ \log \left( \frac{p_d(x)}{p_d(x) + p_m(x)} \right) \right] + \mathbb{E}_{x \sim p_m} \left[ \log \left( \frac{p_m(x)}{p_d(x) + p_m(x)} \right) \right]$$<p>
minimizing this divergence equals maximizing some likelihood.</p>
$$
\begin{align*} V(G, D^*) &= \mathbb{E}_{x \sim p_d} \left[ \log \left( \frac{p_d(x)}{(p_d(x) + p_m(x))/2} \cdot \frac{1}{2} \right) \right] + \mathbb{E}_{x \sim p_m} \left[ \log \left( \frac{p_m(x)}{(p_d(x) + p_m(x))/2} \cdot \frac{1}{2} \right) \right] \\ &= \mathbb{E}_{x \sim p_d} \left[ \log \left( \frac{2 p_d(x)}{p_d(x) + p_m(x)} \right) - \log(2) \right] + \mathbb{E}_{x \sim p_m} \left[ \log \left( \frac{2 p_m(x)}{p_d(x) + p_m(x)} \right) - \log(2) \right] \\ &= -\log(2) + \mathbb{E}_{x \sim p_d} \left[ \log \left( \frac{2 p_d(x)}{p_d(x) + p_m(x)} \right) \right] - \log(2) + \mathbb{E}_{x \sim p_m} \left[ \log \left( \frac{2 p_m(x)}{p_d(x) + p_m(x)} \right) \right] \\ &= -2\log(2) + \int_x p_d(x) \log \left( \frac{2 p_d(x)}{p_d(x) + p_m(x)} \right) dx + \int_x p_m(x) \log \left( \frac{2 p_m(x)}{p_d(x) + p_m(x)} \right) dx \\ &= -2\log(2) + \int_x p_d(x) \log \left( \frac{p_d(x)}{(p_d(x) + p_m(x))/2} \right) dx + \int_x p_m(x) \log \left( \frac{p_m(x)}{(p_d(x) + p_m(x))/2} \right) dx \\ &= -2\log(2) + D_{KL} \left( p_d(x) \middle\| \frac{p_d(x) + p_m(x)}{2} \right) + D_{KL} \left( p_m(x) \middle\| \frac{p_d(x) + p_m(x)}{2} \right) \\ &= -2\log(2) + 2 D_{JS}(p_d(x) \| p_m(x)) \end{align*}
$$<p>Where:</p>
<ul>
<li>$p_d(x)$ is the true data distribution.</li>
<li>$p_m(x)$ is the distribution of the generated samples (implicitly defined by the generator $G$).</li>
<li>$D^*$ is the optimal discriminator.</li>
<li>$\mathbb{E}_{x \sim p}$ denotes the expectation over the distribution $p$.</li>
<li>$D_{KL}(p \| q)$ is the Kullback-Leibler divergence between distributions $p$ and $q$.</li>
<li>$D_{JS}(p \| q)$ is the Jensen-Shannon divergence between distributions $p$ and $q$.</li>
</ul>
<p><strong>Key Takeaway:</strong></p>
<p>The maximum value of the GAN&rsquo;s discriminator loss is related to the Jensen-Shannon divergence between the real data distribution and the distribution of the generated samples. Minimizing the GAN loss (for the generator) corresponds to minimizing the Jensen-Shannon divergence between these two distributions, ideally leading $p_m(x)$ to become equal to $p_d(x)$.</p>
<h4 id="tranining-issues-">Tranining Issues 🟩<a hidden class="anchor" aria-hidden="true" href="#tranining-issues-">#</a></h4>
<ul>
<li><strong>Vanishing Gradients</strong> (the same problem we have seen in <a href="/notes/recurrent-neural-networks">Recurrent Neural Networks</a>).
<ul>
<li>Using <strong>non-saturation loss</strong>.</li>
</ul>
</li>
<li><strong>Model Collapse</strong>: this is a problem where the generator produces a <strong>limited</strong> variety of samples, leading to a lack of diversity in the generated images. This can happen when the generator finds a small set of images that fool the discriminator, but doesn&rsquo;t explore other possibilities.
<ul>
<li>Unrolled GAN: they basically do less updates, more diffused iterations (unrolling parameter with gradient accumulation).</li>
<li>It is very easy to see that when the discriminator is too good, then the signal is very low for the generator to learn fast, leading to slow convergence.
<ul>
<li>One idea to circumvent this is to have smoother discriminator lines, so that the signal to the generator is stronger (See Mao et al 2016 or Sønderby)</li>
<li>Another is changing the signal to maximize the real image discrimination (changes the error signal by a lot!)</li>
</ul>
</li>
</ul>
</li>
<li>Training Instability: GANs can be sensitive to hyperparameters, and small changes in the learning rate or architecture can lead to large changes in performance. This can make training GANs difficult and unpredictable
<ul>
<li><strong>Gradient penalty</strong>: basically adding a penalty to the gradient of the discriminator&rsquo;s output with respect to its input, which helps stabilize the training process. This is also known as <strong>Wasserstein GAN</strong>, since it is inspired by that distance.</li>
</ul>
</li>
</ul>
<h3 id="gan-applications">GAN applications<a hidden class="anchor" aria-hidden="true" href="#gan-applications">#</a></h3>
<h4 id="stylegan---">StyleGAN 🟨&ndash;<a hidden class="anchor" aria-hidden="true" href="#stylegan---">#</a></h4>
<p><a href="http://arxiv.org/abs/1812.04948">(Karras et al. 2019)</a>.
They gradually trained  stacked model from low resolution to high resolution, both generator and discriminator jointly. downsample the original dataset to train at that resolution.</p>
$$
c' = \gamma c + \beta
$$<p>
This is called <strong>AdaIN</strong> (Instance normalization and feature modulation), different kind of normalization, close to batch normalization.</p>
<figure class="center">
<img src="/images/notes/Generative Adversarial Networks-20250420223803048.webp" style="width: 100%"   alt="Generative Adversarial Networks-20250420223803048" title="Generative Adversarial Networks-20250420223803048"/>
<figcaption><p style="text-align:center;">Image from StyleGAN paper</p></figcaption>
</figure>
#### Image to Image translation 🟩--
Introduced in Pix2Pix <a href="http://arxiv.org/abs/1611.07004">(Isola et al. 2018)</a>.
With this problem we want to start from one kind of image, like a segmentation, and output the original image that created it, or from real image to a segmentation image, which is a cool idea.
<p>We add here a L1 loss to the original loss. One drawback is that we need <strong>paired</strong> images in this case, cycleGAN solves this problem, see <a href="http://arxiv.org/abs/1703.10593">(Zhu et al. 2020)</a> (guarantee that we can map back to the original image, which is another added loss), then extended to bycycleGAN (one to many, instead of one-to-one, others extended to video-to-video and autoregressive modelling).</p>
$$
\mathcal{L}(G, F, D_{X}, D_{Y}) = \mathcal{L}_{GAN}(G, D_{Y}, X, Y) + \mathcal{L}_{GAN}(F, D_{X}, Y, X) + \lambda \mathcal{L}_{cyc}(G, F)
$$<p>
Where we have a cycle consistent loss, and $X$ and $Y$ are start and finish images. The idea is that when a starting image is translated to an end image and back, they should be close to each other.</p>
<p>Regarding CycleGAN:
Here we have pairs of generators and discriminators.
Ones should generate the first kind of image, others the second kind of image, same for the generators.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Karras et al. <a href="http://arxiv.org/abs/1812.04948">“A Style-Based Generator Architecture for Generative Adversarial Networks”</a> arXiv preprint arXiv:1812.04948 2019</p>
<p>[2] Zhu et al. <a href="http://arxiv.org/abs/1703.10593">“Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks”</a> arXiv preprint arXiv:1703.10593 2020</p>
<p>[3] Isola et al. <a href="http://arxiv.org/abs/1611.07004">“Image-to-Image Translation with Conditional Adversarial Networks”</a> arXiv preprint arXiv:1611.07004 2018</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machine-perception/">Machine-Perception</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Generative Adversarial Networks on x"
            href="https://x.com/intent/tweet/?text=Generative%20Adversarial%20Networks&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fgenerative-adversarial-networks%2f&amp;hashtags=machine-perception">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Generative Adversarial Networks on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fgenerative-adversarial-networks%2f&amp;title=Generative%20Adversarial%20Networks&amp;summary=Generative%20Adversarial%20Networks&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fgenerative-adversarial-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Generative Adversarial Networks on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fgenerative-adversarial-networks%2f&title=Generative%20Adversarial%20Networks">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Generative Adversarial Networks on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fgenerative-adversarial-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Generative Adversarial Networks on whatsapp"
            href="https://api.whatsapp.com/send?text=Generative%20Adversarial%20Networks%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fgenerative-adversarial-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Generative Adversarial Networks on telegram"
            href="https://telegram.me/share/url?text=Generative%20Adversarial%20Networks&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fgenerative-adversarial-networks%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Generative Adversarial Networks on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Generative%20Adversarial%20Networks&u=https%3a%2f%2fflecart.github.io%2fnotes%2fgenerative-adversarial-networks%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
