<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Support Vector Machines | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning">
<meta name="description" content="This is a quite good resource about this part of Support Vector Machines (step by step derivation). (Bishop 2006) chapter 7 is a good resource. The main idea about this supervised method is separating with a large gap. The thing is that we have a hyperplane, when this plane is projected to lower dimensional data, it can look like a non-linear separator. After we have found this separator, we can intuitively have an idea of confidence based on the distance of the separator.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/support-vector-machines/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/support-vector-machines/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Support Vector Machines" />
<meta property="og:description" content="This is a quite good resource about this part of Support Vector Machines (step by step derivation). (Bishop 2006) chapter 7 is a good resource. The main idea about this supervised method is separating with a large gap. The thing is that we have a hyperplane, when this plane is projected to lower dimensional data, it can look like a non-linear separator. After we have found this separator, we can intuitively have an idea of confidence based on the distance of the separator." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/support-vector-machines/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Support Vector Machines"/>
<meta name="twitter:description" content="This is a quite good resource about this part of Support Vector Machines (step by step derivation). (Bishop 2006) chapter 7 is a good resource. The main idea about this supervised method is separating with a large gap. The thing is that we have a hyperplane, when this plane is projected to lower dimensional data, it can look like a non-linear separator. After we have found this separator, we can intuitively have an idea of confidence based on the distance of the separator."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Support Vector Machines",
      "item": "https://flecart.github.io/notes/support-vector-machines/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Support Vector Machines",
  "name": "Support Vector Machines",
  "description": "This is a quite good resource about this part of Support Vector Machines (step by step derivation). (Bishop 2006) chapter 7 is a good resource. The main idea about this supervised method is separating with a large gap. The thing is that we have a hyperplane, when this plane is projected to lower dimensional data, it can look like a non-linear separator. After we have found this separator, we can intuitively have an idea of confidence based on the distance of the separator.",
  "keywords": [
    "machinelearning"
  ],
  "articleBody": "This is a quite good resource about this part of Support Vector Machines (step by step derivation). (Bishop 2006) chapter 7 is a good resource. The main idea about this supervised method is separating with a large gap. The thing is that we have a hyperplane, when this plane is projected to lower dimensional data, it can look like a non-linear separator. After we have found this separator, we can intuitively have an idea of confidence based on the distance of the separator.\nMid 90‚Äô and early 2000 these models were very popular. With support vector machines, we generalize the idea of a linear separator.\nDefinitions We consider a dataset $S = \\left\\{ (x^{(1)}, y^{(1)}), \\dots, (x^{(n)}, y^{(n)}) \\right\\}$. This is a classical dataset with points, and dimensionality $d$, and labels $y^{(i)} \\in \\left\\{ -1, 1 \\right\\}$.\nFunctional Margin üü®‚Äì Given a hyperplane $w^{T}x + b$ a margin for the $i$-th point is just $\\lambda^{(i)} = y^{(i)}(w^{T}x^{(i)} + b)$. Why is this a good definition? We discussed that the main idea is that we want a large gap between the hyperplane and the points. When a classification is correct we have that $\\lambda^{(i)} \u003e 0$. And the value of this margin can be interpreted as a sign of confidence. One observation is that $w$ are not scaled, so we should usually take the norm, or any other transformation that is useful for us. The important thing to notice is the sign.\nAfter we have defined this for every point, we have that the functional margin for this dataset is $$ \\lambda = \\min_{i} \\lambda^{(i)} $$ Geometrical Margin üü®‚Äì This margin has the easy interpretation given its geometry. Let‚Äôs say we have a point $A$, and its projection to our hyperplane, call this point $B$. The small trick is to express point $B$ as $A$ minus the projection distance, which is our geometrical margin. So we have $$ w^{T}\\left( x^{(i)} - \\frac{w}{\\lVert w \\rVert }\\lambda^{(i)} \\right) + b = 0 \\implies \\lambda^{(i)} = \\frac{w^{T}x^{(i) }+ b}{\\lVert w \\rVert } $$ We know that the vector $w$ is perpendicular to the hyperplane (this is easy to prove, just take a vector of the hyperplane, say $a - b$ with $a, b$ points on the hyperplane, then we know that $w^{T}(a - b) = 0$ because $w^{T}a + b = 0$ and the same for $b$), so we just use that as a unit vector for having the distance. After we have this definition, we just define the geometrical margin to be the minimum: $$ \\lambda = \\min_{i}\\lambda^{(i)} $$ We note that if $\\lVert w \\rVert = 1$ we have that geometrical margin = functional margin, so we have the properties of correct classification that the functional margin gives, and we also have the easy geometrical interpretation of the geometrical margin.\nThe technique The primal problem üü®‚Äì We have said in the previous sections that the main idea of the SVM classifier is to find the best margin to separate our data. This motivates a simple modeling of the problem. We will use Lagrange Multipliers to find the solution of the optimization problem above. We first formalize the problem in this way: $$ \\begin{array} \\\\ \\max \\lambda \\\\ \\lVert w \\rVert = 1 \\\\ y^{(i)}(w^{T}x^{(i)} + b) \\geq \\lambda \\\\ \\end{array} $$ Which means we want to find the best geometrical margin. But some conditions are not so nice to handle (the norm constraint is not so easy, because it is not convex), this allows us to try to re-scale the $w$ and the $b$ such that the third condition is $1$, and the second is implicit. This is possible because scaling $w$ and $b$ does not change the final solution of the margin. Let‚Äôs see this. Given the support points $x^{+}$ and $x^{-}$ we want to find the best $w$ and $b$ such that the margin is maximized. We can write the optimization problem in this way:\n$$ \\begin{align} \\max \\lVert proj_{+} - proj_{-} \\rVert = \\max \\frac{\\lVert w^{T}x^{+} - w^{T}x^{-} \\rVert}{\\lVert w \\rVert } \\implies \\max \\frac{1}{\\lVert w \\rVert} \\\\ y^{(i)}(w^{T}x^{(i)} + b) \\geq 1 \\end{align} $$ Where we have used the definition of vector projection, and the scaling property, which allowed us to build that constraint. We need to note that in this case we are still assuming linear separability of the data.\nIn this way we reduce this problem to be $$ \\begin{array} \\ \\min \\frac{1}{2} \\lVert w \\rVert ^{2} \\ y^{(i)}(w^{T}x^{(i)} + b) \\geq 1\\\n\\end{array} $$ Which could be solved by Quadratic Programming and the use of Lagrange Multiplies. We see now how is this possible.\nFunctional and Geometrical Margin The original problem‚Äôs margin is called geometrical margin (because the distance actually corresponds to a real geometrical distance) while, if we set the constraint $\\lVert w \\rVert = 1$, it is called the functional margin, as the optimization objective is the same, which makes it scalable.\nThe dual üü•++ Using Lagrange Multipliers we obtain he following optimization function: $$ \\mathcal{L}(w, b, \\alpha) = \\frac{1}{2}\\lVert w \\rVert ^{2} - \\sum_{i} \\alpha_{i}(y^{(i)}(w^{T}x^{(i)} + b) - 1) $$ Where $\\alpha_{i} \\geq 0 \\forall i$. We check Slater‚Äôs condition hold, which imply strong duality on this optimization problem. In order for this to hold we need to find $w$ and $b$ such that strict inequality hold. This is simple, as it is scaling invariant, given some $w$ and $b$ such that the equality holds (this is true by construction, because we have set the support vectors to be 1), then just re-scale them by a strictly positive value $\\lambda \u003e 0$ and you have your new $w$ and $b$.\nNow we find the gradient and substitute in the Lagrange Multiplier equation, and we obtain the dual problem: $$\n\\begin{align} \\min_{\\alpha} \\frac{1}{2} \\sum_{i, j} \\alpha_{i}\\alpha_{j}y^{(i)}y^{(j)}x^{(i)T}x^{(j)} - \\sum_{i} \\alpha_{i} \\ \\text{subject to} \\sum_{i} \\alpha_{i}y^{(i)} = 0 \\ \\alpha_{i} \\geq 0 \\end{align} $$\nFinding support vectors üü®+ This is a quadratic optimization problem, and it is usually solved by the SMO algorithm. After we have found this value, we can look for the support vectors in the following manner: We first observe that by complementary slackness we have that $\\alpha_{i}(1 - y^{(i)}(w^{T}x^{(i)} + b)) = 0$. This implies that if $\\alpha_{i} \u003e 0$ then $y^{(i)}(w^{T}x^{(i)} + b) = 1$. This is the condition for a support vector, and if it‚Äôs not a support vector then $\\alpha_{i}$ which implies that $\\boldsymbol{\\alpha}$ is a quite sparse vector. Now you see that a simple linear search over the dataset is enough to find the support vectors! If you have derived the dual, at one step of the gradient calculation you would have found that $$ w^{*} = \\sum_{i} \\alpha_{i}y^{(i)}x^{(i)} $$ Now you see that this is just a linear combination of the support vectors! And we can also find $b$ noting that $y^{(i)}(w^{T}x^{(i)} + b) = 1$ for a support vector, so we can find $b$ by solving for it. After we have one positive and one negative support vector, it is easy to find the intercept in the following way: $$ w^{*T}x^{+} + b = 1 \\implies b = 1 - w^{*T}x^{+} $$ Extensions of SVM Soft SVM In this case we assume the data is not linearly separable so we add a relaxation on the optimization problem. We add a slack variable $\\xi_{i} \\geq 0$ such that the optimization problem is now: $$ \\begin{array} \\\\ \\min \\frac{1}{2} \\lVert w \\rVert ^{2} + C\\sum_{i} \\xi_{i} \\\\ y^{(i)}(w^{T}x^{(i)} + b) \\geq 1 - \\xi_{i} \\\\ \\xi_{i} \\geq 0 \\end{array} $$ Intuitively, we are now allowing for some points to be misclassified, but we are penalizing this misclassification. The parameter $C$ is a hyper-parameter that we can tune to adjust the tradeoff between the margin and the misclassification. Qualitatively, if $\\xi_{i} = 0$ then the point is correctly classified, if $\\xi_{i} = 1$ then the point is on the margin, and if $\\xi_{i} \u003e 1$ then the point is misclassified. We see that if we have higher value ok $\\xi$ this is penalized by the $C$ parameter. So, you see $C$ tells you how much you penalize misclassification.\nRelation with the Hinge Loss The objective of the Soft SVM can be related to the hinge loss. The hinge loss is defined as $$ \\mathcal{L}_{\\text{Hinge}}(y, x) = \\max(0, 1 - y^{(i)}(w^{T}x^{(i)} + b)) = \\max(0, 1 - y^{(i)}f(x^{(i)})) $$ Then, it‚Äôs just minimizing a regularized Hinge Loss: $$ \\frac{1}{2} \\lVert w \\rVert^{2} + C\\sum_{i = 1}^{N}\\mathcal{L}_{\\text{Hinge}}(y^{(i)}, x^{(i)}) $$ , which is the same as the Soft SVM loss function. This is because for correctly classified points the hinge loss is zero, and for misclassified points the hinge loss is $1 - y^{(i)}f(x^{(i)})$ which is the same as the slack variable $\\xi_{i}$.\nDeriving the optimization objective üü® One can see using the same techniques that the optimization objective in this case is: $$ \\begin{align} \\mathcal{L}(w, b, \\xi,\\alpha, \\mu) = \\frac{1}{2}\\lVert w \\rVert ^{2} + C\\sum_{i} \\xi_{i} - \\sum_{i} \\alpha_{i}(y^{(i)}(w^{T}x^{(i)} + b) - 1 + \\xi_{i}) - \\sum_{i} \\mu_{i}\\xi_{i} \\end{align} $$ Then the stationary conditions are: $$ \\begin{cases} \\frac{\\partial \\mathcal{L}}{\\partial w} = 0 \\implies w = \\sum_{i} \\alpha_{i}y^{(i)}x^{(i)} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial b} = 0 \\implies \\sum_{i} \\alpha_{i}y^{(i)} = 0 \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\xi_{i}} = 0 \\implies \\alpha_{i} = C - \\mu_{i} \\end{cases} $$ Substituting these in, we have that the dual of the soft margin SVM classifier is: $$ \\begin{align} \\min_{\\alpha} \\frac{1}{2} \\sum_{i, j} \\alpha_{i}\\alpha_{j}y^{(i)}y^{(j)}x^{(i)T}x^{(j)} - \\sum_{i} \\alpha_{i} \\\\ \\text{subject to} \\sum_{i} \\alpha_{i}y^{(i)} = 0 \\\\ 0 \\leq \\alpha_{i} \\leq C \\end{align} $$ Which is very similar to the original problem, but with the added constraint that $\\alpha_{i} \\leq C$.\nNOTE: we have compacted three conditions into one: $$ \\forall i \\begin{cases} C = \\alpha_{i} + \\mu_{i} \\\\ \\mu_{i} \\geq 0 \\\\ \\alpha_{i} \\geq 0 \\end{cases} \\implies 0 \\leq \\alpha_{i} \\leq C $$ Kernelized SVM We have seen in the previous analysis of SVM that $w^{*} = \\sum_{i} \\alpha_{i}y^{(i)}x^{(i)}$ then you see that if we define a feature function $\\varphi$ which a corresponding kernel (see Kernel Methods) then at inference we just need to compute: $$ w^{*}\\varphi(x) = \\sum_{i} \\alpha_{i}y^{(i)}\\varphi(x^{(i)}) \\varphi(x) = \\sum_{i} \\alpha_{i}y^{(i)}K(x^{(i)}, x) $$ We see kernels again! Usually Kernelized SVM is justified by the observation that data might be separable in higher dimensional spaces, so we just lift the data on that space and compute everything there. Example, if the $1-D$ data is as follows: $[0, 1] \\to 1$ else where in the real line is $0$ this is clearly not linearly separable. But if we lift to $(x, x^{2})$ it is separable. This is the intuition behind how kernels work for SVMs too!\nKernelized SVM loss üü® If we do the math, we will get exactly the similar conditions for quadratic optimization: $$ \\begin{align} \\min_{\\alpha} \\frac{1}{2} \\sum_{i, j} \\alpha_{i}\\alpha_{j}y^{(i)}y^{(j)}K(x^{(i)}, x^{(j)}) - \\sum_{i} \\alpha_{i} \\\\ \\text{subject to} \\sum_{i} \\alpha_{i}y^{(i)} = 0 \\\\ 0 \\leq \\alpha_{i} \\end{align} $$ Inference with K-SVM üü©- And for inference, we do something quite similar: $$ w^{*T}\\varphi(x) + b = \\sum_{i} \\alpha_{i}y^{(i)}K(x^{(i)}, x) + b $$ As in this case we have that $w^{*} = \\sum_{i} \\alpha_{i}y^{(i)}\\varphi(x^{(i)})$.\nStructured SVM Structured SVMs attempt to generalize the idea of SVMs into multi-class prediction, or prediction of structured objects (like parse threes covered in Probabilistic Parsing or tags in Part of Speech Tagging).\nThe generalized Margin The main idea of SSVMs is to extend the idea of margins to a more general form (Crmmer and Singer JMLR 2001). Given $k$ classes and $y$ one possible configuration of these, then the new constrain that we have is $$ (w_{i}^{T}x + w_{i,0}) - \\max_{j \\neq i} (w^{T}_{j}x + w_{j, 0}) \\geq m $$ where $m$ is our margin, and $i$ is the current considered class among the $k$ possible (note we have different weight vectors for each class). If we want to extend this to structured data, then instead of taking indexes, we should take the maximum margin different from the one we are considering inside the space of possible structures (which is often huge).\nThis generalization is not exactly the generalization we would expect for the binary classification case, but in any case it is useful to extend the technique to structured objects.\nThen after having made this consideration, we can still use the functional margin idea of re-normalizing, and using the soft-SVM idea of adding a slack variable accounting for possible errors.\nAnother way to generalize SVM to multiclass is training many models in a one versus rest manner, and then just take the model that has the highest margin among the present. But we will not discuss this case.\nProblems with SSVMs From the slides:\nCompact representation of the output space If we allowed even just one parameter for each class, we would already have more parameters than we could ever hope to have enough training data for. Efficient prediction: Sequentially enumerating all possible classes may be infeasible, hence making a single prediction on a new example may be computationally challenging. Prediction error: The notion of a prediction error must be refined ‚Äì for example, predicting a parse tree that is almost correct should be treated differently from predicting a tree that is completely wrong. Furthermore, the training data may be inconsistent. Efficient training: Efficient training algorithms are needed that have a run-time complexity sub-linear in the number of classes. Scores and feature maps To solve the first two problems, we usually define a feature map $\\psi(y, x)$ that tells us something about the input and possible relations with the structures. This is often considered the hard part that needs a lot of problem knowledge. In order to solve the prediction error problem, we define a notion of score, which often comes naturally with a given problem. This is often modeled as $f_{y, x} = w^{T}\\psi(y, x)$. Which allows us to compare even similar solutions. These ideas have been also developed in the NLP course. e.g. Log Linear Models clearly define the concept of scores, which are quite common in the other community. While the features are often derived from data, with some neural network models nowadays (e.g. Part of Speech Tagging they used Bert to have an estimate of the emission, which is the probability of a certain word to have a certain tagging).\nThen we just choose the class with the highest score as our prediction.\nReferences [1] Bishop ‚ÄúPattern Recognition and Machine Learning‚Äù Springer 2006\n",
  "wordCount" : "2392",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/support-vector-machines/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Support Vector Machines
    </h1>
    <div class="post-meta">12 min&nbsp;¬∑&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#definitions" aria-label="Definitions">Definitions</a><ul>
                        
                <li>
                    <a href="#functional-margin---" aria-label="Functional Margin üü®&ndash;">Functional Margin üü®&ndash;</a></li>
                <li>
                    <a href="#geometrical-margin---" aria-label="Geometrical Margin üü®&ndash;">Geometrical Margin üü®&ndash;</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#the-technique" aria-label="The technique">The technique</a><ul>
                        <ul>
                        
                <li>
                    <a href="#the-primal-problem---" aria-label="The primal problem üü®&ndash;">The primal problem üü®&ndash;</a></li>
                <li>
                    <a href="#functional-and-geometrical-margin" aria-label="Functional and Geometrical Margin">Functional and Geometrical Margin</a></li>
                <li>
                    <a href="#the-dual-" aria-label="The dual üü•&#43;&#43;">The dual üü•++</a></li>
                <li>
                    <a href="#finding-support-vectors-" aria-label="Finding support vectors üü®&#43;">Finding support vectors üü®+</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#extensions-of-svm" aria-label="Extensions of SVM">Extensions of SVM</a><ul>
                        
                <li>
                    <a href="#soft-svm" aria-label="Soft SVM">Soft SVM</a><ul>
                        
                <li>
                    <a href="#relation-with-the-hinge-loss" aria-label="Relation with the Hinge Loss">Relation with the Hinge Loss</a></li>
                <li>
                    <a href="#deriving-the-optimization-objective-" aria-label="Deriving the optimization objective üü®">Deriving the optimization objective üü®</a></li></ul>
                </li>
                <li>
                    <a href="#kernelized-svm" aria-label="Kernelized SVM">Kernelized SVM</a><ul>
                        
                <li>
                    <a href="#kernelized-svm-loss-" aria-label="Kernelized SVM loss üü®">Kernelized SVM loss üü®</a></li>
                <li>
                    <a href="#inference-with-k-svm--" aria-label="Inference with K-SVM üü©-">Inference with K-SVM üü©-</a></li></ul>
                </li>
                <li>
                    <a href="#structured-svm" aria-label="Structured SVM">Structured SVM</a><ul>
                        
                <li>
                    <a href="#the-generalized-margin" aria-label="The generalized Margin">The generalized Margin</a></li>
                <li>
                    <a href="#problems-with-ssvms" aria-label="Problems with SSVMs">Problems with SSVMs</a></li>
                <li>
                    <a href="#scores-and-feature-maps" aria-label="Scores and feature maps">Scores and feature maps</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><a href="https://cs229.stanford.edu/main_notes.pdf">This</a> is a quite good resource about this part of Support Vector Machines (step by step derivation). (Bishop 2006) chapter 7 is a good resource. The main idea about this <em>supervised</em> method is separating with a <strong>large gap</strong>. The thing is that we have a hyperplane, when this plane is projected to lower dimensional data, it can look like a non-linear separator. After we have found this separator, we can intuitively have an idea of <em>confidence</em> based on the distance of the separator.</p>
<p>Mid 90&rsquo; and early 2000 these models were very popular. With support vector machines, we generalize the idea of a linear separator.</p>
<h3 id="definitions">Definitions<a hidden class="anchor" aria-hidden="true" href="#definitions">#</a></h3>
<p>We consider a dataset $S = \left\{ (x^{(1)}, y^{(1)}), \dots, (x^{(n)}, y^{(n)})  \right\}$. This is a classical dataset with points, and dimensionality $d$, and labels $y^{(i)} \in \left\{ -1, 1 \right\}$.</p>
<h4 id="functional-margin---">Functional Margin üü®&ndash;<a hidden class="anchor" aria-hidden="true" href="#functional-margin---">#</a></h4>
<p>Given a hyperplane $w^{T}x + b$ a margin for the $i$-th point is just $\lambda^{(i)} = y^{(i)}(w^{T}x^{(i)} + b)$. Why is this a good definition? We discussed that the main idea is that we want a <em>large gap</em> between the hyperplane and the points. When a classification is correct we have that $\lambda^{(i)} > 0$. And the value of this margin can be interpreted as a sign of confidence.
One observation is that $w$ are not scaled, so we should usually take the norm, or any other transformation that is useful for us. The important thing to notice is the sign.</p>
<p>After we have defined this for every point, we have that the functional margin for this dataset is
</p>
$$
\lambda = \min_{i} \lambda^{(i)}
$$
<h4 id="geometrical-margin---">Geometrical Margin üü®&ndash;<a hidden class="anchor" aria-hidden="true" href="#geometrical-margin---">#</a></h4>
<p>This margin has the easy interpretation given its geometry. Let&rsquo;s say we have a point $A$, and its projection to our hyperplane, call this point $B$. The small trick is to express point $B$ as $A$ minus the projection distance, which is our geometrical margin. So we have
</p>
$$
w^{T}\left( x^{(i)} - \frac{w}{\lVert w \rVert }\lambda^{(i)} \right) + b = 0
\implies
\lambda^{(i)} = \frac{w^{T}x^{(i)  }+ b}{\lVert w \rVert }
$$
<p>
We know that the vector $w$  is perpendicular to the hyperplane (this is easy to prove, just take a vector of the hyperplane, say $a - b$ with $a, b$ points on the hyperplane, then we know that $w^{T}(a - b) = 0$ because $w^{T}a + b = 0$ and the same for $b$), so we just use that as a unit vector for having the distance.
After we have this definition, we just define the geometrical margin to be the minimum:
</p>
$$
\lambda = \min_{i}\lambda^{(i)}
$$
<p>We note that if $\lVert w \rVert = 1$ we have that geometrical margin = functional margin, so we have the properties of correct classification that the functional margin gives, and we also have the easy geometrical interpretation of the geometrical margin.</p>
<h2 id="the-technique">The technique<a hidden class="anchor" aria-hidden="true" href="#the-technique">#</a></h2>
<h4 id="the-primal-problem---">The primal problem üü®&ndash;<a hidden class="anchor" aria-hidden="true" href="#the-primal-problem---">#</a></h4>
<p>We have said in the previous sections that the main idea of the SVM classifier is to find the <strong>best margin</strong> to separate our data. This motivates a simple modeling of the problem.
We will use <a href="/notes/lagrange-multipliers/">Lagrange Multipliers</a> to find the solution of the optimization problem above.
We first formalize the problem  in this way:
</p>
$$
\begin{array} \\
\max \lambda \\
\lVert w \rVert  = 1 \\
y^{(i)}(w^{T}x^{(i)} + b) \geq \lambda \\
\end{array}
$$
<p>
Which means we want to find the best geometrical margin. But some conditions are not so nice to handle (the norm constraint is not so easy, because it is not convex), this allows us to try to re-scale the $w$ and the $b$ such that the third condition is $1$, and the second is implicit. This is possible because scaling $w$ and $b$ does not change the final solution of the margin.
Let&rsquo;s see this.
Given the support points $x^{+}$ and $x^{-}$ we want to find the best $w$ and $b$ such that the margin is maximized. We can write the optimization problem in this way:</p>
$$
\begin{align}
\max \lVert proj_{+} - proj_{-} \rVert  = \max \frac{\lVert w^{T}x^{+} - w^{T}x^{-} \rVert}{\lVert w \rVert } \implies  \max \frac{1}{\lVert w \rVert} \\
y^{(i)}(w^{T}x^{(i)} + b) \geq 1
\end{align}
$$
<p>
Where we have used the definition of vector projection, and the scaling property, which allowed us to build that constraint. We need to note that in this case we are still assuming linear separability of the data.</p>
<p>In this way we reduce this problem to be
$$
\begin{array}
\
\min \frac{1}{2} \lVert w \rVert ^{2} \
y^{(i)}(w^{T}x^{(i)} + b) \geq 1\</p>
<p>\end{array}
$$
Which could be solved by Quadratic Programming and the use of Lagrange Multiplies. We see now how is this possible.</p>
<h4 id="functional-and-geometrical-margin">Functional and Geometrical Margin<a hidden class="anchor" aria-hidden="true" href="#functional-and-geometrical-margin">#</a></h4>
<p>The original problem&rsquo;s margin is called <strong>geometrical margin</strong> (because the distance actually corresponds to a real geometrical distance) while, if we set the constraint $\lVert w \rVert = 1$, it is called the functional margin, as the optimization objective is the same, which makes it scalable.</p>
<h4 id="the-dual-">The dual üü•++<a hidden class="anchor" aria-hidden="true" href="#the-dual-">#</a></h4>
<p>Using Lagrange Multipliers we obtain he following optimization function:
</p>
$$
\mathcal{L}(w, b, \alpha) = \frac{1}{2}\lVert w \rVert ^{2} - \sum_{i} \alpha_{i}(y^{(i)}(w^{T}x^{(i)} + b) - 1)
$$
<p>
Where $\alpha_{i} \geq 0 \forall i$. We check <em>Slater&rsquo;s condition hold</em>, which imply strong duality on this optimization problem.
In order for this to hold we need to find $w$ and $b$ such that strict inequality hold. This is simple, as it is scaling invariant, given some $w$ and $b$ such that the equality holds (this is true by construction, because we have set the support vectors to be 1), then just re-scale them by a strictly positive value $\lambda > 0$ and you have your new $w$ and $b$.</p>
<p>Now we find the gradient and substitute in the Lagrange Multiplier equation, and we obtain the dual problem:
$$</p>
<p>\begin{align}
\min_{\alpha} \frac{1}{2} \sum_{i, j} \alpha_{i}\alpha_{j}y^{(i)}y^{(j)}x^{(i)T}x^{(j)} - \sum_{i} \alpha_{i} \
\text{subject to} \sum_{i} \alpha_{i}y^{(i)} = 0 \
\alpha_{i} \geq 0
\end{align}
$$</p>
<h4 id="finding-support-vectors-">Finding support vectors üü®+<a hidden class="anchor" aria-hidden="true" href="#finding-support-vectors-">#</a></h4>
<p>This is a quadratic optimization problem, and it is usually solved by the SMO algorithm. After we have found this value, we can look for the support vectors in the following manner:
We first observe that by complementary slackness we have that $\alpha_{i}(1 - y^{(i)}(w^{T}x^{(i)} + b)) = 0$. This implies that if $\alpha_{i} > 0$ then $y^{(i)}(w^{T}x^{(i)} + b) = 1$. This is the condition for a support vector, and if it&rsquo;s not a support vector then $\alpha_{i}$ which implies that $\boldsymbol{\alpha}$ is a quite sparse vector. Now you see that a simple linear search over the dataset is enough to find the support vectors!
If you have derived the dual, at one step of the gradient calculation you would have found that
</p>
$$
w^{*} = \sum_{i} \alpha_{i}y^{(i)}x^{(i)}
$$
<p>
Now you see that this is just a linear combination of the <strong>support vectors</strong>!
And we can also find $b$ noting that $y^{(i)}(w^{T}x^{(i)} + b) = 1$ for a support vector, so we can find $b$ by solving for it.
After we have one positive and one negative support vector, it is easy to find the intercept in the following way:
</p>
$$
w^{*T}x^{+} + b = 1 \implies b = 1 - w^{*T}x^{+}
$$
<h2 id="extensions-of-svm">Extensions of SVM<a hidden class="anchor" aria-hidden="true" href="#extensions-of-svm">#</a></h2>
<h3 id="soft-svm">Soft SVM<a hidden class="anchor" aria-hidden="true" href="#soft-svm">#</a></h3>
<p>In this case we assume the data is <strong>not linearly separable</strong> so we add a relaxation on the optimization problem. We add a <strong>slack</strong> variable $\xi_{i} \geq 0$ such that the optimization problem is now:
</p>
$$
\begin{array}
 \\
\min \frac{1}{2} \lVert w \rVert ^{2} + C\sum_{i} \xi_{i} \\ 
y^{(i)}(w^{T}x^{(i)} + b) \geq 1 - \xi_{i} \\ 
\xi_{i} \geq 0
\end{array}
$$
<p>
Intuitively, we are now allowing for some points to be misclassified, but we are <em>penalizing</em> this misclassification. The parameter $C$ is a hyper-parameter that we can tune to adjust the tradeoff between the margin and the misclassification.
Qualitatively, if $\xi_{i} = 0$ then the point is correctly classified, if $\xi_{i} = 1$ then the point is on the margin, and if $\xi_{i} > 1$ then the point is misclassified. We see that if we have higher value ok $\xi$ this is penalized by the $C$ parameter. So, you see $C$ tells you how much you penalize misclassification.</p>
<h4 id="relation-with-the-hinge-loss">Relation with the Hinge Loss<a hidden class="anchor" aria-hidden="true" href="#relation-with-the-hinge-loss">#</a></h4>
<p>The objective of the Soft SVM can be related to the hinge loss. The hinge loss is defined as
</p>
$$
\mathcal{L}_{\text{Hinge}}(y, x) = \max(0, 1 - y^{(i)}(w^{T}x^{(i)} + b)) = \max(0, 1 - y^{(i)}f(x^{(i)}))
$$
<p>Then, it&rsquo;s just minimizing a regularized Hinge Loss:
</p>
$$
\frac{1}{2} \lVert w \rVert^{2} + C\sum_{i = 1}^{N}\mathcal{L}_{\text{Hinge}}(y^{(i)}, x^{(i)})
$$
<p>
, which is the same as the Soft SVM loss function.
This is because for correctly classified points the hinge loss is zero, and for misclassified points the hinge loss is $1 - y^{(i)}f(x^{(i)})$ which is the same as the slack variable $\xi_{i}$.</p>
<h4 id="deriving-the-optimization-objective-">Deriving the optimization objective üü®<a hidden class="anchor" aria-hidden="true" href="#deriving-the-optimization-objective-">#</a></h4>
<p>One can see using the same techniques that the optimization objective in this case is:
</p>
$$
\begin{align}
\mathcal{L}(w, b, \xi,\alpha, \mu) = \frac{1}{2}\lVert w \rVert ^{2} + C\sum_{i} \xi_{i} - \sum_{i} \alpha_{i}(y^{(i)}(w^{T}x^{(i)} + b) - 1 + \xi_{i}) - \sum_{i} \mu_{i}\xi_{i}
\end{align}
$$
<p>Then the stationary conditions are:
</p>
$$
\begin{cases}
\frac{\partial \mathcal{L}}{\partial w} = 0 \implies w = \sum_{i} \alpha_{i}y^{(i)}x^{(i)}  \\
\frac{\partial \mathcal{L}}{\partial b} = 0 \implies \sum_{i} \alpha_{i}y^{(i)} = 0  \\
\frac{\partial \mathcal{L}}{\partial \xi_{i}} = 0 \implies \alpha_{i} = C - \mu_{i}
\end{cases}
$$
<p>
Substituting these in, we have that the dual of the soft margin SVM classifier is:
</p>
$$
\begin{align}
\min_{\alpha} \frac{1}{2} \sum_{i, j} \alpha_{i}\alpha_{j}y^{(i)}y^{(j)}x^{(i)T}x^{(j)} - \sum_{i} \alpha_{i}  \\
\text{subject to} \sum_{i} \alpha_{i}y^{(i)} = 0  \\
0 \leq \alpha_{i} \leq C
\end{align}
$$
<p>
Which is very similar to the original problem, but with the added constraint that $\alpha_{i} \leq C$.</p>
<p>NOTE: we have compacted three conditions into one:
</p>
$$
\forall i
\begin{cases}
C = \alpha_{i} + \mu_{i} \\
\mu_{i} \geq 0 \\
\alpha_{i} \geq 0
\end{cases}
\implies 0 \leq \alpha_{i} \leq C
$$
<h3 id="kernelized-svm">Kernelized SVM<a hidden class="anchor" aria-hidden="true" href="#kernelized-svm">#</a></h3>
<p>We have seen in the previous analysis of SVM that $w^{*} = \sum_{i} \alpha_{i}y^{(i)}x^{(i)}$ then you see that if we define a feature function $\varphi$ which a corresponding kernel (see <a href="/notes/kernel-methods/">Kernel Methods</a>) then at inference we just need to compute:
</p>
$$
w^{*}\varphi(x) = \sum_{i} \alpha_{i}y^{(i)}\varphi(x^{(i)}) \varphi(x) = \sum_{i} \alpha_{i}y^{(i)}K(x^{(i)}, x)
$$
<p>
We see kernels again!
Usually Kernelized SVM is justified by the observation that data might be separable in higher dimensional spaces, so we just lift the data on that space and compute everything there. Example, if the $1-D$ data is as follows: $[0, 1] \to 1$ else where in the real line is $0$ this is clearly not linearly separable. But if we lift to $(x, x^{2})$ it is separable. This is the intuition behind how kernels work for SVMs too!</p>
<h4 id="kernelized-svm-loss-">Kernelized SVM loss üü®<a hidden class="anchor" aria-hidden="true" href="#kernelized-svm-loss-">#</a></h4>
<p>If we do the math, we will get exactly the similar conditions for quadratic optimization:
</p>
$$
\begin{align}
\min_{\alpha} \frac{1}{2} \sum_{i, j} \alpha_{i}\alpha_{j}y^{(i)}y^{(j)}K(x^{(i)}, x^{(j)}) - \sum_{i} \alpha_{i}   \\
\text{subject to} \sum_{i} \alpha_{i}y^{(i)} = 0   \\
0 \leq \alpha_{i}
\end{align}
$$
<h4 id="inference-with-k-svm--">Inference with K-SVM üü©-<a hidden class="anchor" aria-hidden="true" href="#inference-with-k-svm--">#</a></h4>
<p>And for inference, we do something quite similar:
</p>
$$
w^{*T}\varphi(x) + b = \sum_{i} \alpha_{i}y^{(i)}K(x^{(i)}, x) + b
$$
<p>
As in this case we have that $w^{*} = \sum_{i} \alpha_{i}y^{(i)}\varphi(x^{(i)})$.</p>
<h3 id="structured-svm">Structured SVM<a hidden class="anchor" aria-hidden="true" href="#structured-svm">#</a></h3>
<p>Structured SVMs attempt to generalize the idea of SVMs into multi-class prediction, or prediction of structured objects (like parse threes covered in <a href="/notes/probabilistic-parsing/">Probabilistic Parsing</a> or tags in <a href="/notes/part-of-speech-tagging/">Part of Speech Tagging</a>).</p>
<h4 id="the-generalized-margin">The generalized Margin<a hidden class="anchor" aria-hidden="true" href="#the-generalized-margin">#</a></h4>
<p>The main idea of SSVMs is to extend the idea of margins to a more general form (Crmmer and Singer JMLR 2001).
Given $k$ classes and $y$ one possible configuration of these, then the new constrain that we have is
</p>
$$
(w_{i}^{T}x + w_{i,0}) - \max_{j \neq i} (w^{T}_{j}x + w_{j, 0}) \geq m
$$
<p>
where $m$ is our margin, and $i$ is the current considered class among the $k$ possible (note we have different weight vectors for each class).
If we want to extend this to structured data, then instead of taking indexes, we should take the maximum margin different from the one we are considering inside the space of possible structures (which is often huge).</p>
<p>This generalization is <strong>not</strong> exactly the generalization we would expect for the binary classification case, but in any case it is useful to extend the technique to structured objects.</p>
<p>Then after having made this consideration, we can still use the functional margin idea of re-normalizing, and using the soft-SVM idea of adding a slack variable accounting for possible errors.</p>
<p>Another way to generalize SVM to multiclass is training many models in a one versus rest manner, and then just take the model that has the highest margin among the present. But we will not discuss this case.</p>
<h4 id="problems-with-ssvms">Problems with SSVMs<a hidden class="anchor" aria-hidden="true" href="#problems-with-ssvms">#</a></h4>
<p>From the slides:</p>
<ol>
<li><strong>Compact representation</strong> of the output space If we allowed even just one parameter for each class, we would already have more parameters than we could ever hope to have enough training data for.</li>
<li><strong>Efficient prediction:</strong> Sequentially enumerating all possible classes may be infeasible, hence making a single prediction on a new example may be computationally challenging.</li>
<li><strong>Prediction error</strong>: The notion of a prediction error must be refined ‚Äì for example, predicting a parse tree that is almost correct should be treated differently from predicting a tree that is completely wrong. Furthermore, the training data may be inconsistent.</li>
<li><strong>Efficient training</strong>: Efficient training algorithms are needed that have a run-time complexity sub-linear in the number of classes.</li>
</ol>
<h4 id="scores-and-feature-maps">Scores and feature maps<a hidden class="anchor" aria-hidden="true" href="#scores-and-feature-maps">#</a></h4>
<p>To solve the first two problems, we usually define a feature map $\psi(y, x)$ that tells us something about the input and possible relations with the structures. This is often considered the <strong>hard part</strong> that needs a lot of problem knowledge.
In order to solve the prediction error problem, we define a notion of <strong>score</strong>, which often comes naturally with a given problem. This is often modeled as $f_{y, x} = w^{T}\psi(y, x)$. Which allows us to compare even similar solutions.
These ideas have been also developed in the NLP course. e.g. <a href="/notes/log-linear-models/">Log Linear Models</a> clearly define the concept of scores, which are quite common in the other community. While the features are often derived from data, with some neural network models nowadays (e.g. <a href="/notes/part-of-speech-tagging/">Part of Speech Tagging</a> they used Bert to have an estimate of the emission, which is the probability of a certain word to have a certain tagging).</p>
<p>Then we just choose the class with the highest score as our prediction.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Bishop ‚ÄúPattern Recognition and Machine Learning‚Äù Springer 2006</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Support Vector Machines on x"
            href="https://x.com/intent/tweet/?text=Support%20Vector%20Machines&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fsupport-vector-machines%2f&amp;hashtags=machinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Support Vector Machines on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fsupport-vector-machines%2f&amp;title=Support%20Vector%20Machines&amp;summary=Support%20Vector%20Machines&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fsupport-vector-machines%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Support Vector Machines on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fsupport-vector-machines%2f&title=Support%20Vector%20Machines">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Support Vector Machines on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fsupport-vector-machines%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Support Vector Machines on whatsapp"
            href="https://api.whatsapp.com/send?text=Support%20Vector%20Machines%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fsupport-vector-machines%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Support Vector Machines on telegram"
            href="https://telegram.me/share/url?text=Support%20Vector%20Machines&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fsupport-vector-machines%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Support Vector Machines on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Support%20Vector%20Machines&u=https%3a%2f%2fflecart.github.io%2fnotes%2fsupport-vector-machines%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
