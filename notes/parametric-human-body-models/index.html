<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Parametric Human Body Models | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machine-perception">
<meta name="description" content="An historical perspective
The origins of motion capture
One of the earliest starts of motion capturing is the famous horse in 1878 in motion &ldquo;video&rdquo;. This was the start of all the modern cameras. One of the earliest human body motion capture was in military for moving efficiency purposes in 1883. This website has many historical resources on the topic.
The problem is still a problem in modern times. If we want to create models to mimic humans, it surely could be nice to understand how humans move and think. This is the general line of though of this line of research.">
<meta name="author" content="
By Xuanqiang Angelo Huang">
<link rel="canonical" href="https://flecart.github.io/notes/parametric-human-body-models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f790d9af969c56c079c1ce2d5972a04486bf3d6144295d5fba319830e1e55a7a.css" integrity="sha256-95DZr5acVsB5wc4tWXKgRIa/PWFEKV1fujGYMOHlWno=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/parametric-human-body-models/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/parametric-human-body-models/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Parametric Human Body Models">
  <meta property="og:description" content="An historical perspective The origins of motion capture One of the earliest starts of motion capturing is the famous horse in 1878 in motion “video”. This was the start of all the modern cameras. One of the earliest human body motion capture was in military for moving efficiency purposes in 1883. This website has many historical resources on the topic. The problem is still a problem in modern times. If we want to create models to mimic humans, it surely could be nice to understand how humans move and think. This is the general line of though of this line of research.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2025-05-08T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-08T00:00:00+00:00">
    <meta property="article:tag" content="Machine-Perception">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Parametric Human Body Models">
<meta name="twitter:description" content="An historical perspective
The origins of motion capture
One of the earliest starts of motion capturing is the famous horse in 1878 in motion &ldquo;video&rdquo;. This was the start of all the modern cameras. One of the earliest human body motion capture was in military for moving efficiency purposes in 1883. This website has many historical resources on the topic.
The problem is still a problem in modern times. If we want to create models to mimic humans, it surely could be nice to understand how humans move and think. This is the general line of though of this line of research.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Parametric Human Body Models",
      "item": "https://flecart.github.io/notes/parametric-human-body-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Parametric Human Body Models",
  "name": "Parametric Human Body Models",
  "description": "An historical perspective The origins of motion capture One of the earliest starts of motion capturing is the famous horse in 1878 in motion \u0026ldquo;video\u0026rdquo;. This was the start of all the modern cameras. One of the earliest human body motion capture was in military for moving efficiency purposes in 1883. This website has many historical resources on the topic. The problem is still a problem in modern times. If we want to create models to mimic humans, it surely could be nice to understand how humans move and think. This is the general line of though of this line of research.\n",
  "keywords": [
    "machine-perception"
  ],
  "articleBody": "An historical perspective The origins of motion capture One of the earliest starts of motion capturing is the famous horse in 1878 in motion “video”. This was the start of all the modern cameras. One of the earliest human body motion capture was in military for moving efficiency purposes in 1883. This website has many historical resources on the topic. The problem is still a problem in modern times. If we want to create models to mimic humans, it surely could be nice to understand how humans move and think. This is the general line of though of this line of research.\nHuman motion capture One interesting experiment is that humans seem to automatically group dots that move together. It is research from many years ago, in Johansoon. Ten or twelve dots were enough for humans to have enough contextual information to have it make sense. This was the most important point of that research.\nProblems in Human motion capture Following this idea, one interesting approach could be estimating 2D points from human images. The problems is taking into account for\nPlane rotations (head in different orientations) Scaling Perspectives Aspect ratio of different humans Intra-category variation (e.g. masked faces, particular faces with helmets, they should still be considered human faces) This idea first started with modelling with pictorial structures, like rectangles and things similar to that to model human faces. At the time, they didn’t have much data, and it was very difficult to extract those images. One of the earliest breakthroughs was from 2017 Cat et al. paper. Early Models: Mass Spring and Pictorial Structures Take cylinder primitives and fit to some representation. This was some physics based model to fit into some energy function. This inspired a lot of research. This was mainly a problem on representation. At the time it was very difficult to store images on the disk, there was not so much compute and data.\nModern Approaches We know now how to extract features.\nEarliest breakthroughs: Deep Features This is called DeepPose. Predicting heatmats was better than just predicting 2d points, so then people started to predict heatmaps for every single important joints, then you can sample from the joint. Then combined with the iterative approach they invented the next model.\nYou fit narrower and narrowerversions of the image to the model (stage modelling). After this, we went to regress for heatmaps, which was a better feature, meaning we have better results.\nConvolutional Pose Machines The network is able to look at the entire image after some iterations. Spatial correlations are taken into account here. So that deep models have big receptive field, while early stages are quite local, they add belief headmaps of the conditioned joints. OpenPose and Part Affinity Fields The use convolutional pose machines with iterative refinement and another branch with part affinity fields, to help with the association problem. This is a bottom-up approach, since we are feeding the whole image, without any preprocessing with that. The problem is that maybe we have many predictions for a single arm, and you need to join them together.\nPart affinity fields:\nFor every limb you create a unit vector that represents the moving direction of that limb. This helps with association strategies. Gives you direction of limbs that are then useful to solve the problem better, it looks very much like step by step solution in a supervised manner, closer to diffusions? ViTPose This is an example of a top-down approach (first use YOLO or similar things to have a good detection). There are many many other models that attempt to attack this section.\nYou get bounding boxes, and then fit the bounding boxes to a model that gives 2D detection, this helps you solve it without affinity fields, but needs 10x if there are 10 people in the image after you get a bounding box.\nUsually 2D for humans, it is easy to annotate and check, but for 3D it is difficult to record and capture, we don’t have enough data for 3D, but we need to do it for this kind of problems, there are many papers that attacked this kind of zone.\nImpactful models Blanz and Vetter Face model This is one of the classical statistical face models.. It is a statistical model that can be used to generate 3D human faces from 2D images. This example is still relevant today after 26 years.\nLinear face representations $$ S = \\sum_{i = 1}^{m} a_{i} S_{i} $$ We want to have a general model that can generate all the faces.\nThis is desirable to have for:\nSimplicity in generation, you just need to find parameters starting from a fixed set of parameters. Generative model: we can generate new faces starting from the ones we have. It is still hard problem, since just linear interpolation of pixels surely does not work.\nDense correspondence We want point to point correspondence between parts of one face to another, i.e. a right eye corresponds to a right eye on the other face. One solution is adding one template face of fixed topology that helps in solving this representation. Yet this is some sort of a chicken and egg problem: having a morphable model would greatly help with the registration, but to build a morphable model we need to do registration.\nUse bootstrapping, the same idea we usually use to build compilers or RL Tabular Reinforcement Learning, Macchine Astratte, we have an algorithm that gives us a first model, and refine it iteratively.\nSensible Coefficients How can you vary the coefficients to build some faces that do make sense? How to be sure that the produced face does indeed make sense? It was very difficult to get 3D data at the time, so the used Principal Component Analysis. You just assume that all shapes are in full correspondence, with a given topology, and compute the eigenvectors of the given faces dataset, it is exactly that. Given a set of $S_{i}$, you should be able to compute the principal components and take the eigenvectors.\n$$ x = Uc = \\sum_{i = 1}^{m} c_{i} \\boldsymbol{u_{i}} = \\sum_{i= 1}^{m} \\left( U^{T}x_{i} \\right) \\boldsymbol{u_{i}} $$You might also restrict this to a subset of the eigenvectors, to reduce the dimensionality of the problem. The geometry is high quality, and it is able to give 3D shapes from 2D images.\nStatistical -\u003e from data Shape -\u003e from geometry priors.\nSMPL Published in SIGGRAPH Asia 2025. MPI in Tübingen, Germany, also called the SMPL Family.\nSMPL (skinned, multi-person linear model) is a industry-standard built on linear-blend skinning (gives you a model that can be animated), used in both industry and research to model human bodies. It is a parametric model that can be used to generate 3D human bodies from 2D images. It is just an additive model, meaning it is simple, but with lots of parameters.\nThe model $$ S = M(\\theta, \\beta) $$ Where $S$ is the 3D mesh and joints, $M$ is the SMPL model, $\\theta$ is the pose parameters and $\\beta$ is the shape parameters.\n$$ \\begin{align*} M(\\theta, \\beta) = W(T_{p}(\\beta, \\theta), J(\\beta), \\theta, \\mathcal{W}) \\\\ T_{P}(\\beta, \\theta) = \\bar{T} + B_{S}(\\beta) + B_{P}(\\theta) \\end{align*} $$ Where:\n$W \\in \\mathbb{R}^{3N}$ outputs a mesh with $N$ vertices in 3D space $T_{P}(\\beta, \\theta)$ is the pose and shape parameters $J : \\mathbb{R}^{\\lvert \\beta \\rvert} \\to \\mathbb{R}^{3K}$ is the joint locations $\\beta \\in \\mathbb{R}^{\\lvert \\beta \\rvert}$ are the shape parameters, usually 10 $\\theta \\in \\mathbb{R}^{3K}$ are the pose parameters (joint angles for K joints) $\\mathcal{W} \\in \\mathbb{R}^{K' \\times 3N}$ is the skinning weights, with $K' \\ll K$ $\\bar{T} \\in \\mathbb{R}^{3N}$ rest pose $B_{S}(\\beta) \\in \\mathbb{R}^{3N}$ is the shape blend shapes $B_{P}(\\theta) \\in \\mathbb{R}^{3N}$ is the pose blend shapes Rest Pose and Skinning Weights The template was designed by an artist with 6.890 vertices and a segmentation part.\nThe skinning weights (sparse matrix) $\\mathcal{W}$ define how much every vertex is influenced by every joint under articulation (maximum of 4 joints) Joints and Shapes The shape blend shapes are linear offsets to represent the person’s shape. usually first 3 principal components. And it has different models for male and female.\n$$ J(\\beta) = \\mathcal{J}(\\bar{T} + B_{S}(\\beta)) $$$$ B_{S}(\\beta) = \\sum_{i = 1}^{m} b_{i} \\beta_{i} $$ and $b_{i}$ are the shape displacements learned with PCA from data.\nPosing and Skinning The pose assumes bone lengths are fixed, you can just do some rotations for every joint (rotations). So a pose is just a matrix of rotation information. SMPL uses angle-axis formulation around a normalized axis of rotation. Posing is determining the orientation of the joints.\n$$ R_{k}^{\\text{ global }} = R_{k}^{\\text{ local }} R_{A(k)}^{\\text{ global }} $$ Where $A(k)$ is the parent of joint $k$\nFrom the rest position to posed space is named skinning, which is a rigid transformation. The details of the math is not important, so I will not report here.\n$$ \\sum_{k}w_{ji}G_{k}(\\theta, J)t_{i} $$Artifacts LBS produces artifacts such as:\nCandy-wrapper artifact Collapsed joints (no good volume). You add corrections that make the shape look weird in unposed state, but ok in posed state. Artists usually add pose corrections, or you can learn it from the data. We formulate the pose blend shapes as a linear function of the elements of the part rotation matrices.\nSpecifically, SMPL is represented by pose $\\boldsymbol{\\theta}$ and shape $\\boldsymbol{\\beta}$ parameters. Given blend skinning weights $w_{ki}$, rigid bone transformation $\\mathbf{G}_k(\\boldsymbol{\\theta}, \\mathbf{J}(\\boldsymbol{\\beta}))$, joint locations $\\mathbf{J}$, shape correctives $s_i(\\boldsymbol{\\beta})$, and pose correctives $p_i(\\boldsymbol{\\theta})$ of a template mesh in rest pose, a deformed vertex $\\mathbf{v}_i'$ is given by:\n$$ \\mathbf{v}_i' = \\sum_k w_{ki} \\mathbf{G}_k(\\boldsymbol{\\theta}, \\mathbf{J}(\\boldsymbol{\\beta})) (\\mathbf{v}_i + s_i(\\boldsymbol{\\beta}) + p_i(\\boldsymbol{\\theta})) $$Pose Correctives TODO, very strange formulation, difficult to understand…\nDatasets and Registration It starts with data gathering wit 3dMD scanners or similar scanning techniques that return point clouds. You need two datasets, shape (to get shape distributions), and pose datasets.\nThen you use a shape dataset (high resolution scans of people in roughly the same pose) to learn the shape distribution in the population, of about 2k samples. There is a second dataset, the pose dataset, which is not publicly available for SMPL (Faust publishes some datasets). They put patterns on the skin to compare something I did not understood.\nThe high resolution scans are much higher resolution compared to possible templates. The poses can be complex, can have self-contact and lots of smooth areas. This is again some kind of a chicken and egg problem. Here they used co-registration where they solve it in a joint manner, they basically moved the shape into the scan to match.\nTraining the SMPL model Initialization of the parameters Our loss is the minimizing Euclidean surface reconstruction error. Our parameters are $\\Phi = \\left\\{ \\bar{T}, \\mathcal{W}, \\mathcal{J}, \\mathcal{S}, \\mathcal{P} \\right\\}$, with some regularization techniques to:\nPose blend shapes $\\mathcal{P}$ close to 0. Skinning weights $\\mathcal{W}$ are initialized well (don’t know how close) and kept close (else it would probably not converge) Joint influence $\\mathcal{J}$ is assumed to be local. Pose Normalization After, we train the T and S on shape dataset using PCA. and all shapes have the same pose normalization (we don’t want to learn differences due to pose).\noptimization problem is formulated as:\n$$ \\hat{\\mathbf{T}}_j^S = \\underset{\\hat{\\mathbf{T}}}{\\arg \\min} \\| W(\\hat{\\mathbf{T}} + B_P(\\vec{\\theta}_j; P), \\mathcal{J}\\hat{T}, \\vec{\\theta}_j, \\mathcal{W}) - \\mathbf{V}_j^S \\|^2 $$Where:\n$\\hat{\\mathbf{T}}_j^S$: The optimized template shape for the $j$-th registration. $\\hat{\\mathbf{T}}$: The template shape in T-Pose. $B_P(\\vec{\\theta}_j; P)$: A blend shape function that deforms the template based on pose parameters $\\vec{\\theta}_j$ and blend shape bases $P$, so that the only part that is unknown is the shape. $\\mathbf{J}$: The Jacobian matrix relating pose parameters to vertex deformations. $\\vec{\\theta}_j$: The pose parameters for the $j$-th shape. $W$: Weights associated with the vertices or regions. $\\mathbf{V}_j^S$: The $j$-th registered shape in the dataset. PCA is computed on $\\hat{T}_{j}^{S}$, which is the final shape.\nThe deformed template, representing the shape in a specific pose, is given by:\n$$ \\hat{\\mathbf{T}} + B_P(\\vec{\\theta}_j; P) $$ The optimization aims to find the template shape $\\hat{\\mathbf{T}}$ that best fits all the registered shapes in the dataset after accounting for pose variations.\nOther Applications The SMPL family SMPL+H adds hand joints, the standard does not have it. FLAME builds face models SMPL-X adds expressive (with facial expressions, models are more lifelike, they add articulations for the face) STAR adds more shape spaces, they can match more extreme shapes of the human body. SMPL-D (displacement) to add some shapes for the clothes. It has some limitations (fixed resolution, but perhaps clothing needs more), this is some kind of a explicit modelling of the shape. Human Mesh Recovery See paper (Kanazawa et al. 2018). They use direct parameter regression to SMPL parmeters using Neural Networks, starting from single images, using 3D estimations. Statistical human body model to retrieve the correct model: SMPLify $$ \\hat{\\theta}, \\hat{\\beta} = \\underset{\\theta, \\beta}{\\arg \\min} \\| P(\\theta, \\beta) - P_{2D} \\|^2 + \\lambda_1 \\| J(\\theta, \\beta) - J_{2D} \\|^2 + \\lambda_2 \\| S(\\theta, \\beta) - S_{2D} \\|^2 $$ Where:\n$\\hat{\\theta}$: The estimated pose parameters. $\\hat{\\beta}$: The estimated shape parameters. $P(\\theta, \\beta)$: The predicted 3D joint locations. $P_{2D}$: The observed 2D joint locations. $J(\\theta, \\beta)$: The predicted 3D joint locations. $J_{2D}$: The observed 2D joint locations. $S(\\theta, \\beta)$: The predicted 3D mesh. $S_{2D}$: The observed 2D mesh. $\\lambda_1, \\lambda_2$: Regularization parameters. The idea is to add some regularization priors and try to match the 2D joints to the 3D joints. The problem is that the 2D joints are not always accurate, and the optimization can be difficult. One thing to note is that this paper was published in 2016.\nLearned Gradient Descent They show this provides much faster convergence, published in 2020. The problem was that the above descent was handcrafted, in this way you are using some differential network to estimate the difference, which is somewhat close to the ideas in Normalizing Flows for continuous ODE (see (Chen et al. 2019)).\nThey show this technique is much faster to converge compared to SMPLify.\nFitting IMU measurements IMU are Inertial Measurement Units, which are used to measure acceleration and angular velocity. They can be used to fit the SMPL model to the IMU measurements. So that you can learn some actions.\nYou need a cost function for the orientation, and use that to regress to SMPL parameters and check the 3D pose of the human model.\nModelling Clothing This lab proposed to add some modifications of the shape to model clothing. Problem is that the quality is fixed, something which is not always what we want. This is why sometimes implicit representations are better (see Advanced 3D Representations).\nModifying shapes is some form of an explicit representation.\nReferences [1] Chen et al. “Neural Ordinary Differential Equations” arXiv preprint arXiv:1806.07366 2019 [2] Kanazawa et al. “End-to-End Recovery of Human Shape and Pose” IEEE 2018 ",
  "wordCount" : "2464",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "2025-05-08T00:00:00Z",
  "dateModified": "2025-05-08T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang Angelo Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/parametric-human-body-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Parametric Human Body Models
    </h1>
    <div class="post-meta"><span title='2025-05-08 00:00:00 +0000 UTC'>May 8, 2025</span>&nbsp;·&nbsp;Reading Time: 12 minutes&nbsp;·&nbsp;
By Xuanqiang Angelo Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#an-historical-perspective" aria-label="An historical perspective">An historical perspective</a><ul>
                        
                <li>
                    <a href="#the-origins-of-motion-capture" aria-label="The origins of motion capture">The origins of motion capture</a></li>
                <li>
                    <a href="#human-motion-capture" aria-label="Human motion capture">Human motion capture</a></li>
                <li>
                    <a href="#problems-in-human-motion-capture" aria-label="Problems in Human motion capture">Problems in Human motion capture</a></li>
                <li>
                    <a href="#early-models-mass-spring-and-pictorial-structures" aria-label="Early Models: Mass Spring and Pictorial Structures">Early Models: Mass Spring and Pictorial Structures</a></li></ul>
                </li>
                <li>
                    <a href="#modern-approaches" aria-label="Modern Approaches">Modern Approaches</a><ul>
                        
                <li>
                    <a href="#earliest-breakthroughs-deep-features" aria-label="Earliest breakthroughs: Deep Features">Earliest breakthroughs: Deep Features</a></li>
                <li>
                    <a href="#convolutional-pose-machines" aria-label="Convolutional Pose Machines">Convolutional Pose Machines</a></li>
                <li>
                    <a href="#openpose-and-part-affinity-fields" aria-label="OpenPose and Part Affinity Fields">OpenPose and Part Affinity Fields</a></li>
                <li>
                    <a href="#vitpose" aria-label="ViTPose">ViTPose</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#impactful-models" aria-label="Impactful models">Impactful models</a><ul>
                        
                <li>
                    <a href="#blanz-and-vetter-face-model" aria-label="Blanz and Vetter Face model">Blanz and Vetter Face model</a><ul>
                        
                <li>
                    <a href="#linear-face-representations" aria-label="Linear face representations">Linear face representations</a></li>
                <li>
                    <a href="#dense-correspondence" aria-label="Dense correspondence">Dense correspondence</a></li>
                <li>
                    <a href="#sensible-coefficients" aria-label="Sensible Coefficients">Sensible Coefficients</a></li></ul>
                </li>
                <li>
                    <a href="#smpl" aria-label="SMPL">SMPL</a><ul>
                        
                <li>
                    <a href="#the-model" aria-label="The model">The model</a></li>
                <li>
                    <a href="#rest-pose-and-skinning-weights" aria-label="Rest Pose and Skinning Weights">Rest Pose and Skinning Weights</a></li>
                <li>
                    <a href="#joints-and-shapes" aria-label="Joints and Shapes">Joints and Shapes</a></li>
                <li>
                    <a href="#posing-and-skinning" aria-label="Posing and Skinning">Posing and Skinning</a></li>
                <li>
                    <a href="#artifacts" aria-label="Artifacts">Artifacts</a></li>
                <li>
                    <a href="#pose-correctives" aria-label="Pose Correctives">Pose Correctives</a></li>
                <li>
                    <a href="#datasets-and-registration" aria-label="Datasets and Registration">Datasets and Registration</a></li></ul>
                </li>
                <li>
                    <a href="#training-the-smpl-model" aria-label="Training the SMPL model">Training the SMPL model</a><ul>
                        
                <li>
                    <a href="#initialization-of-the-parameters" aria-label="Initialization of the parameters">Initialization of the parameters</a></li>
                <li>
                    <a href="#pose-normalization" aria-label="Pose Normalization">Pose Normalization</a></li></ul>
                </li>
                <li>
                    <a href="#other-applications" aria-label="Other Applications">Other Applications</a><ul>
                        
                <li>
                    <a href="#the-smpl-family" aria-label="The SMPL family">The SMPL family</a></li>
                <li>
                    <a href="#human-mesh-recovery" aria-label="Human Mesh Recovery">Human Mesh Recovery</a></li>
                <li>
                    <a href="#smplify" aria-label="SMPLify">SMPLify</a></li>
                <li>
                    <a href="#learned-gradient-descent" aria-label="Learned Gradient Descent">Learned Gradient Descent</a></li>
                <li>
                    <a href="#fitting-imu-measurements" aria-label="Fitting IMU measurements">Fitting IMU measurements</a></li>
                <li>
                    <a href="#modelling-clothing" aria-label="Modelling Clothing">Modelling Clothing</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="an-historical-perspective">An historical perspective<a hidden class="anchor" aria-hidden="true" href="#an-historical-perspective">#</a></h3>
<h4 id="the-origins-of-motion-capture">The origins of motion capture<a hidden class="anchor" aria-hidden="true" href="#the-origins-of-motion-capture">#</a></h4>
<p>One of the earliest starts of <em>motion capturing</em> is the famous horse in 1878 in motion &ldquo;video&rdquo;. This was the start of all the modern cameras. One of the earliest human body motion capture was in military for moving efficiency purposes in 1883. <a href="https://www.scienceandmediamuseum.org.uk/objects-and-stories/surprising-origins-motion-capture">This</a> website has many historical resources on the topic.
The problem is still a problem in modern times. If we want to create models to mimic humans, it surely could be nice to understand how humans move and think. This is the general line of though of this line of research.</p>
<h4 id="human-motion-capture">Human motion capture<a hidden class="anchor" aria-hidden="true" href="#human-motion-capture">#</a></h4>
<p>One interesting experiment is that humans seem to automatically group dots that move together. It is research from many years ago, in <a href="https://www.scienceandmediamuseum.org.uk/objects-and-stories/surprising-origins-motion-capture">Johansoon</a>. Ten or twelve dots were enough for humans to have enough contextual information to have it make sense. This was the most important point of that research.</p>
<h4 id="problems-in-human-motion-capture">Problems in Human motion capture<a hidden class="anchor" aria-hidden="true" href="#problems-in-human-motion-capture">#</a></h4>
<p>Following this idea, one interesting approach could be estimating 2D points from human images.
The problems is taking into account for</p>
<ul>
<li>Plane rotations (head in different orientations)</li>
<li>Scaling</li>
<li>Perspectives</li>
<li>Aspect ratio of different humans</li>
<li>Intra-category variation (e.g. masked faces, particular faces with helmets, they should still be considered human faces)
This idea first started with modelling with <em>pictorial structures</em>, like rectangles and things similar to that to model human faces.
At the time, they didn&rsquo;t have much data, and it was very difficult to extract those images. One of the earliest breakthroughs was from 2017 Cat et al. paper.</li>
</ul>
<h4 id="early-models-mass-spring-and-pictorial-structures">Early Models: Mass Spring and Pictorial Structures<a hidden class="anchor" aria-hidden="true" href="#early-models-mass-spring-and-pictorial-structures">#</a></h4>
<p>Take cylinder primitives and fit to some representation. This was some physics based model to fit into some energy function. This inspired a lot of research. This was mainly a problem on <em>representation</em>. At the time it was very difficult to store images on the disk, there was not so much compute and data.</p>
<img src="/images/notes/Parametric Human Body Models-20250527153447208.webp" style="width: 100%" class="center" alt="Parametric Human Body Models-20250527153447208">
<h3 id="modern-approaches">Modern Approaches<a hidden class="anchor" aria-hidden="true" href="#modern-approaches">#</a></h3>
<p>We know now how to extract features.</p>
<h4 id="earliest-breakthroughs-deep-features">Earliest breakthroughs: Deep Features<a hidden class="anchor" aria-hidden="true" href="#earliest-breakthroughs-deep-features">#</a></h4>
<p>This is called <strong>DeepPose</strong>.
<img src="/images/notes/Parametric Human Body Models-20250508142438571.webp" style="width: 100%" class="center" alt="Parametric Human Body Models-20250508142438571">
Predicting heatmats was better than just predicting 2d points, so then people started to predict heatmaps for every single important joints, then you can sample from the joint.
Then combined with the iterative approach they invented the next model.</p>
<p>You fit narrower and <strong>narrower</strong>versions of the image to the model (stage modelling).
After this, we went to regress for <em>heatmaps</em>, which was a better feature, meaning we have better results.</p>
<h4 id="convolutional-pose-machines">Convolutional Pose Machines<a hidden class="anchor" aria-hidden="true" href="#convolutional-pose-machines">#</a></h4>
<p>The network is able to look at the entire image after some iterations. Spatial correlations are taken into account here.
So that deep models have big receptive field, while early stages are quite local, they add belief headmaps of the conditioned joints.
<img src="/images/notes/Parametric Human Body Models-20250508142624190.webp" style="width: 100%" class="center" alt="Parametric Human Body Models-20250508142624190"></p>
<h4 id="openpose-and-part-affinity-fields">OpenPose and Part Affinity Fields<a hidden class="anchor" aria-hidden="true" href="#openpose-and-part-affinity-fields">#</a></h4>
<p>The use convolutional pose machines with iterative refinement and another branch with <strong>part affinity fields</strong>, to help with the association problem. This is a <em>bottom-up approach</em>, since we are feeding the whole image, without any preprocessing with that.
The problem is that maybe we have many predictions for a single arm, and you need to join them together.</p>
<p>Part <strong>affinity fields</strong>:</p>
<ul>
<li>For every limb you create a unit vector that represents the moving direction of that limb. This helps with association strategies.</li>
<li>Gives you direction of limbs that are then useful to solve the problem better, it looks very much like step by step solution in a supervised manner, closer to diffusions?</li>
</ul>
<h4 id="vitpose">ViTPose<a hidden class="anchor" aria-hidden="true" href="#vitpose">#</a></h4>
<p>This is an example of a <em>top-down</em> approach (first use YOLO or similar things to have a good detection). There are many many other models that attempt to attack this section.</p>
<p>You get bounding boxes, and then fit the bounding boxes to a model that gives 2D detection, this helps you solve it without affinity fields, but needs 10x if there are 10 people in the image after you get a bounding box.</p>
<p>Usually 2D for humans, it is easy to annotate and check, but for 3D it is difficult to record and capture, we don&rsquo;t have enough data for 3D, but we need to do it for this kind of problems, there are many papers that attacked this kind of zone.</p>
<h2 id="impactful-models">Impactful models<a hidden class="anchor" aria-hidden="true" href="#impactful-models">#</a></h2>
<h3 id="blanz-and-vetter-face-model">Blanz and Vetter Face model<a hidden class="anchor" aria-hidden="true" href="#blanz-and-vetter-face-model">#</a></h3>
<p>This is one of the classical statistical face models.. It is a statistical model that can be used to generate 3D human faces from 2D images. This example is still relevant today after 26 years.</p>
<h4 id="linear-face-representations">Linear face representations<a hidden class="anchor" aria-hidden="true" href="#linear-face-representations">#</a></h4>
$$
S = \sum_{i = 1}^{m} a_{i} S_{i}
$$<p>
We want to have a general model that can generate all the faces.</p>
<p>This is desirable to have for:</p>
<ul>
<li><strong>Simplicity</strong> in generation, you just need to find parameters starting from a fixed set of parameters.</li>
<li><strong>Generative</strong> model: we can generate new faces starting from the ones we have.</li>
</ul>
<p>It is still hard problem, since just linear interpolation of pixels surely does not work.</p>
<h4 id="dense-correspondence">Dense correspondence<a hidden class="anchor" aria-hidden="true" href="#dense-correspondence">#</a></h4>
<p>We want point to point correspondence between parts of one face to another, i.e. a right eye corresponds to a right eye on the other face. One solution is adding one <strong>template face</strong> of <em>fixed topology</em> that helps in solving this representation.
Yet this is some sort of a chicken and egg problem: having a morphable model would greatly help
with the registration, but to build a morphable model we need to do registration.</p>
<img src="/images/notes/Parametric Human Body Models-20250527155226372.webp" style="width: 100%" class="center" alt="Parametric Human Body Models-20250527155226372">
<p>Use <strong>bootstrapping</strong>, the same idea we usually use to build compilers or RL <a href="/notes/tabular-reinforcement-learning">Tabular Reinforcement Learning</a>, <a href="/notes/macchine-astratte">Macchine Astratte</a>, we have an algorithm that gives us a first model, and refine it iteratively.</p>
<h4 id="sensible-coefficients">Sensible Coefficients<a hidden class="anchor" aria-hidden="true" href="#sensible-coefficients">#</a></h4>
<p>How can you vary the coefficients to build some faces that do make sense? How to be sure that the produced face does indeed make sense? It was very difficult to get 3D data at the time, so the used Principal Component Analysis. You just assume that all shapes are in full correspondence, with a given topology, and compute the eigenvectors of the given faces dataset, it is exactly that.
Given a set of $S_{i}$, you should be able to compute the principal components and take the eigenvectors.</p>
$$
x = Uc = \sum_{i = 1}^{m} c_{i} \boldsymbol{u_{i}} = \sum_{i= 1}^{m} \left( U^{T}x_{i} \right) \boldsymbol{u_{i}}
$$<p>You might also restrict this to a subset of the eigenvectors, to reduce the dimensionality of the problem.
The geometry is high quality, and it is able to give 3D shapes from 2D images.</p>
<p>Statistical -&gt; from data
Shape -&gt; from geometry priors.</p>
<h3 id="smpl">SMPL<a hidden class="anchor" aria-hidden="true" href="#smpl">#</a></h3>
<p>Published in SIGGRAPH Asia 2025. MPI in Tübingen, Germany, also called the SMPL Family.</p>
<p>SMPL (skinned, multi-person linear model) is a industry-standard built on linear-blend skinning (gives you a model that <strong>can be animated</strong>), used in both industry and research to model human bodies. It is a parametric model that can be used to generate 3D human bodies from 2D images.
It is just an <em>additive model</em>, meaning it is simple, but with lots of parameters.</p>
<h4 id="the-model">The model<a hidden class="anchor" aria-hidden="true" href="#the-model">#</a></h4>
$$
S = M(\theta, \beta)
$$<p>
Where $S$ is the 3D mesh and joints, $M$ is the SMPL model, $\theta$ is the pose parameters and $\beta$ is the shape parameters.</p>
$$
\begin{align*}
M(\theta, \beta) = W(T_{p}(\beta, \theta), J(\beta), \theta, \mathcal{W}) \\
T_{P}(\beta, \theta) = \bar{T} + B_{S}(\beta) + B_{P}(\theta)
\end{align*}
$$<p>
Where:</p>
<ul>
<li>$W \in \mathbb{R}^{3N}$ outputs a mesh with $N$ vertices in 3D space</li>
<li>$T_{P}(\beta, \theta)$ is the pose and shape parameters</li>
<li>$J : \mathbb{R}^{\lvert \beta \rvert} \to \mathbb{R}^{3K}$ is the joint locations</li>
<li>$\beta \in \mathbb{R}^{\lvert \beta \rvert}$ are the shape parameters, usually 10</li>
<li>$\theta \in \mathbb{R}^{3K}$ are the pose parameters (joint angles for K joints)</li>
<li>$\mathcal{W} \in \mathbb{R}^{K' \times 3N}$ is the skinning weights, with $K' \ll K$</li>
<li>$\bar{T} \in \mathbb{R}^{3N}$ rest pose</li>
<li>$B_{S}(\beta) \in \mathbb{R}^{3N}$ is the shape blend shapes</li>
<li>$B_{P}(\theta) \in \mathbb{R}^{3N}$ is the pose blend shapes</li>
</ul>
<img src="/images/notes/Parametric Human Body Models-20250518162343115.webp" style="width: 100%" class="center" alt="Parametric Human Body Models-20250518162343115">
<h4 id="rest-pose-and-skinning-weights">Rest Pose and Skinning Weights<a hidden class="anchor" aria-hidden="true" href="#rest-pose-and-skinning-weights">#</a></h4>
<p>The template was designed by an artist with 6.890 vertices and a segmentation part.</p>
<ul>
<li>The skinning weights (sparse matrix) $\mathcal{W}$ define how much every vertex is influenced by every joint under articulation (maximum of 4 joints)</li>
</ul>
<h4 id="joints-and-shapes">Joints and Shapes<a hidden class="anchor" aria-hidden="true" href="#joints-and-shapes">#</a></h4>
<p>The shape blend shapes are linear offsets to represent the person&rsquo;s shape. usually first 3 principal components. And it has different models for male and female.</p>
$$
J(\beta) = \mathcal{J}(\bar{T} + B_{S}(\beta))
$$$$
B_{S}(\beta) = \sum_{i = 1}^{m} b_{i} \beta_{i}
$$<p>
and $b_{i}$ are the shape displacements learned with PCA from data.</p>
<h4 id="posing-and-skinning">Posing and Skinning<a hidden class="anchor" aria-hidden="true" href="#posing-and-skinning">#</a></h4>
<p>The pose assumes bone lengths are <strong>fixed</strong>, you can just do some rotations for every joint (rotations). So a pose is just a matrix of rotation information.
SMPL uses <strong>angle-axis</strong> formulation around a normalized axis of rotation. Posing is determining the orientation of the joints.</p>
$$
R_{k}^{\text{ global }} = R_{k}^{\text{ local }} R_{A(k)}^{\text{ global }}
$$<p>
Where $A(k)$ is the parent of joint $k$</p>
<p>From the rest position to posed space is named <strong>skinning</strong>, which is a rigid transformation.
The details of the math is not important, so I will not report here.</p>
$$
\sum_{k}w_{ji}G_{k}(\theta, J)t_{i}
$$<h4 id="artifacts">Artifacts<a hidden class="anchor" aria-hidden="true" href="#artifacts">#</a></h4>
<p>LBS produces artifacts such as:</p>
<ul>
<li>Candy-wrapper artifact</li>
<li>Collapsed joints (no good volume).
You add corrections that make the shape look weird in unposed state, but ok in posed state. Artists usually add <strong>pose corrections</strong>, or you can learn it from the data.</li>
</ul>
<blockquote>
<p>We formulate the pose blend shapes as a linear function of the elements of the part rotation matrices.</p></blockquote>
<p>Specifically, SMPL is represented by pose $\boldsymbol{\theta}$ and shape $\boldsymbol{\beta}$ parameters. Given blend skinning weights $w_{ki}$, rigid bone transformation $\mathbf{G}_k(\boldsymbol{\theta}, \mathbf{J}(\boldsymbol{\beta}))$, joint locations $\mathbf{J}$, shape correctives $s_i(\boldsymbol{\beta})$, and pose correctives $p_i(\boldsymbol{\theta})$ of a template mesh in rest pose, a deformed vertex $\mathbf{v}_i'$ is given by:</p>
$$
\mathbf{v}_i' = \sum_k w_{ki} \mathbf{G}_k(\boldsymbol{\theta}, \mathbf{J}(\boldsymbol{\beta})) (\mathbf{v}_i + s_i(\boldsymbol{\beta}) + p_i(\boldsymbol{\theta}))
$$<h4 id="pose-correctives">Pose Correctives<a hidden class="anchor" aria-hidden="true" href="#pose-correctives">#</a></h4>
<p>TODO, very strange formulation, difficult to understand&hellip;</p>
<h4 id="datasets-and-registration">Datasets and Registration<a hidden class="anchor" aria-hidden="true" href="#datasets-and-registration">#</a></h4>
<p>It starts with data gathering wit 3dMD scanners or similar scanning techniques that return point clouds.
You need two datasets, shape (to get shape distributions), and pose datasets.</p>
<p>Then you use a <strong>shape dataset</strong> (high resolution scans of people in roughly the same pose) to learn the shape distribution in the population, of about 2k samples.
There is a second dataset, the <strong>pose dataset</strong>, which is not publicly available for SMPL (Faust publishes some datasets). They put patterns on the skin to compare something I did not understood.</p>
<p>The high resolution scans are much higher resolution compared to possible templates. The poses can be complex, can have self-contact and lots of smooth areas. This is again some kind of a chicken and egg problem. Here they used <strong>co-registration</strong> where they solve it in a joint manner, they basically <em>moved the shape into the scan to match</em>.</p>
<h3 id="training-the-smpl-model">Training the SMPL model<a hidden class="anchor" aria-hidden="true" href="#training-the-smpl-model">#</a></h3>
<h4 id="initialization-of-the-parameters">Initialization of the parameters<a hidden class="anchor" aria-hidden="true" href="#initialization-of-the-parameters">#</a></h4>
<p>Our loss is the <em>minimizing Euclidean surface reconstruction error</em>.
Our parameters are $\Phi = \left\{ \bar{T}, \mathcal{W}, \mathcal{J}, \mathcal{S}, \mathcal{P} \right\}$, with some regularization techniques to:</p>
<ul>
<li>Pose blend shapes $\mathcal{P}$ close to 0.</li>
<li>Skinning weights $\mathcal{W}$ are initialized well (don&rsquo;t know how close) and <strong>kept close</strong> (else it would probably not converge)</li>
<li>Joint influence $\mathcal{J}$ is assumed to be local.</li>
</ul>
<h4 id="pose-normalization">Pose Normalization<a hidden class="anchor" aria-hidden="true" href="#pose-normalization">#</a></h4>
<p>After, we train the T and S on shape dataset <strong>using PCA</strong>. and all shapes have the same <strong>pose normalization</strong> (we don&rsquo;t want to learn differences due to pose).</p>
<p>optimization problem is formulated as:</p>
$$
\hat{\mathbf{T}}_j^S = \underset{\hat{\mathbf{T}}}{\arg \min} \| W(\hat{\mathbf{T}} + B_P(\vec{\theta}_j; P), \mathcal{J}\hat{T}, \vec{\theta}_j, \mathcal{W}) - \mathbf{V}_j^S \|^2
$$<p>Where:</p>
<ul>
<li>$\hat{\mathbf{T}}_j^S$: The optimized template shape for the $j$-th registration.</li>
<li>$\hat{\mathbf{T}}$: The template shape in T-Pose.</li>
<li>$B_P(\vec{\theta}_j; P)$: A blend shape function that deforms the template based on pose parameters $\vec{\theta}_j$ and blend shape bases $P$, so that the only part that is unknown is the <strong>shape</strong>.</li>
<li>$\mathbf{J}$: The Jacobian matrix relating pose parameters to vertex deformations.</li>
<li>$\vec{\theta}_j$: The pose parameters for the $j$-th shape.</li>
<li>$W$: Weights associated with the vertices or regions.</li>
<li>$\mathbf{V}_j^S$: The $j$-th <em>registered</em> shape in the dataset.</li>
</ul>
<p>PCA is computed  on $\hat{T}_{j}^{S}$, which is the final shape.</p>
<p>The deformed template, representing the shape in a specific pose, is given by:</p>
$$
\hat{\mathbf{T}} + B_P(\vec{\theta}_j; P)
$$<p>
The optimization aims to find the template shape $\hat{\mathbf{T}}$ that best fits all the registered shapes in the dataset after accounting for pose variations.</p>
<h3 id="other-applications">Other Applications<a hidden class="anchor" aria-hidden="true" href="#other-applications">#</a></h3>
<h4 id="the-smpl-family">The SMPL family<a hidden class="anchor" aria-hidden="true" href="#the-smpl-family">#</a></h4>
<img src="/images/notes/Parametric Human Body Models-20250519134103329.webp" style="width: 100%" class="center" alt="Parametric Human Body Models-20250519134103329">
<ul>
<li>SMPL+H adds hand joints, the standard does not have it.</li>
<li>FLAME builds face models</li>
<li>SMPL-X adds expressive (with facial expressions, models are more lifelike, they add articulations for the face)</li>
<li>STAR adds more shape spaces, they can match more extreme shapes of the human body.</li>
<li>SMPL-D (displacement) to add some shapes for the clothes.
<ul>
<li>It has some limitations (fixed resolution, but perhaps clothing needs more), this is some kind of a explicit modelling of the shape.</li>
</ul>
</li>
</ul>
<h4 id="human-mesh-recovery">Human Mesh Recovery<a hidden class="anchor" aria-hidden="true" href="#human-mesh-recovery">#</a></h4>
<p>See paper <a href="https://ieeexplore.ieee.org/document/8578842/">(Kanazawa et al. 2018)</a>.
They use <strong>direct parameter regression</strong> to SMPL parmeters using <a href="/notes/neural-networks">Neural Networks</a>, starting from single images, using 3D estimations.
Statistical human body model to retrieve the correct model:
<img src="/images/notes/Parametric Human Body Models-20250519134459923.webp" style="width: 100%" class="center" alt="Parametric Human Body Models-20250519134459923"></p>
<h4 id="smplify">SMPLify<a hidden class="anchor" aria-hidden="true" href="#smplify">#</a></h4>
$$
\hat{\theta}, \hat{\beta} = \underset{\theta, \beta}{\arg \min} \| P(\theta, \beta) - P_{2D} \|^2 + \lambda_1 \| J(\theta, \beta) - J_{2D} \|^2 + \lambda_2 \| S(\theta, \beta) - S_{2D} \|^2
$$<p>
Where:</p>
<ul>
<li>$\hat{\theta}$: The estimated pose parameters.</li>
<li>$\hat{\beta}$: The estimated shape parameters.</li>
<li>$P(\theta, \beta)$: The predicted 3D joint locations.</li>
<li>$P_{2D}$: The observed 2D joint locations.</li>
<li>$J(\theta, \beta)$: The predicted 3D joint locations.</li>
<li>$J_{2D}$: The observed 2D joint locations.</li>
<li>$S(\theta, \beta)$: The predicted 3D mesh.</li>
<li>$S_{2D}$: The observed 2D mesh.</li>
<li>$\lambda_1, \lambda_2$: Regularization parameters.</li>
</ul>
<p>The idea is to add some regularization <strong>priors</strong> and try to match the 2D joints to the 3D joints. The problem is that the 2D joints are not always accurate, and the optimization can be difficult.
One thing to note is that this paper was published in 2016.</p>
<h4 id="learned-gradient-descent">Learned Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#learned-gradient-descent">#</a></h4>
<img src="/images/notes/Parametric Human Body Models-20250519135006873.webp" style="width: 100%" class="center" alt="Parametric Human Body Models-20250519135006873">
<p>They show this provides much <strong>faster</strong> convergence, published in 2020.
The problem was that the above descent was handcrafted, in this way you are using some differential network to estimate the difference, which is somewhat close to the ideas in <a href="/notes/normalizing-flows">Normalizing Flows</a> for continuous ODE (see <a href="http://arxiv.org/abs/1806.07366">(Chen et al. 2019)</a>).</p>
<p>They show this technique is much faster to converge compared to SMPLify.</p>
<h4 id="fitting-imu-measurements">Fitting IMU measurements<a hidden class="anchor" aria-hidden="true" href="#fitting-imu-measurements">#</a></h4>
<p>IMU are Inertial Measurement Units, which are used to measure acceleration and angular velocity. They can be used to fit the SMPL model to the IMU measurements. So that you can learn some actions.</p>
<p>You need a cost function for the orientation, and use that to regress to SMPL parameters and check the 3D pose of the human model.</p>
<h4 id="modelling-clothing">Modelling Clothing<a hidden class="anchor" aria-hidden="true" href="#modelling-clothing">#</a></h4>
<p>This lab proposed to add some modifications of the shape to model clothing. Problem is that the quality is fixed, something which is not always what we want. This is why sometimes implicit representations are better (see <a href="/notes/advanced-3d-representations">Advanced 3D Representations</a>).</p>
<p>Modifying shapes is some form of an explicit representation.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=chenNeuralOrdinaryDifferential2019>[1] Chen et al. <a href="http://arxiv.org/abs/1806.07366">“Neural Ordinary Differential Equations”</a> arXiv preprint arXiv:1806.07366 2019
 </p>
<p id=kanazawaEndtoEndRecoveryHuman2018>[2] Kanazawa et al. <a href="https://ieeexplore.ieee.org/document/8578842/">“End-to-End Recovery of Human Shape and Pose”</a> IEEE  2018
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machine-perception/">Machine-Perception</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Parametric Human Body Models on x"
            href="https://x.com/intent/tweet/?text=Parametric%20Human%20Body%20Models&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fparametric-human-body-models%2f&amp;hashtags=machine-perception">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Parametric Human Body Models on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fparametric-human-body-models%2f&amp;title=Parametric%20Human%20Body%20Models&amp;summary=Parametric%20Human%20Body%20Models&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fparametric-human-body-models%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Parametric Human Body Models on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fparametric-human-body-models%2f&title=Parametric%20Human%20Body%20Models">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Parametric Human Body Models on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fparametric-human-body-models%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Parametric Human Body Models on whatsapp"
            href="https://api.whatsapp.com/send?text=Parametric%20Human%20Body%20Models%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fparametric-human-body-models%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Parametric Human Body Models on telegram"
            href="https://telegram.me/share/url?text=Parametric%20Human%20Body%20Models&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fparametric-human-body-models%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Parametric Human Body Models on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Parametric%20Human%20Body%20Models&u=https%3a%2f%2fflecart.github.io%2fnotes%2fparametric-human-body-models%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
