<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Recurrent Neural Networks | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="ðŸ’¬natural-language-processing, machine-perception">
<meta name="description" content="Recurrent Neural Networks allows us to model arbitrarily long sequence dependencies, at least in theory (this is also why they seem a very nice choice in theory for time series). This is very handy, and has many interesting theoretical implication. But here we are also interested in the practical applicability, so we may need to analyze common architectures used to implement these models, the main limitation and drawbacks, the nice properties and some applications.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/recurrent-neural-networks/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/recurrent-neural-networks/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/recurrent-neural-networks/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Recurrent Neural Networks">
  <meta property="og:description" content="Recurrent Neural Networks allows us to model arbitrarily long sequence dependencies, at least in theory (this is also why they seem a very nice choice in theory for time series). This is very handy, and has many interesting theoretical implication. But here we are also interested in the practical applicability, so we may need to analyze common architectures used to implement these models, the main limitation and drawbacks, the nice properties and some applications.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="ðŸ’¬Natural-Language-Processing">
    <meta property="article:tag" content="Machine-Perception">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Recurrent Neural Networks">
<meta name="twitter:description" content="Recurrent Neural Networks allows us to model arbitrarily long sequence dependencies, at least in theory (this is also why they seem a very nice choice in theory for time series). This is very handy, and has many interesting theoretical implication. But here we are also interested in the practical applicability, so we may need to analyze common architectures used to implement these models, the main limitation and drawbacks, the nice properties and some applications.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Recurrent Neural Networks",
      "item": "https://flecart.github.io/notes/recurrent-neural-networks/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Recurrent Neural Networks",
  "name": "Recurrent Neural Networks",
  "description": "Recurrent Neural Networks allows us to model arbitrarily long sequence dependencies, at least in theory (this is also why they seem a very nice choice in theory for time series). This is very handy, and has many interesting theoretical implication. But here we are also interested in the practical applicability, so we may need to analyze common architectures used to implement these models, the main limitation and drawbacks, the nice properties and some applications.\n",
  "keywords": [
    "ðŸ’¬natural-language-processing", "machine-perception"
  ],
  "articleBody": "Recurrent Neural Networks allows us to model arbitrarily long sequence dependencies, at least in theory (this is also why they seem a very nice choice in theory for time series). This is very handy, and has many interesting theoretical implication. But here we are also interested in the practical applicability, so we may need to analyze common architectures used to implement these models, the main limitation and drawbacks, the nice properties and some applications.\nThese network can bee seen as chaotic systems (non-linear dynamical systems), see Introduction to Chaos Theory.\nIntroduction to Recurrent Neural Networks A simple theoretical motivation Recall the content presented in Language Models (this is a hard prerequisite): we want an efficient way to model $p(y \\mid \\boldsymbol{y}_{ \\lt t})$ . We have presented n-gram models (see Language Models#N-gram models) that fixate the context window thus fixating the infinite context window of a locally normalized language model, and we have developed the classical neural n-gram model. We want to build onto this and naturally invent the recurrent neural networks. Letâ€™s start with the new content now.\nWe can compose the hidden state of a previous time step and the known word of the previous context in a new hidden state, and continue in this manner to produce the whole string. This allows, in theory, to have infinite context-length. The difficult thing here is to describe $f$, that is how we combine the previous hidden state with the new token. In this context, we will call $f$ the combiner function.\nVanilla RNNs ðŸŸ©â€“ $$ h_{t} = f(h_{t - 1}, x_{t}; \\theta) $$ where we have a single function that is able to handle different kinds of inputs. We have a single hidden vector $h$.\nVanilla RNNs have the following equations:\n$$ h_{t} = \\tanh (W_{h} h_{t - 1} + W_{x} x_{t} + b) $$$$ o_{t} = W_{o} h_{t} + b_{o} $$Two ways to model dynamical systems ðŸŸ¨++ RNN chose to model it with the second system, the first system is what Autoregressive Modelling does (e.g. Transformers with GPTs). You can see that the second option is much simpler. Types of problems We mainly classify the problems that RNN architectures attack are:\nSequence-To-Sequence: Input and output are sequences of different lengths. Example: Machine translation, where the input is a sentence in one language and the output is a sentence in another language. One-To-One: Input and output are single values. Example: Image classification, where the input is an image and the output is a single class label (difficult to use this for this kind of problem). One-To-Many: - Input is a single value and output is a sequence. - Example: Image captioning, where the input is an image and the output is a sequence of words describing the image. Many-To-One: - Input is a sequence and output is a single value. - Example: Sentiment analysis, where the input is a sequence of words and the output is a single value indicating the sentiment of the text. Some Combiner functions Elman Networks This is the easiest RNN model. LSTM Networks ðŸŸ¨++ The idea here is to have a leaky unit to access past information more directly, so that the gradient flows more regularly and solves that vanishing and exploding gradient problem.\nHere, you donâ€™t need to remember the exact structure of the $f$ function, just remember that there is a way to intuitively imbue in the architecture the idea of retaining information, forgetting information and updating information. Three operations defined with weights. Itâ€™s not important to remember the architecture per se, at least if you donâ€™t want to implement one.\nCell state:Â The cell state is a vector that is maintained by the LSTM cell and is used to store information over time. Gates:Â LSTMs use three gates to control the flow of information into and out of the cell state: Forget gate:Â The forget gate determines which information from the previous cell state to discard. Input gate:Â The input gate determines which new information to add to the cell state. Output gate:Â The output gate determines which information from the cell state to output. LSTM and similar architectures solve the vanishing or exploding gradient problem by substituting them with additions.\nEverything can be rewritten in a more compact manner as follows: $$ \\begin{align*}\n\\begin{pmatrix} i_t \\ f_t \\ o_t \\ \\tilde{c}t \\end{pmatrix} = \\sigma \\left( W \\begin{pmatrix} h{t - 1} \\ x_t \\end{pmatrix} + b \\right) \\ c_t = f_t \\odot c_{t - 1} + i_t \\odot \\tilde{c}_t \\ h_t = o_t \\odot \\tanh(c_t) \\end{align*} $$ Where $W$ is a matrix that contains all the weights of the network, and $\\sigma$ is the sigmoid activation function. The $\\odot$ operator denotes element-wise multiplication. NOTE: $\\sigma$ at the beginning in reality should be tanh for the forget part, here it denotes activations. The LSTM architecture is designed to learn long-term dependencies in sequential data by using a combination of gates and cell states to control the flow of information.\nDrawbacks of LSTM One drawback of LSTMs is that they add a lot of computation compared to RNNs, so they have developed some more lightweight versions of these networks (e.g. GRU).\nMulti Level LSTMs GRU Networks Stochastic RNNs One of the bottlenecks of RNNs is their limited representation space, which is believed to constrain their generation ability.\nDue to the deterministic evolution of the hidden state, RNNs model the variability in the data by means of the conditional output distributions, which may be insufficient for highly structured data such as speech and images. Such data can be characterized by the correlation between the output variables (i.e., multivariate data) and strong dependencies between variables at different time-steps (i.e., long-term dependencies). There also exists a complex relationship between the underlying factors of variation and the observed data.\nThis kind of network contains a VAE at every step.\n$$ h_{t } = f_{\\theta}(h_{t - 1}, z_{t}, x_{t}) $$ During the recurrence step.\nFormal Definitions of RNN Real-valued Recurrent Neural Network A deterministic real-valued recurrent neural network $\\mathcal{R}$ is a four-tuple $(\\Sigma, D, f, \\mathbf{h}_0)$ where:\n$\\Sigma$ is the alphabet of input symbols. $D$ is the dimension of $\\mathcal{R}$. $f: \\mathbb{R}^D \\times \\Sigma \\rightarrow \\mathbb{R}^D$ is the dynamics map, i.e., a function defining the transitions between subsequent states. $\\mathbf{h}_0 \\in \\mathbb{R}^D$ is the initial state. $$h_t = \\frac{1}{2}h_{t-1} + \\frac{1}{h_{t-1}}$$ This corresponds to a rational-valued RNN $(\\Sigma, D, f, h_0)$ where: $\\Sigma = \\{a\\}$ (The input alphabet consists of a single symbol â€˜aâ€™). $D = 1$ (The dimension of the RNN is 1). $f: (\\mathbb{R} \\times \\Sigma) \\rightarrow \\mathbb{R}$ is given by $f(x, a) = \\frac{1}{2}x + \\frac{1}{x}$. * $h_0 = 2$ (The initial state is 2).\nThe Hidden State The idea is to define a neural encoding function, and use that to define a language model on top of the previous definition of recurrent neural network.\nLet $\\mathcal{R} = (\\Sigma, D, f, \\mathbf{h}_0)$ be a real-valued RNN. For an input sequence $\\sigma = \\sigma_1 \\sigma_2 \\dots \\sigma_T \\in \\Sigma^*$, the sequence of states $\\mathbf{h}_0, \\mathbf{h}_1, \\dots, \\mathbf{h}_T \\in \\mathbb{R}^D$ is defined inductively as follows:\n$\\mathbf{h}_0$ is the initial state. For $1 \\le t \\le T$, the state at time $t$ is computed by applying the dynamics map $f$ to the previous state $\\mathbf{h}_{t-1}$ and the current input symbol $\\sigma_t$: $$\\mathbf{h}_t = f(\\mathbf{h}_{t-1}, \\sigma_t)$$ The output of the RNN on the input sequence $\\sigma$ is the final state $\\mathbf{h}_T$. Recurrent neural sequence model Let $\\mathcal{R} = (\\Sigma, D, f, \\mathbf{h}_0)$ be a recurrent neural network and $\\mathbf{E} \\in \\mathbb{R}^{|\\Sigma| \\times D}$ a symbol representation matrix. A $D$-dimensional recurrent neural sequence model over an alphabet $\\Sigma$ is a tuple $(\\Sigma, D, f, \\mathbf{E}, \\mathbf{h}_0)$ defining the sequence model of the form\n$$ p_{\\text{SM}}(y_t \\mid \\mathbf{y}_{",
  "wordCount" : "1287",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/recurrent-neural-networks/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;Â»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Recurrent Neural Networks
    </h1>
    <div class="post-meta">7 min&nbsp;Â·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction-to-recurrent-neural-networks" aria-label="Introduction to Recurrent Neural Networks">Introduction to Recurrent Neural Networks</a><ul>
                        
                <li>
                    <a href="#a-simple-theoretical-motivation" aria-label="A simple theoretical motivation">A simple theoretical motivation</a></li>
                <li>
                    <a href="#vanilla-rnns---" aria-label="Vanilla RNNs ðŸŸ©&ndash;">Vanilla RNNs ðŸŸ©&ndash;</a></li>
                <li>
                    <a href="#two-ways-to-model-dynamical-systems-" aria-label="Two ways to model dynamical systems ðŸŸ¨&#43;&#43;">Two ways to model dynamical systems ðŸŸ¨++</a></li>
                <li>
                    <a href="#types-of-problems" aria-label="Types of problems">Types of problems</a></li></ul>
                </li>
                <li>
                    <a href="#some-combiner-functions" aria-label="Some Combiner functions">Some Combiner functions</a><ul>
                        
                <li>
                    <a href="#elman-networks" aria-label="Elman Networks">Elman Networks</a></li>
                <li>
                    <a href="#lstm-networks-" aria-label="LSTM Networks ðŸŸ¨&#43;&#43;">LSTM Networks ðŸŸ¨++</a><ul>
                        
                <li>
                    <a href="#drawbacks-of-lstm" aria-label="Drawbacks of LSTM">Drawbacks of LSTM</a></li>
                <li>
                    <a href="#multi-level-lstms" aria-label="Multi Level LSTMs">Multi Level LSTMs</a></li></ul>
                </li>
                <li>
                    <a href="#gru-networks" aria-label="GRU Networks">GRU Networks</a></li>
                <li>
                    <a href="#stochastic-rnns" aria-label="Stochastic RNNs">Stochastic RNNs</a></li></ul>
                </li>
                <li>
                    <a href="#formal-definitions-of-rnn" aria-label="Formal Definitions of RNN">Formal Definitions of RNN</a><ul>
                        
                <li>
                    <a href="#real-valued-recurrent-neural-network" aria-label="Real-valued Recurrent Neural Network">Real-valued Recurrent Neural Network</a></li>
                <li>
                    <a href="#the-hidden-state" aria-label="The Hidden State">The Hidden State</a></li>
                <li>
                    <a href="#recurrent-neural-sequence-model" aria-label="Recurrent neural sequence model">Recurrent neural sequence model</a></li></ul>
                </li>
                <li>
                    <a href="#training-rnns" aria-label="Training RNNs">Training RNNs</a><ul>
                        
                <li>
                    <a href="#schematic-presentation-of-bptt-" aria-label="Schematic presentation of BPTT ðŸŸ©">Schematic presentation of BPTT ðŸŸ©</a></li>
                <li>
                    <a href="#backpropagation-through-time-" aria-label="Backpropagation Through Time ðŸŸ¥&#43;&#43;">Backpropagation Through Time ðŸŸ¥++</a></li>
                <li>
                    <a href="#trucated-bptt-" aria-label="Trucated BPTT ðŸŸ©">Trucated BPTT ðŸŸ©</a></li>
                <li>
                    <a href="#vanishing-and-exploding-gradient-problem---" aria-label="Vanishing and Exploding gradient problem ðŸŸ©&ndash;">Vanishing and Exploding gradient problem ðŸŸ©&ndash;</a></li></ul>
                </li>
                <li>
                    <a href="#limitations-of-rnn" aria-label="Limitations of RNN">Limitations of RNN</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Recurrent Neural Networks allows us to model <em>arbitrarily long</em> sequence dependencies, at least in theory (this is also why they seem a very nice choice in theory for time series). This is very handy, and has many interesting theoretical implication. But here we are also interested in the practical applicability, so we may need to analyze common architectures used to implement these models, the main limitation and drawbacks, the nice properties and some applications.</p>
<p>These network can bee seen as <strong>chaotic</strong> systems (non-linear dynamical systems), see Introduction to Chaos Theory.</p>
<h3 id="introduction-to-recurrent-neural-networks">Introduction to Recurrent Neural Networks<a hidden class="anchor" aria-hidden="true" href="#introduction-to-recurrent-neural-networks">#</a></h3>
<h4 id="a-simple-theoretical-motivation">A simple theoretical motivation<a hidden class="anchor" aria-hidden="true" href="#a-simple-theoretical-motivation">#</a></h4>
<p>Recall the content presented in <a href="/notes/language-models">Language Models</a> (this is a hard prerequisite): we want an efficient way to model $p(y \mid \boldsymbol{y}_{ \lt t})$ . We have presented n-gram models (see <a href="/notes/language-models#n-gram-models">Language Models#N-gram models</a>) that fixate the context window thus fixating the infinite context window of a locally normalized language model, and we have developed the classical neural n-gram model. We want to build onto this and naturally invent the recurrent neural networks. Let&rsquo;s start with the new content now.</p>
<p>We can compose the hidden state of a previous time step and the known word of the previous context in a new hidden state, and continue in this manner to produce the whole string.
<img src="/images/notes/Recurrent Neural Networks-20240908132034082.webp" style="width: 100%" class="center" alt="Recurrent Neural Networks-20240908132034082">
This allows, in theory, to have <strong>infinite context-length</strong>.
The difficult thing here is to describe $f$, that is <em>how</em> we combine the previous hidden state with the new token. In this context, we will call $f$ the <em>combiner</em> function.</p>
<h4 id="vanilla-rnns---">Vanilla RNNs ðŸŸ©&ndash;<a hidden class="anchor" aria-hidden="true" href="#vanilla-rnns---">#</a></h4>
$$
h_{t} = f(h_{t - 1}, x_{t}; \theta)
$$<p>
where we have a single function that is able to handle different kinds of inputs. We have a <strong>single hidden vector</strong> $h$.</p>
<p>Vanilla RNNs have the following equations:</p>
$$
h_{t} = \tanh (W_{h} h_{t - 1} + W_{x} x_{t} + b)
$$$$
o_{t} = W_{o} h_{t} + b_{o}
$$<h4 id="two-ways-to-model-dynamical-systems-">Two ways to model dynamical systems ðŸŸ¨++<a hidden class="anchor" aria-hidden="true" href="#two-ways-to-model-dynamical-systems-">#</a></h4>
<img src="/images/notes/Recurrent Neural Networks-20250521113720859.webp" style="width: 100%" class="center" alt="Recurrent Neural Networks-20250521113720859">
RNN chose to model it with the second system, the first system is what <a href="/notes/autoregressive-modelling">Autoregressive Modelling</a> does (e.g. <a href="/notes/transformers">Transformers</a> with GPTs). You can see that the second option is much simpler.
<h4 id="types-of-problems">Types of problems<a hidden class="anchor" aria-hidden="true" href="#types-of-problems">#</a></h4>
<p>We mainly classify the problems that RNN architectures attack are:</p>
<ul>
<li><strong>Sequence-To-Sequence</strong>:
<ul>
<li>Input and output are sequences of different lengths.</li>
<li>Example: Machine translation, where the input is a sentence in one language and the output is a sentence in another language.</li>
</ul>
</li>
<li><strong>One-To-One</strong>:
<ul>
<li>Input and output are single values.</li>
<li>Example: Image classification, where the input is an image and the output is a single class label (difficult to use this for this kind of problem).</li>
</ul>
</li>
<li><strong>One-To-Many</strong>:
- Input is a single value and output is a sequence.
- Example: Image captioning, where the input is an image and the output is a sequence of words describing the image.</li>
<li><strong>Many-To-One</strong>:
- Input is a sequence and output is a single value.
- Example: Sentiment analysis, where the input is a sequence of words and the output is a single value indicating the sentiment of the text.</li>
</ul>
<h3 id="some-combiner-functions">Some Combiner functions<a hidden class="anchor" aria-hidden="true" href="#some-combiner-functions">#</a></h3>
<h4 id="elman-networks">Elman Networks<a hidden class="anchor" aria-hidden="true" href="#elman-networks">#</a></h4>
<img src="/images/notes/Recurrent Neural Networks-20240908132337808.webp" style="width: 100%" class="center" alt="Recurrent Neural Networks-20240908132337808">
This is the easiest RNN model.
<h4 id="lstm-networks-">LSTM Networks ðŸŸ¨++<a hidden class="anchor" aria-hidden="true" href="#lstm-networks-">#</a></h4>
<p>The idea here is to have a <strong>leaky unit</strong> to access past information more directly, so that the gradient flows more regularly and solves that vanishing and exploding gradient problem.</p>
<img src="/images/notes/Recurrent Neural Networks-20240908132411932.webp" style="width: 100%" class="center" alt="Recurrent Neural Networks-20240908132411932">
<p>Here, you don&rsquo;t need to remember the exact structure of the $f$ function, just remember that there is a way to intuitively imbue in the architecture the idea of retaining information, forgetting information and updating information. Three operations defined with weights.
It&rsquo;s not important to remember the architecture per se, at least if you don&rsquo;t want to implement one.</p>
<ul>
<li><strong>Cell state:</strong>Â The cell state is a vector that is maintained by the LSTM cell and is used to store information over time.</li>
<li><strong>Gates:</strong>Â LSTMs use three gates to control the flow of information into and out of the cell state:
<ul>
<li><strong>Forget gate:</strong>Â The forget gate determines which information from the previous cell state to discard.</li>
<li><strong>Input gate:</strong>Â The input gate determines which new information to add to the cell state.</li>
<li><strong>Output gate:</strong>Â The output gate determines which information from the cell state to output.</li>
</ul>
</li>
</ul>
<p>LSTM and similar architectures solve the vanishing or exploding gradient problem by substituting them with additions.</p>
<p>Everything can be rewritten in a more compact manner as follows:
$$
\begin{align*}</p>
<p>\begin{pmatrix}
i_t \
f_t \
o_t \
\tilde{c}<em>t
\end{pmatrix} = \sigma \left( W \begin{pmatrix}
h</em>{t - 1} \
x_t
\end{pmatrix} + b \right)
\
c_t = f_t \odot c_{t - 1} + i_t \odot \tilde{c}_t
\
h_t = o_t \odot \tanh(c_t)
\end{align*}
$$
Where $W$ is a matrix that contains all the weights of the network, and $\sigma$ is the sigmoid activation function. The $\odot$ operator denotes element-wise multiplication. NOTE: $\sigma$ at the beginning in reality should be tanh for the forget part, here it denotes activations.
The LSTM architecture is designed to learn long-term dependencies in sequential data by using a combination of gates and cell states to control the flow of information.</p>
<h5 id="drawbacks-of-lstm">Drawbacks of LSTM<a hidden class="anchor" aria-hidden="true" href="#drawbacks-of-lstm">#</a></h5>
<p>One drawback of LSTMs is that they add a lot of computation compared to RNNs, so they have developed some more lightweight versions of these networks (e.g. GRU).</p>
<h5 id="multi-level-lstms">Multi Level LSTMs<a hidden class="anchor" aria-hidden="true" href="#multi-level-lstms">#</a></h5>
<img src="/images/notes/Recurrent Neural Networks-20250521145022680.webp" style="width: 100%" class="center" alt="Recurrent Neural Networks-20250521145022680">
<h4 id="gru-networks">GRU Networks<a hidden class="anchor" aria-hidden="true" href="#gru-networks">#</a></h4>
<img src="/images/notes/Recurrent Neural Networks-20240908132825214.webp" style="width: 100%" class="center" alt="Recurrent Neural Networks-20240908132825214">
<h4 id="stochastic-rnns">Stochastic RNNs<a hidden class="anchor" aria-hidden="true" href="#stochastic-rnns">#</a></h4>
<p>One of the bottlenecks of RNNs is their limited representation space, which is believed to constrain their generation ability.</p>
<blockquote>
<p>Due to the deterministic evolution of the hidden state, RNNs model the variability in the data by means of the conditional output distributions, which may be insufficient for highly structured data such as speech and images. Such data can be characterized by the correlation between the output variables (i.e., multivariate data) and strong dependencies between variables at different time-steps (i.e., long-term dependencies). There also exists a complex relationship between the underlying factors of variation and the observed data.</p></blockquote>
<img src="/images/notes/Recurrent Neural Networks-20250522175002776.webp" style="width: 100%" class="center" alt="Recurrent Neural Networks-20250522175002776">
<p>This kind of network contains a VAE at every step.</p>
$$
h_{t } = f_{\theta}(h_{t - 1}, z_{t}, x_{t})
$$<p>
During the recurrence step.</p>
<h3 id="formal-definitions-of-rnn">Formal Definitions of RNN<a hidden class="anchor" aria-hidden="true" href="#formal-definitions-of-rnn">#</a></h3>
<h4 id="real-valued-recurrent-neural-network">Real-valued Recurrent Neural Network<a hidden class="anchor" aria-hidden="true" href="#real-valued-recurrent-neural-network">#</a></h4>
<p>A deterministic real-valued recurrent neural network $\mathcal{R}$ is a four-tuple $(\Sigma, D, f, \mathbf{h}_0)$ where:</p>
<ul>
<li>$\Sigma$ is the alphabet of input symbols.</li>
<li>$D$ is the dimension of $\mathcal{R}$.</li>
<li>$f: \mathbb{R}^D \times \Sigma \rightarrow \mathbb{R}^D$ is the dynamics map, i.e., a function defining the transitions between subsequent states.</li>
<li>$\mathbf{h}_0 \in \mathbb{R}^D$ is the initial state.</li>
</ul>
$$h_t = \frac{1}{2}h_{t-1} + \frac{1}{h_{t-1}}$$<p> This corresponds to a rational-valued RNN $(\Sigma, D, f, h_0)$ where: $\Sigma = \{a\}$ (<em>The input alphabet consists of a single symbol &lsquo;a&rsquo;).</em> $D = 1$ (The dimension of the RNN is 1).  $f: (\mathbb{R} \times \Sigma) \rightarrow \mathbb{R}$ is given by $f(x, a) = \frac{1}{2}x + \frac{1}{x}$. * $h_0 = 2$ (The initial state is 2).</p>
<h4 id="the-hidden-state">The Hidden State<a hidden class="anchor" aria-hidden="true" href="#the-hidden-state">#</a></h4>
<p>The idea is to define a neural <em>encoding function</em>, and use that to define a language model on top of the previous definition of recurrent neural network.</p>
<p>Let $\mathcal{R} = (\Sigma, D, f, \mathbf{h}_0)$ be a real-valued RNN. For an input sequence $\sigma = \sigma_1 \sigma_2 \dots \sigma_T \in \Sigma^*$, the sequence of states $\mathbf{h}_0, \mathbf{h}_1, \dots, \mathbf{h}_T \in \mathbb{R}^D$ is defined inductively as follows:</p>
<ul>
<li>$\mathbf{h}_0$ is the initial state.</li>
<li>For $1 \le t \le T$, the state at time $t$ is computed by applying the dynamics map $f$ to the previous state $\mathbf{h}_{t-1}$ and the current input symbol $\sigma_t$:
$$\mathbf{h}_t = f(\mathbf{h}_{t-1}, \sigma_t)$$
The output of the RNN on the input sequence $\sigma$ is the final state $\mathbf{h}_T$.</li>
</ul>
<h4 id="recurrent-neural-sequence-model">Recurrent neural sequence model<a hidden class="anchor" aria-hidden="true" href="#recurrent-neural-sequence-model">#</a></h4>
<p>Let $\mathcal{R} = (\Sigma, D, f, \mathbf{h}_0)$ be a recurrent neural network and $\mathbf{E} \in \mathbb{R}^{|\Sigma| \times D}$ a symbol representation matrix. A $D$-dimensional recurrent neural sequence model over an alphabet $\Sigma$ is a tuple $(\Sigma, D, f, \mathbf{E}, \mathbf{h}_0)$ defining the sequence model of the form</p>
$$
p_{\text{SM}}(y_t \mid \mathbf{y}_{<t}) \stackrel{\text{def}}{=} f_{\Delta^{|\Sigma|-1}}(\mathbf{E} \text{enc}_{\mathcal{R}}(\mathbf{y}_{<t}))_{y_t} = f_{\Delta^{|\Sigma|-1}}(\mathbf{E} \mathbf{h}_{t-1})_{y_t}. \quad (5.7)
$$<p>By far the most common choice of the projection function is the softmax yielding the sequence model</p>
$$
p_{\text{SM}}(y_t \mid \mathbf{y}_{<t}) \stackrel{\text{def}}{=} \text{softmax}(\mathbf{E} \text{enc}_{\mathcal{R}}(\mathbf{y}_{<t}))_{y_t} = \text{softmax}(\mathbf{E} \mathbf{h}_{t-1})_{y_t}. \quad (5.8)
$$<p>For conciseness, we will refer to RNN sequence models whose next-symbol probability distributions are computed using the softmax function as <strong>softmax RNN sequence models</strong>.</p>
<h3 id="training-rnns">Training RNNs<a hidden class="anchor" aria-hidden="true" href="#training-rnns">#</a></h3>
<h4 id="schematic-presentation-of-bptt-">Schematic presentation of BPTT ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#schematic-presentation-of-bptt-">#</a></h4>
<p>To train RNNs we use <strong>backpropagation through time</strong>. The idea is the same as a classical <a href="/notes/backpropagation">Backpropagation</a>, but we <strong>unroll</strong> the network and then use the same algorithm for backpropagation. We will update the same parameters multiple times (probably I haven&rsquo;t understood this point).
<img src="/images/notes/Recurrent Neural Networks-20240908133355952.webp" style="width: 100%" class="center" alt="Recurrent Neural Networks-20240908133355952">
<img src="/images/notes/image/universita/ex-notion/Recurrent Neural Networks/Untitled 2.png" style="width: 100%" class="center" alt="image/universita/ex-notion/Recurrent Neural Networks/Untitled 2"></p>
<h4 id="backpropagation-through-time-">Backpropagation Through Time ðŸŸ¥++<a hidden class="anchor" aria-hidden="true" href="#backpropagation-through-time-">#</a></h4>
<ul>
<li><strong>Backpropagation through time (BPTT):</strong>Â LSTMs use a variant of BPTT called truncated BPTT to train the network. BPTT is a technique for training recurrent neural networks by unrolling the network through time and calculating the gradients of the loss function with respect to the parameters of the network. Truncated BPTT is a simplified version of BPTT that is more computationally efficient.</li>
</ul>
<p>It is treated as a multi-layer network with unbounded number of layers.</p>
<img src="/images/notes/Recurrent Neural Networks-20250313131833328.webp" style="width: 100%" class="center" alt="Recurrent Neural Networks-20250313131833328">
<p>They are <strong>biased</strong> to more recent samples (you can observe this from the gradient information also).</p>
<h4 id="trucated-bptt-">Trucated BPTT ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#trucated-bptt-">#</a></h4>
$$
\frac{ \partial h_{t} }{ \partial h_{k} } = \prod_{i = k + 1}^{t} W_{hh}^{T}\text{diag}(f'(h_{i - 1})) 
$$<p>
Then we have two cases if we consider the spectral norm of the diagonal:</p>
<ul>
<li>If it is less than some value it is a <strong>sufficient condition</strong> that the above quantity is less than $\mu^{t - k} < 1$.</li>
<li>else it is a necessary condition for it to explode, so it could happen that it does indeed explode.</li>
</ul>
<p>This motivates to alleviate the problem by truncating the backpropagation and not going to its end.</p>
<ul>
<li>Vanilla RNNs are too sensitive to recent distractions.</li>
</ul>
<h4 id="vanishing-and-exploding-gradient-problem---">Vanishing and Exploding gradient problem ðŸŸ©&ndash;<a hidden class="anchor" aria-hidden="true" href="#vanishing-and-exploding-gradient-problem---">#</a></h4>
<p>Exploding gradients is often easy to control just with gradient clipping.
The vanishing gradient problem is often more difficult to solve, we could design some:</p>
<ol>
<li>Activation function</li>
<li>Weight initialization</li>
<li>Custom gate cells</li>
<li>Gradient Clipping.
We will not discuss those at this moment.</li>
</ol>
<p>It is quite easy to get an intuition of why they suffer from vanishing or exploding gradient problems.
We see that $h_{t} = Wh_{t - 1}$ if W is diagonalizable then $h_{t} = Q \Lambda^{t}Q^{T} h_{1}$ and the diagonal matrix is either going to explode or going to zero since we are powering it.</p>
<p>Let&rsquo;s do now a more formal analysis of the problem.
Consider $\lambda$ to be the largest Eigen value of $W_{hh}$, and that the activation function is bounded by $\gamma$ (usually true, don&rsquo;t use ReLU, tanh for example is a nice assumption that breaks this proof).
Then it is tru ethat $\lambda < \frac{1}{\gamma} \to$ vanishing gradient:</p>
$$
\forall_{i} \lVert \frac{ \partial h_{i} }{ \partial h_{i - 1} }  \rVert \leq \lVert W^{T}_{hh} \rVert \lVert \text{diag} \left[ f'(h_{i - 1}) \right]  \rVert < \frac{1}{\gamma} \gamma = 1
$$<p>Then you can prove that the signal of distant $h$ is basically zero (recall the product with the h states!).
When you have $\gamma > \frac{1}{\gamma}$ you have exploding gradients, but that is basically the same proof.
Hochreiter and Schmidhuber say that Vanilla RNNs are too sensitive by recent distractions.</p>
<h3 id="limitations-of-rnn">Limitations of RNN<a hidden class="anchor" aria-hidden="true" href="#limitations-of-rnn">#</a></h3>
<ol>
<li>Encoding the information is expected to lose something. We don&rsquo;t have a clear insight about exactly what is encoded into the $h$ vectors.</li>
<li>Very slow! I can&rsquo;t parallelize this computation, since one output depends on the output of the previous token!</li>
<li>In the end of the story, they can&rsquo;t do <strong>long term memory</strong> at all, they say maximum is 100 words with LSTMs.
<a href="/notes/transformers">Transformers</a> solve these problems. They don&rsquo;t have hidden encoding, they just do big forwards, they are parallelizable, and can encode information depending on the context size (like n-grams). But they don&rsquo;t have theoretical guaranties of possible infinite context sizes&hellip;</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/natural-language-processing/">ðŸ’¬Natural-Language-Processing</a></li>
      <li><a href="https://flecart.github.io/tags/machine-perception/">Machine-Perception</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Recurrent Neural Networks on x"
            href="https://x.com/intent/tweet/?text=Recurrent%20Neural%20Networks&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2frecurrent-neural-networks%2f&amp;hashtags=%f0%9f%92%acnatural-language-processing%2cmachine-perception">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Recurrent Neural Networks on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2frecurrent-neural-networks%2f&amp;title=Recurrent%20Neural%20Networks&amp;summary=Recurrent%20Neural%20Networks&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2frecurrent-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Recurrent Neural Networks on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2frecurrent-neural-networks%2f&title=Recurrent%20Neural%20Networks">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Recurrent Neural Networks on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2frecurrent-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Recurrent Neural Networks on whatsapp"
            href="https://api.whatsapp.com/send?text=Recurrent%20Neural%20Networks%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2frecurrent-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Recurrent Neural Networks on telegram"
            href="https://telegram.me/share/url?text=Recurrent%20Neural%20Networks&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2frecurrent-neural-networks%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Recurrent Neural Networks on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Recurrent%20Neural%20Networks&u=https%3a%2f%2fflecart.github.io%2fnotes%2frecurrent-neural-networks%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
