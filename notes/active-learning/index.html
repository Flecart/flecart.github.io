<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Active Learning | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning, ➕probabilistic-artificial-intelligence">
<meta name="description" content="Active Learning concerns methods to decide how to sample the most useful information in a specific domain; how can you select the best sample for an unknown model?
Gathering data is very costly, we would like to create some principled manner to choose the best data point to humanly label in order to have the best model.
In this setting, we are interested in the concept of usefulness of information. One of our main goals is to reduce uncertainty, thus, Entropy-based (mutual information) methods are often used.
For example, we can use active learning to choose what samples needs to be labelled in order to have highest accuracy on the trained model, when labelling is costly.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/active-learning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/active-learning/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/active-learning/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Active Learning">
  <meta property="og:description" content="Active Learning concerns methods to decide how to sample the most useful information in a specific domain; how can you select the best sample for an unknown model? Gathering data is very costly, we would like to create some principled manner to choose the best data point to humanly label in order to have the best model.
In this setting, we are interested in the concept of usefulness of information. One of our main goals is to reduce uncertainty, thus, Entropy-based (mutual information) methods are often used. For example, we can use active learning to choose what samples needs to be labelled in order to have highest accuracy on the trained model, when labelling is costly.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="Machinelearning">
    <meta property="article:tag" content="➕Probabilistic-Artificial-Intelligence">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Active Learning">
<meta name="twitter:description" content="Active Learning concerns methods to decide how to sample the most useful information in a specific domain; how can you select the best sample for an unknown model?
Gathering data is very costly, we would like to create some principled manner to choose the best data point to humanly label in order to have the best model.
In this setting, we are interested in the concept of usefulness of information. One of our main goals is to reduce uncertainty, thus, Entropy-based (mutual information) methods are often used.
For example, we can use active learning to choose what samples needs to be labelled in order to have highest accuracy on the trained model, when labelling is costly.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Active Learning",
      "item": "https://flecart.github.io/notes/active-learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Active Learning",
  "name": "Active Learning",
  "description": "Active Learning concerns methods to decide how to sample the most useful information in a specific domain; how can you select the best sample for an unknown model? Gathering data is very costly, we would like to create some principled manner to choose the best data point to humanly label in order to have the best model.\nIn this setting, we are interested in the concept of usefulness of information. One of our main goals is to reduce uncertainty, thus, Entropy-based (mutual information) methods are often used. For example, we can use active learning to choose what samples needs to be labelled in order to have highest accuracy on the trained model, when labelling is costly.\n",
  "keywords": [
    "machinelearning", "➕probabilistic-artificial-intelligence"
  ],
  "articleBody": "Active Learning concerns methods to decide how to sample the most useful information in a specific domain; how can you select the best sample for an unknown model? Gathering data is very costly, we would like to create some principled manner to choose the best data point to humanly label in order to have the best model.\nIn this setting, we are interested in the concept of usefulness of information. One of our main goals is to reduce uncertainty, thus, Entropy-based (mutual information) methods are often used. For example, we can use active learning to choose what samples needs to be labelled in order to have highest accuracy on the trained model, when labelling is costly.\nSetting Given a safe exploration space $S$, an interesting space $A$ and all possible space $\\mathcal{X}$ and a dataset $\\left\\{ (x_{i}, y_{i}) \\right\\} \\subseteq S \\times \\mathbb{R}$ we want to find the best points that we would like to sample in $S$ to maximize the information we can get about $A$.\nSo we want to collect the most representative sample under constrained budgeting.\nWe can divide the setting in Active Learning as solving two problems:\nQuantifying the concept of utility of a sample. Finding the best sample to select. Utility of samples $$ I(X; Y) = \\underbrace{H(X)}_{ \\begin{align} \\text{Uncertainty of } X \\\\ \\text{ before observing Y} \\end{align}}- \\underbrace{H(X \\mid Y)}_{\\text{ After observing } Y} $$Information Gain in Linear Regression This measures the gain of adding $x$ to the set $A$ for a certain function $F$.\n$$ \\begin{align} I(X, Y) \u0026= H(Y) - H(Y \\mid X) \\\\ \u0026= \\frac{1}{2} \\log ( (2\\pi e)^{d}\\text{det}(\\Sigma + \\sigma^{2}_{n}I)) - \\frac{1}{2} \\log ( (2\\pi e)^{d}\\text{det}(\\sigma^{2}_{n}I)) \\\\ \u0026= \\frac{1}{2} \\log (\\lvert I + \\sigma_{n}^{-2}\\Sigma \\rvert) \\end{align} $$ Where $X \\sim \\mathcal{N}(x; \\mu, \\Sigma)$ and $Y = X + \\varepsilon, \\varepsilon \\sim\\mathcal{N}(\\varepsilon; 0, \\sigma^{2}_{n}I)$.\nMarginal Gain $$ \\Delta_{F}(x \\mid A) = F(A \\cup \\left\\{ x \\right\\}) - F(A) $$$$ F(A) = \\forall y \\in A, f = y + \\varepsilon:I(f; y ) $$ Which is a mutual information for a lot a lot of points. Instead of writing the for all we can write $I(f_{\\mathcal{A}}; y_{A})$.\nThere is a nice property of this Marginal Gain: $$ \\begin{align} F(A \\cup \\left{ x \\right}) - F(A) \u0026= I(f_{A \\cup \\left{ x \\right}} ; y_{A \\cup \\left{ x \\right} }) - I(f_{\\mathcal{A}} ; y_{A}) \\ \u0026= I(f_{A \\cup \\left{ x \\right}} ; y_{A \\cup \\left{ x \\right} }) - I(f_{\\mathcal{A} \\cup \\left{ x \\right} } ; y_{A}) \u0026\\text{ As } f_{x} \\perp y_{A} \\mid f_{A}\\ \u0026= I(f_{A \\cup \\left{ x \\right} };y_{x} \\mid y_{A} ) \u0026\\text{ By chain rule of }I \\ \u0026= I(f_{x}; y_{x} \\mid y_{A}) \u0026 \\text{ As } f_{A} \\perp y_{x} \\mid f_{x} \\ \u0026= H(y_{x} \\mid y_{A}) - H(y_{x} \\mid f_{x}, y_{A}) \\ \u0026= H(y_{x} \\mid y_{A}) - H(\\varepsilon) \\ \\end{align}\n$$ This relation will be quite important for the proof of submodularity of mutual information.\nSubmodularity of Mutual Information $$ F(A \\cup \\left\\{ x \\right\\}) - F(A) \\geq F(B \\cup \\left\\{ x \\right\\}) - F(B) $$$$ \\Delta_{F}(x \\mid A) \\geq \\Delta_{F}(x \\mid B) $$ This property just says that the gain of adding a point to a small set is higher than adding it to a bigger set.\n$$ \\begin{align} \\Delta_{F}(x \\mid A) \u0026= H(y_{x} \\mid y_{A}) - H(\\varepsilon) \\\\ \u0026\\leq H(y_{x} \\mid y_{B}) - H(\\varepsilon) \u0026\\text{ By monotonicity of } H \\\\ \u0026= \\Delta_{F}(x \\mid B) \\end{align} $$ Which ends the proof $\\square$.\nSubmodularity means no synergy $$ I(f_{x}; y_{x}; y_{B - A} \\mid y_{A}) \\geq 0 $$$$ \\begin{align} I(f_{x}; y_{x}; y_{B - A} \\mid y_{A}) \u0026= I(f_{x}; y_{x} \\mid y_{A}) - I(f_{x}; y_{x} \\mid y_{B}) \\\\ \u0026= H(f_{x} \\mid y_{A}) - H(f_{x} \\mid y_{x}, y_{A}) - H(f_{x} \\mid y_{B}) + H(f_{x} \\mid y_{x}, y_{B}) \\\\ \u0026= H(f_{x} \\mid y_{A}) - H(f_{x} \\mid y_{x}) - H(f_{x} \\mid y_{B}) + H(f_{x} \\mid y_{x}) \\\\ \u0026= H(f_{x} \\mid y_{A}) - H(f_{x} \\mid y_{B}) \\\\ \u0026\\geq 0 \\text{ By monotonicy of } H \\end{align} $$Monotonicity of Information $$ \\begin{align} I(A; Y) \u0026= H(Y) - H(Y \\mid A) \\\\ \u0026\\leq H(Y) - H(Y \\mid B) \u0026\\text{By monotonicity of } H\\\\ \u0026= I(B; Y) \\end{align} $$Greedy optimization Uncertainty Sampling $$ x_{t + 1} = \\arg\\max_{x \\in \\mathcal{X}} I(f_{x}; y_{x} \\mid y_{S_{t}}) $$ Where $S_{t}$ are the point chosen until timestep $t$.\n$$ x_{t + 1} = \\arg\\max_{x \\in \\mathcal{X}} \\frac{1}{2}\\log \\left( 1 + \\frac{\\sigma^{2}_{t}(x)}{\\sigma^{2}_{n}} \\right)= \\arg\\max_{x \\in \\mathcal{X}} \\sigma_{t}^{2}(x) $$ Where we assumed the noise to be homoscedastic.\n$$ \\begin{align} I(f_{x}; y_{x}) \u0026= H(y_{x}) - H(y_{x} \\mid f_{x}) \\\\ \u0026= \\frac{1}{2} \\log (2\\pi e \\sigma^{2}_{n} + 2\\pi e \\sigma^{2}_{t}(x)) -\\frac{1}{2} \\log (2\\pi e \\sigma^{2}_{n}) \\\\ \u0026= \\frac{1}{2} \\log \\left( 1 + \\frac{\\sigma^{2}_{t}(x)}{\\sigma^{2}_{n}} \\right) \\end{align} $$Drawbacks of Uncertainty Sampling In heteroscedastic settings, what we should minimize is actually the ratio between the epistemic uncertainty and the aleatoric uncertainty, always choosing for the maximum epistemic uncertainty is not guaranteed to be the most informative choice. But with greedy, we can’t distinguish them cleanly (for some reason I didn’t understood). In the homoskedastic setting, however, it is quite good.\nGreedy mutual information optimization The main idea is to find the point $x$ in our safe space that maximizes mutual information with the interesting space $A$. This solution has been called Information Transductive learning by the AML professor, not sure that this name is correct.\n$$ x_{n} = \\arg \\max_{x \\in S} I(f_{\\mathcal{A}}; y_{x} \\mid \\mathcal{D}_{n - 1}) $$ Phrased in another manner, we would like to know how the uncertainty about our target function $f$ diminishes when we get to know about $y$.\n$$ \\arg\\max_{x \\in D} I(f_{\\mathcal{A}}; y_{x + D_{n - 1}} ) - I(f_{\\mathcal{A}}; y_{D_{n - 1}}) $$ With some simple calculation we find that this objective is exactly the above objective. This is also the marginal gain that we described above.\nIf we assume that $f$ is a Gaussian Processes then the mutual information is interpretable as minimizing general posterior variance:\n$$ \\begin{align} x_{n} \u0026 = \\arg \\max_{x \\in S} I(f_{\\mathcal{A}}; y_{x} \\mid \\mathcal{D}_{n - 1}) \\\\ \u0026 = \\arg \\max_{x \\in S} H(f_{\\mathcal{A}} \\mid \\mathcal{D}_{n - 1}) - H(f_{\\mathcal{A}} \\mid y_{x}, \\mathcal{D}_{n - 1}) \\\\ \\\\ \u0026 = \\arg \\max_{x \\in S} \\frac{1}{2} \\log \\left( 2 \\pi e \\sigma_{f_{\\mathcal{A}} \\mid \\mathcal{D}_{n - 1}}^{2} \\right) - \\frac{1}{2} \\log \\left( 2 \\pi e \\sigma_{f_{\\mathcal{A}} \\mid y_{x}, \\mathcal{D}_{n - 1}}^{2} \\right) \\\\ \\\\ \u0026 = \\arg \\min_{x \\in S} \\frac{1}{2} \\log \\left( \\frac{\\sigma_{f_{\\mathcal{A}} \\mid y_{x}, \\mathcal{D}_{n - 1}}^{2}}{\\sigma_{f_{\\mathcal{A}} \\mid \\mathcal{D}_{n - 1}}^{2}} \\right) \\\\ \\\\ \u0026 = \\arg \\min_{x \\in S} \\sigma_{f_{\\mathcal{A}} \\mid y_{x}, \\mathcal{D}_{n - 1}}^{2} \\end{align} $$It’s easy to interpret: we just want to sample the point were the uncertainty is maximized!\nGreedy optimization bounds $$ F(S_{T}) \\geq \\left( 1 - \\frac{1}{e} \\right) \\max_{T } F(T) $$ Using the notation for mutual information introduced above. This says that greedy uncertainty sampling is at least $1 - 1/e$ of the optimal solution, which is near optimal. The important theoretical step for this is the submodularity of mutual information. We will not provide that here.\n$$ \\begin{align} F(S^{*}) \u0026\\leq F(S^{*} \\cup S_{T}) \u0026\\text{ by Monotonicity} \\\\ \u0026= F(S_{T}) + \\sum_{i = 1}^{n} \\Delta_{F}(x_{i} \\mid S_{T} \\cup x_{j \u003c i}) \\\\ \u0026\\leq F(S_{T}) + \\sum_{i = 1}^{n} \\Delta_{F}(x_{i} \\mid S_{T} ) \u0026\\text{ By Submodularity} \\\\ \u0026 \\leq F(S_{T}) + n \\cdot \\arg\\max_{x \\in \\mathcal{X}} \\Delta_{F}(x \\mid S_{T}) \\\\ \u0026 = F(S_{T}) + n \\cdot (F(S_{T + 1}) - F(S_{T})) \u0026\\text{ By definiiton of Greedy} \\\\ \\end{align} $$ Then if we set $\\delta _t = F(S^{*}) - F(S_{t})$ we can do some standard algebraic manipulation manipulation and get the result.\nTypes of optimal design What we have done so far has been intensively studied in the field of optimal design. This field is interested on how to conduct the most informative experiment. There are many different manners to choose the sampling point. These include\nd-optimal: which attempts to reduce the determinant of the posterior covariance function. a-optimal: minimizes the trace of the posterior covariance matrix. e-optimal: attempts to minimize the maximum eigenvalue of the posterior covariance matrix. All these can be interpreted geometrically as doing operations on the uncertainty ellipsoid. Active Learning for classification The starting Idea: Label Entropy $$ x_{n+1} = \\arg \\max_{x \\in S} H(y_{x}\\mid x_{1:n}, y_{1:n}) $$ But often, this usually leads to sampling points close to the decision boundary (that is where the uncertainty of the labels are higher!). Similar to the case where we have heteroskedastic noise, the entropy at these points could be higher just because of the aleatoric noise, aka label noise. We would like to come up with a more informed approach.\nInformative sampling for classification This is also called BALD (Bayesian Active Learning by Disagreement). Here we distinguish the aleatoric and epistemic noise by adding another random variable $\\theta$ that represents the epistemic noise. Then we build upon basically the same ideas as the section on Regression in #Greedy optimization.\n$$ \\begin{align} x_{n+1} \u0026= \\arg \\max_{x \\in S} I(\\theta; y_{x}\\mid x_{1:t}, y_{1:t}) \\\\ \u0026= \\arg \\max_{x \\in S} H(y_{x} \\mid x_{1:t}, y_{1:t}) - H(y_{x} \\mid x_{1:t}, y_{1:t},\\theta) \\\\ \u0026= \\arg \\max_{x \\in S} H(y_{x} \\mid x_{1:t}, y_{1:t}) - \\mathbb{E}_{\\theta \\mid x_{1:t}, y_{1:t}} \\left[ H(y_{x} \\mid \\theta) \\right] \\\\ \\end{align} $$ We can use approximate inference, like Variational Inference or Monte Carlo Methods to obtain approximations of the above. Entropy of the average prediction versus average entropy of the predictions given a trained model. The first term looks for points were all models are uncertain, the second term can be interpreted as a regularizer (similar to aleatoric uncertainty considerations somehow), and penalizes points where the model is uncertain, and steers for more confident points where the model disagrees with the label. Intuitively, the second part is the aleatoric uncertainty which is the average uncertainty for all models.\nThis is some code to play with on a notebook.\n$$ I[\\mathbf{y}, \\theta \\mid \\mathbf{x}, \\mathcal{D}] = H[\\mathbf{y} \\mid \\mathbf{x}, \\mathcal{D}] - \\mathbb{E}_{p(\\theta \\mid \\mathcal{D})} [H[\\mathbf{y} \\mid \\mathbf{x}, \\theta]] $$ The important thing to understand is that these methods select the most informative samples from the unlabeled pool, i.e., samples for which the model’s predictions are most uncertain due to disagreement across its posterior. The new data point that we are trying to sample is not known in advance.\nAcquisition functions In this case we would like to find some point with a certain property, which is usually local. For example, we might be interested to find the maximum of a certain function. The ideas here could be applied also to the search of other points, given the assumptions hold.\nSee Bayesian Optimization.\nInformation Transductive learning This is an idea of (H{\"u}botter et al. 2024), part of his master’s thesis at ETH with Andreas Krause. The presentation of this part is a not perfectly clear, but probably is not quite important for the exam.\nIntroduction to the Problem The main problem is how to sample in dangerous environments, where the cost of sampling may be high. We would need to define a safe zone in these contexts. With this setting, the space where you can have sample points is inherently different from the testing and evaluation points. We would like to find the maximum of an unknown stochastic process $f^{*}$, while respecting some safety constraints. We can choose a set of points $x_{1}, \\dots, x_{n} \\in \\mathcal{X}$ but we don’t want these to be outside the safe area $S := \\left\\{ x \\in \\mathcal{X} \\mid g(x) \\geq 0 \\right\\}$ where $g$ is the safety function. For each point that we choose, we observe the label value $y_{1}, \\dots y_{n}$ and the safety value $z_{1}, \\dots z_{n}$. We have thus identified three unknown that we would like to take into account, $f, g, S$. Fitting a Gaussian Processes on the dataset $(x_{i}, z_{i})_{i \u003c n}$, we can produce two function that represent our confidence over the safety function $g$ and the safety set $S$. We consider the function $S_{l} = \\left\\{ x \\mid l_{g}(x) \\geq 0 \\right\\}$ where $l$ paired with its upper bound counter part has $95\\%$ confidence bound of the safety function. In this manner, if we sample inside this set, we are almost sure that we are always sampling inside the safe zone, given our model is well-calibrated (see Bayesian neural networks).\nTransductive Learning Solution Then we sample inside this space for the function $f$ where its higher than the most conservative estimate, in formulas, our target space is $A = \\left\\{ x \\in S_{u} \\mid u_{f}(x) \\geq \\max_{x \\in S_{l}} l_{f}(x) \\right\\}$. A graphical representation is probably clearer in the presentation. After we define the safe space and the target space, we could plug in optimization methods, for example the greedy one we explained before.\n$$ x_{n+1} = \\arg \\max_{x \\in S_{l}} I(f_{\\mathcal{A}}; y_{x} \\mid \\mathcal{D}_{n - 1}) $$Batch Active Learning With this, we present the algorithm introduced in (Yehuda et al. 2022).\nGeneralization error for 1-NN $$ \\mathop{\\mathbb{E}}[\\hat{f}(x) \\neq f(x)] \\leq P(x \\not\\in C) + P(x \\in C_{wrong}) $$ Where $C_{wrong}$ is the set of points labelled wrongly. We note that $P(x \\not \\in C) = 1 - P(C(L, \\delta))$ which is the probability of a point not present in our covering set, while $x \\in C_{wrong}$ is the probability of a point in the covering set being wrongly classified. The paper defines the first value to be the high budged case as it can be swiftly lowered by just sampling more points, while in low budget that variable is not controllable, so one would like to reduce the second term. ProbCover attempts to fix the second term.\nThe objective function $$ \\arg\\max _{L \\subseteq \\mathcal{X}, \\lvert L \\rvert = b} P(\\bigcup_{x \\in L} B(x, \\delta) ) $$ For a fixed $\\delta$, we would like to find the best set of points of size $b$ such that it is probably covered by the balls situated in that point. The paper shows that this problem is NP hard. The second problem is that we don’t know the prior distribution of $x$, so it’s hard to make an estimate of that (we can use the empirical distribution).\nDuality with Coreset $$ \\delta(x) = \\min \\left\\{ \\delta \\in R_{+} : P(\\bigcup_{x \\in L}B(x, \\delta)) = 1 \\right\\} $$ Meaning: find best ball range, such that all the points are covered by the balls of that range. This is a loose dual problem to the one we have seen before. In the above problem: we try to fix the ball radius, and find the point that can minimize the coverage.\nProbCover From my understanding, ProbCover tells you how to sample the points that have the maximum coverage for a certain space, independently of the model that you will use. This is motivated by the observation that if we fix the $\\delta$ of the ball covers that we would like to use, then the upper bound of the probability of the error is fixed.\nThe simple algorithm is just maximizing the points that have most neighbors inside its circle. This has some theoretical motivations. The main results you should remember are the following:\nThe probability of impure balls in the coverage is upper bounded by the probability of impure balls in the whole sample space. The generalization error $\\mathbb{E}[\\hat{f}(x) \\neq f(x)]$ is upper bounded by the probability of the uncovered space and the wrongly classified points inside the covered space. If we fix a $\\delta$, which is the radius of a Ball, then we just need to maximize coverage, which could be done by the following algorithm in the image, which just maximizes for most points in the ball. References [1] H{\\\"u}botter et al. “Transductive Active Learning: Theory and Applications” arXiv preprint arXiv:2402.15898 2024 [2] Yehuda et al. “Active Learning Through a Covering Lens” arXiv preprint arXiv:2205.11320 2022 ",
  "wordCount" : "2626",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/active-learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Active Learning
    </h1>
    <div class="post-meta">13 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#setting" aria-label="Setting">Setting</a><ul>
                        
                <li>
                    <a href="#utility-of-samples" aria-label="Utility of samples">Utility of samples</a></li>
                <li>
                    <a href="#information-gain-in-linear-regression" aria-label="Information Gain in Linear Regression">Information Gain in Linear Regression</a></li>
                <li>
                    <a href="#marginal-gain" aria-label="Marginal Gain">Marginal Gain</a></li>
                <li>
                    <a href="#submodularity-of-mutual-information" aria-label="Submodularity of Mutual Information">Submodularity of Mutual Information</a></li>
                <li>
                    <a href="#submodularity-means-no-synergy" aria-label="Submodularity means no synergy">Submodularity means no synergy</a></li>
                <li>
                    <a href="#monotonicity-of-information" aria-label="Monotonicity of Information">Monotonicity of Information</a></li></ul>
                </li>
                <li>
                    <a href="#greedy-optimization" aria-label="Greedy optimization">Greedy optimization</a><ul>
                        
                <li>
                    <a href="#uncertainty-sampling" aria-label="Uncertainty Sampling">Uncertainty Sampling</a></li>
                <li>
                    <a href="#drawbacks-of-uncertainty-sampling" aria-label="Drawbacks of Uncertainty Sampling">Drawbacks of Uncertainty Sampling</a></li>
                <li>
                    <a href="#greedy-mutual-information-optimization" aria-label="Greedy mutual information optimization">Greedy mutual information optimization</a></li>
                <li>
                    <a href="#greedy-optimization-bounds" aria-label="Greedy optimization bounds">Greedy optimization bounds</a></li>
                <li>
                    <a href="#types-of-optimal-design" aria-label="Types of optimal design">Types of optimal design</a></li></ul>
                </li>
                <li>
                    <a href="#active-learning-for-classification" aria-label="Active Learning for classification">Active Learning for classification</a><ul>
                        
                <li>
                    <a href="#the-starting-idea-label-entropy" aria-label="The starting Idea: Label Entropy">The starting Idea: Label Entropy</a></li>
                <li>
                    <a href="#informative-sampling-for-classification" aria-label="Informative sampling for classification">Informative sampling for classification</a></li></ul>
                </li>
                <li>
                    <a href="#acquisition-functions" aria-label="Acquisition functions">Acquisition functions</a></li>
                <li>
                    <a href="#information-transductive-learning" aria-label="Information Transductive learning">Information Transductive learning</a><ul>
                        
                <li>
                    <a href="#introduction-to-the-problem" aria-label="Introduction to the Problem">Introduction to the Problem</a></li>
                <li>
                    <a href="#transductive-learning-solution" aria-label="Transductive Learning Solution">Transductive Learning Solution</a></li></ul>
                </li>
                <li>
                    <a href="#batch-active-learning" aria-label="Batch Active Learning">Batch Active Learning</a><ul>
                        
                <li>
                    <a href="#generalization-error-for-1-nn" aria-label="Generalization error for 1-NN">Generalization error for 1-NN</a></li>
                <li>
                    <a href="#the-objective-function" aria-label="The objective function">The objective function</a></li>
                <li>
                    <a href="#duality-with-coreset" aria-label="Duality with Coreset">Duality with Coreset</a></li>
                <li>
                    <a href="#probcover" aria-label="ProbCover">ProbCover</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Active Learning concerns methods to decide how to sample the most useful information in a specific domain; how can you select the best sample for an unknown model?
Gathering data is very costly, we would like to create some principled manner to choose the best data point to humanly label in order to have the best model.</p>
<p>In this setting, we are interested in the concept of <strong>usefulness of information</strong>. One of our main goals is to <em>reduce uncertainty</em>, thus, <a href="/notes/entropy">Entropy</a>-based (mutual information) methods are often used.
For example, we can use active learning to choose what samples needs to be labelled in order to have highest accuracy on the trained model, when labelling is costly.</p>
<h3 id="setting">Setting<a hidden class="anchor" aria-hidden="true" href="#setting">#</a></h3>
<p>Given a safe exploration space $S$, an interesting space $A$ and all possible space $\mathcal{X}$ and a dataset $\left\{ (x_{i}, y_{i}) \right\} \subseteq S \times \mathbb{R}$ we want to find the best points that we would like to sample in $S$ to maximize the information we can get about $A$.</p>
<p>So we want to collect the most representative sample under constrained budgeting.</p>
<p>We can divide the setting in Active Learning as solving two problems:</p>
<ol>
<li>Quantifying the concept of <strong>utility</strong> of a sample.</li>
<li>Finding the <strong>best</strong> sample to select.</li>
</ol>
<h4 id="utility-of-samples">Utility of samples<a hidden class="anchor" aria-hidden="true" href="#utility-of-samples">#</a></h4>
$$
I(X; Y) = \underbrace{H(X)}_{
\begin{align}
\text{Uncertainty of } X \\ \text{ before observing Y}
\end{align}}- \underbrace{H(X \mid Y)}_{\text{ After observing } Y}
$$<h4 id="information-gain-in-linear-regression">Information Gain in Linear Regression<a hidden class="anchor" aria-hidden="true" href="#information-gain-in-linear-regression">#</a></h4>
<p>This measures the gain of adding $x$ to the set $A$ for a certain function $F$.</p>
$$
\begin{align}
I(X, Y) &= H(Y) - H(Y \mid X)  \\
&= \frac{1}{2} \log ( (2\pi e)^{d}\text{det}(\Sigma + \sigma^{2}_{n}I))  - \frac{1}{2} \log ( (2\pi e)^{d}\text{det}(\sigma^{2}_{n}I)) \\
&= \frac{1}{2} \log (\lvert  I + \sigma_{n}^{-2}\Sigma  \rvert)
\end{align}
$$<p>
Where $X \sim \mathcal{N}(x; \mu, \Sigma)$ and $Y = X + \varepsilon, \varepsilon \sim\mathcal{N}(\varepsilon; 0, \sigma^{2}_{n}I)$.</p>
<h4 id="marginal-gain">Marginal Gain<a hidden class="anchor" aria-hidden="true" href="#marginal-gain">#</a></h4>
$$
\Delta_{F}(x \mid A) = F(A \cup \left\{ x \right\}) - F(A)
$$$$
F(A) = \forall y \in A, f = y + \varepsilon:I(f;  y )
$$<p>
Which is a mutual information for a lot a lot of points.
Instead of writing the for all we can write $I(f_{\mathcal{A}};  y_{A})$.</p>
<p>There is a nice property of this Marginal Gain:
$$
\begin{align}
F(A \cup \left{ x \right}) - F(A) &amp;= I(f_{A \cup \left{ x \right}} ; y_{A \cup \left{ x \right} }) - I(f_{\mathcal{A}} ; y_{A})   \
&amp;= I(f_{A \cup \left{ x \right}} ; y_{A \cup \left{ x \right} }) - I(f_{\mathcal{A} \cup \left{ x \right} } ; y_{A}) &amp;\text{ As } f_{x} \perp y_{A} \mid f_{A}\
&amp;= I(f_{A \cup \left{ x \right} };y_{x} \mid y_{A} ) &amp;\text{ By chain rule of }I  \
&amp;= I(f_{x}; y_{x} \mid y_{A}) &amp; \text{ As } f_{A} \perp y_{x} \mid f_{x}  \
&amp;= H(y_{x} \mid y_{A}) - H(y_{x} \mid f_{x}, y_{A})   \
&amp;= H(y_{x} \mid y_{A}) - H(\varepsilon)   \
\end{align}</p>
<p>$$
This relation will be quite important for the proof of submodularity of mutual information.</p>
<h4 id="submodularity-of-mutual-information">Submodularity of Mutual Information<a hidden class="anchor" aria-hidden="true" href="#submodularity-of-mutual-information">#</a></h4>
$$
F(A \cup \left\{ x \right\}) - F(A) \geq F(B \cup \left\{ x \right\}) - F(B)
$$$$
\Delta_{F}(x \mid A) \geq \Delta_{F}(x \mid B)
$$<p>
This property just says that the gain of adding a point to a small set is higher than adding it to a bigger set.</p>
$$
\begin{align}
\Delta_{F}(x \mid A) &= H(y_{x} \mid y_{A}) - H(\varepsilon)   \\
&\leq H(y_{x} \mid y_{B}) - H(\varepsilon)  &\text{ By monotonicity of } H  \\
&= \Delta_{F}(x \mid B)
\end{align}
$$<p>
Which ends the proof $\square$.</p>
<h4 id="submodularity-means-no-synergy">Submodularity means no synergy<a hidden class="anchor" aria-hidden="true" href="#submodularity-means-no-synergy">#</a></h4>
$$
I(f_{x}; y_{x}; y_{B - A} \mid y_{A}) \geq 0
$$$$
\begin{align}
I(f_{x}; y_{x}; y_{B - A} \mid y_{A}) &= I(f_{x}; y_{x} \mid y_{A}) - I(f_{x}; y_{x} \mid y_{B})   \\
&= H(f_{x} \mid y_{A}) - H(f_{x} \mid y_{x}, y_{A}) - H(f_{x} \mid y_{B}) + H(f_{x} \mid y_{x}, y_{B})    \\ 
&= H(f_{x} \mid y_{A}) - H(f_{x} \mid y_{x}) - H(f_{x} \mid y_{B}) + H(f_{x} \mid y_{x})    \\
&= H(f_{x} \mid y_{A})  - H(f_{x} \mid y_{B})    \\
&\geq 0 \text{ By monotonicy of } H
\end{align}
$$<h4 id="monotonicity-of-information">Monotonicity of Information<a hidden class="anchor" aria-hidden="true" href="#monotonicity-of-information">#</a></h4>
$$
\begin{align}
 I(A; Y) &= H(Y) - H(Y \mid A)  \\
&\leq H(Y) - H(Y \mid B)   &\text{By monotonicity of } H\\
&= I(B; Y)
\end{align}
$$<h3 id="greedy-optimization">Greedy optimization<a hidden class="anchor" aria-hidden="true" href="#greedy-optimization">#</a></h3>
<h4 id="uncertainty-sampling">Uncertainty Sampling<a hidden class="anchor" aria-hidden="true" href="#uncertainty-sampling">#</a></h4>
$$
x_{t + 1} = \arg\max_{x \in \mathcal{X}} I(f_{x}; y_{x} \mid y_{S_{t}})
$$<p>
Where $S_{t}$ are the point chosen until timestep $t$.</p>
$$
x_{t + 1} = \arg\max_{x \in \mathcal{X}} \frac{1}{2}\log \left( 1 + \frac{\sigma^{2}_{t}(x)}{\sigma^{2}_{n}} \right)= \arg\max_{x \in \mathcal{X}} \sigma_{t}^{2}(x)
$$<p>
Where we assumed the noise to be homoscedastic.</p>
$$
\begin{align}
I(f_{x}; y_{x}) &= H(y_{x}) - H(y_{x} \mid f_{x})   \\
&= \frac{1}{2} \log (2\pi e \sigma^{2}_{n} + 2\pi e \sigma^{2}_{t}(x)) -\frac{1}{2} \log (2\pi e \sigma^{2}_{n})    \\
&= \frac{1}{2} \log \left( 1 + \frac{\sigma^{2}_{t}(x)}{\sigma^{2}_{n}} \right)
\end{align}
$$<h4 id="drawbacks-of-uncertainty-sampling">Drawbacks of Uncertainty Sampling<a hidden class="anchor" aria-hidden="true" href="#drawbacks-of-uncertainty-sampling">#</a></h4>
<p>In heteroscedastic settings, what we should minimize is actually the ratio between the epistemic uncertainty and the aleatoric uncertainty, always choosing for the maximum epistemic uncertainty is not guaranteed to be the most informative choice.
But with greedy, we can&rsquo;t distinguish them cleanly (for some reason I didn&rsquo;t understood).
In the <strong>homoskedastic</strong> setting, however, it is quite good.</p>
<h4 id="greedy-mutual-information-optimization">Greedy mutual information optimization<a hidden class="anchor" aria-hidden="true" href="#greedy-mutual-information-optimization">#</a></h4>
<p>The main idea is to find the point $x$ in our safe space that maximizes mutual information with the interesting space $A$. This solution has been called Information Transductive learning by the AML professor, not sure that this name is correct.</p>
$$
x_{n}  = \arg \max_{x \in S} I(f_{\mathcal{A}};  y_{x} \mid \mathcal{D}_{n - 1})
$$<p>
Phrased in another manner, we would like to know how the uncertainty about our target function $f$ diminishes when we get to know about $y$.</p>
$$
\arg\max_{x \in D} I(f_{\mathcal{A}};  y_{x + D_{n - 1}} ) - I(f_{\mathcal{A}};  y_{D_{n - 1}})
$$<p>
With some simple calculation we find that this objective is exactly the above objective.
This is also the marginal gain that we described above.</p>
<p>If we assume that $f$ is a <a href="/notes/gaussian-processes">Gaussian Processes</a> then the mutual information is interpretable as minimizing general posterior variance:</p>
$$
\begin{align}
x_{n}  &  = \arg \max_{x \in S} I(f_{\mathcal{A}};  y_{x} \mid \mathcal{D}_{n - 1}) \\
 & = \arg \max_{x \in S} H(f_{\mathcal{A}} \mid \mathcal{D}_{n - 1}) - H(f_{\mathcal{A}} \mid y_{x}, \mathcal{D}_{n - 1}) \\ \\
 & = \arg \max_{x \in S} \frac{1}{2} \log \left( 2 \pi e \sigma_{f_{\mathcal{A}} \mid \mathcal{D}_{n - 1}}^{2} \right) - \frac{1}{2} \log \left( 2 \pi e \sigma_{f_{\mathcal{A}} \mid y_{x}, \mathcal{D}_{n - 1}}^{2} \right) \\  \\
 & = \arg \min_{x \in S} \frac{1}{2} \log \left( \frac{\sigma_{f_{\mathcal{A}} \mid y_{x}, \mathcal{D}_{n - 1}}^{2}}{\sigma_{f_{\mathcal{A}} \mid \mathcal{D}_{n - 1}}^{2}} \right) \\  \\
 &  = \arg \min_{x \in S} \sigma_{f_{\mathcal{A}} \mid y_{x}, \mathcal{D}_{n - 1}}^{2}
\end{align}
$$<p>It&rsquo;s easy to interpret: we just want to sample the point were the <em>uncertainty is maximized</em>!</p>
<h4 id="greedy-optimization-bounds">Greedy optimization bounds<a hidden class="anchor" aria-hidden="true" href="#greedy-optimization-bounds">#</a></h4>
$$
F(S_{T}) \geq  \left( 1 - \frac{1}{e} \right) \max_{T } F(T)
$$<p>
Using the notation for mutual information introduced above.
This says that greedy uncertainty sampling is at least $1 - 1/e$ of the optimal solution, which is <em>near optimal</em>. The important theoretical step for this is the submodularity of mutual information.
We will not provide that here.</p>
$$
\begin{align}
F(S^{*}) &\leq F(S^{*} \cup S_{T}) &\text{ by Monotonicity} \\
&= F(S_{T}) + \sum_{i = 1}^{n} \Delta_{F}(x_{i} \mid S_{T} \cup x_{j < i})  \\
&\leq F(S_{T}) + \sum_{i = 1}^{n} \Delta_{F}(x_{i} \mid S_{T} ) &\text{ By Submodularity} \\
& \leq F(S_{T}) + n \cdot \arg\max_{x \in \mathcal{X}} \Delta_{F}(x \mid S_{T})   \\
& = F(S_{T}) + n \cdot (F(S_{T + 1}) - F(S_{T})) &\text{ By definiiton of Greedy}  \\
\end{align}
$$<p>
Then if we set $\delta _t = F(S^{*}) - F(S_{t})$ we can do some standard algebraic manipulation manipulation and get the result.</p>
<h4 id="types-of-optimal-design">Types of optimal design<a hidden class="anchor" aria-hidden="true" href="#types-of-optimal-design">#</a></h4>
<p>What we have done so far has been intensively studied in the field of optimal design. This field is interested on how to conduct the <em>most informative experiment</em>.
There are many different manners to choose the sampling point. These include</p>
<ul>
<li><em>d-optimal</em>: which attempts to reduce the determinant of the posterior covariance function.</li>
<li><em>a-optimal</em>: minimizes the trace of the posterior covariance matrix.</li>
<li><em>e-optimal</em>: attempts to minimize the maximum eigenvalue of the posterior covariance matrix.
All these can be interpreted geometrically as doing operations on the <em>uncertainty ellipsoid</em>.</li>
</ul>
<h3 id="active-learning-for-classification">Active Learning for classification<a hidden class="anchor" aria-hidden="true" href="#active-learning-for-classification">#</a></h3>
<h4 id="the-starting-idea-label-entropy">The starting Idea: Label Entropy<a hidden class="anchor" aria-hidden="true" href="#the-starting-idea-label-entropy">#</a></h4>
$$
x_{n+1} = \arg \max_{x \in S} H(y_{x}\mid x_{1:n}, y_{1:n})
$$<p>
But often, this usually leads to sampling points close to the decision boundary (that is where the uncertainty of the labels are higher!). Similar to the case where we have heteroskedastic noise, the entropy at these points could be higher just because of the aleatoric noise, aka label noise. We would like to come up with a more informed approach.</p>
<h4 id="informative-sampling-for-classification">Informative sampling for classification<a hidden class="anchor" aria-hidden="true" href="#informative-sampling-for-classification">#</a></h4>
<p>This is also called BALD (Bayesian Active Learning by Disagreement).
Here we distinguish the aleatoric and epistemic noise by adding another random variable $\theta$ that represents the epistemic noise. Then we build upon basically the same ideas as the section on Regression in <a href="/notes#greedy-optimization">#Greedy optimization</a>.</p>
$$
\begin{align}
x_{n+1} &= \arg \max_{x \in S} I(\theta; y_{x}\mid x_{1:t}, y_{1:t})  \\
&= \arg \max_{x \in S} H(y_{x} \mid x_{1:t}, y_{1:t}) - H(y_{x} \mid x_{1:t}, y_{1:t},\theta) \\
&= \arg \max_{x \in S} H(y_{x} \mid x_{1:t}, y_{1:t}) - \mathbb{E}_{\theta \mid x_{1:t}, y_{1:t}} \left[ H(y_{x} \mid \theta) \right] \\
\end{align}
$$<p>
We can use approximate inference, like <a href="/notes/variational-inference">Variational Inference</a> or <a href="/notes/monte-carlo-methods">Monte Carlo Methods</a> to obtain approximations of the above.
Entropy of the average prediction versus average entropy of the predictions given a trained model.
The first term looks for points were <em>all models are uncertain</em>, the second term can be interpreted as a regularizer (similar to aleatoric uncertainty considerations somehow), and penalizes points where the model is uncertain, and steers for more confident points where the model disagrees with the label. Intuitively, the second part is the <em>aleatoric uncertainty</em> which is the average uncertainty for all models.</p>
<p><a href="https://gist.github.com/Flecart/dc04ca800cf2a9b152e28c93493a6bfe">This</a> is some code to play with on a notebook.</p>
$$
I[\mathbf{y}, \theta \mid \mathbf{x}, \mathcal{D}] = H[\mathbf{y} \mid \mathbf{x}, \mathcal{D}] - \mathbb{E}_{p(\theta \mid \mathcal{D})} [H[\mathbf{y} \mid \mathbf{x}, \theta]]
$$<p>
The important thing to understand is that these methods select the most informative samples from the <strong>unlabeled pool</strong>, i.e., samples for which the model&rsquo;s predictions are most uncertain due to disagreement across its posterior. The new data point that we are trying to sample is not known in advance.</p>
<h3 id="acquisition-functions">Acquisition functions<a hidden class="anchor" aria-hidden="true" href="#acquisition-functions">#</a></h3>
<p>In this case we would like to find some point with a certain property, which is usually local. For example, we might be interested to find the maximum of a certain function. The ideas here could be applied also to the search of other points, given the assumptions hold.</p>
<p>See <a href="/notes/bayesian-optimization">Bayesian Optimization</a>.</p>
<h3 id="information-transductive-learning">Information Transductive learning<a hidden class="anchor" aria-hidden="true" href="#information-transductive-learning">#</a></h3>
<p>This is an idea of <a href="http://arxiv.org/abs/2402.15898">(H{&quot;u}botter et al. 2024)</a>, part of his master&rsquo;s thesis at ETH with Andreas Krause. The presentation of this part is a not perfectly clear, but probably is not quite important for the exam.</p>
<h4 id="introduction-to-the-problem">Introduction to the Problem<a hidden class="anchor" aria-hidden="true" href="#introduction-to-the-problem">#</a></h4>
<p>The main problem is <em>how to sample</em> in dangerous environments, where the cost of sampling may be high. We would need to define a <strong>safe zone</strong> in these contexts. With this setting, the space where you can have sample points is inherently different from the testing and evaluation points.
We would like to find the maximum of an unknown stochastic process $f^{*}$, while respecting some safety constraints.
We can choose a set of points $x_{1}, \dots, x_{n} \in \mathcal{X}$  but we don&rsquo;t want these to be outside the <em>safe area</em> $S := \left\{ x \in \mathcal{X} \mid g(x) \geq 0 \right\}$ where $g$ is the safety function.
For each point that we choose, we observe the label value $y_{1}, \dots y_{n}$ and the safety value $z_{1}, \dots z_{n}$. We have thus identified three unknown that we would like to take into account, $f, g, S$.
Fitting a <a href="/notes/gaussian-processes">Gaussian Processes</a> on the dataset $(x_{i}, z_{i})_{i < n}$, we can produce two function that represent our confidence over the safety function $g$ and the safety set $S$. We consider the function $S_{l} = \left\{ x \mid l_{g}(x) \geq 0 \right\}$ where $l$ paired with its upper bound counter part has $95\%$ confidence bound of the safety function.
In this manner, if we sample inside this set, we are <em>almost sure</em> that we are always sampling inside the safe zone, given our model is well-calibrated (see <a href="/notes/bayesian-neural-networks">Bayesian neural networks</a>).</p>
<h4 id="transductive-learning-solution">Transductive Learning Solution<a hidden class="anchor" aria-hidden="true" href="#transductive-learning-solution">#</a></h4>
<p>Then we sample inside this space for the function $f$ where its higher than the most conservative estimate, in formulas, our target space is $A = \left\{ x \in S_{u} \mid u_{f}(x) \geq \max_{x \in S_{l}} l_{f}(x) \right\}$. A graphical representation is probably clearer in the presentation.
After we define the safe space and the target space, we could plug in optimization methods, for example the greedy one we explained before.</p>
$$
x_{n+1} = \arg \max_{x \in S_{l}} I(f_{\mathcal{A}};  y_{x} \mid \mathcal{D}_{n - 1})
$$<h3 id="batch-active-learning">Batch Active Learning<a hidden class="anchor" aria-hidden="true" href="#batch-active-learning">#</a></h3>
<p>With this, we present the algorithm introduced in <a href="http://arxiv.org/abs/2205.11320">(Yehuda et al. 2022)</a>.</p>
<h4 id="generalization-error-for-1-nn">Generalization error for 1-NN<a hidden class="anchor" aria-hidden="true" href="#generalization-error-for-1-nn">#</a></h4>
$$
\mathop{\mathbb{E}}[\hat{f}(x) \neq f(x)] \leq P(x \not\in C) + P(x \in C_{wrong})
$$<p>
Where $C_{wrong}$ is the set of points labelled wrongly.
We note that $P(x \not \in C) = 1 - P(C(L, \delta))$ which is the probability of a point not present in our covering set, while $x \in C_{wrong}$ is the probability of a point in the covering set being wrongly classified.
The paper defines the first value to be the <strong>high budged case</strong> as it can be swiftly lowered by just sampling more points, while in <strong>low budget</strong> that variable is not controllable, so one would like to reduce the second term. ProbCover attempts to fix the second term.</p>
<h4 id="the-objective-function">The objective function<a hidden class="anchor" aria-hidden="true" href="#the-objective-function">#</a></h4>
$$
\arg\max _{L \subseteq \mathcal{X}, \lvert L \rvert  = b} P(\bigcup_{x \in L} B(x, \delta) )
$$<p>
For a fixed $\delta$, we would like to find the best set of points of size $b$ such that it is probably covered by the balls situated in that point. The paper shows that this problem is NP hard.
The second problem is that we don&rsquo;t know the prior distribution of $x$, so it&rsquo;s hard to make an estimate of that (we can use the empirical distribution).</p>
<h4 id="duality-with-coreset">Duality with Coreset<a hidden class="anchor" aria-hidden="true" href="#duality-with-coreset">#</a></h4>
$$
\delta(x) = \min \left\{ \delta \in R_{+} : P(\bigcup_{x \in L}B(x, \delta)) = 1 \right\} 
$$<p>
Meaning: find best ball range, such that all the points are covered by the balls of that range. This is a loose dual problem to the one we have seen before.
In the above problem: we try to fix the ball radius, and find the point that can minimize the coverage.</p>
<h4 id="probcover">ProbCover<a hidden class="anchor" aria-hidden="true" href="#probcover">#</a></h4>
<p>From my understanding, ProbCover tells you how to sample the points that have the <strong>maximum coverage</strong> for a certain space, independently of the model that you will use. This is motivated by the observation that if we fix the $\delta$ of the ball covers that we would like to use, then the upper bound of the probability of the error is fixed.</p>
<p>The simple algorithm is just maximizing the points that have most neighbors inside its circle. This has some theoretical motivations.
The main results you should remember are the following:</p>
<ol>
<li>The probability of impure balls in the coverage is upper bounded by the probability of impure balls in the whole sample space.</li>
<li>The generalization error $\mathbb{E}[\hat{f}(x) \neq f(x)]$ is upper bounded by the probability of the uncovered space and the wrongly classified points inside the covered space.</li>
<li>If we fix a $\delta$, which is the radius of a Ball, then we just need to maximize coverage, which could be done by the following algorithm in the image, which just maximizes for most points in the ball.</li>
</ol>
<img src="/images/notes/Active Learning-20241119183944079.webp" style="width: 100%" class="center" alt="Active Learning-20241119183944079">
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=hubotterTransductiveActiveLearning2024>[1] H{\"u}botter et al. <a href="http://arxiv.org/abs/2402.15898">“Transductive Active Learning: Theory and Applications”</a> arXiv preprint arXiv:2402.15898 2024
 </p>
<p id=yehudaActiveLearningCovering2022>[2] Yehuda et al. <a href="http://arxiv.org/abs/2205.11320">“Active Learning Through a Covering Lens”</a> arXiv preprint arXiv:2205.11320 2022
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">➕Probabilistic-Artificial-Intelligence</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Active Learning on x"
            href="https://x.com/intent/tweet/?text=Active%20Learning&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2factive-learning%2f&amp;hashtags=machinelearning%2c%e2%9e%95probabilistic-artificial-intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Active Learning on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2factive-learning%2f&amp;title=Active%20Learning&amp;summary=Active%20Learning&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2factive-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Active Learning on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2factive-learning%2f&title=Active%20Learning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Active Learning on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2factive-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Active Learning on whatsapp"
            href="https://api.whatsapp.com/send?text=Active%20Learning%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2factive-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Active Learning on telegram"
            href="https://telegram.me/share/url?text=Active%20Learning&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2factive-learning%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Active Learning on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Active%20Learning&u=https%3a%2f%2fflecart.github.io%2fnotes%2factive-learning%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
