<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Intrinsic Motivation and Playfulness | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="neuroscience">
<meta name="description" content="We have a classical exploration-exploitation tradeoff, see Reinforcement Learning, a introduction.
Why would animals explore, even if there is no immediate reward based on it? Animals are able to adapt and explore nonetheless. It would be thus nice to understand and implement these features in artificial systems.
We will attack this from an evolutionary psychology perspective: Playfulness and intrinsic motivation.
Curiosity
Here we will talk about why animals are pushed to explore.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/intrinsic-motivation-and-playfulness/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/intrinsic-motivation-and-playfulness/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/intrinsic-motivation-and-playfulness/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Intrinsic Motivation and Playfulness">
  <meta property="og:description" content="We have a classical exploration-exploitation tradeoff, see Reinforcement Learning, a introduction. Why would animals explore, even if there is no immediate reward based on it? Animals are able to adapt and explore nonetheless. It would be thus nice to understand and implement these features in artificial systems. We will attack this from an evolutionary psychology perspective: Playfulness and intrinsic motivation.
Curiosity Here we will talk about why animals are pushed to explore.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="Neuroscience">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Intrinsic Motivation and Playfulness">
<meta name="twitter:description" content="We have a classical exploration-exploitation tradeoff, see Reinforcement Learning, a introduction.
Why would animals explore, even if there is no immediate reward based on it? Animals are able to adapt and explore nonetheless. It would be thus nice to understand and implement these features in artificial systems.
We will attack this from an evolutionary psychology perspective: Playfulness and intrinsic motivation.
Curiosity
Here we will talk about why animals are pushed to explore.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Intrinsic Motivation and Playfulness",
      "item": "https://flecart.github.io/notes/intrinsic-motivation-and-playfulness/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Intrinsic Motivation and Playfulness",
  "name": "Intrinsic Motivation and Playfulness",
  "description": "We have a classical exploration-exploitation tradeoff, see Reinforcement Learning, a introduction. Why would animals explore, even if there is no immediate reward based on it? Animals are able to adapt and explore nonetheless. It would be thus nice to understand and implement these features in artificial systems. We will attack this from an evolutionary psychology perspective: Playfulness and intrinsic motivation.\nCuriosity Here we will talk about why animals are pushed to explore.\n",
  "keywords": [
    "neuroscience"
  ],
  "articleBody": "We have a classical exploration-exploitation tradeoff, see Reinforcement Learning, a introduction. Why would animals explore, even if there is no immediate reward based on it? Animals are able to adapt and explore nonetheless. It would be thus nice to understand and implement these features in artificial systems. We will attack this from an evolutionary psychology perspective: Playfulness and intrinsic motivation.\nCuriosity Here we will talk about why animals are pushed to explore.\nIntrinsic motivation One of the classical books about the topic is White, Robert W. ‚ÄúMotivation reconsidered: The concept of competence.‚Äù Psychological review 66.5 (1959): 297.\nThree main motives We want to understand why animals explore (why visiting new places or learning new things brings this small bump of joy? Why would you want to know more things, in the moment it is difficult that it has some reward). and activity and manipulation, for example going to jog and swim, which is costly activity but useless. Or running wheel mouse as another example, or playing with music and games (manipulation), and they feel pleasure by doing these activities.\nWe will focus mainly on curiosity (exploration) and playfulness (manipulation and activity)\nFrameworks for Curiosity Q-Learning See Tabular Reinforcement Learning#Q Learning. We have states-action-value function, and have an update rule for a specific action at a specific state. Usually the error is off-policy, which is usually learnt from a database of experiences, so not updated from\nThe optimal policy can always be retrieved, and it will be an indicator function for the correct action at a certain state.\nThe bads:\nWorld knowledge, gathered through experience, is not utilized to guide exploration (more so with ùúñ-greedy SARSA). More explored state-action paths are not less favored for exploration (even under softmax), meaning we are not exploring enough. This leads to idea like exploration bonus to balance the exploration, akin to some Rmax algorithm we studied in Tabular Reinforcement Learning.\nSARSA See Tabular Reinforcement Learning#SARSA. It‚Äôs basically the same as q-learning, but it is epsilon-greedy, it has some random exploration with some probability. And it is on-policy since we are using the action based on the policy\nK-armed bandit See Reinforcement Learning, a introduction, where we have a very small explanation of this problem.\nWe have tradeoffs between explorations and exploitation!\nEpsilon greedy exploration Convergence for larger ùúñ is faster, but leads to smaller average reward in the long run due to continuous exploration after convergence. Greedy action selection increases rapidly but often does not converge\nIdea for improvement: make $\\varepsilon$ adaptable (anneal it to zero), but it is difficult to tune. A better idea is the following section, the softmax exploration\nSoftmax exploration We are using a softmax function to know which state to explore. See Tabular Reinforcement Learning. And we have a parameter $\\beta$ or a temperature parameter $\\tau$ to control the exploration exploitation tradeoff.\nThis is the most used technique now in this subfield. This doesn‚Äôt take into account the cues of the environment. It‚Äôs bad to do again a bad action just because the temperature is high for example. If the state has been visited often, we would like that to be more stable and less ‚Äúhot‚Äù.\nExploration Bonus with uncertainty The idea is to add a bias for exploration, based on the variance of the function! The problem is that we are attracted by randomness. We don‚Äôt want to be attracted by randomness, if it‚Äôs just noise, its not so interesting.\nCounters for exploration bonus Counters can solve this problem, to bias towards rare state action pairs. But in high dimensional spaces, this is very inefficient. Another drawback is that we are using only the immediate value but not the future values, so it is very much shortsighted measure.\nExploration Values One idea is to introduce exploration values, like Q values in SARSA but without rewards. We assume new states have a very good reward, like 1, and update each state with the new real reward. This can act as a counter. See Tabular Reinforcement Learning, this is very close to RMAX.\nThe more times we visit, the less the reward will be, in this way it acts as a counter. This is some sort of generalized counter. The problem is that some problems are not yet solved by these algorithm because their reward is very sparse, or a lot of noise.\nForward-Inverse Models ‚ÄúDefine the intrinsic reward in proportion to the error made by the agent in predicting the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model.‚Äù\nWe want to predict the parts of the environment that are caused by our agent. We want to work with the transformation of the sensory states. The intrinsic reward is the difference of the predicted features and the actual features. Bigger difference means bigger surprise. The idea is to try to build features that are only relevant to your action. So for the first time it was able to play games with lots of noise.\nOne drawback is that it leads the agent to be interested in the noisy TV problem.\nPlayfulness Difference with Curiosity How can we distinguish manipulation seeking and knowledge seeking behaviours? Manipulation can be used as the discriminator for usefulness against the noisy TV problem.\nOne fundamental difference is that curiosity is consumed, playfulness is not.\nFor example consider sensory substitution.\nSensory substitution Image to sound for the blind. But people hate this sensory substitution:\nA lot of training to learn this They use a lot of auditory information to orient themselves, and this adds a lot of noise. A better device is a vibratory device on the skin, but it takes still a lot of effort. One interesting thing happens for the birds see Birdsong and Song System. Singing towards a female makes them more stereotyped (female is exploiting, else they would explore more). Same thing for humans, when they get older they explore a lot less.\nLight substitution in Zebra finches Hearing birds can learn when the light is off. Deaf birds follow the light, they but I did not understand this part‚Ä¶ They can learn by light. They adapt very fast to light stimuli. These birds don‚Äôt know what they are producing, but they can get light feedback into them. This experiment is about whether if Zebra finches can learn just by feedback of light, when they are deaf.\nTime of adaptation in Zebra finches They can learn to change the pitch very precisely. They learn faster when they are deaf. But their feedback must be reliable! They prefer reliable feedback but low signal against noisy feedback. No reinforcement learning can cleanly explain this.\nA model for sensorimotor behaviour of birds The model must be reliable, we studied something very similar in Conditioning Theory. If not, the association is not so strong.\n$$ R_{j}^{t} = E_{j}^{T} + M_{j}^{T} + r_{j} $$ Where $r_{J}$ is some intrinsic punishment. They maximize by choosing some light off things.\nPitch Contingent Singing Birds like to sing if it is correlated to their pitch, if its random they don‚Äôt like to sing (leading to the empowerment theory). Meaning empowerment is linked to sensory substitution. They actually need to be able to act and modify on that.\nEmpowerment $$ E = C(p(s_{t + 1} \\mid a_{t})) = \\max_{p(a_{t})} I(A_{t} ; S_{t + 1}) $$ This is what the agent wants to do, its related to playfulness and complementary to curiosity so to say.\n$$ I(A; S) = H(S) - H(S \\mid A) $$ Meaning high entropy environments (complex environment) and complete determinism of my own actions, define the largest empowerment signal, which is deep reward.\nSea squirt is some animal that behaves like this.\n",
  "wordCount" : "1283",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/intrinsic-motivation-and-playfulness/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Intrinsic Motivation and Playfulness
    </h1>
    <div class="post-meta">7 min&nbsp;¬∑&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#curiosity" aria-label="Curiosity">Curiosity</a><ul>
                        
                <li>
                    <a href="#intrinsic-motivation" aria-label="Intrinsic motivation">Intrinsic motivation</a><ul>
                        
                <li>
                    <a href="#three-main-motives" aria-label="Three main motives">Three main motives</a></li></ul>
                </li>
                <li>
                    <a href="#frameworks-for-curiosity" aria-label="Frameworks for Curiosity">Frameworks for Curiosity</a><ul>
                        
                <li>
                    <a href="#q-learning" aria-label="Q-Learning">Q-Learning</a></li>
                <li>
                    <a href="#sarsa" aria-label="SARSA">SARSA</a></li>
                <li>
                    <a href="#k-armed-bandit" aria-label="K-armed bandit">K-armed bandit</a></li>
                <li>
                    <a href="#epsilon-greedy-exploration" aria-label="Epsilon greedy exploration">Epsilon greedy exploration</a></li>
                <li>
                    <a href="#softmax-exploration" aria-label="Softmax exploration">Softmax exploration</a></li>
                <li>
                    <a href="#exploration-bonus-with-uncertainty" aria-label="Exploration Bonus with uncertainty">Exploration Bonus with uncertainty</a></li>
                <li>
                    <a href="#counters-for-exploration-bonus" aria-label="Counters for exploration bonus">Counters for exploration bonus</a></li>
                <li>
                    <a href="#exploration-values" aria-label="Exploration Values">Exploration Values</a></li>
                <li>
                    <a href="#forward-inverse-models" aria-label="Forward-Inverse Models">Forward-Inverse Models</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#playfulness" aria-label="Playfulness">Playfulness</a><ul>
                        <ul>
                        
                <li>
                    <a href="#difference-with-curiosity" aria-label="Difference with Curiosity">Difference with Curiosity</a></li></ul>
                    
                <li>
                    <a href="#sensory-substitution" aria-label="Sensory substitution">Sensory substitution</a><ul>
                        
                <li>
                    <a href="#light-substitution-in-zebra-finches" aria-label="Light substitution in Zebra finches">Light substitution in Zebra finches</a></li>
                <li>
                    <a href="#time-of-adaptation-in-zebra-finches" aria-label="Time of adaptation in Zebra finches">Time of adaptation in Zebra finches</a></li>
                <li>
                    <a href="#a-model-for-sensorimotor-behaviour-of-birds" aria-label="A model for sensorimotor behaviour of birds">A model for sensorimotor behaviour of birds</a></li>
                <li>
                    <a href="#pitch-contingent-singing" aria-label="Pitch Contingent Singing">Pitch Contingent Singing</a></li>
                <li>
                    <a href="#empowerment" aria-label="Empowerment">Empowerment</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>We have a classical exploration-exploitation tradeoff, see <a href="/notes/reinforcement-learning,-a-introduction">Reinforcement Learning, a introduction</a>.
Why would animals explore, even if there is no immediate reward based on it? Animals are able to adapt and explore nonetheless. It would be thus nice to understand and implement these features in artificial systems.
We will attack this from an evolutionary psychology perspective: <strong>Playfulness and intrinsic motivation</strong>.</p>
<h2 id="curiosity">Curiosity<a hidden class="anchor" aria-hidden="true" href="#curiosity">#</a></h2>
<p>Here we will talk about why animals are pushed to explore.</p>
<h3 id="intrinsic-motivation">Intrinsic motivation<a hidden class="anchor" aria-hidden="true" href="#intrinsic-motivation">#</a></h3>
<p>One of the classical books about the topic is White, Robert W. &ldquo;Motivation reconsidered: The concept of
competence.&rdquo; Psychological review 66.5 (1959): 297.</p>
<h4 id="three-main-motives">Three main motives<a hidden class="anchor" aria-hidden="true" href="#three-main-motives">#</a></h4>
<p>We want to understand <strong>why animals explore</strong> (why visiting new places or learning new things brings this small bump of joy? Why would you want to know more things, in the moment it is difficult that it has some reward). and <strong>activity and manipulation</strong>, for example going to jog and swim, which is costly activity but useless. Or running wheel mouse as another example, or playing with music and games (manipulation), and they feel pleasure by doing these activities.</p>
<p>We will focus mainly on curiosity (exploration) and playfulness (manipulation and activity)</p>
<h3 id="frameworks-for-curiosity">Frameworks for Curiosity<a hidden class="anchor" aria-hidden="true" href="#frameworks-for-curiosity">#</a></h3>
<h4 id="q-learning">Q-Learning<a hidden class="anchor" aria-hidden="true" href="#q-learning">#</a></h4>
<p>See <a href="/notes/tabular-reinforcement-learning#q-learning">Tabular Reinforcement Learning#Q Learning</a>.
We have states-action-value function, and have an update rule for a specific action at a specific state.
Usually the error is <em>off-policy</em>, which is usually learnt from a database of experiences, so not updated from</p>
<p>The <strong>optimal policy can always be retrieved</strong>, and it will be an indicator function for the correct action at a certain state.</p>
<p><strong>The bads:</strong></p>
<ul>
<li>World knowledge, gathered through experience, is not utilized to guide exploration (more so with ùúñ-greedy SARSA).</li>
<li>More explored state-action paths are not less favored for exploration (even under softmax), meaning <strong>we are not exploring enough</strong>.</li>
</ul>
<p>This leads to idea like exploration bonus to balance the exploration, akin to some Rmax algorithm we studied in <a href="/notes/tabular-reinforcement-learning">Tabular Reinforcement Learning</a>.</p>
<h4 id="sarsa">SARSA<a hidden class="anchor" aria-hidden="true" href="#sarsa">#</a></h4>
<p>See <a href="/notes/tabular-reinforcement-learning#sarsa">Tabular Reinforcement Learning#SARSA</a>.
It&rsquo;s basically the same as q-learning, but it is <strong>epsilon-greedy</strong>, it has some random exploration with some probability.
And it is <strong>on-policy</strong> since we are using the action based on the policy</p>
<h4 id="k-armed-bandit">K-armed bandit<a hidden class="anchor" aria-hidden="true" href="#k-armed-bandit">#</a></h4>
<p>See <a href="/notes/reinforcement-learning,-a-introduction">Reinforcement Learning, a introduction</a>, where we have a very small explanation of this problem.</p>
<img src="/images/notes/Intrinsic Motivation-20250721211914633.webp" style="width: 100%" class="center" alt="Intrinsic Motivation-20250721211914633">
<p>We have tradeoffs between explorations and exploitation!</p>
<h4 id="epsilon-greedy-exploration">Epsilon greedy exploration<a hidden class="anchor" aria-hidden="true" href="#epsilon-greedy-exploration">#</a></h4>
<p>Convergence for larger ùúñ is faster, but leads to smaller average reward in the long run due to continuous exploration after convergence. Greedy action selection increases rapidly but often does not converge</p>
<p>Idea for improvement: make $\varepsilon$ adaptable (anneal it to zero), but it is <em>difficult to tune</em>.
A better idea is the following section, the softmax exploration</p>
<h4 id="softmax-exploration">Softmax exploration<a hidden class="anchor" aria-hidden="true" href="#softmax-exploration">#</a></h4>
<p>We are using a softmax function to know which state to explore. See <a href="/notes/tabular-reinforcement-learning">Tabular Reinforcement Learning</a>.
And we have a parameter $\beta$ or a temperature parameter $\tau$ to control the exploration exploitation tradeoff.</p>
<img src="/images/notes/Intrinsic Motivation-20250721212343218.webp" style="width: 100%" class="center" alt="Intrinsic Motivation-20250721212343218">
<p>This is the <strong>most used</strong> technique now in this subfield.
This doesn&rsquo;t take into account the <em>cues</em> of the environment. It&rsquo;s bad to do again a bad action just because the temperature is high for example. If the state has been visited often, we would like that to be more stable and less &ldquo;hot&rdquo;.</p>
<h4 id="exploration-bonus-with-uncertainty">Exploration Bonus with uncertainty<a hidden class="anchor" aria-hidden="true" href="#exploration-bonus-with-uncertainty">#</a></h4>
<p>The idea is to add a bias for exploration, based on the variance of the function!
<img src="/images/notes/Intrinsic Motivation-20250721212800645.webp" style="width: 100%" class="center" alt="Intrinsic Motivation-20250721212800645"></p>
<p>The problem is that we are attracted by randomness. We <strong>don&rsquo;t want to be attracted by randomness</strong>, if it&rsquo;s just noise, its not so interesting.</p>
<h4 id="counters-for-exploration-bonus">Counters for exploration bonus<a hidden class="anchor" aria-hidden="true" href="#counters-for-exploration-bonus">#</a></h4>
<p>Counters can solve this problem, to bias towards rare state action pairs. But in high dimensional spaces, this is very inefficient.
Another drawback is that we are using only the <strong>immediate value</strong> but not the future values, so it is very much shortsighted measure.</p>
<h4 id="exploration-values">Exploration Values<a hidden class="anchor" aria-hidden="true" href="#exploration-values">#</a></h4>
<p>One idea is to introduce <strong>exploration values</strong>, like Q values in SARSA but without rewards. We assume new states have a very good reward, like 1, and update each state with the new real reward. This can act as a counter.
See <a href="/notes/tabular-reinforcement-learning">Tabular Reinforcement Learning</a>, this is very close to RMAX.</p>
<img src="/images/notes/Intrinsic Motivation-20250721215000851.webp" style="width: 100%" class="center" alt="Intrinsic Motivation-20250721215000851">
The more times we visit, the less the reward will be, in this way it acts as a counter.
This is some sort of generalized counter.
<p>The problem is that some problems are not yet solved by these algorithm because their reward is <strong>very sparse</strong>, or a lot of <strong>noise</strong>.</p>
<h4 id="forward-inverse-models">Forward-Inverse Models<a hidden class="anchor" aria-hidden="true" href="#forward-inverse-models">#</a></h4>
<blockquote>
<p>‚ÄúDefine the intrinsic reward in proportion to the error made by the agent in predicting the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model.‚Äù</p></blockquote>
<p>We want to predict the parts of the environment that are caused by <strong>our agent</strong>. We want to work with the transformation of the sensory states.
<img src="/images/notes/Intrinsic Motivation-20250721215400567.webp" style="width: 100%" class="center" alt="Intrinsic Motivation-20250721215400567"></p>
<p>The intrinsic reward is the difference of the predicted features and the actual features. Bigger difference means bigger surprise. The idea is to try to build features that are only relevant to your action.
So for the first time it was able to play games with lots of noise.</p>
<p>One drawback is that it leads the agent to be interested in the noisy TV problem.</p>
<h2 id="playfulness">Playfulness<a hidden class="anchor" aria-hidden="true" href="#playfulness">#</a></h2>
<h4 id="difference-with-curiosity">Difference with Curiosity<a hidden class="anchor" aria-hidden="true" href="#difference-with-curiosity">#</a></h4>
<p>How can we distinguish manipulation seeking and knowledge seeking behaviours?
Manipulation can be used as the discriminator for usefulness against the noisy TV problem.</p>
<p>One fundamental difference is that curiosity is <strong>consumed</strong>, playfulness is not.</p>
<p>For example consider sensory substitution.</p>
<h3 id="sensory-substitution">Sensory substitution<a hidden class="anchor" aria-hidden="true" href="#sensory-substitution">#</a></h3>
<p>Image to sound for the blind. But people hate this sensory substitution:</p>
<ul>
<li>A lot of training to learn this</li>
<li>They use a lot of auditory information to orient themselves, and this adds a lot of noise.</li>
</ul>
<p>A better device is a vibratory device on the skin, but it takes still a lot of effort.
One interesting thing happens for the birds see <a href="/notes/birdsong-and-song-system">Birdsong and Song System</a>.
Singing towards a female makes them more stereotyped (female is exploiting, else they would explore more).
Same thing for humans, when they get older they explore a lot less.</p>
<h4 id="light-substitution-in-zebra-finches">Light substitution in Zebra finches<a hidden class="anchor" aria-hidden="true" href="#light-substitution-in-zebra-finches">#</a></h4>
<p>Hearing birds can learn when the light is off.
Deaf birds follow the light, they but I did not understand this part&hellip; They can learn by light.
They adapt very fast to light stimuli.
These birds don&rsquo;t know what they are producing, but they can get light feedback into them.
This experiment is about whether if Zebra finches can learn just by feedback of light, when they are deaf.</p>
<h4 id="time-of-adaptation-in-zebra-finches">Time of adaptation in Zebra finches<a hidden class="anchor" aria-hidden="true" href="#time-of-adaptation-in-zebra-finches">#</a></h4>
<p>They can learn to change the pitch very precisely.
They learn faster when they are deaf.
But their feedback must be <strong>reliable</strong>! They prefer reliable feedback but low signal against noisy feedback.
No reinforcement learning can cleanly explain this.</p>
<h4 id="a-model-for-sensorimotor-behaviour-of-birds">A model for sensorimotor behaviour of birds<a hidden class="anchor" aria-hidden="true" href="#a-model-for-sensorimotor-behaviour-of-birds">#</a></h4>
<p>The model must be reliable, we studied something very similar in <a href="/notes/conditioning-theory">Conditioning Theory</a>.
If not, the association is not so strong.</p>
<img src="/images/notes/Intrinsic Motivation and Playfulness-20250722223656113.webp" style="width: 100%" class="center" alt="Intrinsic Motivation and Playfulness-20250722223656113">
$$
R_{j}^{t} = E_{j}^{T} + M_{j}^{T} + r_{j}
$$<p>
Where $r_{J}$ is some intrinsic punishment.
They maximize by choosing some light off things.</p>
<h4 id="pitch-contingent-singing">Pitch Contingent Singing<a hidden class="anchor" aria-hidden="true" href="#pitch-contingent-singing">#</a></h4>
<p>Birds like to sing if it is correlated to their pitch, if its random they don&rsquo;t like to sing (leading to the empowerment theory). Meaning empowerment is linked to sensory substitution.
They actually need to be able to act and modify on that.</p>
<h4 id="empowerment">Empowerment<a hidden class="anchor" aria-hidden="true" href="#empowerment">#</a></h4>
$$
E = C(p(s_{t + 1} \mid a_{t})) = \max_{p(a_{t})} I(A_{t} ; S_{t + 1})
$$<p>
This is what the agent wants to do, its related to playfulness and complementary to curiosity so to say.</p>
$$
I(A; S) = H(S) - H(S \mid A)
$$<p>
Meaning high entropy environments (complex environment) and complete determinism of my own actions, define the largest empowerment signal, which is deep reward.</p>
<p>Sea squirt is some animal that behaves like this.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/neuroscience/">Neuroscience</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intrinsic Motivation and Playfulness on x"
            href="https://x.com/intent/tweet/?text=Intrinsic%20Motivation%20and%20Playfulness&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fintrinsic-motivation-and-playfulness%2f&amp;hashtags=neuroscience">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intrinsic Motivation and Playfulness on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fintrinsic-motivation-and-playfulness%2f&amp;title=Intrinsic%20Motivation%20and%20Playfulness&amp;summary=Intrinsic%20Motivation%20and%20Playfulness&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fintrinsic-motivation-and-playfulness%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intrinsic Motivation and Playfulness on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fintrinsic-motivation-and-playfulness%2f&title=Intrinsic%20Motivation%20and%20Playfulness">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intrinsic Motivation and Playfulness on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fintrinsic-motivation-and-playfulness%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intrinsic Motivation and Playfulness on whatsapp"
            href="https://api.whatsapp.com/send?text=Intrinsic%20Motivation%20and%20Playfulness%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fintrinsic-motivation-and-playfulness%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intrinsic Motivation and Playfulness on telegram"
            href="https://telegram.me/share/url?text=Intrinsic%20Motivation%20and%20Playfulness&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fintrinsic-motivation-and-playfulness%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intrinsic Motivation and Playfulness on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Intrinsic%20Motivation%20and%20Playfulness&u=https%3a%2f%2fflecart.github.io%2fnotes%2fintrinsic-motivation-and-playfulness%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
