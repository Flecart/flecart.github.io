<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Systems for Artificial Intelligence | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="‚òÅcloud-computing">
<meta name="description" content="At the time of writing, the compute requirements for machine learning models and artificial intelligence are growing at a staggering rate of 200% every 3.5 months. Interest in the area is being quantified as 10k papers per month on the topic, while dollar investments on compute (energy, cooling, sustainability of compute in general) have had a hard time keeping up with the continuous new requests.


 From https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec03/03_ml-lifecycle.pdf">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/systems-for-artificial-intelligence/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/systems-for-artificial-intelligence/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/systems-for-artificial-intelligence/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Systems for Artificial Intelligence">
  <meta property="og:description" content="At the time of writing, the compute requirements for machine learning models and artificial intelligence are growing at a staggering rate of 200% every 3.5 months. Interest in the area is being quantified as 10k papers per month on the topic, while dollar investments on compute (energy, cooling, sustainability of compute in general) have had a hard time keeping up with the continuous new requests.
From https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec03/03_ml-lifecycle.pdf">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="‚òÅCloud-Computing">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Systems for Artificial Intelligence">
<meta name="twitter:description" content="At the time of writing, the compute requirements for machine learning models and artificial intelligence are growing at a staggering rate of 200% every 3.5 months. Interest in the area is being quantified as 10k papers per month on the topic, while dollar investments on compute (energy, cooling, sustainability of compute in general) have had a hard time keeping up with the continuous new requests.


 From https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec03/03_ml-lifecycle.pdf">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Systems for Artificial Intelligence",
      "item": "https://flecart.github.io/notes/systems-for-artificial-intelligence/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Systems for Artificial Intelligence",
  "name": "Systems for Artificial Intelligence",
  "description": "At the time of writing, the compute requirements for machine learning models and artificial intelligence are growing at a staggering rate of 200% every 3.5 months. Interest in the area is being quantified as 10k papers per month on the topic, while dollar investments on compute (energy, cooling, sustainability of compute in general) have had a hard time keeping up with the continuous new requests.\nFrom https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec03/03_ml-lifecycle.pdf\n",
  "keywords": [
    "‚òÅcloud-computing"
  ],
  "articleBody": "At the time of writing, the compute requirements for machine learning models and artificial intelligence are growing at a staggering rate of 200% every 3.5 months. Interest in the area is being quantified as 10k papers per month on the topic, while dollar investments on compute (energy, cooling, sustainability of compute in general) have had a hard time keeping up with the continuous new requests.\nFrom https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec03/03_ml-lifecycle.pdf\nThis is the general pipeline of model development, training, and inference.\nOne note is that the real ML code in this systems is very small, see (Sculley et al. 2015), there are massive maintenance costs in common ML systems.\nData Curation Collection and Cleaning üü© Garbage in -\u003e Garbage out ~old wisdom\nThe first part of every machine learning system is collect and curate good quality data. Work in this part of building machine learning systems mostly consist in:\nAggregating data Identifying correct sources of the data Remove not good data, labeling it so that it has high quality, and similar. Identify anomalies/outliers, and filtering it. Very difficult to distinguish from noise. Fill empty values Having correct labels (e.g. (Deng et al. 2009) initial big effort, which is very expensive) Analyzing preprocessing costs üü©‚Äì We need to distinguish from online and offline preprocessing requirements. These functions will use lots of resources on the CPU and often become part of the bottleneck (see here), from personal statistics, it was about 30% just for data preprocessing. Sometimes, it also consumes more data than the training itself! The professor Ana Klimovic shows how scaling the preprocessing and training client can speed up sometimes by 100x in some cases, see here.\nImage from the course slides\nModel Development In this section, we want to build a nice model for our problem, design new architectures, tune hyperparameters, and validate the accuracy. There is not so much from the system side that can be said about model development; this is another area of research.\nAutomated searches üü©‚Äì This is somewhat similar to what fast performance optimization community has done with LAPACK, BLAS, see Fast Linear Algebra.\nWe have: Model search:\nExplore automatically different architectures to find the best one, obviously very performance intensive. Neural Architecture Search, where you could also use things like RL to find the best one. Hyper-parameter tuning:\nSearch for the best hyperparameters for a model batch size, learning rate, number of layers, activation fucntions, optimizers, and other similar things that could be considered to tune this aspect. Training Pipeline The training pipeline is the output of the model development phase. If you do not track all the dependencies, you wont be able to do things like this:\nRetrain models with new data Track data and code for debugging Capture dependencies for deployment Audit training for compliance (e.g., data privacy policies) Then it would also enable for validation and versioning and usually does not need lot of expertise in machine learning, this is the MLOps side of things.\nUsual Training Stages After (Bommasani et al. 2022), we mostly train big ML models and then:\nPretrain: we train the model on a large dataset, usually unsupervised, to learn general features of the data. Fine-tune: we train the model on a smaller dataset, usually supervised, to learn specific features of the data. Alignment: we train the model to align its behavior with human values and preferences or based on some reliability metrics. Continual Training Usually models need to be retrained to\nInclude new data Adapt do domain shifts or data shifts E.g. hairstyle, fashion shifts. Data deletion E.g. GDPR, or similar laws. This means you need to incorporate new training methods starting from an existing one. It is probably difficult to balance different tradeoff, from accuracy to reliability and compliance. For example some threshold to trigger retraining to check for concept drifts. And this would then enable for data selection policies to take the correct data for retraining. See (B√∂ther et al. 2025).\nTraining Parallelization Training is one of the most compute intensive parts of the machine learning pipeline. We need to distinguish the data itself with the parameters data (weights of the model most of the time, or hyperparameters and similar things).\nData parallelism: partition the data and run multiple copies of the model, which synchronize to exchange model weight updates.\nAbout model parallelism:\nTensor Parallelism: we divide layer inference across nodes Pipeline Parallelism: We divide models across nodes. Parameter Server üü© This is a classical way to implement distributed training. Basically, every worker has a copy of the model, and they send the gradients to the parameter server, which then aggregates them and sends them back to all workers. This is a good way to implement distributed training, but it has some drawbacks:\nThe parameter server can become a bottleneck, especially if there are many workers. The parameter server can become a single point of failure, which can be problematic if it goes down. AllReduce Parallelism This is a way to implement distributed training, the GPUs share the exchange directly with each other and gradient aggregation is done across workers.\nThis is the most common way to implement GPU GPU interconnect thanks to fast networks between GPUs.\nReduce-Scatter phase of an all-reduce parallelism\nNext phase is just gathering the weight together so that each worker has the same weights.\nPipeline Parallelism üü©‚Äì Due to data dependencies, it is usually difficult to have high hardware utilization with the standard forward and backward version. This form of parallelism is also called model parallelism.\npipeline parallelism partitions the layers of a model into multiple stages and distributes them across the GPUs. While training, two consecutive pipeline stages exchange intermediate activations or gradients with point-topoint communication. Since the point-to-point communication of pipeline parallelism adds small overheads compared to the synchronization of tensor parallelism, the pipeline parallelism degree can increase along with the model size. From (Kim et al. 2023) paper.\nUsing micro-batches, you can improve the utilization of the hardware, it is like pipelining, see Central Processing Unit, but on a different level. Introduced in (Huang et al. 2019). Pipeline parallelism might suffer from stalls (bubbles) due to dependencies and load imbalance between the devices (imbalanced partitioning).\nThis form of parallelism is useful if you do not have a machine big enough to fit the whole model, but many machines, so you load the weights at each stage of the pipeline to compute the forward pass, pipelining makes the bubble time smaller, thus making the model more efficient.\nMemory efficient Pipeline Another example of a pipeline is (Kim et al. 2023), they introduce the one forward and one backward approach, in figure:\nImage from the paper: An illustration of a 4-way 1F1B pipeline schedule with eight micro-batches. The memory pressure of each pipeline stage increases during the warmup phase, stays constant at the steady phase, and decreases within the cooldown phase. After the cooldown phase, parameters are updated with accumulated gradients of each micro-batch. A number in either forward or backward denotes the micro-batch index.\n$$ M(s) = W(s) + A(s) $$$$ M(s) = W_{0} + A_{0} \\mu(s) $$ Where $\\mu$ is the number of micro batches.\nData Parallelism üü© This is the easiest method for parallelism: you replicate the model many times as use that to train on different batches of data at the same times. But you can quickly observe if the model takes a lot of data then this is quite consuming in terms of memory.\nTensor Parallelism üü© Computing different parts of the tensor can be done in parallel, with different parts of the data. Even if a model can fit on a single GPU, it could benefit from tensor parallelism by splitting that part.\nSync or Async Training üü®- Synchronous training is usually slow, there will be probably stragglers, for example see (Dean \u0026 Barroso 2013). Synchronous training: workers operate in lockstep to update the model weights at each iteration\nAsynchronous training: each worker independently updates model weights Synchronous training achieves better model quality, but lower throughput Accuracy vs. throughput trade-off. Useful metric: time to train to a target accuracy Fault Tolerance üü•++ Single failures can wipe out entire model parameters and waste lots of compute time. Restarting the training process is quite expensive. Common problem that make a node fail are:\nHardware Misconfigurations Network problems A common way to deal with this problem is using checkpointing If you use spot VMs with some GPUs and TPUs, the amount of failures is higher, difficult to ask a GPU in the cloud currently because everybody is asking for that currently.\nPipelined Checkpoints üü®‚Äì We want to checkpoint our current model parameters to some persistent storage. The problem is how can we balance the time needed to checkpoint and training? We don‚Äôt want too much overhead due to this part. The idea is to start to copy to RAM after the first update has finished, but then start the computation for the next iteration immediately, and stall only for the remaining time, then disk I/O can be done in parallel.\nModel Inference After we have trained the model, we want to optimize for response time.\nComparison between training and inference Training: iterate through dataset, compute forward \u0026 backward pass on each batch of data to update model parameters.\nCare about high throughput and fast convergence to high accuracy Inference: compute forward pass on a (small) batch of data Care about low latency, high throughput and high accuracy Inference location We can mainly use two types of inference location:\nOn the edge: Meaning it is running on your computer, locally, or on the device. It is probably the best option for privacy and latency. Yet it raises some challenges for energy consumption, and hardware types. On the cloud: Meaning it is running on a remote server, in the cloud. It is probably the best option for scalability and flexibility. Yet it raises some challenges for privacy and latency. e.g. OpenAI runs its model on Azure Cloud. ML Compilers They apply optimizations to the model, to make it run faster on the hardware on different devices. TVM from Tianqi Chen is an example of ML compiler, and it is open source. Goal: efficiently serve a trained model on different kinds of devices\nML compiler applies multiple rounds of optimizations on computation graph, e.g., vectorization, loop tiling, operation fusion (one example is (Dao et al. 2022)) Many ML compilers use ML to to some of the optimizations! https://mlc.ai is a free course on ML optimizations for TVM.\nTechniques for Cloud Inference Clipper and Clockwork are some examples for Cloud inference.\nBatches Cache results Compression and Model quantization Model selection based on latency and accuracy See an example: Model-less . GPU resources allocation We talked about dominance resource fairness in Cluster Management Policies. Here the authors of THEMIS propose the idea of finish time fairness, and minimize max finish time fairness across ML apps. That is the ration of exec time\nGPU underutilization There are some reasons why the GPUs are underutilized.\nLow batch sizes in ML inference Network congestion Input data scheduling Individual ML jobs do not make good use of GPUs, some are more memory intensive, others are CPU intensive. GPU Sharing Temporal Colocation Spatial Colocation Orion is an example of this.\nReferences [1] B√∂ther et al. ‚ÄúModyn: Data-Centric Machine Learning Pipeline Orchestration‚Äù arXiv preprint arXiv:2312.06254 2025 [2] Deng et al. ‚ÄúImageNet: A Large-Scale Hierarchical Image Database‚Äù 2009 IEEE Conference on Computer Vision and Pattern Recognition 2009 [3] Dao et al. ‚ÄúFLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-awareness‚Äù Curran Associates Inc. 2022 [4] Bommasani et al. ‚ÄúOn the Opportunities and Risks of Foundation Models‚Äù arXiv preprint arXiv:2108.07258 2022 [5] Kim et al. ‚ÄúBPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models‚Äù PMLR 2023 [6] Dean \u0026 Barroso ‚ÄúThe Tail at Scale‚Äù Vol. 56(2), pp. 74--80 2013 [7] Huang et al. ‚ÄúGPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism‚Äù Curran Associates, Inc. 2019 [8] Sculley et al. ‚ÄúHidden Technical Debt in Machine Learning Systems‚Äù Curran Associates, Inc. 2015 ",
  "wordCount" : "2001",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/systems-for-artificial-intelligence/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Systems for Artificial Intelligence
    </h1>
    <div class="post-meta">10 min&nbsp;¬∑&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#data-curation" aria-label="Data Curation">Data Curation</a><ul>
                        
                <li>
                    <a href="#collection-and-cleaning-" aria-label="Collection and Cleaning üü©">Collection and Cleaning üü©</a></li>
                <li>
                    <a href="#analyzing-preprocessing-costs---" aria-label="Analyzing preprocessing costs üü©&ndash;">Analyzing preprocessing costs üü©&ndash;</a></li></ul>
                </li>
                <li>
                    <a href="#model-development" aria-label="Model Development">Model Development</a><ul>
                        
                <li>
                    <a href="#automated-searches---" aria-label="Automated searches üü©&ndash;">Automated searches üü©&ndash;</a></li>
                <li>
                    <a href="#training-pipeline" aria-label="Training Pipeline">Training Pipeline</a></li>
                <li>
                    <a href="#usual-training-stages" aria-label="Usual Training Stages">Usual Training Stages</a></li>
                <li>
                    <a href="#continual-training" aria-label="Continual Training">Continual Training</a></li></ul>
                </li>
                <li>
                    <a href="#training-parallelization" aria-label="Training Parallelization">Training Parallelization</a><ul>
                        
                <li>
                    <a href="#parameter-server-" aria-label="Parameter Server üü©">Parameter Server üü©</a></li>
                <li>
                    <a href="#allreduce-parallelism" aria-label="AllReduce Parallelism">AllReduce Parallelism</a></li>
                <li>
                    <a href="#pipeline-parallelism---" aria-label="Pipeline Parallelism üü©&ndash;">Pipeline Parallelism üü©&ndash;</a></li>
                <li>
                    <a href="#memory-efficient-pipeline" aria-label="Memory efficient Pipeline">Memory efficient Pipeline</a></li>
                <li>
                    <a href="#data-parallelism-" aria-label="Data Parallelism üü©">Data Parallelism üü©</a></li>
                <li>
                    <a href="#tensor-parallelism-" aria-label="Tensor Parallelism üü©">Tensor Parallelism üü©</a></li>
                <li>
                    <a href="#sync-or-async-training--" aria-label="Sync or Async Training üü®-">Sync or Async Training üü®-</a></li>
                <li>
                    <a href="#fault-tolerance-" aria-label="Fault Tolerance üü•&#43;&#43;">Fault Tolerance üü•++</a></li>
                <li>
                    <a href="#pipelined-checkpoints---" aria-label="Pipelined Checkpoints üü®&ndash;">Pipelined Checkpoints üü®&ndash;</a></li></ul>
                </li>
                <li>
                    <a href="#model-inference" aria-label="Model Inference">Model Inference</a><ul>
                        
                <li>
                    <a href="#comparison-between-training-and-inference" aria-label="Comparison between training and inference">Comparison between training and inference</a></li>
                <li>
                    <a href="#inference-location" aria-label="Inference location">Inference location</a></li>
                <li>
                    <a href="#ml-compilers" aria-label="ML Compilers">ML Compilers</a></li>
                <li>
                    <a href="#techniques-for-cloud-inference" aria-label="Techniques for Cloud Inference">Techniques for Cloud Inference</a></li>
                <li>
                    <a href="#gpu-resources-allocation" aria-label="GPU resources allocation">GPU resources allocation</a></li>
                <li>
                    <a href="#gpu-underutilization" aria-label="GPU underutilization">GPU underutilization</a></li>
                <li>
                    <a href="#gpu-sharing" aria-label="GPU Sharing">GPU Sharing</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>At the time of writing, the compute requirements for machine learning models and artificial intelligence are growing at a staggering rate of 200% every 3.5 months. Interest in the area is being quantified as 10k papers per month on the topic, while dollar investments on compute (energy, cooling, sustainability of compute in general) have had a hard time keeping up with the continuous new requests.</p>
<figure class="center">
<img src="/images/notes/Systems for Artificial Intelligence-20250429113134097.webp" style="width: 100%"   alt="Systems for Artificial Intelligence-20250429113134097" title="Systems for Artificial Intelligence-20250429113134097"/>
<figcaption><p style="text-align:center;"> From <a href="https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec03/03_ml-lifecycle.pdf">https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec03/03_ml-lifecycle.pdf</a></p></figcaption>
</figure>
<p>This is the general pipeline of model development, training, and inference.</p>
<p>One note is that the real ML code in this systems is very small, see <a href="https://papers.nips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html">(Sculley et al. 2015)</a>, there are massive maintenance costs in common ML systems.</p>
<h3 id="data-curation">Data Curation<a hidden class="anchor" aria-hidden="true" href="#data-curation">#</a></h3>
<h4 id="collection-and-cleaning-">Collection and Cleaning üü©<a hidden class="anchor" aria-hidden="true" href="#collection-and-cleaning-">#</a></h4>
<blockquote>
<p>Garbage in -&gt; Garbage out ~<em>old wisdom</em></p></blockquote>
<p>The first part of every machine learning system is collect and curate good quality data.
Work in this part of building machine learning systems mostly consist in:</p>
<ul>
<li>Aggregating data</li>
<li>Identifying correct sources of the data</li>
<li>Remove not good data, labeling it so that it has high quality, and similar.
<ul>
<li>Identify anomalies/outliers, and filtering it. Very difficult to distinguish from noise.</li>
<li>Fill empty values</li>
<li>Having correct labels (e.g. <a href="https://ieeexplore.ieee.org/document/5206848">(Deng et al. 2009)</a> initial big effort, which is very expensive)</li>
</ul>
</li>
</ul>
<h4 id="analyzing-preprocessing-costs---">Analyzing preprocessing costs üü©&ndash;<a hidden class="anchor" aria-hidden="true" href="#analyzing-preprocessing-costs---">#</a></h4>
<p>We need to distinguish from <strong>online</strong> and <strong>offline</strong> preprocessing requirements. These functions will use lots of resources on the CPU and often become part of the bottleneck (see <a href="https://arxiv.org/pdf/2111.04131">here</a>), from personal statistics, it was about 30% just for data preprocessing.
Sometimes, it also consumes more data than the training itself!
The professor Ana Klimovic shows how scaling the preprocessing and training client can speed up sometimes by 100x in some cases, see <a href="https://anakli.inf.ethz.ch/papers/tfdata_service_SoCC23.pdf">here</a>.</p>
<figure class="center">
<img src="/images/notes/Systems for Artificial Intelligence-20250429114519991.webp" style="width: 100%"   alt="Systems for Artificial Intelligence-20250429114519991" title="Systems for Artificial Intelligence-20250429114519991"/>
<figcaption><p style="text-align:center;">Image from the course slides</p></figcaption>
</figure>
<h3 id="model-development">Model Development<a hidden class="anchor" aria-hidden="true" href="#model-development">#</a></h3>
<p>In this section, we want to build a nice model for our problem, design new architectures, tune hyperparameters, and validate the accuracy.
There is not so much from the system side that can be said about model development; this is another area of research.</p>
<h4 id="automated-searches---">Automated searches üü©&ndash;<a hidden class="anchor" aria-hidden="true" href="#automated-searches---">#</a></h4>
<p>This is somewhat similar to what fast performance optimization community has done with LAPACK, BLAS, see <a href="/notes/fast-linear-algebra">Fast Linear Algebra</a>.</p>
<p>We have:
<strong>Model search</strong>:</p>
<ul>
<li>Explore automatically different architectures to find the best one, obviously very performance intensive.</li>
<li>Neural Architecture Search, where you could also use things like RL to find the best one.</li>
</ul>
<p><strong>Hyper-parameter tuning</strong>:</p>
<ul>
<li>Search for the best hyperparameters for a model
<ul>
<li>batch size, learning rate,  number of layers, activation fucntions, optimizers, and other similar things that could be considered to tune this aspect.</li>
</ul>
</li>
</ul>
<h4 id="training-pipeline">Training Pipeline<a hidden class="anchor" aria-hidden="true" href="#training-pipeline">#</a></h4>
<p>The training pipeline is the output of the model development phase.
If you do not track all the dependencies, you wont be able to do things like this:</p>
<ul>
<li>Retrain models with new data</li>
<li>Track data and code for debugging</li>
<li>Capture dependencies for deployment</li>
<li>Audit training for compliance (e.g., data privacy policies)</li>
</ul>
<p>Then it would also enable for validation and versioning and usually does not need lot of expertise in machine learning, this is the MLOps side of things.</p>
<h4 id="usual-training-stages">Usual Training Stages<a hidden class="anchor" aria-hidden="true" href="#usual-training-stages">#</a></h4>
<p>After <a href="http://arxiv.org/abs/2108.07258">(Bommasani et al. 2022)</a>, we mostly train big ML models and then:</p>
<ul>
<li><strong>Pretrain</strong>: we train the model on a large dataset, usually unsupervised, to learn general features of the data.</li>
<li><strong>Fine-tune</strong>: we train the model on a smaller dataset, usually supervised, to learn specific features of the data.</li>
<li><strong>Alignment</strong>: we train the model to align its behavior with human values and preferences or based on some reliability metrics.</li>
</ul>
<h4 id="continual-training">Continual Training<a hidden class="anchor" aria-hidden="true" href="#continual-training">#</a></h4>
<p>Usually models need to be retrained to</p>
<ul>
<li>Include new data</li>
<li>Adapt do domain shifts or data shifts
<ul>
<li>E.g. hairstyle, fashion shifts.</li>
</ul>
</li>
<li>Data deletion
<ul>
<li>E.g. GDPR, or similar laws.
This means you need to incorporate new training methods starting from an existing one. It is probably difficult to balance different tradeoff, from accuracy to reliability and compliance.</li>
</ul>
</li>
</ul>
<p>For example some threshold to trigger retraining to check for concept drifts. And this would then enable for data selection policies to take the correct data for retraining. See <a href="http://arxiv.org/abs/2312.06254">(B√∂ther et al. 2025)</a>.</p>
<h3 id="training-parallelization">Training Parallelization<a hidden class="anchor" aria-hidden="true" href="#training-parallelization">#</a></h3>
<p>Training is one of the most compute intensive parts of the machine learning pipeline.
We need to distinguish the data itself with the parameters data (weights of the model most of the time, or hyperparameters and similar things).</p>
<p><strong>Data parallelism</strong>: partition the data and run multiple copies of the model, which synchronize to exchange model weight updates.</p>
<p>About model parallelism:</p>
<ul>
<li><strong>Tensor Parallelism</strong>: we divide layer inference across nodes</li>
<li><strong>Pipeline Parallelism</strong>: We divide models across nodes.</li>
</ul>
<img src="/images/notes/Systems for Artificial Intelligence-20250501112925572.webp" style="width: 100%" class="center" alt="Systems for Artificial Intelligence-20250501112925572">
<h4 id="parameter-server-">Parameter Server üü©<a hidden class="anchor" aria-hidden="true" href="#parameter-server-">#</a></h4>
<p>This is a classical way to implement distributed training.
<img src="/images/notes/Systems for Artificial Intelligence-20250430162302571.webp" style="width: 100%" class="center" alt="Systems for Artificial Intelligence-20250430162302571"></p>
<p>Basically, every worker has a copy of the model, and they send the gradients to the parameter server, which then aggregates them and sends them back to all workers.
This is a good way to implement distributed training, but it has some drawbacks:</p>
<ul>
<li>The parameter server can become a bottleneck, especially if there are many workers.</li>
<li>The parameter server can become a single point of failure, which can be problematic if it goes down.</li>
</ul>
<h4 id="allreduce-parallelism">AllReduce Parallelism<a hidden class="anchor" aria-hidden="true" href="#allreduce-parallelism">#</a></h4>
<p>This is a way to implement distributed training, the GPUs share the exchange directly with each other and gradient aggregation is done across workers.</p>
<p>This is the most common way to implement GPU GPU interconnect thanks to fast networks between GPUs.</p>
<figure class="center">
<img src="/images/notes/Systems for Artificial Intelligence-20250501113509714.webp" style="width: 100%"   alt="Systems for Artificial Intelligence-20250501113509714" title="Systems for Artificial Intelligence-20250501113509714"/>
<figcaption><p style="text-align:center;">Reduce-Scatter phase of an all-reduce parallelism</p></figcaption>
</figure>
<p>Next phase is just gathering the weight together so that each worker has the same weights.</p>
<h4 id="pipeline-parallelism---">Pipeline Parallelism üü©&ndash;<a hidden class="anchor" aria-hidden="true" href="#pipeline-parallelism---">#</a></h4>
<p>Due to data dependencies, it is usually difficult to have high hardware utilization with the standard forward and backward version. This form of parallelism is also called <strong>model parallelism</strong>.</p>
<blockquote>
<p>pipeline parallelism partitions the layers of a model into multiple stages and distributes them across
the GPUs. While training, two consecutive pipeline stages exchange intermediate activations or gradients with point-topoint communication. Since the point-to-point communication of pipeline parallelism adds small overheads compared to the synchronization of tensor parallelism, the pipeline parallelism degree can increase along with the model size. From <a href="https://proceedings.mlr.press/v202/kim23l.html">(Kim et al. 2023)</a> paper.</p></blockquote>
<p>Using <strong>micro-batches</strong>, you can improve the utilization of the hardware, it is like pipelining, see <a href="/notes/central-processing-unit">Central Processing Unit</a>, but on a different level. Introduced in <a href="https://papers.nips.cc/paper_files/paper/2019/hash/093f65e080a295f8076b1c5722a46aa2-Abstract.html">(Huang et al. 2019)</a>. Pipeline parallelism might suffer from stalls (bubbles) due to dependencies and load imbalance
between the devices (imbalanced partitioning).</p>
<img src="/images/notes/Systems for Artificial Intelligence-20250501113842247.webp" style="width: 100%" class="center" alt="Systems for Artificial Intelligence-20250501113842247">
<p>This form of parallelism is useful if you do not have a machine big enough to fit the whole model, but many machines, so you load the weights at each stage of the pipeline to compute the forward pass, pipelining makes the bubble time smaller, thus making the model more efficient.</p>
<h4 id="memory-efficient-pipeline">Memory efficient Pipeline<a hidden class="anchor" aria-hidden="true" href="#memory-efficient-pipeline">#</a></h4>
<p>Another example of a pipeline is <a href="https://proceedings.mlr.press/v202/kim23l.html">(Kim et al. 2023)</a>, they introduce the one forward and one backward approach, in figure:</p>
<figure class="center">
<img src="/images/notes/Systems for Artificial Intelligence-20250518150939553.webp" style="width: 100%"   alt="Systems for Artificial Intelligence-20250518150939553" title="Systems for Artificial Intelligence-20250518150939553"/>
<figcaption><p style="text-align:center;">Image from the paper: An illustration of a 4-way 1F1B pipeline schedule with eight micro-batches. The memory pressure of each pipeline stage increases during the warmup phase, stays constant at the steady phase, and decreases within the cooldown phase. After the cooldown phase, parameters are updated with accumulated gradients of each micro-batch. A number in either forward or backward denotes the micro-batch index.</p></figcaption>
</figure>
$$
M(s) = W(s) + A(s)
$$$$
M(s) = W_{0} + A_{0} \mu(s)
$$<p>
Where $\mu$ is the number of micro batches.</p>
<h4 id="data-parallelism-">Data Parallelism üü©<a hidden class="anchor" aria-hidden="true" href="#data-parallelism-">#</a></h4>
<p>This is the easiest method for parallelism: you replicate the model many times as use that to train on different batches of data at the same times. But you can quickly observe if the model takes a lot of data then this is quite consuming in terms of memory.</p>
<h4 id="tensor-parallelism-">Tensor Parallelism üü©<a hidden class="anchor" aria-hidden="true" href="#tensor-parallelism-">#</a></h4>
<p>Computing different parts of the tensor can be done in parallel, with different parts of the data. Even if a model can fit on a single GPU, it could benefit from tensor parallelism by splitting that part.</p>
<img src="/images/notes/Systems for Artificial Intelligence-20250501114138612.webp" style="width: 100%" class="center" alt="Systems for Artificial Intelligence-20250501114138612">
<h4 id="sync-or-async-training--">Sync or Async Training üü®-<a hidden class="anchor" aria-hidden="true" href="#sync-or-async-training--">#</a></h4>
<p>Synchronous training is usually slow, there will be probably <strong>stragglers</strong>, for example see <a href="https://dl.acm.org/doi/10.1145/2408776.2408794">(Dean &amp; Barroso 2013)</a>.
Synchronous training: workers operate in lockstep to update the model weights at each iteration</p>
<ul>
<li>Asynchronous training: each worker independently updates model weights</li>
<li>Synchronous training achieves better model quality, but lower throughput
Accuracy vs. throughput trade-off.
Useful metric: time to train to a target accuracy</li>
</ul>
<h4 id="fault-tolerance-">Fault Tolerance üü•++<a hidden class="anchor" aria-hidden="true" href="#fault-tolerance-">#</a></h4>
<p>Single failures can wipe out entire model parameters and waste lots of compute time. Restarting the training process is quite expensive.
Common problem that make a node fail are:</p>
<ul>
<li>Hardware</li>
<li>Misconfigurations</li>
<li>Network problems</li>
</ul>
<p>A common way to deal with this problem is using <strong>checkpointing</strong>
If you use spot VMs with some GPUs and TPUs, the amount of failures is higher, difficult to ask a GPU in the cloud currently because everybody is asking for that currently.</p>
<h4 id="pipelined-checkpoints---">Pipelined Checkpoints üü®&ndash;<a hidden class="anchor" aria-hidden="true" href="#pipelined-checkpoints---">#</a></h4>
<p>We want to checkpoint our current model parameters to some <strong>persistent storage</strong>. The problem is how can we balance the time needed to checkpoint and training? We don&rsquo;t want too much overhead due to this part.
<img src="/images/notes/Systems for Artificial Intelligence-20250501114945159.webp" style="width: 100%" class="center" alt="Systems for Artificial Intelligence-20250501114945159"></p>
<p>The idea is to start to copy to RAM after the first update has finished, but then start the computation for the next iteration immediately, and stall only for the remaining time, then disk I/O can be done in parallel.</p>
<h3 id="model-inference">Model Inference<a hidden class="anchor" aria-hidden="true" href="#model-inference">#</a></h3>
<p>After we have trained the model, we want to optimize for response time.</p>
<h4 id="comparison-between-training-and-inference">Comparison between training and inference<a hidden class="anchor" aria-hidden="true" href="#comparison-between-training-and-inference">#</a></h4>
<p>Training: iterate through dataset, compute forward &amp; backward pass
on each batch of data to update model parameters.</p>
<ul>
<li>Care about high throughput and fast convergence to high accuracy</li>
<li>Inference: compute forward pass on a (small) batch of data</li>
<li>Care about low latency, high throughput and high accuracy</li>
</ul>
<h4 id="inference-location">Inference location<a hidden class="anchor" aria-hidden="true" href="#inference-location">#</a></h4>
<p>We can mainly use two types of inference location:</p>
<ul>
<li>On the <strong>edge</strong>:
<ul>
<li>Meaning it is running on your computer, locally, or on the device.</li>
<li>It is probably the best option for privacy and latency.</li>
<li>Yet it raises some challenges for energy consumption, and hardware types.</li>
</ul>
</li>
<li>On the <strong>cloud</strong>:
<ul>
<li>Meaning it is running on a remote server, in the cloud.</li>
<li>It is probably the best option for scalability and flexibility.</li>
<li>Yet it raises some challenges for privacy and latency.</li>
<li>e.g. OpenAI runs its model on Azure Cloud.</li>
</ul>
</li>
</ul>
<h4 id="ml-compilers">ML Compilers<a hidden class="anchor" aria-hidden="true" href="#ml-compilers">#</a></h4>
<p>They apply optimizations to the model, to make it run faster on the hardware on different devices.
TVM from Tianqi Chen is an example of ML compiler, and it is open source.
Goal: efficiently serve a trained model on different kinds of devices</p>
<ul>
<li>ML compiler applies multiple rounds of optimizations on computation graph, e.g., vectorization, loop tiling, operation fusion (one example is <a href="/notes/systems-for-artificial-intelligence#daoFLASHATTENTIONFastMemoryefficient2022">(Dao et al. 2022)</a>)</li>
<li>Many ML compilers use ML to to some of the optimizations!</li>
</ul>
<p><a href="https://mlc.ai"><a href="https://mlc.ai">https://mlc.ai</a></a> is a free course on ML optimizations for TVM.</p>
<h4 id="techniques-for-cloud-inference">Techniques for Cloud Inference<a hidden class="anchor" aria-hidden="true" href="#techniques-for-cloud-inference">#</a></h4>
<p>Clipper and Clockwork are some examples for Cloud inference.</p>
<ul>
<li>Batches</li>
<li>Cache results</li>
<li>Compression and Model quantization</li>
<li>Model selection based on latency and accuracy
<ul>
<li>See an example: <a href="https://www.usenix.org/system/files/atc21-romero.pdf">Model-less </a>.</li>
</ul>
</li>
</ul>
<h4 id="gpu-resources-allocation">GPU resources allocation<a hidden class="anchor" aria-hidden="true" href="#gpu-resources-allocation">#</a></h4>
<p>We talked about dominance resource fairness in <a href="/notes/cluster-management-policies">Cluster Management Policies</a>.
Here the authors of THEMIS propose the idea of <strong>finish time fairness</strong>, and minimize max finish time fairness across ML apps.
That is the ration of exec time</p>
<h4 id="gpu-underutilization">GPU underutilization<a hidden class="anchor" aria-hidden="true" href="#gpu-underutilization">#</a></h4>
<p>There are some reasons why the GPUs are underutilized.</p>
<ul>
<li>Low batch sizes in ML inference</li>
<li>Network congestion</li>
<li>Input data scheduling
Individual ML jobs do not make good use of GPUs, some are more memory intensive, others are CPU intensive.</li>
</ul>
<h4 id="gpu-sharing">GPU Sharing<a hidden class="anchor" aria-hidden="true" href="#gpu-sharing">#</a></h4>
<ul>
<li>Temporal Colocation</li>
<li>Spatial Colocation
<img src="/images/notes/Systems for Artificial Intelligence-20250430140138928.webp" style="width: 100%" class="center" alt="Systems for Artificial Intelligence-20250430140138928"></li>
</ul>
<p>Orion is an example of this.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=botherModynDataCentricMachine2025>[1] B√∂ther et al. <a href="http://arxiv.org/abs/2312.06254">‚ÄúModyn: Data-Centric Machine Learning Pipeline Orchestration‚Äù</a> arXiv preprint arXiv:2312.06254 2025
 </p>
<p id=dengImageNetLargescaleHierarchical2009>[2] Deng et al. <a href="https://ieeexplore.ieee.org/document/5206848">‚ÄúImageNet: A Large-Scale Hierarchical Image Database‚Äù</a> 2009 IEEE Conference on Computer Vision and Pattern Recognition  2009
 </p>
<p id=daoFLASHATTENTIONFastMemoryefficient2022>[3] Dao et al. ‚ÄúFLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-awareness‚Äù Curran Associates Inc.  2022
 </p>
<p id=bommasaniOpportunitiesRisksFoundation2022>[4] Bommasani et al. <a href="http://arxiv.org/abs/2108.07258">‚ÄúOn the Opportunities and Risks of Foundation Models‚Äù</a> arXiv preprint arXiv:2108.07258 2022
 </p>
<p id=kimBPipeMemoryBalancedPipeline2023>[5] Kim et al. <a href="https://proceedings.mlr.press/v202/kim23l.html">‚ÄúBPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models‚Äù</a> PMLR  2023
 </p>
<p id=deanTailScale2013>[6] Dean & Barroso <a href="https://dl.acm.org/doi/10.1145/2408776.2408794">‚ÄúThe Tail at Scale‚Äù</a>  Vol. 56(2), pp. 74--80 2013
 </p>
<p id=huangGPipeEfficientTraining2019>[7] Huang et al. <a href="https://papers.nips.cc/paper_files/paper/2019/hash/093f65e080a295f8076b1c5722a46aa2-Abstract.html">‚ÄúGPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism‚Äù</a> Curran Associates, Inc.  2019
 </p>
<p id=sculleyHiddenTechnicalDebt2015>[8] Sculley et al. <a href="https://papers.nips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html">‚ÄúHidden Technical Debt in Machine Learning Systems‚Äù</a> Curran Associates, Inc.  2015
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/cloud-computing/">‚òÅCloud-Computing</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Systems for Artificial Intelligence on x"
            href="https://x.com/intent/tweet/?text=Systems%20for%20Artificial%20Intelligence&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fsystems-for-artificial-intelligence%2f&amp;hashtags=%e2%98%81cloud-computing">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Systems for Artificial Intelligence on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fsystems-for-artificial-intelligence%2f&amp;title=Systems%20for%20Artificial%20Intelligence&amp;summary=Systems%20for%20Artificial%20Intelligence&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fsystems-for-artificial-intelligence%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Systems for Artificial Intelligence on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fsystems-for-artificial-intelligence%2f&title=Systems%20for%20Artificial%20Intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Systems for Artificial Intelligence on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fsystems-for-artificial-intelligence%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Systems for Artificial Intelligence on whatsapp"
            href="https://api.whatsapp.com/send?text=Systems%20for%20Artificial%20Intelligence%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fsystems-for-artificial-intelligence%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Systems for Artificial Intelligence on telegram"
            href="https://telegram.me/share/url?text=Systems%20for%20Artificial%20Intelligence&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fsystems-for-artificial-intelligence%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Systems for Artificial Intelligence on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Systems%20for%20Artificial%20Intelligence&u=https%3a%2f%2fflecart.github.io%2fnotes%2fsystems-for-artificial-intelligence%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
