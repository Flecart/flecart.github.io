<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Introduction to Advanced Machine Learning | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning">
<meta name="description" content="Introduction to the course Machine learning gives a new way of thinking about reality: instead of trying to capture a part of reality, as most of the old sciences, go to the meta-level and try to produce an automated method to capture reality. This first lesson will be more a lesson of philosophy. There a paradigm shift in the sense of Thomas Kuhn&rsquo;s scientific revolutions.
An interesting observation is that in the last 100 years, we have had more progress in the field of computer science than the 5000 years of computer science.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/introduction-to-advanced-machine-learning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/introduction-to-advanced-machine-learning/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Introduction to Advanced Machine Learning" />
<meta property="og:description" content="Introduction to the course Machine learning gives a new way of thinking about reality: instead of trying to capture a part of reality, as most of the old sciences, go to the meta-level and try to produce an automated method to capture reality. This first lesson will be more a lesson of philosophy. There a paradigm shift in the sense of Thomas Kuhn&rsquo;s scientific revolutions.
An interesting observation is that in the last 100 years, we have had more progress in the field of computer science than the 5000 years of computer science." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/introduction-to-advanced-machine-learning/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Introduction to Advanced Machine Learning"/>
<meta name="twitter:description" content="Introduction to the course Machine learning gives a new way of thinking about reality: instead of trying to capture a part of reality, as most of the old sciences, go to the meta-level and try to produce an automated method to capture reality. This first lesson will be more a lesson of philosophy. There a paradigm shift in the sense of Thomas Kuhn&rsquo;s scientific revolutions.
An interesting observation is that in the last 100 years, we have had more progress in the field of computer science than the 5000 years of computer science."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Introduction to Advanced Machine Learning",
      "item": "https://flecart.github.io/notes/introduction-to-advanced-machine-learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Introduction to Advanced Machine Learning",
  "name": "Introduction to Advanced Machine Learning",
  "description": "Introduction to the course Machine learning gives a new way of thinking about reality: instead of trying to capture a part of reality, as most of the old sciences, go to the meta-level and try to produce an automated method to capture reality. This first lesson will be more a lesson of philosophy. There a paradigm shift in the sense of Thomas Kuhn\u0026rsquo;s scientific revolutions.\nAn interesting observation is that in the last 100 years, we have had more progress in the field of computer science than the 5000 years of computer science.",
  "keywords": [
    "machinelearning"
  ],
  "articleBody": "Introduction to the course Machine learning gives a new way of thinking about reality: instead of trying to capture a part of reality, as most of the old sciences, go to the meta-level and try to produce an automated method to capture reality. This first lesson will be more a lesson of philosophy. There a paradigm shift in the sense of Thomas Kuhn’s scientific revolutions.\nAn interesting observation is that in the last 100 years, we have had more progress in the field of computer science than the 5000 years of computer science. This is because we have more data, we have more scientists, we are developing with the Kurzweil’s exponential growth. When comparing brain and computers, one have more creativity, the other has more storage capacity. This gives a trade-off.\nIn principle, epistemology tells you how to extract knowledge from data, from this point of view it can be seen as the precursor of modern information retrieval systems. This tells you something about methods of derivation. The first of which is deduction (Euclid, Hilbert’s grundlagen der geometrie, David Mumford for bayesian reasoning and image processing, but this person is still living). Deduction allows us to create facts by starting with the core principles of our thinking. Later induction was born (philosophically created by Bacon), where you create theory from data, and it’s a little bit more machine learning like. The former goes by logic, the latter by intuition (but could be wrong, and it’s not formalized), now modelled by statistics. These two processes of reasoning can be modelled as computational processes. And this allows us to create nice machines that are able to achieve these :D.\nWe use digital data because we can control it:\nEfficient to manage Discover models test and simulate hypotheses Find solutions The last three points can perhaps be called intelligence. For Joachim M. Buhmann (the prof.), it’s impossible to understand the models, and this can be motivated to the limited storage capacity of our brains, so the attempt of interpretability (i.e. correctly understanding exactly what is happening, the law) is just a bias in the history of science for us humans. While if we understand interpretability as a way to summarize useful information, visualizing it, then it is ok.\nClassically we considered algorithms as a way to process data, efficiently, now we want to treat them as a relation between data and decisions. At the end, the decisions are what is important for us humans. Algorithm’s makes it easier for us. Classically we have studied a concept of complexity of runtime, memory, or energy, but we have not have a clear theory of abstraction or robustness. In our case, we have a probability distribution going in, and another going out. But modelling them as random variables, it’s difficult to have a clear definition of correctness.\nWe can now divide algorithms in three macro categories, based on human-expertise\nClassical algos: humans are able to solve this problems, and are also able to specify those kinds of problems in a logical way Supervised algos: humans are able to solve these kinds of problems, but don’t know how to formalize their logic. Unsupervised and self-play: humans are not able to solve these problem good enough, this gave rise to alpha fold, alpha go and stuff similar to this. For Prof. Buhmann a fundamental ability of intelligence is being able to do counterfactual reasoning and planning. So being able to simulate the future, and be able to plan the today’s action in order to change the future. This is similar to (Choi 2022) and abductive reasoning by Choi.\nEvery law is a social experiment on a not understood population\nWhen Buhmann tried to say that nobody exactly understands everything, but a correct level of abstraction is often enough (i.e. chemical processes of the combustion engine)\nStudying algorithms is the most complicated part of mathematics when mathematics becomes concrete. 29 September 2023 Joachim M. Buhmann, minute 5.44 in ETHz AML lesson\nHe said that Kolmogorov Complexity is Shannon’s Theory for Random variables?? What does it even mean?\nNoise is your friend because it prevents you to make a statement so precise that you cannot compute it\nNoise is an indicator that you can’t solve your job.\nFramework for learning algorithms We want to have a theory used to validate learning algorithms $\\mathcal{A}$, which admits stochastic data as input.\nWe define the data to be observations of some experiment of some genre. Mathematically we write $e \\in \\varepsilon \\to \\mathcal{X}$ , $e \\to \\mathcal{X}(e)$. Meaning: we have an experiment that contributes to the data $\\mathcal{X}$. We define a hypothesis class $\\mathcal{C}$ where possible hypothesis are located. We use these hypothesis to interpret the dataset $\\mathcal{X}$.\nSome definitions A definition of Data Science We say that data science studies the algorithms $\\mathcal{A}$ that map the data $\\mathcal{X}$ to the space of possible hypothesis $\\mathcal{C}$.\nStatistics usually studies the distribution of $\\mathcal{X}$ data science studies the mappings. Physics and Mathematics, and other sciences study the space of possible hypothesis.\nWhat is Data? Encyclopedia Britannica: Association of numbers with physical quantities and natural phenomena by comparing an unknown quantity with a known quantity of the same kind.\nMeasurements by sensors, factual information, numbers. It’s not very clear, I would say any numbers that can help you do some interesting inferences in some world (it has to affect some entities (i.e. humans))\nWhat are features? But we can’t use this data directly so we use features of data, that could be arbitrary transformations of the initial data (edges, corners, etc…)\nTypes of data Measurements We say that we have a world of $R$ objects, and we do observations about these objects. We say $X$ is a measurement (so a data point) where we have a function $X : \\mathcal{O}^{(1)} \\times \\dots \\times \\mathcal{O}^{(R)} \\to \\mathbb{K}$\nFour types of data Feature vectors: $X: \\mathcal{O} \\to \\mathbb{R}^{d}$ Categorical data $X : \\mathcal{O} \\to \\mathbb{R}^{d} \\times \\left\\{ 1, \\dots, k \\right\\}$ Regression data $X : \\mathcal{O} \\to \\mathbb{R}^{d} \\times \\mathbb{R}$ Proximity data $X : \\mathcal{O} \\times \\mathcal{O} \\to \\mathbb{R}$\nThis division should be pretty intuitive. See the slides for some real examples. Scales Nominal or Categorical scales Example: binary $\\mathcal{X} = \\left\\{ 0, 1 \\right\\}$, or some nominal categories (sweet, sour etc)\nQuantitative scales We can divide these scales as interval scales for fahrenheit temperature scale. The important information is between the values, so scale and translation invariant. Ratio scale: the information is the difference with a focal point, for example Kelvin temperature scale Absolute scales where we the important is absolute thing (grade scale ahah).\nBuhmann believes humans are only good in relative scale judgement, when they try to do absolute scaling, they can bias negatively or positively.\nDesiderata for Data Science In this section we would like to describe what exactly is a good data science algorithm.\nHypothesis consistency The most important requirement is: If $x$ and $x'$ are drawn from the same distribution, then $\\mathbb{P}^{\\mathcal{A}}(c \\mid x) \\approx \\mathbb{P}^{\\mathcal{A}}(c\\mid x')$ meaning the probability of the hypothesis should be similar.\nWe can call this control experiment and interpret $x$ as training data and $x'$ as test data in this setting. But we need to take some care about this statement:\nWe want similar inputs to have similar hypothesis We want dissimilar inputs to have dissimilar hypothesis If we only had requirement 1, then always outputting the same thing could solve our problem, but more variability is more useful, so we also consider dissimilar hypothesis :). The classical problem When starting to model a problem, we Computer Scientists that have to work on data have to decide the format of the data that we want to use. We want a way to evaluate this problem, discover when it’s a good model or not. Usually the trade-off is between the expected classification error, while trying to maximize the generalization ability. We don’t have the true expected error, so we use a proxy, the empirical error we can find during the experiments.\nFollowing (Vapnik 2006) we define a learning problem a search of a $f(x) \\in \\mathcal{C}$, and we write $f : \\mathcal{X} \\to \\mathcal{Y}$. Commonly $f$ is parameterised by a $\\theta$. Now we want to compute the risk, which is a loss, a way to tell how much our hypothesis is wrong. There are many possible losses, we won’t talk about them here. We call this loss function $Q(Y, f(X))$ We say that the conditional expected risk is $$ R(f, X) = \\int _{\\mathcal{Y}} P(Y \\mid X) Q(Y, f(X)) \\, dY $$ And the total expected risk is $$ \\mathbf{E}_{x}\\left[ R(f, X) \\right] = \\int _{\\mathcal{X}} \\int _{\\mathcal{Y}} P(X, Y) Q(Y, f(X)) \\, dY \\, dX $$ But these values are marginalizations, and often infeasible to compute. But the most difficult part of this formula is estimating the $P(X, Y)$, which we don’t know. We try to estimate this value by splitting the dataset in training and testing data. But how well can we estimate this? How close is the estimate? We want to know the value of $$ \\mathbb{P} \\left( \\lvert \\hat{R}(\\hat{f}, Z^{test}) - \\mathbf{E}_{X} \\left[ R(\\hat{f}, X) \\right] \\rvert \u003e \\varepsilon \\right) = \\,? $$ Where $\\hat{f}$ is the best hypothesis for our training data, and $\\hat{R}$ is the loss for the test data. We need to have some statistics over this value. k-splitting is a good way to have these statistics.\nWhen we want to mathematically characterize this, we need to keep in mind topology, Inner product spaces, Spazi vettoriali, and Spazi di probabilita.\nLog posterior agreement This section is not exam material (but 🟨–), did not understand this part quite well, the takeaway are following:\nRobust bounds on correctness of ML systems can be achieved (noisy bounds) We care about out-of-distribution generalization and fixed entropy of our hypothesis space. There are algorithms to efficiently minimize the cost (aka risk, hamiltonian) of out-of-distribution inputs. We want a way to measure how well similar distributions induce similar hypothesis. (this is sort of a soundness property). Let’s consider this value $$ \\mathbf{E}_{x, x'} \\log \\sum_{c \\in \\mathcal{C}}p(c \\mid x) \\frac{p(c \\mid x')}{p(c)} $$ We know that $x$ and $x'$ are independently sampled from the input distribution. The $p(c)$ is the same as $\\mathbf{E}_{x'} p(c \\mid x')$ (mean) which is a normalizing factor for that probability. Remember this: $$ \\mathbf{E}_{x'} p(c \\mid x') = \\int p(x') p(c \\mid x') \\, dx' = \\int p(x', c) \\, dx' = p(c) $$ It’s just marginalization.\nNow we do some maths, also Jensen’s inequality will be needed: $$ \\mathbf{E}_{x, x'} \\log \\mathbf{E}_{c \\mid x} \\frac{p(c \\mid x')}{p(c)} \\leq -\\mathbf{E}_{x, x'} \\mathbf{E}_{c \\mid x} \\log \\frac{p(c \\mid x')}{p(c)} = -\\mathbf{E}_{x, x'} \\mathbf{E}_{c \\mid x'}\\log p(c \\mid x') + \\mathbf{E}_{c} \\log p(c) $$ Now we can do some interpretation: The first part is out of sample description length of the hypothesis $c$ given $x$ the second one is minus the entropy of $\\mathcal{C}$.\nWe want a way to describe the quality of an hypothesis (stats people call it risk, physics people call it Hamiltonian) is a function $\\mathcal{R} : \\mathcal{X} \\times \\mathcal{C} \\to \\mathbb{R}$.\nThe gibbs distribution is often a good way to do this: $$ p(c \\mid x) = \\frac{\\exp(- \\beta \\mathcal{R}(x, c))}{\\sum_{c'} \\exp(- \\beta \\mathcal{R}(x, c'))} $$ Some times this is written with the free energy parameter (normalization part) $$ p(c \\mid x) = e^{-\\beta (\\mathcal{R}(x, c) - F(x))} $$ Where $F(x) = - \\frac{1}{\\beta} \\log \\sum_{c'}e^{-\\beta\\mathcal{R(x, c'})}$.\nWe can plug this back into the upper bound (so we can try to minimize the upper bound)\n$$ -\\mathbf{E}_{x, x'} \\mathbf{E}_{c \\mid x'} \\left( -\\beta \\mathcal{R}(x', c) + \\beta F(x')\\right) + \\mathbf{E}_{c} \\log p(c) $$ So we may want to do this minimization: $$ \\min_{\\beta, \\mathcal{R}} \\left\\{ \\mathbf{E}_{c} \\mathbf{E}_{x'} \\left( \\beta \\mathcal{R}(x', c) - \\beta E_{x} F(x') \\right) - \\text{ entropy}(c) \\right\\} $$ I have not understood exactly why we are doing this.\nReferences [1] Vapnik “Estimation of Dependences Based on Empirical Data” Springer 2006\n[2] Choi “The Curious Case of Commonsense Intelligence” Daedalus Vol. 151(2), pp. 139–155 2022\n",
  "wordCount" : "2008",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/introduction-to-advanced-machine-learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Introduction to Advanced Machine Learning
    </h1>
    <div class="post-meta">10 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#introduction-to-the-course" aria-label="Introduction to the course">Introduction to the course</a></li>
                <li>
                    <a href="#framework-for-learning-algorithms" aria-label="Framework for learning algorithms">Framework for learning algorithms</a><ul>
                        
                <li>
                    <a href="#some-definitions" aria-label="Some definitions">Some definitions</a><ul>
                        
                <li>
                    <a href="#a-definition-of-data-science" aria-label="A definition of Data Science">A definition of Data Science</a></li>
                <li>
                    <a href="#what-is-data" aria-label="What is Data?">What is Data?</a></li>
                <li>
                    <a href="#what-are-features" aria-label="What are features?">What are features?</a></li></ul>
                </li>
                <li>
                    <a href="#types-of-data" aria-label="Types of data">Types of data</a><ul>
                        
                <li>
                    <a href="#measurements" aria-label="Measurements">Measurements</a></li>
                <li>
                    <a href="#four-types-of-data" aria-label="Four types of data">Four types of data</a></li></ul>
                </li>
                <li>
                    <a href="#scales" aria-label="Scales">Scales</a><ul>
                        
                <li>
                    <a href="#nominal-or-categorical-scales" aria-label="Nominal or Categorical scales">Nominal or Categorical scales</a></li>
                <li>
                    <a href="#quantitative-scales" aria-label="Quantitative scales">Quantitative scales</a></li></ul>
                </li>
                <li>
                    <a href="#desiderata-for-data-science" aria-label="Desiderata for Data Science">Desiderata for Data Science</a><ul>
                        
                <li>
                    <a href="#hypothesis-consistency" aria-label="Hypothesis consistency">Hypothesis consistency</a></li>
                <li>
                    <a href="#the-classical-problem" aria-label="The classical problem">The classical problem</a></li>
                <li>
                    <a href="#log-posterior-agreement" aria-label="Log posterior agreement">Log posterior agreement</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction-to-the-course">Introduction to the course<a hidden class="anchor" aria-hidden="true" href="#introduction-to-the-course">#</a></h2>
<p>Machine learning gives a new way of thinking about reality: instead of trying to capture a part of reality, as most of the old sciences, go to the meta-level and try to produce an automated method to capture reality.
This first lesson will be more a lesson of philosophy. There a <strong>paradigm shift</strong> in the sense of Thomas Kuhn&rsquo;s scientific revolutions.</p>
<p>An interesting observation is that in the last 100 years, we have had more progress in the field of computer science than the 5000 years of computer science. This is because we have more data, we have more scientists, we are developing with the Kurzweil&rsquo;s exponential growth.
When comparing brain and computers, one have more creativity, the other has more storage capacity. This gives a trade-off.</p>
<p>In principle, <em>epistemology</em> tells you how to extract knowledge from data, from this point of view it can be seen as the precursor of modern information retrieval systems. This tells you something about methods of derivation. The first of which is <strong>deduction</strong> (Euclid, Hilbert&rsquo;s grundlagen der geometrie, David Mumford for bayesian reasoning and image processing, but this person is still living).
Deduction allows us to create facts by starting with the <em>core principles</em> of our thinking.
Later <strong>induction</strong> was born (philosophically created by Bacon), where you create theory from data, and it&rsquo;s a little bit more machine learning like. The former goes by logic, the latter by intuition (but could be wrong, and it&rsquo;s not formalized), now modelled by statistics.
These two processes of reasoning can be modelled as <em>computational processes</em>. And this allows us to create nice machines that are able to achieve these :D.</p>
<p>We use digital data because we can <strong>control it</strong>:</p>
<ul>
<li>Efficient to manage</li>
<li>Discover models</li>
<li>test and simulate hypotheses</li>
<li>Find solutions
The last three points can perhaps be called <em>intelligence</em>.</li>
</ul>
<p>For Joachim M. Buhmann (the prof.), it&rsquo;s impossible to understand the models, and this can be motivated to the limited storage capacity of our brains, so the attempt of interpretability (i.e. correctly understanding exactly what is happening, the law) is just a bias in the history of science for us humans.
While if we understand interpretability as a way to <em>summarize</em> useful information, visualizing it, then it is ok.</p>
<p>Classically we considered algorithms as a way to process data, efficiently, now we want to treat them as a relation between data and decisions. At the end, the decisions are what is important for us humans. Algorithm&rsquo;s makes it easier for us.
Classically we have studied a concept of complexity of runtime, memory, or energy, but we have not have a clear theory of abstraction or robustness. In our case, we have a probability distribution going in, and another going out. But modelling them as random variables, it&rsquo;s difficult to have a clear definition of correctness.</p>
<p>We can now divide algorithms in three macro categories, based on human-expertise</p>
<ol>
<li>Classical algos: humans are able to solve this problems, and are also able to specify those kinds of problems in a logical way</li>
<li>Supervised algos: humans are able to solve these kinds of problems, but don&rsquo;t know how to formalize their logic.</li>
<li>Unsupervised and self-play: humans are not able to solve these problem good enough, this gave rise to alpha fold, alpha go and stuff similar to this.</li>
</ol>
<p>For Prof. Buhmann a fundamental ability of intelligence is being able to do <strong>counterfactual reasoning</strong> and <strong>planning</strong>. So being able to simulate the future, and be able to plan the today&rsquo;s action in order to change the future. This is similar to <a href="https://direct.mit.edu/daed/article/151/2/139/110627/The-Curious-Case-of-Commonsense-Intelligence">(Choi 2022)</a> and <strong>abductive reasoning</strong> by Choi.</p>
<blockquote>
<p>Every law is a social experiment on a not understood population</p>
</blockquote>
<p>When Buhmann tried to say that nobody exactly understands everything, but a correct level of abstraction is often enough (i.e. chemical processes of the combustion engine)</p>
<blockquote>
<p>Studying algorithms is the most complicated part of mathematics when mathematics becomes concrete.
<em>29 September 2023 Joachim M. Buhmann, minute 5.44 in ETHz AML lesson</em></p>
</blockquote>
<p>He said that Kolmogorov Complexity is Shannon&rsquo;s Theory for Random variables?? What does it even mean?</p>
<blockquote>
<p>Noise is your friend because it prevents you to make a statement so precise that you cannot compute it</p>
<p>Noise is an indicator that you <strong>can&rsquo;t</strong> solve your job.</p>
</blockquote>
<h2 id="framework-for-learning-algorithms">Framework for learning algorithms<a hidden class="anchor" aria-hidden="true" href="#framework-for-learning-algorithms">#</a></h2>
<p>We want to have a theory used to <em>validate</em> learning algorithms $\mathcal{A}$, which admits stochastic data as input.</p>
<p>We define the <strong>data</strong> to be observations of some experiment of some genre. Mathematically we write
$e \in \varepsilon \to \mathcal{X}$ , $e \to \mathcal{X}(e)$. Meaning: we have an experiment that contributes to the data $\mathcal{X}$.
We define a hypothesis class $\mathcal{C}$ where possible hypothesis are located. We use these hypothesis to <em>interpret</em> the dataset $\mathcal{X}$.</p>
<h3 id="some-definitions">Some definitions<a hidden class="anchor" aria-hidden="true" href="#some-definitions">#</a></h3>
<h4 id="a-definition-of-data-science">A definition of Data Science<a hidden class="anchor" aria-hidden="true" href="#a-definition-of-data-science">#</a></h4>
<p>We say that data science studies the algorithms $\mathcal{A}$ that map the data $\mathcal{X}$ to the space of possible hypothesis $\mathcal{C}$.</p>
<img src="/images/notes/Introduction to Advanced Machine Learning-20240809131138774.webp" alt="Introduction to Advanced Machine Learning-20240809131138774">
<p>Statistics usually studies the distribution of $\mathcal{X}$ data science studies the mappings. Physics and Mathematics, and other sciences study the space of possible hypothesis.</p>
<h4 id="what-is-data">What is Data?<a hidden class="anchor" aria-hidden="true" href="#what-is-data">#</a></h4>
<blockquote>
<p><em>Encyclopedia Britannica</em>: Association of numbers with physical quantities and natural phenomena by comparing an unknown quantity with a known quantity of the same kind.</p>
</blockquote>
<p>Measurements by sensors, factual information, numbers. It&rsquo;s not very clear, I would say any numbers that can help you do some interesting inferences in some world (it has to affect some entities (i.e. humans))</p>
<h4 id="what-are-features">What are features?<a hidden class="anchor" aria-hidden="true" href="#what-are-features">#</a></h4>
<p>But we can&rsquo;t use this data directly so we use <strong>features</strong> of data, that could be arbitrary transformations of the initial data (edges, corners, etc&hellip;)</p>
<h3 id="types-of-data">Types of data<a hidden class="anchor" aria-hidden="true" href="#types-of-data">#</a></h3>
<h4 id="measurements">Measurements<a hidden class="anchor" aria-hidden="true" href="#measurements">#</a></h4>
<p>We say that we have a world of $R$ objects, and we do observations about these objects.
We say $X$ is a measurement (so a data point) where we have a function $X : \mathcal{O}^{(1)} \times \dots \times \mathcal{O}^{(R)} \to \mathbb{K}$</p>
<h4 id="four-types-of-data">Four types of data<a hidden class="anchor" aria-hidden="true" href="#four-types-of-data">#</a></h4>
<p>Feature vectors: $X: \mathcal{O} \to \mathbb{R}^{d}$
Categorical data $X : \mathcal{O} \to \mathbb{R}^{d} \times \left\{ 1, \dots, k \right\}$
Regression data $X : \mathcal{O} \to \mathbb{R}^{d} \times \mathbb{R}$
Proximity data $X : \mathcal{O} \times \mathcal{O} \to \mathbb{R}$</p>
<p>This division should be pretty intuitive. See the slides for some real examples.
<img src="/images/notes/Introduction to Advanced Machine Learning-20240810121935711.webp" alt="Introduction to Advanced Machine Learning-20240810121935711"></p>
<h3 id="scales">Scales<a hidden class="anchor" aria-hidden="true" href="#scales">#</a></h3>
<h4 id="nominal-or-categorical-scales">Nominal or Categorical scales<a hidden class="anchor" aria-hidden="true" href="#nominal-or-categorical-scales">#</a></h4>
<p>Example: binary $\mathcal{X} = \left\{ 0, 1 \right\}$, or some nominal categories (sweet, sour etc)</p>
<h4 id="quantitative-scales">Quantitative scales<a hidden class="anchor" aria-hidden="true" href="#quantitative-scales">#</a></h4>
<p>We can divide these scales as <strong>interval scales</strong> for fahrenheit temperature scale. The important information is between the values, so scale and translation invariant.
<strong>Ratio scale</strong>: the information is the difference with a focal point, for example Kelvin temperature scale
<strong>Absolute scales</strong> where we the important is absolute thing (grade scale ahah).</p>
<p>Buhmann believes humans are only good in relative scale judgement, when they try to do absolute scaling, they can bias negatively or positively.</p>
<h3 id="desiderata-for-data-science">Desiderata for Data Science<a hidden class="anchor" aria-hidden="true" href="#desiderata-for-data-science">#</a></h3>
<p>In this section we would like to describe what exactly is a good data science algorithm.</p>
<h4 id="hypothesis-consistency">Hypothesis consistency<a hidden class="anchor" aria-hidden="true" href="#hypothesis-consistency">#</a></h4>
<p>The most important requirement is:
If $x$ and $x'$ are drawn from the same distribution, then $\mathbb{P}^{\mathcal{A}}(c \mid x) \approx \mathbb{P}^{\mathcal{A}}(c\mid x')$ meaning the probability of the hypothesis should be similar.</p>
<p>We can call this <em>control experiment</em> and interpret $x$ as training data and $x'$ as test data in this setting.
But we need to take some care about this statement:</p>
<ol>
<li>We want similar inputs to have similar hypothesis</li>
<li>We want dissimilar inputs to have dissimilar hypothesis
If we only had requirement 1, then always outputting the same thing could solve our problem, but more variability is more useful, so we also consider dissimilar hypothesis :).</li>
</ol>
<h4 id="the-classical-problem">The classical problem<a hidden class="anchor" aria-hidden="true" href="#the-classical-problem">#</a></h4>
<p>When starting to model a problem, we Computer Scientists that have to work on data have to decide the format of the data that we want to use. We want a way to <em>evaluate</em> this problem, discover when it&rsquo;s a good model or not. Usually the trade-off is between the <em>expected classification error</em>, while trying to maximize the <em>generalization</em> ability. We don&rsquo;t have the true expected error, so we use a proxy, the <strong>empirical error</strong> we can find during the experiments.</p>
<p>Following <a href="http://link.springer.com/10.1007/0-387-34239-7">(Vapnik 2006)</a> we define a learning problem a <strong>search</strong> of a $f(x) \in \mathcal{C}$, and we write $f : \mathcal{X} \to \mathcal{Y}$. Commonly $f$ is parameterised by a $\theta$.
Now we want to compute the risk, which is a loss, a way to tell how much our hypothesis is wrong. There are many possible losses, we won&rsquo;t talk about them here. We call this loss function $Q(Y, f(X))$
We say that the conditional expected risk is
</p>
$$
R(f, X) = \int _{\mathcal{Y}} P(Y \mid X) Q(Y, f(X)) \, dY
$$
<p>And the total expected risk is
</p>
$$
\mathbf{E}_{x}\left[ R(f, X) \right] = \int _{\mathcal{X}} \int _{\mathcal{Y}} P(X, Y) Q(Y, f(X)) \, dY  \, dX 
$$
<p>But these values are marginalizations, and often infeasible to compute. But the most difficult part of this formula is estimating the $P(X, Y)$, which we don&rsquo;t know. We try to estimate this value by splitting the dataset in training and testing data. But how well can we estimate this? How close is the estimate? We want to know the value of
</p>
$$
\mathbb{P} \left(  \lvert \hat{R}(\hat{f}, Z^{test}) - \mathbf{E}_{X} \left[ R(\hat{f}, X) \right]  \rvert  > \varepsilon \right)  = \,?
$$
<p>
Where $\hat{f}$ is the best hypothesis for our training data, and $\hat{R}$ is the loss for the test data.
We need to have some statistics over this value. k-splitting is a good way to have these statistics.</p>
<p>When we want to mathematically characterize this, we need to keep in mind <a href="/notes/introduction-to-topology/">topology</a>, <a href="/notes/inner-product-spaces/">Inner product spaces</a>, <a href="/notes/spazi-vettoriali/">Spazi vettoriali</a>, and <a href="/notes/spazi-di-probabilita/">Spazi di probabilita</a>.</p>
<h4 id="log-posterior-agreement">Log posterior agreement<a hidden class="anchor" aria-hidden="true" href="#log-posterior-agreement">#</a></h4>
<p>This section is <strong>not</strong> exam material (but 🟨&ndash;), did not understand this part quite well, the takeaway are following:</p>
<ol>
<li>Robust bounds on correctness of ML systems can be achieved (noisy bounds)</li>
<li>We care about out-of-distribution generalization and fixed entropy of our hypothesis space.</li>
<li>There are algorithms to efficiently minimize the cost (aka risk, hamiltonian) of out-of-distribution inputs.</li>
<li>We want a way to measure how well similar distributions induce similar hypothesis. (this is sort of a soundness property).</li>
</ol>
<p>Let&rsquo;s consider this value
</p>
$$
\mathbf{E}_{x, x'} \log \sum_{c \in \mathcal{C}}p(c \mid x) \frac{p(c \mid x')}{p(c)}
$$
<p>
We know that $x$ and $x'$ are independently sampled from the input distribution.
The $p(c)$ is the same as $\mathbf{E}_{x'} p(c \mid x')$ (mean) which is a normalizing factor for that probability.
Remember this:
</p>
$$
\mathbf{E}_{x'} p(c \mid x') = \int p(x') p(c \mid x') \, dx'  = \int p(x', c) \, dx' = p(c)
$$
<p>
It&rsquo;s just marginalization.</p>
<p>Now we do some maths, also <a href="/notes/analisi-di-convessit%C3%A0/#jensen">Jensen&rsquo;s inequality</a> will be needed:
</p>
$$
\mathbf{E}_{x, x'} \log \mathbf{E}_{c \mid x} \frac{p(c \mid x')}{p(c)} \leq -\mathbf{E}_{x, x'} \mathbf{E}_{c \mid x} \log \frac{p(c \mid x')}{p(c)} 
=  -\mathbf{E}_{x, x'} \mathbf{E}_{c \mid x'}\log p(c \mid x') + \mathbf{E}_{c} \log p(c)
$$
<p>
Now we can do some interpretation:
The first part is <em>out of sample description length</em> of the hypothesis $c$ given $x$ the second one is minus the entropy of $\mathcal{C}$.</p>
<p>We want a way to describe the quality of an hypothesis (stats people call it <em>risk</em>, physics people call it Hamiltonian) is a function $\mathcal{R} : \mathcal{X} \times \mathcal{C} \to \mathbb{R}$.</p>
<p>The gibbs distribution is often a good way to do this:
</p>
$$
p(c \mid x) = \frac{\exp(- \beta \mathcal{R}(x, c))}{\sum_{c'} \exp(- \beta \mathcal{R}(x, c'))}
$$
<p>Some times this is written with the free energy parameter (normalization part)
</p>
$$
p(c \mid x) = e^{-\beta (\mathcal{R}(x, c) - F(x))}
$$
<p>
Where $F(x) = - \frac{1}{\beta} \log \sum_{c'}e^{-\beta\mathcal{R(x, c'})}$.</p>
<p>We can plug this back into the upper bound (so we can try to minimize the upper bound)</p>
$$
-\mathbf{E}_{x, x'} \mathbf{E}_{c \mid x'} \left( -\beta \mathcal{R}(x', c) + \beta F(x')\right)  + \mathbf{E}_{c} \log p(c) 
$$
<p>
So we may want to do this minimization:
</p>
$$
\min_{\beta, \mathcal{R}} \left\{  \mathbf{E}_{c} \mathbf{E}_{x'} \left( \beta \mathcal{R}(x', c) - \beta E_{x} F(x') \right)  - \text{ entropy}(c) \right\} 
$$
<p>
I have not understood exactly why we are doing this.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Vapnik <a href="http://link.springer.com/10.1007/0-387-34239-7">“Estimation of Dependences Based on Empirical Data”</a> Springer 2006</p>
<p>[2] Choi <a href="https://direct.mit.edu/daed/article/151/2/139/110627/The-Curious-Case-of-Commonsense-Intelligence">“The Curious Case of Commonsense Intelligence”</a> Daedalus Vol. 151(2), pp. 139&ndash;155 2022</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Advanced Machine Learning on x"
            href="https://x.com/intent/tweet/?text=Introduction%20to%20Advanced%20Machine%20Learning&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fintroduction-to-advanced-machine-learning%2f&amp;hashtags=machinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Advanced Machine Learning on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fintroduction-to-advanced-machine-learning%2f&amp;title=Introduction%20to%20Advanced%20Machine%20Learning&amp;summary=Introduction%20to%20Advanced%20Machine%20Learning&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fintroduction-to-advanced-machine-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Advanced Machine Learning on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fintroduction-to-advanced-machine-learning%2f&title=Introduction%20to%20Advanced%20Machine%20Learning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Advanced Machine Learning on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fintroduction-to-advanced-machine-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Advanced Machine Learning on whatsapp"
            href="https://api.whatsapp.com/send?text=Introduction%20to%20Advanced%20Machine%20Learning%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fintroduction-to-advanced-machine-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Advanced Machine Learning on telegram"
            href="https://telegram.me/share/url?text=Introduction%20to%20Advanced%20Machine%20Learning&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fintroduction-to-advanced-machine-learning%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Advanced Machine Learning on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Introduction%20to%20Advanced%20Machine%20Learning&u=https%3a%2f%2fflecart.github.io%2fnotes%2fintroduction-to-advanced-machine-learning%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
