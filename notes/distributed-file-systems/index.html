<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Distributed file systems | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="üììbig-data">
<meta name="description" content="We want to know how to handle systems that have a large number of data. In previous lesson we have discovered how to quickly access and make Scalable systems with huge dimensions, see Cloud Storage. Object storage could store billions of files, we want to handle millions of petabyte files.
Desiderata of distributed file systems In this case we have a Filesystem. In 2004 google created his own FS. With hundreds or thousands of machines the systems are practically guaranteed to fail.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/distributed-file-systems/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/distributed-file-systems/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Distributed file systems" />
<meta property="og:description" content="We want to know how to handle systems that have a large number of data. In previous lesson we have discovered how to quickly access and make Scalable systems with huge dimensions, see Cloud Storage. Object storage could store billions of files, we want to handle millions of petabyte files.
Desiderata of distributed file systems In this case we have a Filesystem. In 2004 google created his own FS. With hundreds or thousands of machines the systems are practically guaranteed to fail." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/distributed-file-systems/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Distributed file systems"/>
<meta name="twitter:description" content="We want to know how to handle systems that have a large number of data. In previous lesson we have discovered how to quickly access and make Scalable systems with huge dimensions, see Cloud Storage. Object storage could store billions of files, we want to handle millions of petabyte files.
Desiderata of distributed file systems In this case we have a Filesystem. In 2004 google created his own FS. With hundreds or thousands of machines the systems are practically guaranteed to fail."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Distributed file systems",
      "item": "https://flecart.github.io/notes/distributed-file-systems/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Distributed file systems",
  "name": "Distributed file systems",
  "description": "We want to know how to handle systems that have a large number of data. In previous lesson we have discovered how to quickly access and make Scalable systems with huge dimensions, see Cloud Storage. Object storage could store billions of files, we want to handle millions of petabyte files.\nDesiderata of distributed file systems In this case we have a Filesystem. In 2004 google created his own FS. With hundreds or thousands of machines the systems are practically guaranteed to fail.",
  "keywords": [
    "üììbig-data"
  ],
  "articleBody": "We want to know how to handle systems that have a large number of data. In previous lesson we have discovered how to quickly access and make Scalable systems with huge dimensions, see Cloud Storage. Object storage could store billions of files, we want to handle millions of petabyte files.\nDesiderata of distributed file systems In this case we have a Filesystem. In 2004 google created his own FS. With hundreds or thousands of machines the systems are practically guaranteed to fail. This leads to a requirement of fault tolerance. We need to detect these errors so we need monitoring and error detection and we would like the system to be self-regulatory and have the ability of automatic recovery\nEfficiency in operations We want to be efficient in\nScanning the file Appending information to file atomically So that we can have a temporary place for intermediate data or sensors or logs. This needs to be very efficient for appends. Immutability is ok in this case. Latency should not be the bottleneck (with plenty of small files we are losing a lot of files to lookup) but throughput should be the bottleneck. This is why the Distributed File System that we will be studying is optimized for high-throughput data processing rather than low-latency access to small files.\nHadoop Primarily:\n‚Ä¢ Distributed File System (HDFS)\n‚Ä¢ MapReduce\n‚Ä¢ Wide column store (HBase)\nStarting form 188 in 2006 when the filesystem has been developed, now it should handle more about 100k files, about 600PB of data!\nWe will start first by looking at the logical model and then to the physical model, following Codd‚Äôs data independence idea (Codd 1970).\nThe logical model In this case we have a file hierarchy. And we have block storage (also called chunk, split, shard, partition) instead of unstructured object storage we studied in Cloud Storage#Object Stores, which are seen as a single big object. These documents have a far larger block size, usually of 64-128MB. We have 64 for Google and 128 for Hadoop. In Hadoop, blocks are files, so we don‚Äôt have fragmentation on the filesystem. This is because on a network cable having many files takes more time than a single bigger file, because of the latency.\nThe architecture üü© With HDFS we have a centralized architecture where we have a main node, called Coordinator, Primary, Leader, NameNode and then secondary nodes called Worker, Secondary, Follower, DataNodes.\nWhen we have a file, we split it into chunks and store it multiple times (3) into different machines. RAID is redundant in this case, so we won‚Äôt use it, see Devices OS#RAID for more information.\nThe physical model Structure of the NameNode Memory Mapping of file -\u003e Block. Tracking block locations More in particular:\nthe file namespace, that is, the hierarchy of directory names and file names, as well as any access control (ACL) information similar to Unix-based systems. a mapping from each file to the list of its blocks. Each block, in this list, is represented with a 64-bit identifier; the content of the blocks is not on the NameNode. Blocks are local files. a mapping from each block, represented with its 64-bit identifier, to the locations of its replicas, that is, the list of the DataNodes that store a copy of this block. The NameNode never initiates connection with the DataNode. It can just answer to DataNodes‚Äô heartbeats.\nStructure of the DataNode One important fact that the DataNode has to handle are the heartbeats. These are small communications that the DataNode sends to the NameNode to make known that everything is all right (peculiar observation is that also human communication frequency has something related to this), this heartbeat could sometimes also contain information about storage of blocks, like the ACKs for successful storage. It can also contain notification of block corruptions. This is usually done once every 3 seconds but can be configured. Along side this pattern, the DataNode also sends every six hours (configurable) a full report of every block that it contains.\nAnother responsibility of the DataNode is the replication pipeline. When it receives a file, it should replicate it to the other nodes.\nThe file systems operations Read The client asks the NameNode for the blocks and DataNodes. The NameNode answers with the list of each block of the file and the DataNodes that store it ordered from distance to the client. Then the client asks the DataNodes for the files, which is usually a stream of bytes. It downloads every block in turn. Write This operations is a little bit more complex and needs many machines to be organized with each other:\nThe client asks the NameNode for each block a series of DataNodes with which he can initialize connection. Then client instructs the given DataNodes to create the replication pipeline Then client sends the block and waits for acks by the DataNodes This is done for every block of the file, when it‚Äôs finished the client tells the NameNode to release the lock. After a bit of time the NameNode should receive information about correct storage by the DataNodes, and checks if every replica is ok, and gives ACK to the client. Then if one gets corrupted the NameNode automatically regulates the replicas Replication strategy Blocks are replicated with some general guidelines in mind. There is first a notion of distance from nodes that is defined by a physical connection distance: if the two nodes are in the same rack then the distance is 2, if they are in different racks then the distance id 4. There is a general guideline that each node should have at maximum a single replica, and each rack should have at maximum two replicas.\nAddressing the single point of failure If the NameNode crashes, then everything could be lost, the whole cluster would be useless because we don‚Äôt have a way to recover the mappings between the block ids and the files. For this reason a snapshot is usually created. We also need to balance the creation of snapshots and the number of updates. It would be quite infeasible to create a whole new snapshot after every update, yet at the same time we need to track every change to avoid to lose any. This brings up the use of logs for every change. There are also other proposals for merging the logs to the snapshot, so that it would not take half an hour to restart the NameNode in the case of a crash, these are called standby NameNodes that keep track of the current mappings between block ids and files.\nReferences [1] Codd ‚ÄúA Relational Model of Data for Large Shared Data Banks‚Äù Communications of the ACM Vol. 13(6), pp. 377‚Äì387 1970\n",
  "wordCount" : "1120",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/distributed-file-systems/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Distributed file systems
    </h1>
    <div class="post-meta">6 min&nbsp;¬∑&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul><ul>
                <li>
                    <a href="#desiderata-of-distributed-file-systems" aria-label="Desiderata of distributed file systems">Desiderata of distributed file systems</a></li>
                <li>
                    <a href="#efficiency-in-operations" aria-label="Efficiency in operations">Efficiency in operations</a></li></ul>
                    </ul>
                    
                <li>
                    <a href="#hadoop" aria-label="Hadoop">Hadoop</a><ul>
                        
                <li>
                    <a href="#the-logical-model" aria-label="The logical model">The logical model</a><ul>
                        
                <li>
                    <a href="#the-architecture-" aria-label="The architecture üü©">The architecture üü©</a></li></ul>
                </li>
                <li>
                    <a href="#the-physical-model" aria-label="The physical model">The physical model</a><ul>
                        
                <li>
                    <a href="#structure-of-the-namenode" aria-label="Structure of the NameNode">Structure of the NameNode</a></li>
                <li>
                    <a href="#structure-of-the-datanode" aria-label="Structure of the DataNode">Structure of the DataNode</a></li>
                <li>
                    <a href="#the-file-systems-operations" aria-label="The file systems operations">The file systems operations</a><ul>
                        
                <li>
                    <a href="#read" aria-label="Read">Read</a></li>
                <li>
                    <a href="#write" aria-label="Write">Write</a></li></ul>
                </li>
                <li>
                    <a href="#replication-strategy" aria-label="Replication strategy">Replication strategy</a></li>
                <li>
                    <a href="#addressing-the-single-point-of-failure" aria-label="Addressing the single point of failure">Addressing the single point of failure</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>We want to know how to handle systems that have a large number of data. In previous lesson we have discovered how to quickly access and make Scalable systems with huge dimensions, see <a href="/notes/cloud-storage/">Cloud Storage</a>. Object storage could store billions of files, we want to handle millions of petabyte files.</p>
<h4 id="desiderata-of-distributed-file-systems">Desiderata of distributed file systems<a hidden class="anchor" aria-hidden="true" href="#desiderata-of-distributed-file-systems">#</a></h4>
<p>In this case we have a <em><a href="/notes/filesystem/">Filesystem</a></em>. In 2004 google created his own FS. With hundreds or thousands of machines the systems are practically guaranteed to fail. This leads to a requirement of <strong>fault tolerance</strong>. We need to detect these errors so we need <strong>monitoring and error detection</strong> and we would like the system to be self-regulatory and have the ability of <strong>automatic recovery</strong></p>
<h4 id="efficiency-in-operations">Efficiency in operations<a hidden class="anchor" aria-hidden="true" href="#efficiency-in-operations">#</a></h4>
<p>We want to be efficient in</p>
<ul>
<li>Scanning the file</li>
<li>Appending information to file <strong>atomically</strong>
So that we can have a temporary place for intermediate data or sensors or logs. This needs to be very efficient for appends. Immutability is ok in this case.</li>
</ul>
<p>Latency should not be the bottleneck (with plenty of small files we are losing a lot of files to lookup) but throughput should be the bottleneck.
This is why the Distributed File System that we will be studying is optimized for high-throughput data processing rather than low-latency access to small files.</p>
<h2 id="hadoop">Hadoop<a hidden class="anchor" aria-hidden="true" href="#hadoop">#</a></h2>
<p>Primarily:<br>
‚Ä¢ Distributed File System (HDFS)<br>
‚Ä¢ MapReduce<br>
‚Ä¢ Wide column store (HBase)</p>
<p>Starting form 188 in 2006 when the filesystem has been developed, now it should handle more about 100k files, about 600PB of data!</p>
<p>We will start first by looking at the logical model and then to the physical model, following Codd&rsquo;s data independence idea <a href="https://dl.acm.org/doi/10.1145/362384.362685">(Codd 1970)</a>.</p>
<h3 id="the-logical-model">The logical model<a hidden class="anchor" aria-hidden="true" href="#the-logical-model">#</a></h3>
<p>In this case we have a <strong>file hierarchy</strong>.
And we have block storage (also called chunk, split, shard, partition) instead of unstructured object storage we studied in <a href="/notes/cloud-storage/#object-stores">Cloud Storage#Object Stores</a>, which are seen as a single big object.
These documents have a far larger block size, usually of <strong>64-128MB</strong>. We have 64 for Google and 128 for Hadoop. In Hadoop, blocks are files, so we don&rsquo;t have fragmentation on the filesystem.
This is because on a network cable having many files takes more time than a single bigger file, because of the <strong>latency.</strong></p>
<h4 id="the-architecture-">The architecture üü©<a hidden class="anchor" aria-hidden="true" href="#the-architecture-">#</a></h4>
<p>With HDFS we have a <strong>centralized architecture</strong> where we have a main node, called Coordinator, Primary, Leader, NameNode and then secondary nodes called Worker, Secondary, Follower, DataNodes.</p>
<p>When we have a file, we split it into chunks and store it multiple times (3) into different machines. RAID is redundant in this case, so we won&rsquo;t use it, see <a href="/notes/devices-os/#raid">Devices OS#RAID</a> for more information.</p>
<h3 id="the-physical-model">The physical model<a hidden class="anchor" aria-hidden="true" href="#the-physical-model">#</a></h3>
<h4 id="structure-of-the-namenode">Structure of the NameNode<a hidden class="anchor" aria-hidden="true" href="#structure-of-the-namenode">#</a></h4>
<ol>
<li>Memory</li>
<li>Mapping of file -&gt; Block.</li>
<li>Tracking block locations</li>
</ol>
<p>More in particular:</p>
<ul>
<li><strong>the file namespace</strong>, that is, the hierarchy of directory names and
file names, as well as any access control (ACL) information similar
to Unix-based systems.</li>
<li><strong>a mapping from each file to the list of its blocks</strong>. Each block, in
this list, is represented with a 64-bit identifier; the content of the
blocks is not on the NameNode. <em>Blocks are local files</em>.</li>
<li>a mapping from each block, represented with its 64-bit identifier,
to the <strong>locations of its replicas</strong>, that is, the list of the DataNodes
that store a copy of this block.</li>
</ul>
<p>The NameNode <strong>never initiates</strong> connection with the DataNode. It can just answer to DataNodes&rsquo; heartbeats.</p>
<h4 id="structure-of-the-datanode">Structure of the DataNode<a hidden class="anchor" aria-hidden="true" href="#structure-of-the-datanode">#</a></h4>
<p>One important fact that the DataNode has to handle are the <strong>heartbeats</strong>. These are small communications that the DataNode sends to the NameNode to make known that everything is all right (peculiar observation is that also human communication frequency has something related to this), this heartbeat could sometimes also contain information about storage of blocks, like the ACKs for successful storage. It can also contain notification of block corruptions.
This is usually done once every <strong>3 seconds</strong> but can be configured.
Along side this pattern, the DataNode also sends every <strong>six hours</strong> (configurable) a full report of every block that it contains.</p>
<p>Another responsibility of the DataNode is the <strong>replication pipeline</strong>. When it receives a file, it should replicate it to the other nodes.</p>
<h4 id="the-file-systems-operations">The file systems operations<a hidden class="anchor" aria-hidden="true" href="#the-file-systems-operations">#</a></h4>
<h5 id="read">Read<a hidden class="anchor" aria-hidden="true" href="#read">#</a></h5>
<ul>
<li>The client asks the NameNode for the blocks and DataNodes.</li>
<li>The NameNode answers with the list of each block of the file and the DataNodes that store it ordered from distance to the client.</li>
<li>Then the client asks the DataNodes for the files, which is usually a stream of bytes. It downloads every block in turn.</li>
</ul>
<h5 id="write">Write<a hidden class="anchor" aria-hidden="true" href="#write">#</a></h5>
<p>This operations is a little bit more complex and needs many machines to be organized with each other:</p>
<ul>
<li>The client asks the NameNode for each block a series of DataNodes with which he can initialize connection.</li>
<li>Then client instructs the given DataNodes to create the replication pipeline</li>
<li>Then client sends the block and waits for acks by the DataNodes</li>
<li>This is done for every block of the file, when it&rsquo;s finished the client tells the NameNode to release the lock.</li>
<li>After a bit of time the NameNode should receive information about correct storage by the DataNodes, and checks if every replica is ok, and gives ACK to the client.</li>
<li>Then if one gets corrupted the NameNode automatically regulates the replicas</li>
</ul>
<h4 id="replication-strategy">Replication strategy<a hidden class="anchor" aria-hidden="true" href="#replication-strategy">#</a></h4>
<p>Blocks are replicated with some general guidelines in mind. There is first a notion of distance from nodes that is defined by a physical connection distance: if the two nodes are in the same rack then the distance is 2, if they are in different racks then the distance id 4.
There is a general guideline that each node should have at maximum a single replica, and each rack should have at maximum two replicas.</p>
<h4 id="addressing-the-single-point-of-failure">Addressing the single point of failure<a hidden class="anchor" aria-hidden="true" href="#addressing-the-single-point-of-failure">#</a></h4>
<p>If the NameNode crashes, then everything could be lost, the whole cluster would be useless because we don&rsquo;t have a way to recover the mappings between the block ids and the files.
For this reason a <strong>snapshot</strong> is usually created. We also need to balance the creation of snapshots and the number of updates. It would be quite infeasible to create a whole new snapshot after every update, yet at the same time we need to track every change to avoid to lose any.
This brings up the use of <strong>logs</strong> for every change.
There are also other proposals for merging the logs to the snapshot, so that it would not take half an hour to restart the NameNode in the case of a crash, these are called <strong>standby NameNodes</strong> that keep track of the current mappings between block ids and files.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Codd <a href="https://dl.acm.org/doi/10.1145/362384.362685">‚ÄúA Relational Model of Data for Large Shared Data Banks‚Äù</a> Communications of the ACM Vol. 13(6), pp. 377&ndash;387 1970</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/big-data/">üììBig-Data</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Distributed file systems on x"
            href="https://x.com/intent/tweet/?text=Distributed%20file%20systems&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fdistributed-file-systems%2f&amp;hashtags=%f0%9f%93%93big-data">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Distributed file systems on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fdistributed-file-systems%2f&amp;title=Distributed%20file%20systems&amp;summary=Distributed%20file%20systems&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fdistributed-file-systems%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Distributed file systems on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fdistributed-file-systems%2f&title=Distributed%20file%20systems">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Distributed file systems on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fdistributed-file-systems%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Distributed file systems on whatsapp"
            href="https://api.whatsapp.com/send?text=Distributed%20file%20systems%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fdistributed-file-systems%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Distributed file systems on telegram"
            href="https://telegram.me/share/url?text=Distributed%20file%20systems&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fdistributed-file-systems%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Distributed file systems on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Distributed%20file%20systems&u=https%3a%2f%2fflecart.github.io%2fnotes%2fdistributed-file-systems%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
