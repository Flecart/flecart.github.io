<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Notes | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="Notes - X. Angelo Huang&#39;s Blog">
<meta name="author" content="
By Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f790d9af969c56c079c1ce2d5972a04486bf3d6144295d5fba319830e1e55a7a.css" integrity="sha256-95DZr5acVsB5wc4tWXKgRIa/PWFEKV1fujGYMOHlWno=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://flecart.github.io/notes/index.xml">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Notes">
  <meta property="og:description" content="Here I write and share about interesting topics I learn.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="website">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Notes">
<meta name="twitter:description" content="Here I write and share about interesting topics I learn.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }
  ]
}
</script>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span class="active">Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header"><div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a></div>
  <h1>
    Notes
    <a href="/notes/index.xml" title="RSS" aria-label="RSS">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
        stroke-linecap="round" stroke-linejoin="round" height="23">
        <path d="M4 11a9 9 0 0 1 9 9" />
        <path d="M4 4a16 16 0 0 1 16 16" />
        <circle cx="5" cy="19" r="1" />
      </svg>
    </a>
  </h1>
</header>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Autoencoders
    </h2>
  </header>
  <div class="entry-content">
    <p>In questa serie di appunti proviamo a descrivere tutto quello che sappiamo al meglio riguardanti gli autoencoders Blog di riferimento Blog secondario che sembra buono
Introduzione agli autoencoders L’idea degli autoencoders è rappresentare la stessa cosa attraverso uno spazio minore, in un certo senso è la compressione con loss. Per cosa intendiamo qualunque tipologia di dato, che può spaziare fra immagini, video, testi, musica e simili. Qualunque cosa che noi possiamo rappresentare in modo digitale possiamo costruirci un autoencoder. Una volta scelta una tipologia di dato, come per gli algoritmi di compressione, valutiamo come buono il modello che riesce a comprimere in modo efficiente e decomprimere in modo fedele rispetto all’originale. Abbiamo quindi un trade-off fra spazio latente, che è lo spazio in cui sono presenti gli elementi compressi, e la qualità della ricostruzione. Possiamo infatti osservare che se spazio latente = spazio originale, loss di ricostruzione = 0 perché basta imparare l’identità. In questo senso si può dire che diventa sensato solo quando lo spazio originale sia minore di qualche fattore rispetto all’originale. Quando si ha questo, abbiamo più difficoltà di ricostruzione, e c’è una leggera perdita in questo senso.
...</p>
  </div>
  <footer class="entry-footer">Reading Time: 9 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang</footer>
  <a class="entry-link" aria-label="post link to Autoencoders" href="https://flecart.github.io/notes/autoencoders/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Autoregressive Modelling
    </h2>
  </header>
  <div class="entry-content">
    <p>On Autoregressivity The main idea of autoregressivity is to use previous prediction to predict the next state.
The Autoregressive property Autoregressive models model a joint distribution of aleatoric variables by assuming a chain rule like decomposition:
$$ p(x) = \prod_{i=1}^{n} p(x_i | x_{1:i-1}) $$ If we assume independence between the variables, we don’t need many variables to model it $2T$, but this assumption is too strong. If we just use a tabular approach, we’ll have a combinatorial explosion: we will have about $2^{T - 1}$ possible states (if we assume the aleatoric variables are binary, and we are creating a table for each intermediate variable).
...</p>
  </div>
  <footer class="entry-footer">Reading Time: 2 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang</footer>
  <a class="entry-link" aria-label="post link to Autoregressive Modelling" href="https://flecart.github.io/notes/autoregressive-modelling/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Backpropagation
    </h2>
  </header>
  <div class="entry-content">
    <p>Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well. An important observation is that this algorithm is linear: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover. See colah’s blog. Karpathy has a nice resource for this topic too! Stanford lecture on backpropagation is another resource.
...</p>
  </div>
  <footer class="entry-footer">Reading Time: 8 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang</footer>
  <a class="entry-link" aria-label="post link to Backpropagation" href="https://flecart.github.io/notes/backpropagation/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">CMMRS 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>A motorcycle talk founded the CMMRS. I am wondering if small applications like these are enough in having actual impact. It does have some impacts on the people there mmm. The seed was eaten, as his metaphor, industry is eating out most of the talent. And academia is still important as a propulsion engine.
I don’t think it is something general, you need something specific for sure. They say its not recruiting informercial and repeat it a lot. But it seems very strange. They say its more christian like opportunity to
...</p>
  </div>
  <footer class="entry-footer">Reading Time: 17 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang</footer>
  <a class="entry-link" aria-label="post link to CMMRS 2025" href="https://flecart.github.io/notes/cmmrs-2025/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Convolutional Neural Network
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction to Convolutional NN Design Goals We want to be invariant to some transformations but also at the same time to be specific to some thing. Convolutional Neural Networks (CNNs) are a class of deep neural networks that are particularly effective for image processing tasks. They are designed to automatically and adaptively learn spatial hierarchies of features from images. Compared to standard Fully connected Neural Networks, they reuse weights, making their number of parameter much fewer.
...</p>
  </div>
  <footer class="entry-footer">Reading Time: 13 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang</footer>
  <a class="entry-link" aria-label="post link to Convolutional Neural Network" href="https://flecart.github.io/notes/convolutional-neural-network/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Egocentric Vision
    </h2>
  </header>
  <div class="entry-content">
    <p>Egocentric vision is a sub-field of computer vision that studies vision understanding from a centered point of view, that typical of animals. One historical thing is MIT 1997 they had to bring around very heavy cameras. Now we have glasses. Other examples of egocentric vision are cars with cameras that see their surrounding, or robots equipped with cameras mimicking human vision. The difference of egocentric vision compared to standard vision techniques is the high variability and instability of the video, and the concept of movement and interactions inside the image. Standard computer vision is disembodied and controlled field of view.
...</p>
  </div>
  <footer class="entry-footer">Reading Time: 7 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang</footer>
  <a class="entry-link" aria-label="post link to Egocentric Vision" href="https://flecart.github.io/notes/egocentric-vision/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Generative Adversarial Networks
    </h2>
  </header>
  <div class="entry-content">
    <p>Generative Adversarial Network has been introduced in 2014 by Ian Goodfellow (at that time they where still gray and white). Now the images have been improved with Diffusion Models, that can be considered the new paradigm. This idea has been considered by Yann LeCun as one of the most important ideas. Nowadays (2025), they are still used for super-resolution and other applications, but it has still some limitations (mainly stability), and now has good competition against other models. The resolution purported by GAN is much higher than VAE (see Autoencoders#Variational Autoencoders). This is a easy plugin to improve the results of other models (VAE, flow, Diffusion). Also ChatGPT has some sort of adversarial learning for example, not explained in the same manner as here.
...</p>
  </div>
  <footer class="entry-footer">Reading Time: 14 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang</footer>
  <a class="entry-link" aria-label="post link to Generative Adversarial Networks" href="https://flecart.github.io/notes/generative-adversarial-networks/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Neural Networks
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction: a neuron I am lazy, so I’m skipping the introduction for this set of notes. Look at Andrew Ng’s Coursera course for this part (here are the notes). Historical paper is (Rosenblatt 1958). One can view a perceptron to be a Log Linear Models with the temperature of the softmax that goes to 0 (so that it is an argmax). Trained with a stochastic gradient descent with a batch of 1 (this is called the perceptron update rule, see The Perceptron Model).
...</p>
  </div>
  <footer class="entry-footer">Reading Time: 8 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang</footer>
  <a class="entry-link" aria-label="post link to Neural Networks" href="https://flecart.github.io/notes/neural-networks/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Normalizing Flows
    </h2>
  </header>
  <div class="entry-content">
    <p>Normalizing flows have both latent space and can produce tractable explicit probability distributions (closer to Autoregressive Modelling, they have tractable distributions, but not a latent space). This means we are able to get the likelihoods of a certain sample.
This approach to modelling a flexible distribution is called a normalizing flow because the transformation of a probability distribution through a sequence of mappings is somewhat analogous to the flow of a fluid. From (Bishop &amp; Bishop 2024)
...</p>
  </div>
  <footer class="entry-footer">Reading Time: 10 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang</footer>
  <a class="entry-link" aria-label="post link to Normalizing Flows" href="https://flecart.github.io/notes/normalizing-flows/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Parametric Human Body Models
    </h2>
  </header>
  <div class="entry-content">
    <p>An historical perspective The origins of motion capture One of the earliest starts of motion capturing is the famous horse in 1878 in motion “video”. This was the start of all the modern cameras. One of the earliest human body motion capture was in military for moving efficiency purposes in 1883. This website has many historical resources on the topic. The problem is still a problem in modern times. If we want to create models to mimic humans, it surely could be nice to understand how humans move and think. This is the general line of though of this line of research.
...</p>
  </div>
  <footer class="entry-footer">Reading Time: 12 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang</footer>
  <a class="entry-link" aria-label="post link to Parametric Human Body Models" href="https://flecart.github.io/notes/parametric-human-body-models/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="https://flecart.github.io/notes/page/4/">
      «&nbsp;Prev&nbsp;4/38
    </a>
    <a class="next" href="https://flecart.github.io/notes/page/6/">Next&nbsp;6/38&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
