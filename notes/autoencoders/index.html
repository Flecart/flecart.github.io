<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Autoencoders | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning, machine-perception">
<meta name="description" content="In questa serie di appunti proviamo a descrivere tutto quello che sappiamo al meglio riguardanti gli autoencoders
Blog di riferimento
Blog secondario che sembra buono
Introduzione agli autoencoders
L&rsquo;idea degli autoencoders è rappresentare la stessa cosa attraverso uno spazio minore, in un certo senso è la compressione con loss. Per cosa intendiamo qualunque tipologia di dato, che può spaziare fra immagini, video, testi, musica e simili. Qualunque cosa che noi possiamo rappresentare in modo digitale possiamo costruirci un autoencoder.
Una volta scelta una tipologia di dato, come per gli algoritmi di compressione, valutiamo come buono il modello che riesce a comprimere in modo efficiente e decomprimere in modo fedele rispetto all&rsquo;originale.
Abbiamo quindi un trade-off fra spazio latente, che è lo spazio in cui sono presenti gli elementi compressi, e la qualità della ricostruzione.
Possiamo infatti osservare che se spazio latente = spazio originale, loss di ricostruzione = 0 perché basta imparare l&rsquo;identità. In questo senso si può dire che diventa sensato solo quando lo spazio originale sia minore di qualche fattore rispetto all&rsquo;originale. Quando si ha questo, abbiamo più difficoltà di ricostruzione, e c&rsquo;è una leggera perdita in questo senso.">
<meta name="author" content="
By Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/autoencoders/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f790d9af969c56c079c1ce2d5972a04486bf3d6144295d5fba319830e1e55a7a.css" integrity="sha256-95DZr5acVsB5wc4tWXKgRIa/PWFEKV1fujGYMOHlWno=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/autoencoders/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/autoencoders/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Autoencoders">
  <meta property="og:description" content="In questa serie di appunti proviamo a descrivere tutto quello che sappiamo al meglio riguardanti gli autoencoders Blog di riferimento Blog secondario che sembra buono
Introduzione agli autoencoders L’idea degli autoencoders è rappresentare la stessa cosa attraverso uno spazio minore, in un certo senso è la compressione con loss. Per cosa intendiamo qualunque tipologia di dato, che può spaziare fra immagini, video, testi, musica e simili. Qualunque cosa che noi possiamo rappresentare in modo digitale possiamo costruirci un autoencoder. Una volta scelta una tipologia di dato, come per gli algoritmi di compressione, valutiamo come buono il modello che riesce a comprimere in modo efficiente e decomprimere in modo fedele rispetto all’originale. Abbiamo quindi un trade-off fra spazio latente, che è lo spazio in cui sono presenti gli elementi compressi, e la qualità della ricostruzione. Possiamo infatti osservare che se spazio latente = spazio originale, loss di ricostruzione = 0 perché basta imparare l’identità. In questo senso si può dire che diventa sensato solo quando lo spazio originale sia minore di qualche fattore rispetto all’originale. Quando si ha questo, abbiamo più difficoltà di ricostruzione, e c’è una leggera perdita in questo senso.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="Machinelearning">
    <meta property="article:tag" content="Machine-Perception">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Autoencoders">
<meta name="twitter:description" content="In questa serie di appunti proviamo a descrivere tutto quello che sappiamo al meglio riguardanti gli autoencoders
Blog di riferimento
Blog secondario che sembra buono
Introduzione agli autoencoders
L&rsquo;idea degli autoencoders è rappresentare la stessa cosa attraverso uno spazio minore, in un certo senso è la compressione con loss. Per cosa intendiamo qualunque tipologia di dato, che può spaziare fra immagini, video, testi, musica e simili. Qualunque cosa che noi possiamo rappresentare in modo digitale possiamo costruirci un autoencoder.
Una volta scelta una tipologia di dato, come per gli algoritmi di compressione, valutiamo come buono il modello che riesce a comprimere in modo efficiente e decomprimere in modo fedele rispetto all&rsquo;originale.
Abbiamo quindi un trade-off fra spazio latente, che è lo spazio in cui sono presenti gli elementi compressi, e la qualità della ricostruzione.
Possiamo infatti osservare che se spazio latente = spazio originale, loss di ricostruzione = 0 perché basta imparare l&rsquo;identità. In questo senso si può dire che diventa sensato solo quando lo spazio originale sia minore di qualche fattore rispetto all&rsquo;originale. Quando si ha questo, abbiamo più difficoltà di ricostruzione, e c&rsquo;è una leggera perdita in questo senso.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Autoencoders",
      "item": "https://flecart.github.io/notes/autoencoders/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Autoencoders",
  "name": "Autoencoders",
  "description": "In questa serie di appunti proviamo a descrivere tutto quello che sappiamo al meglio riguardanti gli autoencoders Blog di riferimento Blog secondario che sembra buono\nIntroduzione agli autoencoders L\u0026rsquo;idea degli autoencoders è rappresentare la stessa cosa attraverso uno spazio minore, in un certo senso è la compressione con loss. Per cosa intendiamo qualunque tipologia di dato, che può spaziare fra immagini, video, testi, musica e simili. Qualunque cosa che noi possiamo rappresentare in modo digitale possiamo costruirci un autoencoder. Una volta scelta una tipologia di dato, come per gli algoritmi di compressione, valutiamo come buono il modello che riesce a comprimere in modo efficiente e decomprimere in modo fedele rispetto all\u0026rsquo;originale. Abbiamo quindi un trade-off fra spazio latente, che è lo spazio in cui sono presenti gli elementi compressi, e la qualità della ricostruzione. Possiamo infatti osservare che se spazio latente = spazio originale, loss di ricostruzione = 0 perché basta imparare l\u0026rsquo;identità. In questo senso si può dire che diventa sensato solo quando lo spazio originale sia minore di qualche fattore rispetto all\u0026rsquo;originale. Quando si ha questo, abbiamo più difficoltà di ricostruzione, e c\u0026rsquo;è una leggera perdita in questo senso.\n",
  "keywords": [
    "machinelearning", "machine-perception"
  ],
  "articleBody": "In questa serie di appunti proviamo a descrivere tutto quello che sappiamo al meglio riguardanti gli autoencoders Blog di riferimento Blog secondario che sembra buono\nIntroduzione agli autoencoders L’idea degli autoencoders è rappresentare la stessa cosa attraverso uno spazio minore, in un certo senso è la compressione con loss. Per cosa intendiamo qualunque tipologia di dato, che può spaziare fra immagini, video, testi, musica e simili. Qualunque cosa che noi possiamo rappresentare in modo digitale possiamo costruirci un autoencoder. Una volta scelta una tipologia di dato, come per gli algoritmi di compressione, valutiamo come buono il modello che riesce a comprimere in modo efficiente e decomprimere in modo fedele rispetto all’originale. Abbiamo quindi un trade-off fra spazio latente, che è lo spazio in cui sono presenti gli elementi compressi, e la qualità della ricostruzione. Possiamo infatti osservare che se spazio latente = spazio originale, loss di ricostruzione = 0 perché basta imparare l’identità. In questo senso si può dire che diventa sensato solo quando lo spazio originale sia minore di qualche fattore rispetto all’originale. Quando si ha questo, abbiamo più difficoltà di ricostruzione, e c’è una leggera perdita in questo senso.\nThree desiderata Informative: it should be possible to reconstruct the original image. Disentangle: features inside the representation space should be cleanly separated. Robust: if the input is similar, then we would like the representation to be close (somewhat similar to the notion of continuity. Representativeness: if we take some points in the encoding, we would like this to have some corresponding value in the original space. (Somewhat similar to completeness) First Ideas This idea was from Lisker 1988. We would like the encoding and the original input space to share more information as possible: if $Z = \\text{enc}(X)$ then we would like to maximize the mutual information between $Z$ and $X$, $\\arg\\max_{\\theta} I(X ; enc_{\\theta}(X))$. This is also called the Infomax principle.\nThe drawback is that this method does not produce disentangled neither robust representations.\n$$ \\frac{1}{n} \\sum_{i \\leq n} \\mathop{\\mathbb{E}}_{Z \\mid x_{i}} \\left[ \\log p(x_{i} \\mid Z) \\right] $$Autoencoders Classical Autoencoders Simple Linear Encoders The simplest form of an autoencoder is a linear encoder, where the encoding function is a linear transformation and the decoding function is the transpose of the encoding function. In this simple linear case, one can prove that the optimal encoding function is the principal component analysis (PCA) of the data. See Principal Component Analysis. So if we have a simple three layer autoencoder deep neural networks, one can say it computes the PCA of the original data. The same cannot be said for multi-linear neural networks.\n$$ E(w) = \\sum_{i = 1}^{N} \\lVert x_{i} - \\hat{x}_{i} \\rVert^{2} $$ Where $\\hat{x} = \\text{decode}(\\text{encode}(x))$ In that case, the weights here are just the subspace that we are projecting to.The difference compared to PCA is that these vectors don’t need to be orthogonal with each other. People have proven that the solution for this optimization objective is exactly the same as the solution for PCA.\nDeep Autoencoders The difference compared to the previous section is that here we add some non-linear activation functions.\nSuch a network effectively performs a nonlinear form of PCA. It has the advantage of not being limited to linear transformations\nSparse Autoencoders The difference compared to other methods is that here we constrain the internal representation is to use a regularizer to encourage a sparse representation, leading to a lower effective dimensionality.\n$$ \\tilde{E}(w) = E(w) + \\lambda \\sum_{k = 1}^{K} \\lVert z_{k} \\rVert $$Denoising Autoencoders In this case, we add some noise to the input and we try to reconstruct the original image. This is a way to make the network more robust to noise. So the difference compared to\nVariational Autoencoders Questi sono Autoencoders con un approccio variazionale, che abbiamo studiato in Variational Inference, ossia invochiamo in aiuto distribuzioni note e conosciute per cercare di approssimare altre distribuzioni molto più complesse, e teniamo questo come distribuzione vera da cui andiamo a prendere.\nIntuizione L’idea sembra avere uno spazio regolarizzato, ossia un $z \\sim \\mathcal{N}(\\mu, \\sigma^{2}I)$ con $\\sigma$ vettore di dimensione spazio latente e $\\mu$ degli offset che rappresentano media. Quindi il decoder parametrizzato secondo $\\theta$ dovrà essere in una forma dipendente da questa. Questo è un prior che segue un approccio di genere generativo.\nInsieme a questo utilizziamo anche un encoder parametrizzato con $\\phi$ che dovrà darci indicazioni su $z$, per esempio media e varianza.\nSecondo Murphy-1, Questo dovrebbe essere molto simile a un lavoro di uno 95, vedi capitolo su VAE in que libro. La formulazione dei VAE sembra molto simile ai Factor Analysis. Che è una caratterizzazione di un certo tipo sia spazio latente che quello normale.\nGeneral framework Quello che andiamo a fare è computare una rappresentazione $p_{\\theta}(z \\mid x)$ dove $z$ è il nostro spazio latente con un certo prior (questo è un posterior), e poi rigenerare con un $p_{\\theta'}(x \\mid z)$ che è la nostra likelihood.\nSetting del problema $$ p(x | z) \\sim \\mathrm{N}(media, varianza) $$ Ossia i samples della parte condizionata nello spazio latente non sono altro che una media e varianza dipendenti solo dalla parte condizionale, mentre $p(z) = N(0, 1)$ multidimensionale (quindi varianza $I$)\nELBO e derivazione Per trattare ELBO, andare a rivedersi le note in Variational Inference. Se assumiamo questo, allora la loss di Kullback-Leibler diventa abbastanza carina, perché infatti abbiamo che\n$$ KL(q_{x}(z), p(z|x)) = E_{x \\sim q_{x} }(\\log(q_{x}(z))) - E_{x \\sim q_{x}}\\left(\\log( \\frac{p(x, z)}{p(x)}) \\right) $$ $$$$ Ora le ultime due si chiamano rispettivamente **evidence** e **ELBO** che sta per Evidence Lower Bound Notiamo che la evidence non dipende da $z$, infatti avremmo che $$ E_{z \\sim q_{x}}(p(x)) = \\int {-\\infty}^{+\\infty} q{x}(z) p(x) , dz = p(x) \\int {-\\infty}^{+\\infty} q{x}(z) dz = p(x) $$ Quindi se vogliamo minimizzare la divergenza, ci basta Massimizzare ELBO nel nostro caso.\nEsplicitazione di ELBO Possiamo lavorare ancora di più su ELBO, provando ad esplicitarne alcuni valori, infatti possiamo considerare\n$$ ELBO = E_{z \\sim q_{x}}\\left( \\log\\left( \\frac{p(x, z)}{q_{x}(z)} \\right) \\right) =E_{z \\sim q_{x}}\\left( \\log\\left( p(x|z) \\right) \\right) + E_{z \\sim q_{x}}\\left( \\log\\left( \\frac{p(z)}{q_{x}(z)} \\right) \\right) $$$$ = E_{z \\sim q_{x}}\\left( \\log\\left( p(x|z) \\right) \\right) - KL(q_{x}(z), p(z)) $$Ossia abbiamo il secondo termine che prova a regolarizzare la distribuzione $q$ trovata, e il primo termine che è un maximum likelihood, simile a quanto trovato per Naïve Bayes nel corso di Asperti, che è spesso chiamato errore di ricostruzione.\nOra l’ultimo passo sarebbe come esplicitare ELBO in modo che possa essere implementato come loss di una net?\nDerivazione della loss per VAE Vedere qui, è calcolosa, ma molto carina, e ti permette di impratichirti con gaussiane multi-variabili.\nAlla fine si avrà come risultato:\n$$ KL(q_{x}(z), p(z)) = -\\frac{1}{2} \\sum_{j=1}^{J}(1 + \\log \\sigma^{2}_{j} - \\mu^{2}_{j} - \\sigma^{2}_{j}) $$ Derivazione di KL per la loss vedere Gaussians.\nPer l’expectation della forma quadratica vedere qui https://statproofbook.github.io/P/mean-qf.html. Allora, sappiamo che $p(z) = \\mathcal{N}(0, \\mathcal{I})$ quindi ha una forma ben nota, dovremo cercare di fare questa piccolissima derivazione.\nTraining di VAE Una volta ben definito la loss di ricostruzione e la loss di regolarizzazione, possiamo procedere con l’addestramento del modello allenando sia l’encoder che il decoder assieme.\nBeta-VAE Beta-VAE allows for more disentangled latent spaces.\nThe disentangling Idea We want to clearly disentangle the latent dimensions of the VAE. Here we explore how individual dimensions control specific features in the output. This is useful for example introduced in (Higgins et al. 2017), going over a direction changes also identity, while we would only like to change face orientation. With this we assume we have conditionally dependent and independent factors in the latent space.\nModification of the Loss Beta-VAE achieves the above objective by modifying the training objective of the VAE. We assume there are some conditionally independent and dependent factors in the $z$ latent variable. We use Lagrange Multipliers to model the current loss: The loss now becomes:\n$$ \\begin{align*} \\max_{\\phi, \\theta}x \\mathop{\\mathbb{E}}_{z \\sim q_{\\phi}( \\cdot \\mid x)} \\left[ \\log p_{\\theta}(x \\mid z) \\right] - \\beta \\cdot (KL(q_{\\phi}(z \\mid x) \\parallel p(z)) - \\delta) \\\\ \\geq \\mathop{\\mathbb{E}}_{z \\sim q_{\\phi}( \\cdot \\mid x)} \\left[ \\log p_{\\theta}(x \\mid z) \\right] - \\beta \\cdot KL(q_{\\phi}(z \\mid x) \\parallel p(z)) \\end{align*} $$ Where $\\delta, \\beta \u003e 0$, this can be rewritten in some sort of ELBO, $\\beta = 1$ we have the same loss of the VAE. We are basically adding a parameter to check how much is the regularization of the KL doing.\nLimitations blurry images generation Probably because of the injected noise of the Gaussians Or weak inference models. They show that beta VAE effectively disentangles all the possible features correctly, while Vanilla VAE are not able to learn disentangled directions. Hierarchical Latent Variable Models $$ q_{\\phi}(z_{1}, \\dots, z_{L} \\mid x) = \\prod_{l=1}^{L} q_{\\phi}(z_{l} \\mid x) $$$$ p_{\\theta}(z_{1}, \\dots, z_{L} \\mid x) = \\prod_{l=1}^{L} p_{\\theta}(z_{l} \\mid z_{l-1}) $$ Which is quite close to some kind of autoregressive model (See Autoregressive Modelling) or n-gram model (see Language Models), and also has some similarities of how Diffusion Models are trained..\nVector Quantized Variational Autoencoders A way to map images into discrete tokens via quantization: see (Oord et al. 2018).\nIn normal autoencoders you just encode the input: $E(x) = z$, while in VAE you produce a distribution $q(z|x)$. In VQ-VAE you encode the input into a discrete latent space, and then you quantize it to a finite set of values (we do similar things when we do vector search, see Optimizations for DNN#Vector Search). Meaning: You try to find the most similar vector in a embeddings database -\u003e discrete latent representations. The codebook is learned with the autoencoder.\nIntuition on VQ-VAE VQ-VAE combines the benefits of:\nAutoencoders: for dimensionality reduction and feature learning. Discrete latent spaces: to better model data like text, images, or audio. Vector quantization: to replace the continuous latent vectors with discrete ones. This enables to use Transformers in the latent space, which is a nice thing. Image from the paper.\nArchitecture Components Encoder: $x \\rightarrow z_e(x) \\in \\mathbb{R}^D$ This maps the input to a continuous latent space. Vector Quantizer: $z_e(x) \\rightarrow z_q(x) \\in \\mathbb{R}^D$ This replaces the encoder output with the closest vector from a learned codebook of embeddings: $$ z_q(x) = e_k \\text{ where } k = \\arg\\min_j \\| z_e(x) - e_j \\|^2 $$ So you’re snapping the encoder output to its nearest discrete prototype. Decoder: $z_q(x) \\rightarrow \\hat{x}$ Reconstructs the input from the quantized latent. Codebook / Embedding Table: $e \\in \\mathbb{R}^{K \\times D}$ Stores $K$ learnable vectors (i.e., the “vocabulary” of your latent space). Loss Function (Three-Part) $$ \\mathcal{L} = \\underbrace{\\| x - \\hat{x} \\|^2}_{\\text{Reconstruction}} + \\underbrace{\\| \\text{sg}[z_e(x)] - e \\|^2}_{\\text{Codebook Loss}} + \\underbrace{\\beta \\| z_e(x) - \\text{sg}[e] \\|^2}_{\\text{Commitment Loss}} $$ This is equation (3) of the paper. Where:\nReconstruction Loss: makes the output match the input. Codebook Loss: moves the selected codebook entry closer to encoder output. Commitment Loss: encourages encoder to commit to a codebook vector. Note: sg means “stop gradient”, i.e., the gradient is not backpropagated through this path. This is how they handle the non-differentiability of the quantization step.\nFeature VAE VQ-VAE Latent Space Continuous Discrete (via codebook) Prior Gaussian Typically learned separately Sampling Easy via reparameterization Uses nearest neighbor (non-differentiable) Training KL divergence + recon Vector quantization + recon Expressivity Limited by Gaussian prior Much richer References [1] Oord et al. “Neural Discrete Representation Learning” arXiv preprint arXiv:1711.00937 2018 [2] Higgins et al. “Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework” International Conference on Learning Representations 2017 ",
  "wordCount" : "1908",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/autoencoders/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Autoencoders
    </h1>
    <div class="post-meta">Reading Time: 9 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#introduzione-agli-autoencoders" aria-label="Introduzione agli autoencoders">Introduzione agli autoencoders</a><ul>
                        
                <li>
                    <a href="#three-desiderata" aria-label="Three desiderata">Three desiderata</a></li>
                <li>
                    <a href="#first-ideas" aria-label="First Ideas">First Ideas</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#autoencoders" aria-label="Autoencoders">Autoencoders</a><ul>
                        
                <li>
                    <a href="#classical-autoencoders" aria-label="Classical Autoencoders">Classical Autoencoders</a><ul>
                        
                <li>
                    <a href="#simple-linear-encoders" aria-label="Simple Linear Encoders">Simple Linear Encoders</a></li>
                <li>
                    <a href="#deep-autoencoders" aria-label="Deep Autoencoders">Deep Autoencoders</a></li>
                <li>
                    <a href="#sparse-autoencoders" aria-label="Sparse Autoencoders">Sparse Autoencoders</a></li>
                <li>
                    <a href="#denoising-autoencoders" aria-label="Denoising Autoencoders">Denoising Autoencoders</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#variational-autoencoders" aria-label="Variational Autoencoders">Variational Autoencoders</a><ul>
                        
                <li>
                    <a href="#intuizione" aria-label="Intuizione">Intuizione</a><ul>
                        
                <li>
                    <a href="#general-framework" aria-label="General framework">General framework</a></li></ul>
                </li>
                <li>
                    <a href="#setting-del-problema" aria-label="Setting del problema">Setting del problema</a></li>
                <li>
                    <a href="#elbo-e-derivazione" aria-label="ELBO e derivazione">ELBO e derivazione</a><ul>
                        
                <li>
                    <a href="#esplicitazione-di-elbo" aria-label="Esplicitazione di ELBO">Esplicitazione di ELBO</a></li>
                <li>
                    <a href="#derivazione-della-loss-per-vae" aria-label="Derivazione della loss per VAE">Derivazione della loss per VAE</a></li>
                <li>
                    <a href="#training-di-vae" aria-label="Training di VAE">Training di VAE</a></li></ul>
                </li>
                <li>
                    <a href="#beta-vae" aria-label="Beta-VAE">Beta-VAE</a><ul>
                        
                <li>
                    <a href="#the-disentangling-idea" aria-label="The disentangling Idea">The disentangling Idea</a></li>
                <li>
                    <a href="#modification-of-the-loss" aria-label="Modification of the Loss">Modification of the Loss</a></li>
                <li>
                    <a href="#limitations" aria-label="Limitations">Limitations</a></li></ul>
                </li>
                <li>
                    <a href="#hierarchical-latent-variable-models" aria-label="Hierarchical Latent Variable Models">Hierarchical Latent Variable Models</a></li>
                <li>
                    <a href="#vector-quantized-variational-autoencoders" aria-label="Vector Quantized Variational Autoencoders">Vector Quantized Variational Autoencoders</a><ul>
                        
                <li>
                    <a href="#intuition-on-vq-vae" aria-label="Intuition on VQ-VAE">Intuition on VQ-VAE</a></li>
                <li>
                    <a href="#architecture-components" aria-label="Architecture Components">Architecture Components</a></li>
                <li>
                    <a href="#loss-function-three-part" aria-label="Loss Function (Three-Part)">Loss Function (Three-Part)</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In questa serie di appunti proviamo a descrivere tutto quello che sappiamo al meglio riguardanti gli autoencoders
<a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">Blog di riferimento</a>
<a href="https://mbernste.github.io/posts/vae/">Blog secondario che sembra buono</a></p>
<h3 id="introduzione-agli-autoencoders">Introduzione agli autoencoders<a hidden class="anchor" aria-hidden="true" href="#introduzione-agli-autoencoders">#</a></h3>
<p>L&rsquo;idea degli autoencoders è rappresentare la stessa cosa attraverso uno spazio minore, in un certo senso è la compressione con loss. Per cosa intendiamo qualunque tipologia di dato, che può spaziare fra immagini, video, testi, musica e simili. Qualunque cosa che noi possiamo rappresentare in modo digitale possiamo costruirci un autoencoder.
Una volta scelta una tipologia di dato, come per gli algoritmi di compressione, valutiamo come buono il modello che riesce a comprimere in modo efficiente e decomprimere in modo fedele rispetto all&rsquo;originale.
Abbiamo quindi un trade-off fra spazio latente, che è lo spazio in cui sono presenti gli elementi compressi, e la qualità della ricostruzione.
Possiamo infatti osservare che se <strong>spazio latente = spazio originale, loss di ricostruzione = 0</strong> perché basta imparare l&rsquo;identità. In questo senso si può dire che diventa sensato solo quando lo spazio originale sia minore di qualche fattore rispetto all&rsquo;originale. Quando si ha questo, abbiamo più difficoltà di ricostruzione, e c&rsquo;è una leggera perdita in questo senso.</p>
<h4 id="three-desiderata">Three desiderata<a hidden class="anchor" aria-hidden="true" href="#three-desiderata">#</a></h4>
<ul>
<li><strong>Informative</strong>: it should be possible to reconstruct the original image.</li>
<li><strong>Disentangle</strong>: features inside the representation space should be cleanly separated.</li>
<li><strong>Robust</strong>: if the input is similar, then we would like the representation to be close (<em>somewhat similar to the notion of continuity</em>.</li>
<li><strong>Representativeness</strong>: if we take some points in the encoding, we would like this to have some corresponding value in the original space. (Somewhat similar to <em>completeness</em>)</li>
</ul>
<h4 id="first-ideas">First Ideas<a hidden class="anchor" aria-hidden="true" href="#first-ideas">#</a></h4>
<p>This idea was from Lisker 1988. We would like the encoding and the original input space to share more information as possible:
if $Z = \text{enc}(X)$ then we would like to maximize the mutual information between $Z$ and $X$, $\arg\max_{\theta} I(X ; enc_{\theta}(X))$. This is also called the Infomax principle.</p>
<p>The drawback is that this method does not produce disentangled neither robust representations.</p>
$$
\frac{1}{n} \sum_{i \leq n} \mathop{\mathbb{E}}_{Z \mid x_{i}} \left[ \log p(x_{i} \mid Z) \right] 
$$<h2 id="autoencoders">Autoencoders<a hidden class="anchor" aria-hidden="true" href="#autoencoders">#</a></h2>
<h3 id="classical-autoencoders">Classical Autoencoders<a hidden class="anchor" aria-hidden="true" href="#classical-autoencoders">#</a></h3>
<h4 id="simple-linear-encoders">Simple Linear Encoders<a hidden class="anchor" aria-hidden="true" href="#simple-linear-encoders">#</a></h4>
<p>The simplest form of an autoencoder is a linear encoder, where the encoding function is a linear transformation and the decoding function is the transpose of the encoding function.
In this simple linear case, one can prove that the optimal encoding function is the principal component analysis (PCA) of the data. See Principal Component Analysis.
So if we have a simple three layer autoencoder deep neural networks, one can say it computes the PCA of the original data.
The same cannot be said for multi-linear neural networks.</p>
$$
E(w) = \sum_{i = 1}^{N} \lVert x_{i} - \hat{x}_{i} \rVert^{2}
$$<p>
Where $\hat{x} = \text{decode}(\text{encode}(x))$
In that case, the weights here are just the subspace that we are projecting to.The difference compared to PCA is that these vectors don&rsquo;t need to be orthogonal with each other.
People have proven that the solution for this optimization objective is exactly the same as the solution for PCA.</p>
<h4 id="deep-autoencoders">Deep Autoencoders<a hidden class="anchor" aria-hidden="true" href="#deep-autoencoders">#</a></h4>
<p>The difference compared to the previous section is that here we add some <strong>non-linear</strong> activation functions.</p>
<blockquote>
<p>Such a network effectively performs a nonlinear form of PCA. It has the advantage of not being limited to linear transformations</p></blockquote>
<h4 id="sparse-autoencoders">Sparse Autoencoders<a hidden class="anchor" aria-hidden="true" href="#sparse-autoencoders">#</a></h4>
<p>The difference compared to other methods is that here we constrain the internal representation is to use a <strong>regularizer</strong> to encourage a sparse representation, leading to a lower effective dimensionality.</p>
$$
\tilde{E}(w) = E(w) + \lambda \sum_{k = 1}^{K} \lVert z_{k} \rVert
$$<h4 id="denoising-autoencoders">Denoising Autoencoders<a hidden class="anchor" aria-hidden="true" href="#denoising-autoencoders">#</a></h4>
<p>In this case, we add some noise to the input and we try to reconstruct the original image. This is a way to make the network more robust to noise.
So the difference compared to</p>
<h2 id="variational-autoencoders">Variational Autoencoders<a hidden class="anchor" aria-hidden="true" href="#variational-autoencoders">#</a></h2>
<p>Questi sono Autoencoders con un approccio variazionale, che abbiamo studiato in <a href="/notes/variational-inference">Variational Inference</a>, ossia invochiamo in aiuto distribuzioni note e conosciute per cercare di approssimare altre distribuzioni molto più complesse, e teniamo questo come distribuzione vera da cui andiamo a prendere.</p>
<h3 id="intuizione">Intuizione<a hidden class="anchor" aria-hidden="true" href="#intuizione">#</a></h3>
<p>L&rsquo;idea sembra avere uno spazio regolarizzato, ossia un $z \sim \mathcal{N}(\mu, \sigma^{2}I)$ con $\sigma$ vettore di dimensione spazio latente e $\mu$ degli offset che rappresentano media.
Quindi il decoder parametrizzato secondo $\theta$ dovrà essere in una forma dipendente da questa. Questo è un <em>prior</em> che segue un approccio di genere generativo.</p>
<p>Insieme a questo utilizziamo anche un encoder parametrizzato con $\phi$ che dovrà darci indicazioni su $z$, per esempio media e varianza.</p>
<p>Secondo Murphy-1, Questo dovrebbe essere molto simile a un lavoro di uno 95, vedi capitolo su VAE in que libro.
La formulazione dei VAE sembra molto simile ai Factor Analysis. Che è una caratterizzazione di un certo tipo sia spazio latente che quello normale.</p>
<h4 id="general-framework">General framework<a hidden class="anchor" aria-hidden="true" href="#general-framework">#</a></h4>
<p>Quello che andiamo a fare è computare una <strong>rappresentazione</strong> $p_{\theta}(z \mid x)$ dove $z$ è il nostro spazio latente con un certo prior (questo è un posterior),  e poi rigenerare con un $p_{\theta'}(x \mid z)$ che è la nostra likelihood.</p>
<h3 id="setting-del-problema">Setting del problema<a hidden class="anchor" aria-hidden="true" href="#setting-del-problema">#</a></h3>
$$
p(x | z) \sim \mathrm{N}(media, varianza)
$$<p>
Ossia i samples della parte condizionata nello spazio latente non sono altro che una media e varianza dipendenti solo dalla parte condizionale, mentre $p(z) = N(0, 1)$ multidimensionale (quindi varianza $I$)</p>
<h3 id="elbo-e-derivazione">ELBO e derivazione<a hidden class="anchor" aria-hidden="true" href="#elbo-e-derivazione">#</a></h3>
<p>Per trattare ELBO, andare a rivedersi le note in <a href="/notes/variational-inference">Variational Inference</a>.
Se assumiamo questo, allora la loss di Kullback-Leibler diventa abbastanza carina, perché infatti abbiamo che</p>
<h1 id="klq_xz-pzx--e_x-sim-q_x-logq_xz---e_x-sim-q_xleftlog-fracpx-zpx-right">$$
KL(q_{x}(z), p(z|x)) = E_{x \sim q_{x} }(\log(q_{x}(z))) - E_{x \sim q_{x}}\left(\log( \frac{p(x, z)}{p(x)}) \right)</h1>
$$
$$$$
Ora le ultime due si chiamano rispettivamente **evidence** e **ELBO** che sta per Evidence Lower Bound
Notiamo che la evidence non dipende da $z$, infatti avremmo che
$$<p>
E_{z \sim q_{x}}(p(x))  = \int <em>{-\infty}^{+\infty} q</em>{x}(z) p(x) , dz = p(x)  \int <em>{-\infty}^{+\infty} q</em>{x}(z) dz = p(x)
$$
Quindi se vogliamo minimizzare la divergenza, ci basta Massimizzare ELBO nel nostro caso.</p>
<h4 id="esplicitazione-di-elbo">Esplicitazione di ELBO<a hidden class="anchor" aria-hidden="true" href="#esplicitazione-di-elbo">#</a></h4>
<p>Possiamo lavorare ancora di più su ELBO, provando ad esplicitarne alcuni valori, infatti possiamo considerare</p>
$$
ELBO = E_{z \sim q_{x}}\left( \log\left( \frac{p(x, z)}{q_{x}(z)} \right) \right) 
=E_{z \sim q_{x}}\left( \log\left( p(x|z) \right) \right)  + E_{z \sim q_{x}}\left( \log\left( \frac{p(z)}{q_{x}(z)} \right) \right) 
$$$$
= E_{z \sim q_{x}}\left( \log\left( p(x|z) \right) \right)  - KL(q_{x}(z), p(z))
$$<p>Ossia abbiamo il secondo termine che prova a regolarizzare la distribuzione $q$ trovata, e il primo termine che è un maximum likelihood, simile a quanto trovato per <a href="/notes/naïve-bayes">Naïve Bayes</a> nel corso di Asperti, che è spesso chiamato errore di <strong>ricostruzione</strong>.</p>
<p>Ora l&rsquo;ultimo passo sarebbe come esplicitare ELBO in modo che possa essere implementato come loss di una net?</p>
<h4 id="derivazione-della-loss-per-vae">Derivazione della loss per VAE<a hidden class="anchor" aria-hidden="true" href="#derivazione-della-loss-per-vae">#</a></h4>
<p>Vedere <a href="https://mbernste.github.io/posts/vae/#appendix-derivation-of-the-kl-divergence-term-when-the-variational-posterior-and-prior-are-gaussian">qui</a>, è calcolosa, ma molto carina, e ti permette di impratichirti con gaussiane multi-variabili.</p>
<p>Alla fine si avrà come risultato:</p>
$$
KL(q_{x}(z), p(z)) = -\frac{1}{2} \sum_{j=1}^{J}(1 + \log \sigma^{2}_{j} - \mu^{2}_{j} - \sigma^{2}_{j})
$$<p>
<strong>Derivazione di KL</strong> per la loss vedere <a href="/notes/gaussians">Gaussians</a>.</p>
<p>Per l&rsquo;expectation della forma quadratica vedere qui <a href="https://statproofbook.github.io/P/mean-qf.html."><a href="https://statproofbook.github.io/P/mean-qf.html">https://statproofbook.github.io/P/mean-qf.html</a>.</a>
Allora, sappiamo che $p(z) = \mathcal{N}(0, \mathcal{I})$ quindi ha una forma ben nota, dovremo cercare di fare questa piccolissima derivazione.</p>
<h4 id="training-di-vae">Training di VAE<a hidden class="anchor" aria-hidden="true" href="#training-di-vae">#</a></h4>
<p>Una volta ben definito la loss di ricostruzione e la loss di regolarizzazione, possiamo procedere con l&rsquo;addestramento del modello allenando sia l&rsquo;encoder che il decoder assieme.</p>
<h3 id="beta-vae">Beta-VAE<a hidden class="anchor" aria-hidden="true" href="#beta-vae">#</a></h3>
<p>Beta-VAE allows for more disentangled latent spaces.</p>
<h4 id="the-disentangling-idea">The disentangling Idea<a hidden class="anchor" aria-hidden="true" href="#the-disentangling-idea">#</a></h4>
<p>We want to clearly <strong>disentangle</strong> the latent dimensions of the VAE. Here we explore how individual dimensions control specific features in the output.
This is useful for example introduced in <a href="https://openreview.net/forum?id=Sy2fzU9gl">(Higgins et al. 2017)</a>, going over a direction changes also identity, while we would only like to change face orientation.
With this we assume we have <strong>conditionally dependent and independent</strong> factors in the latent space.</p>
<h4 id="modification-of-the-loss">Modification of the Loss<a hidden class="anchor" aria-hidden="true" href="#modification-of-the-loss">#</a></h4>
<p>Beta-VAE achieves the above objective by modifying the training objective of the VAE.
We assume there are some conditionally independent and dependent factors in the $z$ latent variable.
We use <a href="/notes/lagrange-multipliers">Lagrange Multipliers</a> to model the current loss:
The loss now becomes:</p>
$$
\begin{align*}
\max_{\phi, \theta}x \mathop{\mathbb{E}}_{z \sim q_{\phi}( \cdot \mid x)} \left[ \log p_{\theta}(x \mid z) \right] - \beta \cdot (KL(q_{\phi}(z \mid x) \parallel p(z)) - \delta) \\
\geq \mathop{\mathbb{E}}_{z \sim q_{\phi}( \cdot \mid x)} \left[ \log p_{\theta}(x \mid z) \right] - \beta \cdot KL(q_{\phi}(z \mid x) \parallel p(z))
\end{align*}
$$<p>
Where $\delta, \beta > 0$, this can be rewritten in some sort of ELBO, $\beta = 1$ we have the same loss of the VAE. We are basically adding a parameter to check how much is the regularization of the KL doing.</p>
<h4 id="limitations">Limitations<a hidden class="anchor" aria-hidden="true" href="#limitations">#</a></h4>
<ul>
<li>blurry images generation</li>
<li>Probably because of the injected noise of the Gaussians</li>
<li>Or weak inference models.</li>
</ul>
<img src="/images/notes/Autoencoders-20250521150505228.webp" style="width: 100%" class="center" alt="Autoencoders-20250521150505228">
They show that beta VAE effectively disentangles all the possible features correctly, while Vanilla VAE are not able to learn disentangled directions.
<h3 id="hierarchical-latent-variable-models">Hierarchical Latent Variable Models<a hidden class="anchor" aria-hidden="true" href="#hierarchical-latent-variable-models">#</a></h3>
$$
q_{\phi}(z_{1}, \dots, z_{L} \mid x) = \prod_{l=1}^{L} q_{\phi}(z_{l} \mid  x)
$$$$
p_{\theta}(z_{1}, \dots, z_{L} \mid x) = \prod_{l=1}^{L} p_{\theta}(z_{l} \mid z_{l-1})
$$<p>
Which is quite close to some kind of autoregressive model (See <a href="/notes/autoregressive-modelling">Autoregressive Modelling</a>) or n-gram model (see <a href="/notes/language-models">Language Models</a>), and also has some similarities of how <a href="/notes/diffusion-models">Diffusion Models</a> are trained..</p>
<img src="/images/notes/Autoencoders-20250521162549752.webp" style="width: 100%" class="center" alt="Autoencoders-20250521162549752">
<h3 id="vector-quantized-variational-autoencoders">Vector Quantized Variational Autoencoders<a hidden class="anchor" aria-hidden="true" href="#vector-quantized-variational-autoencoders">#</a></h3>
<p>A way to map images into discrete tokens via quantization: see <a href="http://arxiv.org/abs/1711.00937">(Oord et al. 2018)</a>.</p>
<p>In normal autoencoders you just encode the input: $E(x) = z$, while in VAE you produce a distribution $q(z|x)$.
In VQ-VAE you encode the input into a discrete latent space, and then you quantize it to a finite set of values (we do similar things when we do vector search, see <a href="/notes/optimizations-for-dnn#vector-search">Optimizations for DNN#Vector Search</a>).
Meaning: You try to find the <strong>most similar vector</strong> in a embeddings database -&gt; <strong>discrete latent representations</strong>.
The codebook is learned with the autoencoder.</p>
<h4 id="intuition-on-vq-vae">Intuition on VQ-VAE<a hidden class="anchor" aria-hidden="true" href="#intuition-on-vq-vae">#</a></h4>
<p>VQ-VAE combines the benefits of:</p>
<ul>
<li><strong>Autoencoders</strong>: for dimensionality reduction and feature learning.</li>
<li><strong>Discrete latent spaces</strong>: to better model data like text, images, or audio.</li>
<li><strong>Vector quantization</strong>: to replace the continuous latent vectors with discrete ones.
<ul>
<li>This enables to use <a href="/notes/transformers">Transformers</a> in the latent space, which is a nice thing.</li>
</ul>
</li>
</ul>
<figure class="center">
<img src="/images/notes/Autoencoders-20250521163916683.webp" style="width: 100%"   alt="Autoencoders-20250521163916683" title="Autoencoders-20250521163916683"/>
<figcaption><p style="text-align:center;">Image from the paper.</p></figcaption>
</figure>
<h4 id="architecture-components">Architecture Components<a hidden class="anchor" aria-hidden="true" href="#architecture-components">#</a></h4>
<ol>
<li><strong>Encoder</strong>: $x \rightarrow z_e(x) \in \mathbb{R}^D$
This maps the input to a continuous latent space.</li>
<li><strong>Vector Quantizer</strong>: $z_e(x) \rightarrow z_q(x) \in \mathbb{R}^D$
This replaces the encoder output with the closest vector from a learned <strong>codebook</strong> of embeddings:
$$
   z_q(x) = e_k \text{ where } k = \arg\min_j \| z_e(x) - e_j \|^2
   $$
So you&rsquo;re snapping the encoder output to its nearest discrete prototype.</li>
<li><strong>Decoder</strong>: $z_q(x) \rightarrow \hat{x}$
Reconstructs the input from the quantized latent.</li>
<li><strong>Codebook / Embedding Table</strong>: $e \in \mathbb{R}^{K \times D}$
Stores $K$ learnable vectors (i.e., the “vocabulary” of your latent space).</li>
</ol>
<h4 id="loss-function-three-part">Loss Function (Three-Part)<a hidden class="anchor" aria-hidden="true" href="#loss-function-three-part">#</a></h4>
$$
\mathcal{L} = \underbrace{\| x - \hat{x} \|^2}_{\text{Reconstruction}} + \underbrace{\| \text{sg}[z_e(x)] - e \|^2}_{\text{Codebook Loss}} + \underbrace{\beta \| z_e(x) - \text{sg}[e] \|^2}_{\text{Commitment Loss}}
$$<p>
This is equation (3) of the paper.
Where:</p>
<ul>
<li><strong>Reconstruction Loss</strong>: makes the output match the input.</li>
<li><strong>Codebook Loss</strong>: moves the selected codebook entry closer to encoder output.</li>
<li><strong>Commitment Loss</strong>: encourages encoder to commit to a codebook vector.</li>
</ul>
<p><strong>Note</strong>: <code>sg</code> means &ldquo;stop gradient&rdquo;, i.e., the gradient is not backpropagated through this path. This is how they handle the non-differentiability of the quantization step.</p>
<table>
  <thead>
      <tr>
          <th>Feature</th>
          <th>VAE</th>
          <th>VQ-VAE</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Latent Space</td>
          <td>Continuous</td>
          <td>Discrete (via codebook)</td>
      </tr>
      <tr>
          <td>Prior</td>
          <td>Gaussian</td>
          <td>Typically learned separately</td>
      </tr>
      <tr>
          <td>Sampling</td>
          <td>Easy via reparameterization</td>
          <td>Uses nearest neighbor (non-differentiable)</td>
      </tr>
      <tr>
          <td>Training</td>
          <td>KL divergence + recon</td>
          <td>Vector quantization + recon</td>
      </tr>
      <tr>
          <td>Expressivity</td>
          <td>Limited by Gaussian prior</td>
          <td>Much richer</td>
      </tr>
  </tbody>
</table>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=oordNeuralDiscreteRepresentation2018>[1] Oord et al. <a href="http://arxiv.org/abs/1711.00937">“Neural Discrete Representation Learning”</a> arXiv preprint arXiv:1711.00937 2018
 </p>
<p id=higginsBetaVAELearningBasic2017>[2] Higgins et al. <a href="https://openreview.net/forum?id=Sy2fzU9gl">“Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework”</a> International Conference on Learning Representations  2017
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
      <li><a href="https://flecart.github.io/tags/machine-perception/">Machine-Perception</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Autoencoders on x"
            href="https://x.com/intent/tweet/?text=Autoencoders&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fautoencoders%2f&amp;hashtags=machinelearning%2cmachine-perception">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Autoencoders on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fautoencoders%2f&amp;title=Autoencoders&amp;summary=Autoencoders&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fautoencoders%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Autoencoders on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fautoencoders%2f&title=Autoencoders">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Autoencoders on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fautoencoders%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Autoencoders on whatsapp"
            href="https://api.whatsapp.com/send?text=Autoencoders%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fautoencoders%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Autoencoders on telegram"
            href="https://telegram.me/share/url?text=Autoencoders&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fautoencoders%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Autoencoders on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Autoencoders&u=https%3a%2f%2fflecart.github.io%2fnotes%2fautoencoders%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
