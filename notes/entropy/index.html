<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Entropy | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="‚úèinformation-theory">
<meta name="description" content="Questo √® stato creato da 1948 Shannon in (Shannon 1948). Questa nozione √® basata sulla nozione di probabilit√†, perch√© le cose rare sono pi√π informative rispetto a qualcosa che accade spesso.
Introduction to Entropy The Shannon Information Content This is dependent on the notion of the Shannon information content defined as $$ h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/entropy/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/entropy/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Entropy" />
<meta property="og:description" content="Questo √® stato creato da 1948 Shannon in (Shannon 1948). Questa nozione √® basata sulla nozione di probabilit√†, perch√© le cose rare sono pi√π informative rispetto a qualcosa che accade spesso.
Introduction to Entropy The Shannon Information Content This is dependent on the notion of the Shannon information content defined as $$ h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/entropy/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Entropy"/>
<meta name="twitter:description" content="Questo √® stato creato da 1948 Shannon in (Shannon 1948). Questa nozione √® basata sulla nozione di probabilit√†, perch√© le cose rare sono pi√π informative rispetto a qualcosa che accade spesso.
Introduction to Entropy The Shannon Information Content This is dependent on the notion of the Shannon information content defined as $$ h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Entropy",
      "item": "https://flecart.github.io/notes/entropy/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Entropy",
  "name": "Entropy",
  "description": "Questo √® stato creato da 1948 Shannon in (Shannon 1948). Questa nozione √® basata sulla nozione di probabilit√†, perch√© le cose rare sono pi√π informative rispetto a qualcosa che accade spesso.\nIntroduction to Entropy The Shannon Information Content This is dependent on the notion of the Shannon information content defined as $$ h(x = a_{i}) = \\log_{2}\\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.",
  "keywords": [
    "‚úèinformation-theory"
  ],
  "articleBody": "Questo √® stato creato da 1948 Shannon in (Shannon 1948). Questa nozione √® basata sulla nozione di probabilit√†, perch√© le cose rare sono pi√π informative rispetto a qualcosa che accade spesso.\nIntroduction to Entropy The Shannon Information Content This is dependent on the notion of the Shannon information content defined as $$ h(x = a_{i}) = \\log_{2}\\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.\nKolmogorov complexity √® un modo diverso per definire la complessit√†. Legato √® Neural Networks#Kullback-Leibler Divergence.\nWe can model the classical view of entropy as the from [^1]\nExpected value of Surprisal which is the uncertainty of a random variable $X$ taking a certain value which is $p(X = x) = P(x)$, but we want to measure it using log-likelihood.\nWith $\\lvert \\mathcal{X} \\rvert \u003c \\infty$, $P_{X}(\\cdot)$. $$ H(\\mathcal{X}) = - \\sum_{x \\in \\mathcal{X}, P_{\\mathcal{X}}(x) \u003e 0} p(x)\\log(p(x)) \\tag{1.1} $$ Ossia, possiamo dire in modo intuitivo quanto sarebbe sorprendente vedere che si avverasse quell‚Äôevento.\nThis is the graph for the binary case: $$ H(\\mathcal{X}) = p\\log \\frac{1}{p} + (1- p) \\log \\frac{1}{1 - p} $$ Oppure come descritto da [^2] $$ \\begin{align*} H(X) := E[I(X)] \u0026= \\sum_{i=1}^n P(x_i)I(x_i) \\\\ \u0026= \\sum_{i=1}^n p_i \\log(1/p_i) \\\\ \u0026= -\\sum_{i=1}^n p_i \\log(p_i) \\tag{1.2} \\end{align*} $$ Axiomatic Approach to Entropy We can define the entropy as the only function that satisfies the following properties (the function is commonly called surprise):\n$H(X) \\geq 0$ for every discrete r.v. which is related to the code length, which we don‚Äôt want to be negative $H(X, Y) = H(X) + H(Y)$ when $X, Y$ are independent random variables. $H(X) \\text{ is maximal }$ when $X \\sim Unif(0, 1)$ this concept is related to hard to guess the results. Then one can prove that the only function that satisfies these properties is the entropy. Which is: $$ H(X) = - \\sum_{x \\in \\mathcal{X}} p(x) \\log p(x) $$ Properties of the entropy One observation is that labels don‚Äôt matter, we just need the probability vector and don‚Äôt care about what it represents.\nThe entropy is always positive. It‚Äôs easy to prove because all probabilities are $0 \u003c p \\leq 1$ so every term in the sum is positive because log in that interval is also positive.\nAxiomatic approach One could derive the entropy from an axiomatic point of view, with the idea of Surprise We just need three requirements: Given a probability space $\\Omega, \\mathcal{A}, \\mathbb{P}$ , then the surprise of an even $E \\subseteq \\mathcal{A}$ is equal to a function $S([\\mathbb{P}(E)])$ which satisfies the following properties:\n$S(1) = 0$ $S$ is continuous $S$ is monotonic decreasing, meaning that if $p \u003e q$ then $S(p) \u003c S(q)$ $S$ is additive, meaning that $S(pq) = S(p) + S(q)$ when $p, q$ are independent. Then we can prove that the only function satisfying these properties has the form: $$ S(p) = - \\log(p) $$ The entropy is just the expected value of the surprise.\nChain Rule non fare. Una propriet√† random √® $$ H(X, Y) = H(X) + H(X|Y) $$ La dimostrazione √® abbastanza banale una volta che si conoscono le definizioni‚Ä¶. La cosa interessante √® che si pu√≤ generale per qualunque numero di variabili aleatorie: $$ H(X_{0}, X_{1}, \\dots, X_{i}) = \\sum_{i}H(X_{i}|X_{i-1}\\dots X_{0}) $$ Upper bound $$ H(X) \\leq \\log \\lvert \\mathcal{X} \\rvert $$ Con $\\mathcal{X}$ l‚Äôinsieme immagine della variabile aleatoria discreta $X$. Importante in questo caso che la nostra variabile sia discreta, altrimenti il teorema provvisto in (Cover \u0026 Thomas 2012) 2.6.4 non funziona. Non √® molto banale l‚Äôidea di utilizzare la uniforme per modellare il numero di elementi. e usare la positivit√† di KL per finire l‚Äôupper bound.\nWe can prove the equality by having an uniform distribution. The computation is easy: If $\\mathcal{X}$ is uniform then: $$ P_{X}(x) = \\frac{1}{\\lvert \\mathcal{X} \\rvert } $$ And so we have $$ \\sum P_{X}(x) \\log \\frac{1}{P_{X}(x)} = \\log \\frac{1}{P_{X}(x)} = \\log \\lvert X \\rvert $$ Proving the upper bound Let‚Äôs take $D(P_{X} \\mid \\mid U)$ into consideration, then we have that this value is $$ \\sum P_{X}(x) \\log \\frac{P_{X}(x)}{\\frac{1}{\\lvert \\mathcal{X} \\rvert }} = \\sum P_{X} \\log P(x) + \\sum P_{X}(x) \\log \\lvert \\mathcal{X} \\rvert = \\log \\lvert \\mathcal{X} \\rvert - H(X) $$ And by knowing that the Kullback Leibler divergence is positive we know that $$ \\log \\lvert \\mathcal{X} \\rvert - H(X) \\geq 0 $$ Which ends the proof.\nEntropy is concave Uso l‚Äôupper bound e il fatto che KL √® convesso per dimostrare questa cosa.\nFunctional dependency Non fare Se $Y = f(X)$ per qualche funzione, allora $H(Y|X) = H(X|Y) = 0$ si pu√≤ risolvere con qualche ragionamento sul supporto di entropia. Interessante vedere che ha una piccola relazione con Normalizzazione dei database#Dipendenze funzionali.\nMonotonicity of Entropy This theorem states that $$ H(X) \\geq H(X \\mid Y) $$ It is also called the principle Information never hurts meaning you are never more uncertain about a random variable when you have more information about it.\nIt‚Äôs easy to see why its true! $$ H(X) - H(X \\mid Y) = \\sum P(x) \\log P(x) - \\sum P(x, y) \\log P(x \\mid y) = \\sum P(x, y) \\log \\frac{P(x, y)}{P(x)} \\underbrace{\\geq}_{\\text{Jensen}} 0 $$ This is also the reason why mutual information is always positive.\nTypes of entropy Conditional Entropy $$ H(Y|X) = \\sum_{x \\in \\mathcal{X}}p(x) H(Y|X=x) = \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x, y) \\log \\frac{1}{P(y|x)} = \\mathbf{E}\\left[ \\log\\frac{1}{p(Y|X)} \\right] $$ La nozione con il valore atteso √® la pi√π semplice anche in questo caso.\nconditional entropy corresponds to the expected remaining uncertainty in $Y$ after we observe $X$\nWe can also express conditional entropy as $$ H(Y \\mid X) = \\mathbb{E}_{(x, y) \\sim p(x, y)}\\left[ \\log \\frac{1}{p(y \\mid x)} \\right] $$ Joint Entropy $$ H(X, Y) = - \\sum_{x, y} p(x, y) \\log p(x, y) $$ We observe that there is a relation between Joint entropy and the conditional entropy: $$ H(X, Y) = H(X) + H(Y \\mid X) = H(Y) + H(X \\mid Y) $$ Relative Entropy or Kullback-Leibler Let‚Äôs take $\\lvert \\mathcal{X} \\rvert \u003c \\infty$, and take distributions P, Q, $\\mathcal{X} \\to \\mathbb{R}$ such that $p(x) \\geq 0 \\forall x \\in \\mathcal{X}$ and the sum is 1, same thing for $Q$, then we define the Kullback-Leibler Divergence between those distributions to be\n$$ D(P \\mid \\mid Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)} $$ We need to define some corner cases:\nIf $P(x) = 0$ and $Q$ is anything then its 0 If $\\exists \\xi \\in \\mathcal{X}$ such that $P(\\xi) \u003e 0$ and $Q(\\xi) = 0$ =\u003e $D(P \\mid \\mid Q) = + \\infty$. If $P \\ll Q$ then $D(P \\mid \\mid Q) \u003c \\infty$ (I did not understand why) This has some relations with the entropy, we can use some log properties and have the following result:\n$$ D(P \\mid \\mid Q) = - H(P) - \\sum P(x) \\log Q(x) $$ The second addendum can be called cross-entropy.\nIn modo praticamente equivalente possiamo definire una versione condizionata. e si pu√≤ applicare anche in questo caso una chain rule\n$$ DL(P(x, y) \\mid\\mid Q(x, y) = DL(P(x) \\mid\\mid Q(x)) + DL(P(x|y) \\mid\\mid Q(x|y)) $$ Relative entropy is not a distance not a metric, so its incorrect to say it is a distance, but for practical purposes it seems to work well: if the Relative entropy is small also the probability vectors are small.\nKL is positive or null Ossia per ogni distribuzione $p$ o $q$ si ha che $$ DL(P \\mid\\mid Q) \\geq 0 $$ Con uguaglianza se hanno esattamente la stessa distribuzione. We have that $D(P \\mid \\mid Q) = 0 \\iff P = Q$ .\nE ricordandoci che $\\log$ √® una funzione concava, quindi si pu√≤ utilizzare Jensen. Lo dimostriamo ora in breve. Sappiamo che la funzione $-\\log(x) = \\log\\left( \\frac{1}{x} \\right)$ √® una funzione convessa, perch√© il negativo di una funzione concava, che √® il logaritmo.\nAllora consideriamo $\\frac{1}{u} = \\frac{Q(x)}{P(x)}$ che √® la parte dentro al logaritmo perch√© cos√¨ possiamo usare Jensen Allora comunque abbiamo\n$$ \\sum_{x} P(x) \\log\\left( \\frac{Q(x)}{P(x)} \\right) \\geq \\log\\left( \\sum_{x} P(x) \\cdot \\frac{Q(x)}{P(x)} \\right) = \\log(1) = 0 $$ Che conclude che $$ D_{KL}(P \\mid \\mid Q) \\geq 0 $$ In modo facile.\nKL is convex $DL(p\\mid\\mid q)$ √® convesso sulla coppia $(p, q)$, 2.7.2 di (Cover \u0026 Thomas 2012). Anche sula 2.26 di McKay √® buono, anche se non esattamente parla di questo.\nMutual information Questa nozione definisce quanta informazione hanno in comune due variabili aleatorie\nDefinizione $$ I(X;Y) = \\sum_{x}\\sum_{y} p(x, y) \\log\\left( \\frac{p(x, y)}{p(x)p(y)} \\right) = H(X) - H(X|Y) $$ Si pu√≤ fare dopo un po‚Äô di calcoli che qui ho omesso, ma non dovrebbe essere difficile farlo.888\nSi pu√≤ intendere la mutual information anche come KL fra le distribuzioni $p(x, y)$ e $p(x)p(y)$ si pu√≤ notare che queste due sono uguali quando le due sono indipendenti, che √® coerente con la nostra nozione che abbiamo dell‚Äôindipendenza.\nPropriet√† Another property: $$ I(X; Y\\mid Z) = I(X ; Y, Z) - I(X ; Z) $$ Redundancy and Synergy üü® If we consider the interaction information $$ I(X;Y;Z) = I(X;Y) - I(X;Y\\mid Z) $$ Then synergy between $Y$ and $Z$ is present when the interaction information is negative, and redundant when its positive. This is because, intuitively, if it is negative it means $Z$ can provide more information with $Y$ about $Z$, else, they both have some information about $X$ which is probably not enough.\nExample: Independent Variable in MI We see that if $Y \\mid X \\perp Z$ then $I(X;Z) = I(X, Y; Z)$ this means then that the relation between $Y$ and $Z$ is neither redundant or synergic\n$$ \\begin{align} I(X, Y; Z) \u0026= \\mathbb{E}\\left[ \\log \\frac{P(X, Y, Z)}{P(X, Y) P(Z)} \\right] \\\\ \u0026= \\mathbb{E}\\left[ \\log \\frac{P(X, Z)}{P(X) P(Z)} \\right] + \\mathbb{E}\\left[ \\log \\frac{P(Y \\mid X, Z)}{P(Y \\mid X)} \\right] \\\\ \u0026= I(X; Z) + \\mathbb{E}\\left[ \\log \\frac{P(Y \\mid X, Z)}{P(Y \\mid X)} \\right] \\\\ \u0026= I(X; Z) \\end{align} $$ Where in the last step we used the fact that $Y \\perp Z \\mid X \\implies P(Y \\mid X, Z) = P(Y \\mid X)$.\nThis means that if the independence condition is satified, then we can add random variables as we like without changing the mutual information.\nSufficient Statistics Possiamo rappresentare il sampling da una certa famiglia di distribuzioni $f_{\\theta}(x)$ , rappresentato da $X$, e una sua statistica a caso (media varianza etc, che credo basti una funzione sul valore) come T, allora possiamo rappresentarlo come una Markov Chains#Catena di 3 variabili $\\theta \\to X \\to T(X)$ E vale il teorema di information processing\n$$ I(\\theta; T(X)) \\leq I(\\theta; X) $$ Si pu√≤ chiamare una statistica per $\\theta$ sufficiente se $X$ contiene tutta l‚Äôinformazione di $\\theta$. Non so bene cosa significhi. La cosa importante √® che la statistica sufficiente preserva la mutua informazione ossia si ha una uguaglianza in quella relazione di sopra. Vedere 2.9 di (Cover \u0026 Thomas 2012) per esempi .\nQuesta cosa potrebbe permettere di dire che usando quella statistica io posso dimenticarmi del parametro, perch√© riesco a ricavarmelo senza problemi credo‚Ä¶.\nThe purpose of sufficiency is to demonstrate that statistics that satisfy this property do not discard information about the parameter, and as such, estimators that might be based on a sufficient statistic are in a sense ‚Äúgood‚Äù ones to choose.\nDa https://math.stackexchange.com/questions/1186645/understanding-sufficient-statistic. Sufficient statistics also have a quite nice relationship with The Exponential Family.\nFano‚Äôs inequality L\"idea principale √® utilizzare una variabile aleatoria per stimarne una altra, usando l‚Äôentropia condizionale fra le due.\nEnunciato fano Per ogni estimantore $\\hat{X}$ tale per cui $X \\to Y \\to \\hat{X}$ sia una catena di markov, e con $P_{e} = Pr(X \\not= \\hat{X})$ ossia la probabilit√† di errore, abbiamo che vale $$ H(P_{e}) + P_{e}\\log \\lvert \\mathcal{X} \\rvert \\geq H(X|\\hat{X}) \\geq H(X|Y) $$ Ci sono forme pi√π deboli che possiamo considerare in un certo senso corollari, ossia che $$ 1 + P_{e} \\log \\lvert \\mathcal{X} \\rvert \\geq H(X|Y) $$ Dimostrazione Fano Questa √® una bomba da fare. Poi per√≤ ha un sacco di conseguenze non applicabili in modo immediato (cio√® non ci arrivi subito se non le fai un po‚Äô prima).\nMaximum Distribution entropy Un problema classico nella teoria dell‚Äôinformazione √® trovare la distribuzione che massimizzi l‚Äôentropia (quindi l‚Äôinformazione contenuta credo) dati certe conoscenze a priori, Ossia data una funzione $f$ e certe condizioni che deve rispettare, massimizzare l‚Äôentropia.\nSI pu√≤ dimostrare (lo si pu√≤ vedere da una reference di sopra) che la distribuzione che massimizza l‚Äôentropia, avendo solamente la condizione di probabilit√†, ossia che $\\sum_{x}p(x) = 1$ √® la distribuzione uniforme. Mentre se assumo anche media $\\mu$ e varianza $\\sigma^{2}$ allora √® la gaussiana (dimostrato in Maximum Entropy Principle. In un certo senso possiamo dire che queste distribuzioni sono molto ricche di informazioni.\nCodewords Jensen‚Äôs Inequality Questo √® un teorema fondamentale per moltissime cose, e da un certo punto di vista √® una cosa banale per le cose convesse/concave. Allora, sia data una funzione in $[a, b]$ tale che sia convessa (concava) in questo intervallo, allora vale che $$ f\\left( \\sum_{i} \\lambda_{i} x_{i} \\right) \\leq \\sum_{i}\\lambda_{i}f(x_{i}) $$ Con $\\sum_{i}\\lambda_{i} = 1$. Questa cosa si estende in modo molto semplice a variabili aleatorie e $E$ quando al posto di $\\lambda_{i}$ mettiamo una probabilit√† in un punto.\nLa dimostrazione non dovrebbe essere molto difficile. La strategia √® utilizzare l‚Äôinduzione in modo abbastanza classico. Non so in che modo si estende su funzioni continue, ma quelle sono cose tecniche matematiche non interessantissime.\nLog sum inequality Siano $a_{1}, a_{2}, \\dots a_{n}$ e $b_{1}, b_{2}, \\dots, b_{n}$ numeri non negativi, allora vale che $$ \\sum_{i=1}^{n}a_{i} \\log\\left( \\frac{a_{i}}{b_{i}} \\right) \\geq \\left( \\sum_{i=1}^{n}a_{i} \\right)\\log \\frac{\\left( \\sum_{i=1}^{n} a_{i} \\right)}{\\sum_{i=1}^{n}b_{i}} $$ Con uguaglianza se vale che $\\forall i, \\frac{a_{i}}{b_{i}}= const$\nKrafts Inequality https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality Questo teorema interessa cose dei codewords, perch√© ci interessano dei set di prefixfree che sono molto pi√π gestibili probabilmente dal punto di vista dell‚Äôinterpretazione. La cosa interessante √®:\nSiano $l_{1}, l_{2}, l_{3}, \\dots, l_{n}$ lunghezze di code-words all‚Äôinterno del nostro alfabeto, allora vale che esistono dei code-words (stringhe binarie) che hanno quelle lunghezze se e solo se viene soddisfatta la propriet√† $$ \\sum_{x} 2^{-l(x)} \\leq 1 $$ Il motivo √® abbastanza semplice, questo si spiega in modo grafico in maniera praticamente immediata quando facciamo il disegno. Si pu√≤ vedere dall‚Äôalbero binario corrispondente di un insieme di set binari con prefissi che se un parente √® scelto (colorato nel disegno), allora nessun discendente pu√≤ essere scelto perch√© altrimenti avresti un prefisso. Inoltre se colori quelli sopra, significa che al massimo se sommi tutti quei valori otterrai 1 sse hai utilizzato tutti i rami a tua disposizione (meaning, che non puoi scegliere altri code-work, altrimenti perdi la prefix property). Source coding theorem for symbol codes Chiamata anche come la relazione con Kolmogorov. 1.11.3 di (Li \u0026 Vit√°nyi 2019), allora se prendiamo un set di code-words con $L$ il minimo prefix code che possiamo mai avere Ossia $L = \\sum_{x} P(x)l(x)$, con $l$ scelto il minimo possibile. Allora vale che $$ H(P) \\leq L \\leq H(P) + 1 $$ Ossia la lunghezza migliore possibile √® boundata da valori di entropia. Che √® una cosa abbastanza forte perch√© relaziona come deve essere fatto il code-words, con la complessit√† dell‚Äôinformazione che vogliamo andare a utilizzare. La dimostrazione non la facciamo qui, ma √® fattibile con le tue conoscenze credo, ti serve la Gibbs inequality qui sotto per una freccia\nDimostrazione Prendiamo $q_{i} = 2^{-l_{i}}$, allora abbiamo $l_{i} = \\log\\left( \\frac{1}{q_{i}} \\right)$ vale $$ L = \\sum_{x} p(x) l(x) = \\sum_{x} \\left( p(x) \\log\\left( \\frac{1}{q(x)} \\right) \\right) \\geq \\sum_{x}p(x) \\log\\left( \\frac{1}{p(x)} \\right) =H(x) $$ Dove abbiamo usato anche l‚Äôineguaglianza di Gibbs #Gibbs Inequality e il fatto che vale #Krafts Inequality.\nProvando a dimostrare l‚Äôaltro bound, supponiamo che $$ l_{i} = \\lceil -\\log_{2}(p_{i}) \\rceil $$ Si pu√≤ dimostrare che questo √® un prefix code, perch√© soddisfa Kraft. Allora si pu√≤ notare come $$ L = \\sum_{x} p(x) l(x) = \\sum_{x} p(x) \\lceil -\\log_{2}(p_{x}) \\rceil \\leq \\sum_{x}p(x) \\left( \\log_{2}\\left( \\frac{1}{p(x)}\\right) +1 \\right) = H(x) + 1 $$ Una nota interessante √® questo teorema ci permette di definire un concetto di efficienza di rappresentazione. Tutto quanto dato da KL divergence √® una specie di inefficienza.\nInfatti possiamo scrivere $$ L = H(x) + D_{KL}(P \\mid \\mid Q) $$ Con $Q$ la probabilit√† associata alle singole codewords, assumendo che siano uniformi e simili per dire.\nGibbs Inequality Afferma che l‚Äôentropia √® minore rispetto alla cross-entropy di qualunque cosa, ossia $$ \\sum_{x} P(x) \\log\\left( \\frac{1}{P(x)} \\right) \\leq \\sum_{x} P(x) \\log\\left( \\frac{1}{Q(x)} \\right) $$ Qualunque sia l‚Äôaltra distribuzione. Si pu√≤ dimostrare in modo abbastanza diretto utilizzando il fatto che la Kullback Leibler divergence, presentato in Neural Networks, √® sempre positiva o uguale a 0. Infatti la parte di sopra si pu√≤ riscrivere come\n$$ -\\sum_{x} P(x) \\log\\left( \\frac{P(x)}{Q(x)} \\right) = D_{KL}(P \\mid \\mid Q) $$ References [1] Cover \u0026 Thomas ‚ÄúElements of Information Theory‚Äù John Wiley \u0026 Sons 2012\n[2] Shannon ‚ÄúA Mathematical Theory of Communication‚Äù The Bell System Technical Journal Vol. 27, pp. 379‚Äì423, 623‚Äì656 1948\n[3] Li \u0026 Vit√°nyi ‚ÄúAn Introduction to Kolmogorov Complexity and Its Applications‚Äù Springer International Publishing 2019\n",
  "wordCount" : "2849",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/entropy/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Entropy
    </h1>
    <div class="post-meta">14 min&nbsp;¬∑&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#introduction-to-entropy" aria-label="Introduction to Entropy">Introduction to Entropy</a><ul>
                        
                <li>
                    <a href="#the-shannon-information-content" aria-label="The Shannon Information Content">The Shannon Information Content</a></li>
                <li>
                    <a href="#axiomatic-approach-to-entropy" aria-label="Axiomatic Approach to Entropy">Axiomatic Approach to Entropy</a></li></ul>
                </li>
                <li>
                    <a href="#properties-of-the-entropy" aria-label="Properties of the entropy">Properties of the entropy</a><ul>
                        
                <li>
                    <a href="#axiomatic-approach" aria-label="Axiomatic approach">Axiomatic approach</a></li>
                <li>
                    <a href="#chain-rule" aria-label="Chain Rule">Chain Rule</a></li>
                <li>
                    <a href="#upper-bound" aria-label="Upper bound">Upper bound</a></li>
                <li>
                    <a href="#entropy-is-concave" aria-label="Entropy is concave">Entropy is concave</a></li>
                <li>
                    <a href="#functional-dependency" aria-label="Functional dependency">Functional dependency</a></li>
                <li>
                    <a href="#monotonicity-of-entropy" aria-label="Monotonicity of Entropy">Monotonicity of Entropy</a></li></ul>
                </li>
                <li>
                    <a href="#types-of-entropy" aria-label="Types of entropy">Types of entropy</a><ul>
                        
                <li>
                    <a href="#conditional-entropy" aria-label="Conditional Entropy">Conditional Entropy</a></li>
                <li>
                    <a href="#joint-entropy" aria-label="Joint Entropy">Joint Entropy</a></li></ul>
                </li>
                <li>
                    <a href="#relative-entropy-or-kullback-leibler" aria-label="Relative Entropy or Kullback-Leibler">Relative Entropy or Kullback-Leibler</a><ul>
                        
                <li>
                    <a href="#kl-is-positive-or-null" aria-label="KL is positive or null">KL is positive or null</a></li>
                <li>
                    <a href="#kl-is-convex" aria-label="KL is convex">KL is convex</a></li></ul>
                </li>
                <li>
                    <a href="#mutual-information" aria-label="Mutual information">Mutual information</a><ul>
                        
                <li>
                    <a href="#definizione" aria-label="Definizione">Definizione</a></li>
                <li>
                    <a href="#propriet%c3%a0" aria-label="Propriet√†">Propriet√†</a></li>
                <li>
                    <a href="#redundancy-and-synergy-" aria-label="Redundancy and Synergy üü®">Redundancy and Synergy üü®</a></li>
                <li>
                    <a href="#example-independent-variable-in-mi" aria-label="Example: Independent Variable in MI">Example: Independent Variable in MI</a></li></ul>
                </li>
                <li>
                    <a href="#sufficient-statistics" aria-label="Sufficient Statistics">Sufficient Statistics</a></li>
                <li>
                    <a href="#fanos-inequality" aria-label="Fano&rsquo;s inequality">Fano&rsquo;s inequality</a><ul>
                        
                <li>
                    <a href="#enunciato-fano" aria-label="Enunciato fano">Enunciato fano</a></li>
                <li>
                    <a href="#dimostrazione-fano" aria-label="Dimostrazione Fano">Dimostrazione Fano</a></li></ul>
                </li>
                <li>
                    <a href="#maximum-distribution-entropy" aria-label="Maximum Distribution entropy">Maximum Distribution entropy</a></li>
                <li>
                    <a href="#codewords" aria-label="Codewords">Codewords</a><ul>
                        
                <li>
                    <a href="#jensens-inequality" aria-label="Jensen&rsquo;s Inequality">Jensen&rsquo;s Inequality</a></li>
                <li>
                    <a href="#log-sum-inequality" aria-label="Log sum inequality">Log sum inequality</a></li>
                <li>
                    <a href="#krafts-inequality" aria-label="Krafts Inequality">Krafts Inequality</a></li>
                <li>
                    <a href="#source-coding-theorem-for-symbol-codes" aria-label="Source coding theorem for symbol codes">Source coding theorem for symbol codes</a></li>
                <li>
                    <a href="#gibbs-inequality" aria-label="Gibbs Inequality">Gibbs Inequality</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Questo √® stato creato da 1948 Shannon in <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">(Shannon 1948)</a>. Questa nozione √® basata sulla nozione di probabilit√†, perch√© le cose rare sono pi√π informative rispetto a qualcosa che accade spesso.</p>
<h3 id="introduction-to-entropy">Introduction to Entropy<a hidden class="anchor" aria-hidden="true" href="#introduction-to-entropy">#</a></h3>
<h4 id="the-shannon-information-content">The Shannon Information Content<a hidden class="anchor" aria-hidden="true" href="#the-shannon-information-content">#</a></h4>
<p>This is dependent on the notion of the <strong>Shannon information content</strong> defined as
</p>
$$
h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})}
$$
<p>
We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.</p>
<p><a href="/notes/kolmogorov-complexity/">Kolmogorov complexity</a> √® un modo diverso per definire la complessit√†.
Legato √® <a href="/notes/neural-networks/#kullback-leibler-divergence">Neural Networks#Kullback-Leibler Divergence</a>.</p>
<p>We can model the classical view of entropy as the from [^1]</p>
<blockquote>
<p>Expected value of <strong>Surprisal</strong> which is the uncertainty of a random variable $X$ taking a certain value which is $p(X = x) = P(x)$, but we want to measure it using log-likelihood.</p>
</blockquote>
<p>With $\lvert \mathcal{X} \rvert < \infty$, $P_{X}(\cdot)$.
</p>
$$
H(\mathcal{X}) = - \sum_{x \in \mathcal{X}, P_{\mathcal{X}}(x) > 0} p(x)\log(p(x)) \tag{1.1}
$$
<p>
Ossia, possiamo dire in modo intuitivo quanto sarebbe sorprendente vedere che si avverasse quell&rsquo;evento.</p>
<img src="/images/notes/Entropy-20240918151806980.webp" alt="Entropy-20240918151806980">
This is the graph for the binary case:
$$
H(\mathcal{X}) = p\log \frac{1}{p} + (1- p) \log \frac{1}{1 - p}
$$
<p>Oppure come descritto da [^2]
</p>
$$
\begin{align*}
H(X) := E[I(X)] &= \sum_{i=1}^n P(x_i)I(x_i) \\
&= \sum_{i=1}^n p_i \log(1/p_i) \\
&= -\sum_{i=1}^n p_i \log(p_i) \tag{1.2}
\end{align*}
$$
<h4 id="axiomatic-approach-to-entropy">Axiomatic Approach to Entropy<a hidden class="anchor" aria-hidden="true" href="#axiomatic-approach-to-entropy">#</a></h4>
<p>We can define the entropy as the only function that satisfies the following properties (the function is commonly called <strong>surprise</strong>):</p>
<ol>
<li>$H(X) \geq 0$ for every <strong>discrete</strong> r.v. which is related to the code length, which we don&rsquo;t want to be negative</li>
<li>$H(X, Y) = H(X) + H(Y)$ when $X, Y$ are independent random variables.</li>
<li>$H(X) \text{ is maximal }$ when $X \sim Unif(0, 1)$ this concept is related to <strong>hard to guess</strong> the results.</li>
</ol>
<p>Then one can prove that the only function that satisfies these properties is the entropy.
Which is:
</p>
$$
H(X) = - \sum_{x \in \mathcal{X}} p(x) \log p(x)
$$
<h3 id="properties-of-the-entropy">Properties of the entropy<a hidden class="anchor" aria-hidden="true" href="#properties-of-the-entropy">#</a></h3>
<p>One observation is that <strong>labels don&rsquo;t matter</strong>, we just need the <em>probability vector</em> and don&rsquo;t care about what it represents.</p>
<p>The entropy is <strong>always positive</strong>. It&rsquo;s easy to prove because all probabilities are $0 < p \leq 1$ so every term in the sum is positive because log in that interval is also positive.</p>
<h4 id="axiomatic-approach">Axiomatic approach<a hidden class="anchor" aria-hidden="true" href="#axiomatic-approach">#</a></h4>
<p>One could derive the entropy from an axiomatic point of view, with the idea of <strong>Surprise</strong>
We just need three requirements:
Given a probability space $\Omega, \mathcal{A}, \mathbb{P}$ , then the surprise of an even $E \subseteq \mathcal{A}$ is equal to a function $S([\mathbb{P}(E)])$ which satisfies the following properties:</p>
<ol>
<li>$S(1) = 0$</li>
<li>$S$ is continuous</li>
<li>$S$ is monotonic decreasing, meaning that if $p > q$ then $S(p) < S(q)$</li>
<li>$S$ is additive, meaning that $S(pq) = S(p) + S(q)$ when $p, q$ are independent.</li>
</ol>
<p>Then we can prove that the only function satisfying these properties has the form:
</p>
$$
S(p) = - \log(p)
$$
<p>
The entropy is just the expected value of the surprise.</p>
<h4 id="chain-rule">Chain Rule<a hidden class="anchor" aria-hidden="true" href="#chain-rule">#</a></h4>
<p>non fare.
Una propriet√† random √®
</p>
$$
H(X, Y) = H(X) + H(X|Y)
$$
<p>
La dimostrazione √® abbastanza banale una volta che si conoscono le definizioni&hellip;.
La cosa interessante √® che si pu√≤ generale per qualunque numero di variabili aleatorie:
</p>
$$
H(X_{0}, X_{1}, \dots, X_{i}) = \sum_{i}H(X_{i}|X_{i-1}\dots X_{0})
$$
<h4 id="upper-bound">Upper bound<a hidden class="anchor" aria-hidden="true" href="#upper-bound">#</a></h4>
$$
H(X) \leq \log \lvert \mathcal{X} \rvert 
$$
<p>
Con $\mathcal{X}$ l&rsquo;insieme immagine della variabile aleatoria <strong>discreta</strong> $X$. Importante in questo caso che la nostra variabile sia discreta, altrimenti il teorema provvisto in <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">(Cover &amp; Thomas 2012)</a> 2.6.4 non funziona.
Non √® molto banale l&rsquo;idea di utilizzare la uniforme per modellare il numero di elementi. e usare la positivit√† di KL per finire l&rsquo;upper bound.</p>
<p>We can prove the equality by having an uniform distribution. The computation is easy:
If $\mathcal{X}$ is uniform then:
</p>
$$
P_{X}(x) = \frac{1}{\lvert \mathcal{X} \rvert }
$$
<p>
And so we have
</p>
$$
\sum P_{X}(x) \log \frac{1}{P_{X}(x)} = \log \frac{1}{P_{X}(x)} = \log \lvert X \rvert 
$$
<p><strong>Proving the upper bound</strong>
Let&rsquo;s take $D(P_{X} \mid \mid U)$ into consideration, then we have that this value is
</p>
$$
\sum P_{X}(x) \log \frac{P_{X}(x)}{\frac{1}{\lvert \mathcal{X} \rvert }}
= \sum P_{X} \log P(x) + \sum P_{X}(x) \log \lvert \mathcal{X} \rvert 
= \log \lvert \mathcal{X} \rvert  - H(X)
$$
<p>
And by knowing that the Kullback Leibler divergence is positive we know that
</p>
$$
\log \lvert \mathcal{X} \rvert  - H(X) \geq 0
$$
<p>
Which ends the proof.</p>
<h4 id="entropy-is-concave">Entropy is concave<a hidden class="anchor" aria-hidden="true" href="#entropy-is-concave">#</a></h4>
<p>Uso l&rsquo;upper bound e il fatto che KL √® convesso per dimostrare questa cosa.</p>
<h4 id="functional-dependency">Functional dependency<a hidden class="anchor" aria-hidden="true" href="#functional-dependency">#</a></h4>
<p>Non fare
Se $Y = f(X)$ per qualche funzione, allora $H(Y|X) = H(X|Y) = 0$ si pu√≤ risolvere con qualche ragionamento sul supporto di entropia.
Interessante vedere che ha una piccola relazione con <a href="/notes/normalizzazione-dei-database/#dipendenze-funzionali">Normalizzazione dei database#Dipendenze funzionali</a>.</p>
<h4 id="monotonicity-of-entropy">Monotonicity of Entropy<a hidden class="anchor" aria-hidden="true" href="#monotonicity-of-entropy">#</a></h4>
<p>This theorem states that
</p>
$$
H(X) \geq H(X \mid Y)
$$
<p>
It is also called the principle <em>Information never hurts</em> meaning you are never more uncertain about a random variable when you have more information about it.</p>
<p>It&rsquo;s easy to see why its true!
</p>
$$
H(X) - H(X \mid Y) = \sum P(x) \log P(x) - \sum P(x, y) \log P(x \mid y) = \sum P(x, y) \log \frac{P(x, y)}{P(x)} \underbrace{\geq}_{\text{Jensen}} 0
$$
<p>
This is also the reason why mutual information is always positive.</p>
<h3 id="types-of-entropy">Types of entropy<a hidden class="anchor" aria-hidden="true" href="#types-of-entropy">#</a></h3>
<h4 id="conditional-entropy">Conditional Entropy<a hidden class="anchor" aria-hidden="true" href="#conditional-entropy">#</a></h4>
$$
H(Y|X) = \sum_{x \in \mathcal{X}}p(x) H(Y|X=x)
= \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log \frac{1}{P(y|x)}
= \mathbf{E}\left[ \log\frac{1}{p(Y|X)} \right] 
$$
<p>
La nozione con il valore atteso √® la pi√π semplice anche in questo caso.</p>
<blockquote>
<p>conditional entropy corresponds to the expected remaining uncertainty in $Y$ after we observe $X$</p>
</blockquote>
<p>We can also express conditional entropy as
</p>
$$
H(Y \mid X) = \mathbb{E}_{(x, y) \sim p(x, y)}\left[ \log \frac{1}{p(y \mid x)} \right]
$$
<h4 id="joint-entropy">Joint Entropy<a hidden class="anchor" aria-hidden="true" href="#joint-entropy">#</a></h4>
$$
H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y)
$$
<p>
We observe that there is a relation between Joint entropy and the conditional entropy:
</p>
$$
H(X, Y) = H(X) + H(Y \mid X) = H(Y) + H(X \mid Y)
$$
<h3 id="relative-entropy-or-kullback-leibler">Relative Entropy or Kullback-Leibler<a hidden class="anchor" aria-hidden="true" href="#relative-entropy-or-kullback-leibler">#</a></h3>
<p>Let&rsquo;s take
$\lvert \mathcal{X} \rvert < \infty$, and take distributions P, Q, $\mathcal{X} \to \mathbb{R}$ such that  $p(x) \geq 0 \forall x \in \mathcal{X}$ and the sum is 1, same thing for $Q$, then we define the Kullback-Leibler Divergence between those distributions to be</p>
$$
D(P \mid \mid Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}
$$
<p>
We need to define some corner cases:</p>
<ul>
<li>If $P(x) = 0$ and $Q$ is anything then its 0</li>
<li>If $\exists \xi \in \mathcal{X}$ such that $P(\xi) > 0$ and $Q(\xi) = 0$ =&gt; $D(P \mid \mid Q) = + \infty$.</li>
<li>If $P \ll Q$ then $D(P \mid \mid Q) < \infty$ (I did not understand why)</li>
</ul>
<p>This has some relations with the entropy, we can use some log properties and have the following result:</p>
$$
D(P \mid \mid Q) = - H(P) - \sum P(x) \log Q(x)
$$
<p>
The second addendum can be called <em>cross-entropy</em>.</p>
<p>In modo praticamente equivalente possiamo definire una versione condizionata. e si pu√≤ applicare anche in questo caso una chain rule</p>
$$
DL(P(x, y) \mid\mid Q(x, y) = DL(P(x) \mid\mid Q(x)) + DL(P(x|y) \mid\mid Q(x|y))
$$
<p>Relative entropy <strong>is not a distance</strong> not a metric, so its incorrect to say it is a distance, but for practical purposes it seems to work well: if the Relative entropy is small also the probability vectors are small.</p>
<h4 id="kl-is-positive-or-null">KL is positive or null<a hidden class="anchor" aria-hidden="true" href="#kl-is-positive-or-null">#</a></h4>
<p>Ossia per ogni distribuzione $p$ o $q$ si ha che
</p>
$$
DL(P \mid\mid Q) \geq 0
$$
<p>
Con uguaglianza se hanno esattamente la stessa distribuzione.
We have that $D(P \mid \mid Q) = 0 \iff P = Q$ .</p>
<p>E ricordandoci che $\log$ √® una funzione concava, quindi si pu√≤ utilizzare <a href="/notes/analisi-di-convessit%C3%A0/#jensen">Jensen</a>.
Lo dimostriamo ora in breve.
Sappiamo che la funzione $-\log(x) = \log\left( \frac{1}{x} \right)$ √® una funzione convessa, perch√© il negativo di una funzione concava, che √®  il logaritmo.</p>
<p>Allora consideriamo $\frac{1}{u} = \frac{Q(x)}{P(x)}$ che √® la parte dentro al logaritmo perch√© cos√¨ possiamo usare Jensen
Allora comunque abbiamo</p>
$$
\sum_{x} P(x) \log\left( \frac{Q(x)}{P(x)} \right) \geq \log\left( \sum_{x} P(x) \cdot \frac{Q(x)}{P(x)} \right) = \log(1) = 0
$$
<p>Che conclude che
</p>
$$
D_{KL}(P \mid \mid Q) \geq 0
$$
<p>
In modo facile.</p>
<h4 id="kl-is-convex">KL is convex<a hidden class="anchor" aria-hidden="true" href="#kl-is-convex">#</a></h4>
<p>$DL(p\mid\mid q)$ √® convesso sulla coppia $(p, q)$, 2.7.2 di <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">(Cover &amp; Thomas 2012)</a>.
Anche sula 2.26 di McKay √® buono, anche se non esattamente parla di questo.</p>
<h3 id="mutual-information">Mutual information<a hidden class="anchor" aria-hidden="true" href="#mutual-information">#</a></h3>
<p>Questa nozione definisce quanta informazione hanno in comune due variabili aleatorie</p>
<h4 id="definizione">Definizione<a hidden class="anchor" aria-hidden="true" href="#definizione">#</a></h4>
$$
I(X;Y) = \sum_{x}\sum_{y} p(x, y) \log\left(  \frac{p(x, y)}{p(x)p(y)} \right)
= H(X) - H(X|Y)
$$
<p>
Si pu√≤ fare dopo un po&rsquo; di calcoli che qui ho omesso, ma non dovrebbe essere difficile farlo.888</p>
<p>Si pu√≤ intendere la mutual information anche come KL fra le distribuzioni $p(x, y)$ e $p(x)p(y)$ si pu√≤ notare che queste due sono uguali quando le due sono indipendenti, che √® coerente con la nostra nozione che abbiamo dell&rsquo;indipendenza.</p>
<h4 id="propriet√†">Propriet√†<a hidden class="anchor" aria-hidden="true" href="#propriet√†">#</a></h4>
<img src="/images/notes/Entropy-20240229150751912.webp" alt="Entropy-20240229150751912">
<img src="/images/notes/Entropy-20240229150807093.webp" alt="Entropy-20240229150807093">
Another property:
$$
I(X; Y\mid Z) = I(X ; Y, Z) - I(X ; Z)
$$
<h4 id="redundancy-and-synergy-">Redundancy and Synergy üü®<a hidden class="anchor" aria-hidden="true" href="#redundancy-and-synergy-">#</a></h4>
<p>If we consider the interaction information
</p>
$$
I(X;Y;Z) = I(X;Y) - I(X;Y\mid Z)
$$
<p>
Then <strong>synergy</strong> between $Y$ and $Z$ is present when the interaction information is negative, and <strong>redundant</strong> when its positive.
This is because, intuitively, if it is negative it means $Z$ can provide more information with $Y$ about $Z$, else, they both have some information about $X$ which is probably not enough.</p>
<h4 id="example-independent-variable-in-mi">Example: Independent Variable in MI<a hidden class="anchor" aria-hidden="true" href="#example-independent-variable-in-mi">#</a></h4>
<p>We see that if $Y \mid X \perp Z$ then $I(X;Z) = I(X, Y; Z)$ this means then that the relation between $Y$ and $Z$ is neither redundant or synergic</p>
$$
\begin{align}
I(X, Y; Z) &= \mathbb{E}\left[ \log \frac{P(X, Y, Z)}{P(X, Y) P(Z)} \right] \\
&= \mathbb{E}\left[ \log \frac{P(X, Z)}{P(X) P(Z)} \right] + \mathbb{E}\left[ \log \frac{P(Y \mid X, Z)}{P(Y \mid X)} \right] \\
&= I(X; Z) + \mathbb{E}\left[ \log \frac{P(Y \mid X, Z)}{P(Y \mid X)} \right] \\
&= I(X; Z)
\end{align}
$$
<p>
Where in the last step we used the fact that $Y \perp Z \mid X \implies P(Y \mid X, Z) = P(Y \mid X)$.</p>
<p>This means that if the independence condition is satified, then we can add random variables as we like without changing the mutual information.</p>
<h3 id="sufficient-statistics">Sufficient Statistics<a hidden class="anchor" aria-hidden="true" href="#sufficient-statistics">#</a></h3>
<p>Possiamo rappresentare il sampling da una certa famiglia di distribuzioni $f_{\theta}(x)$ , rappresentato da $X$, e una sua statistica a caso (media varianza etc, che credo basti una funzione sul valore) come T, allora possiamo rappresentarlo come una <a href="/notes/markov-chains/#catena-di-3-variabili">Markov Chains#Catena di 3 variabili</a> $\theta \to X \to T(X)$
E vale il teorema di information processing</p>
$$
I(\theta; T(X)) \leq I(\theta; X)
$$
<p>
Si pu√≤ chiamare una statistica per $\theta$ sufficiente se $X$ contiene tutta l&rsquo;informazione di $\theta$. Non so bene cosa significhi.
La cosa importante √® che la statistica sufficiente <strong>preserva la mutua informazione</strong> ossia si ha una uguaglianza in quella relazione di sopra. Vedere 2.9 di <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">(Cover &amp; Thomas 2012)</a> per esempi .</p>
<p>Questa cosa potrebbe permettere di dire che usando quella statistica io posso dimenticarmi del parametro, perch√© riesco a ricavarmelo senza problemi credo&hellip;.</p>
<blockquote>
<p>The purpose of sufficiency is to demonstrate that statistics that satisfy this property do not discard information about the parameter, and as such, estimators that might be based on a sufficient statistic are in a sense &ldquo;good&rdquo; ones to choose.</p>
</blockquote>
<p>Da <a href="https://math.stackexchange.com/questions/1186645/understanding-sufficient-statistic.">https://math.stackexchange.com/questions/1186645/understanding-sufficient-statistic.</a>
Sufficient statistics also have a quite nice relationship with <a href="/notes/the-exponential-family/">The Exponential Family</a>.</p>
<h3 id="fanos-inequality">Fano&rsquo;s inequality<a hidden class="anchor" aria-hidden="true" href="#fanos-inequality">#</a></h3>
<p>L&quot;idea principale √® utilizzare una variabile aleatoria per stimarne una altra, usando l&rsquo;entropia condizionale fra le due.</p>
<h4 id="enunciato-fano">Enunciato fano<a hidden class="anchor" aria-hidden="true" href="#enunciato-fano">#</a></h4>
<p>Per ogni estimantore $\hat{X}$ tale per cui $X \to Y \to \hat{X}$ sia una catena di markov, e con $P_{e} = Pr(X \not= \hat{X})$ ossia la probabilit√† di errore, abbiamo che vale
</p>
$$
H(P_{e}) + P_{e}\log \lvert \mathcal{X} \rvert \geq H(X|\hat{X}) \geq H(X|Y)
$$
<p>
Ci sono forme pi√π deboli che possiamo considerare in un certo senso corollari, ossia che
</p>
$$
1 + P_{e} \log \lvert \mathcal{X} \rvert  \geq H(X|Y)
$$
<h4 id="dimostrazione-fano">Dimostrazione Fano<a hidden class="anchor" aria-hidden="true" href="#dimostrazione-fano">#</a></h4>
<p>Questa √® una bomba da fare. Poi per√≤ ha un sacco di conseguenze non applicabili in modo immediato (cio√® non ci arrivi subito se non le fai un po&rsquo; prima).</p>
<h3 id="maximum-distribution-entropy">Maximum Distribution entropy<a hidden class="anchor" aria-hidden="true" href="#maximum-distribution-entropy">#</a></h3>
<p>Un problema classico nella teoria dell&rsquo;informazione √® trovare la distribuzione che massimizzi l&rsquo;entropia (quindi l&rsquo;informazione contenuta credo) dati certe conoscenze a priori,
Ossia data una funzione $f$  e certe condizioni che deve rispettare, massimizzare l&rsquo;entropia.</p>
<p>SI pu√≤ dimostrare (lo si pu√≤ vedere da una reference di sopra) che la distribuzione che massimizza l&rsquo;entropia, avendo solamente la condizione di probabilit√†, ossia che $\sum_{x}p(x) = 1$ √® la distribuzione uniforme.
Mentre se assumo anche media $\mu$ e varianza $\sigma^{2}$ allora √® la gaussiana (dimostrato in <a href="/notes/maximum-entropy-principle/">Maximum Entropy Principle</a>.
In un certo senso possiamo dire che queste distribuzioni sono molto ricche di informazioni.</p>
<h3 id="codewords">Codewords<a hidden class="anchor" aria-hidden="true" href="#codewords">#</a></h3>
<h4 id="jensens-inequality">Jensen&rsquo;s Inequality<a hidden class="anchor" aria-hidden="true" href="#jensens-inequality">#</a></h4>
<p>Questo √® un teorema fondamentale per moltissime cose, e da un certo punto di vista √® una cosa banale per le cose convesse/concave.
Allora, sia data una funzione in $[a, b]$ tale che sia convessa (concava) in questo intervallo, allora vale che
</p>
$$
f\left( \sum_{i} \lambda_{i} x_{i} \right) \leq \sum_{i}\lambda_{i}f(x_{i})
$$
<p>
Con $\sum_{i}\lambda_{i} = 1$. Questa cosa si estende in modo molto semplice a variabili aleatorie e $E$ quando al posto di $\lambda_{i}$ mettiamo una probabilit√† in un punto.</p>
<p>La dimostrazione non dovrebbe essere molto difficile. La strategia √® utilizzare l&rsquo;induzione in modo abbastanza classico. Non so in che modo si estende su funzioni continue, ma quelle sono cose tecniche matematiche non interessantissime.</p>
<h4 id="log-sum-inequality">Log sum inequality<a hidden class="anchor" aria-hidden="true" href="#log-sum-inequality">#</a></h4>
<p>Siano $a_{1}, a_{2}, \dots a_{n}$ e $b_{1}, b_{2}, \dots, b_{n}$ numeri non negativi, allora vale che
</p>
$$
\sum_{i=1}^{n}a_{i} \log\left( \frac{a_{i}}{b_{i}} \right) \geq \left( \sum_{i=1}^{n}a_{i} \right)\log \frac{\left( \sum_{i=1}^{n} a_{i} \right)}{\sum_{i=1}^{n}b_{i}}
$$
<p>
Con uguaglianza se vale che $\forall i, \frac{a_{i}}{b_{i}}= const$</p>
<h4 id="krafts-inequality">Krafts Inequality<a hidden class="anchor" aria-hidden="true" href="#krafts-inequality">#</a></h4>
<p><a href="https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality">https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality</a>
Questo teorema interessa cose dei codewords, perch√© ci interessano dei <strong>set</strong> di prefixfree che sono molto pi√π gestibili probabilmente dal punto di vista dell&rsquo;interpretazione.
La cosa interessante √®:</p>
<p>Siano $l_{1}, l_{2}, l_{3}, \dots, l_{n}$ lunghezze di code-words all&rsquo;interno del nostro alfabeto, allora vale che esistono dei code-words (stringhe binarie) che hanno quelle lunghezze <strong>se e solo se</strong> viene soddisfatta la propriet√†
</p>
$$
\sum_{x} 2^{-l(x)} \leq 1
$$
<p>Il motivo √® abbastanza semplice, questo si spiega in modo grafico in maniera praticamente immediata quando facciamo il disegno.
Si pu√≤ vedere dall&rsquo;albero binario corrispondente di un insieme di set binari con prefissi che se un parente √® scelto (colorato nel disegno), allora nessun discendente pu√≤ essere scelto perch√© altrimenti avresti un prefisso. Inoltre se colori quelli sopra, significa che al massimo se sommi tutti quei valori otterrai 1 sse hai utilizzato tutti i rami a tua disposizione (meaning, che non puoi scegliere altri code-work, altrimenti perdi la prefix property).
<img src="/images/notes/Entropy-20240212162407876.webp" alt="Entropy-20240212162407876"></p>
<h4 id="source-coding-theorem-for-symbol-codes">Source coding theorem for symbol codes<a hidden class="anchor" aria-hidden="true" href="#source-coding-theorem-for-symbol-codes">#</a></h4>
<p>Chiamata anche come la <strong>relazione con Kolmogorov</strong>.
1.11.3 di <a href="http://link.springer.com/10.1007/978-3-030-11298-1">(Li &amp; Vit√°nyi 2019)</a>, allora se prendiamo un set di code-words con $L$ il minimo prefix code che possiamo mai avere
Ossia $L = \sum_{x} P(x)l(x)$, con $l$ scelto il minimo possibile. Allora vale che
</p>
$$
H(P) \leq L \leq H(P) + 1
$$
<p>
Ossia la lunghezza migliore possibile √® boundata da valori di entropia. Che √® una cosa abbastanza forte perch√© relaziona come deve essere fatto il code-words, con la complessit√† dell&rsquo;informazione che vogliamo andare a utilizzare.
La dimostrazione non la facciamo qui, ma √® fattibile con le tue conoscenze credo, ti serve la Gibbs inequality qui sotto per una freccia</p>
<p><strong>Dimostrazione</strong>
Prendiamo $q_{i} = 2^{-l_{i}}$, allora abbiamo $l_{i} = \log\left( \frac{1}{q_{i}} \right)$ vale
</p>
$$
L = \sum_{x} p(x) l(x) = \sum_{x} \left( p(x) \log\left( \frac{1}{q(x)} \right) \right)
\geq  \sum_{x}p(x) \log\left( \frac{1}{p(x)} \right) =H(x)
$$
<p>Dove abbiamo usato anche l&rsquo;ineguaglianza di Gibbs <a href="/notes/entropy/#gibbs-inequality">#Gibbs Inequality</a> e il fatto che vale <a href="/notes/entropy/#krafts-inequality">#Krafts Inequality</a>.</p>
<p>Provando a dimostrare l&rsquo;altro bound, supponiamo che
</p>
$$
l_{i} = \lceil -\log_{2}(p_{i}) \rceil 
$$
<p>
Si pu√≤ dimostrare che questo √® un prefix code, perch√© soddisfa Kraft.
Allora si pu√≤ notare come
</p>
$$
L = \sum_{x} p(x) l(x) = \sum_{x} p(x) \lceil -\log_{2}(p_{x}) \rceil \leq \sum_{x}p(x) \left( \log_{2}\left( \frac{1}{p(x)}\right) +1 \right) = H(x) + 1
$$
<p>
Una nota interessante √® questo teorema ci permette di definire un concetto di <em>efficienza di rappresentazione</em>. Tutto quanto dato da KL divergence √® una specie di inefficienza.</p>
<p>Infatti possiamo scrivere
</p>
$$
L = H(x) + D_{KL}(P \mid \mid Q)
$$
<p>
Con $Q$ la probabilit√† associata alle singole codewords, assumendo che siano uniformi e simili per dire.</p>
<h4 id="gibbs-inequality">Gibbs Inequality<a hidden class="anchor" aria-hidden="true" href="#gibbs-inequality">#</a></h4>
<p>Afferma che l&rsquo;entropia √® minore rispetto alla cross-entropy di qualunque cosa, ossia
</p>
$$
\sum_{x} P(x) \log\left(  \frac{1}{P(x)} \right) \leq \sum_{x} P(x) \log\left( \frac{1}{Q(x)} \right)
$$
<p>
Qualunque sia l&rsquo;altra distribuzione.
Si pu√≤ dimostrare in modo abbastanza diretto utilizzando il fatto che la Kullback Leibler divergence, presentato in <a href="/notes/neural-networks/">Neural Networks</a>, √® sempre positiva o uguale a 0.
Infatti la parte di sopra si pu√≤ riscrivere come</p>
$$
-\sum_{x} P(x) \log\left( \frac{P(x)}{Q(x)} \right) = D_{KL}(P \mid \mid Q)
$$
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Cover &amp; Thomas <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">‚ÄúElements of Information Theory‚Äù</a> John Wiley &amp; Sons 2012</p>
<p>[2] Shannon <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">‚ÄúA Mathematical Theory of Communication‚Äù</a> The Bell System Technical Journal Vol. 27, pp. 379&ndash;423, 623&ndash;656 1948</p>
<p>[3] Li &amp; Vit√°nyi <a href="http://link.springer.com/10.1007/978-3-030-11298-1">‚ÄúAn Introduction to Kolmogorov Complexity and Its Applications‚Äù</a> Springer International Publishing 2019</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/information-theory/">‚úèInformation-Theory</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on x"
            href="https://x.com/intent/tweet/?text=Entropy&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f&amp;hashtags=%e2%9c%8finformation-theory">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f&amp;title=Entropy&amp;summary=Entropy&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f&title=Entropy">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on whatsapp"
            href="https://api.whatsapp.com/send?text=Entropy%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on telegram"
            href="https://telegram.me/share/url?text=Entropy&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Entropy&u=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
