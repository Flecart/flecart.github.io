<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Entropy | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="✏information-theory">
<meta name="description" content="Questo è stato creato da 1948 Shannon in (Shannon 1948). Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso.
Introduction to Entropy
The Shannon Information Content
$$
h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})}
$$
We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.
Kolmogorov complexity è un modo diverso per definire la complessità.
Legato è Neural Networks#Kullback-Leibler Divergence.">
<meta name="author" content="
By Xuanqiang Angelo Huang">
<link rel="canonical" href="https://flecart.github.io/notes/entropy/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f790d9af969c56c079c1ce2d5972a04486bf3d6144295d5fba319830e1e55a7a.css" integrity="sha256-95DZr5acVsB5wc4tWXKgRIa/PWFEKV1fujGYMOHlWno=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/entropy/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/entropy/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Entropy">
  <meta property="og:description" content="Questo è stato creato da 1948 Shannon in (Shannon 1948). Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso.
Introduction to Entropy The Shannon Information Content $$ h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.
Kolmogorov complexity è un modo diverso per definire la complessità. Legato è Neural Networks#Kullback-Leibler Divergence.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2024-09-20T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-09-20T00:00:00+00:00">
    <meta property="article:tag" content="✏Information-Theory">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Entropy">
<meta name="twitter:description" content="Questo è stato creato da 1948 Shannon in (Shannon 1948). Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso.
Introduction to Entropy
The Shannon Information Content
$$
h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})}
$$
We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.
Kolmogorov complexity è un modo diverso per definire la complessità.
Legato è Neural Networks#Kullback-Leibler Divergence.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Entropy",
      "item": "https://flecart.github.io/notes/entropy/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Entropy",
  "name": "Entropy",
  "description": "Questo è stato creato da 1948 Shannon in (Shannon 1948). Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso.\nIntroduction to Entropy The Shannon Information Content $$ h(x = a_{i}) = \\log_{2}\\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.\nKolmogorov complexity è un modo diverso per definire la complessità. Legato è Neural Networks#Kullback-Leibler Divergence.\n",
  "keywords": [
    "✏information-theory"
  ],
  "articleBody": "Questo è stato creato da 1948 Shannon in (Shannon 1948). Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso.\nIntroduction to Entropy The Shannon Information Content $$ h(x = a_{i}) = \\log_{2}\\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.\nKolmogorov complexity è un modo diverso per definire la complessità. Legato è Neural Networks#Kullback-Leibler Divergence.\nWe can model the classical view of entropy as the from [^1]\nExpected value of Surprisal which is the uncertainty of a random variable $X$ taking a certain value which is $p(X = x) = P(x)$, but we want to measure it using log-likelihood.\n$$ H(\\mathcal{X}) = - \\sum_{x \\in \\mathcal{X}, P_{\\mathcal{X}}(x) \u003e 0} p(x)\\log(p(x)) \\tag{1.1} $$ Ossia, possiamo dire in modo intuitivo quanto sarebbe sorprendente vedere che si avverasse quell’evento.\nThis is the graph for the binary case: $$ H(\\mathcal{X}) = p\\log \\frac{1}{p} + (1- p) \\log \\frac{1}{1 - p} $$ $$ \\begin{align*} H(X) := E[I(X)] \u0026= \\sum_{i=1}^n P(x_i)I(x_i) \\\\ \u0026= \\sum_{i=1}^n p_i \\log(1/p_i) \\\\ \u0026= -\\sum_{i=1}^n p_i \\log(p_i) \\tag{1.2} \\end{align*} $$Axiomatic Approach to Entropy We can define the entropy as the only function that satisfies the following properties (the function is commonly called surprise):\n$H(X) \\geq 0$ for every discrete r.v. which is related to the code length, which we don’t want to be negative $H(X, Y) = H(X) + H(Y)$ when $X, Y$ are independent random variables. $H(X) \\text{ is maximal }$ when $X \\sim Unif(0, 1)$ this concept is related to hard to guess the results. $$ H(X) = - \\sum_{x \\in \\mathcal{X}} p(x) \\log p(x) $$Properties of the entropy One observation is that labels don’t matter, we just need the probability vector and don’t care about what it represents.\nThe entropy is always positive. It’s easy to prove because all probabilities are $0 \u003c p \\leq 1$ so every term in the sum is positive because log in that interval is also positive.\nAxiomatic approach One could derive the entropy from an axiomatic point of view, with the idea of Surprise We just need three requirements: Given a probability space $\\Omega, \\mathcal{A}, \\mathbb{P}$ , then the surprise of an even $E \\subseteq \\mathcal{A}$ is equal to a function $S([\\mathbb{P}(E)])$ which satisfies the following properties:\n$S(1) = 0$ $S$ is continuous $S$ is monotonic decreasing, meaning that if $p \u003e q$ then $S(p) \u003c S(q)$ $S$ is additive, meaning that $S(pq) = S(p) + S(q)$ when $p, q$ are independent. $$ S(p) = - \\log(p) $$ The entropy is just the expected value of the surprise.\nChain Rule $$ H(X, Y) = H(X) + H(X|Y) $$$$ H(X_{0}, X_{1}, \\dots, X_{i}) = \\sum_{i}H(X_{i}|X_{i-1}\\dots X_{0}) $$Upper bound $$ H(X) \\leq \\log \\lvert \\mathcal{X} \\rvert $$ Con $\\mathcal{X}$ l’insieme immagine della variabile aleatoria discreta $X$. Importante in questo caso che la nostra variabile sia discreta, altrimenti il teorema provvisto in (Cover \u0026 Thomas 2012) 2.6.4 non funziona. Non è molto banale l’idea di utilizzare la uniforme per modellare il numero di elementi. e usare la positività di KL per finire l’upper bound.\n$$ P_{X}(x) = \\frac{1}{\\lvert \\mathcal{X} \\rvert } $$$$ \\sum P_{X}(x) \\log \\frac{1}{P_{X}(x)} = \\log \\frac{1}{P_{X}(x)} = \\log \\lvert X \\rvert $$$$ \\sum P_{X}(x) \\log \\frac{P_{X}(x)}{\\frac{1}{\\lvert \\mathcal{X} \\rvert }} = \\sum P_{X} \\log P(x) + \\sum P_{X}(x) \\log \\lvert \\mathcal{X} \\rvert = \\log \\lvert \\mathcal{X} \\rvert - H(X) $$$$ \\log \\lvert \\mathcal{X} \\rvert - H(X) \\geq 0 $$ Which ends the proof.\nEntropy is concave Uso l’upper bound e il fatto che KL è convesso per dimostrare questa cosa.\nFunctional dependency Non fare Se $Y = f(X)$ per qualche funzione, allora $H(Y|X) = H(X|Y) = 0$ si può risolvere con qualche ragionamento sul supporto di entropia. Interessante vedere che ha una piccola relazione con Normalizzazione dei database#Dipendenze funzionali.\nMonotonicity of Entropy $$ H(X) \\geq H(X \\mid Y) $$ It is also called the principle Information never hurts meaning you are never more uncertain about a random variable when you have more information about it.\n$$ H(X) - H(X \\mid Y) = \\sum P(x) \\log P(x) - \\sum P(x, y) \\log P(x \\mid y) = \\sum P(x, y) \\log \\frac{P(x, y)}{P(x)} \\underbrace{\\geq}_{\\text{Jensen}} 0 $$ This is also the reason why mutual information is always positive.\nTypes of entropy Conditional Entropy $$ H(Y|X) = \\sum_{x \\in \\mathcal{X}}p(x) H(Y|X=x) = \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x, y) \\log \\frac{1}{P(y|x)} = \\mathbf{E}\\left[ \\log\\frac{1}{p(Y|X)} \\right] $$ La nozione con il valore atteso è la più semplice anche in questo caso.\nconditional entropy corresponds to the expected remaining uncertainty in $Y$ after we observe $X$\n$$ H(Y \\mid X) = \\mathbb{E}_{(x, y) \\sim p(x, y)}\\left[ \\log \\frac{1}{p(y \\mid x)} \\right] $$Joint Entropy $$ H(X, Y) = - \\sum_{x, y} p(x, y) \\log p(x, y) $$$$ H(X, Y) = H(X) + H(Y \\mid X) = H(Y) + H(X \\mid Y) $$Relative Entropy or Kullback-Leibler Let’s take $\\lvert \\mathcal{X} \\rvert \u003c \\infty$, and take distributions P, Q, $\\mathcal{X} \\to \\mathbb{R}$ such that $p(x) \\geq 0 \\forall x \\in \\mathcal{X}$ and the sum is 1, same thing for $Q$, then we define the Kullback-Leibler Divergence between those distributions to be\n$$ D(P \\mid \\mid Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)} $$ We need to define some corner cases:\nIf $P(x) = 0$ and $Q$ is anything then its 0 If $\\exists \\xi \\in \\mathcal{X}$ such that $P(\\xi) \u003e 0$ and $Q(\\xi) = 0$ =\u003e $D(P \\mid \\mid Q) = + \\infty$. If $P \\ll Q$ then $D(P \\mid \\mid Q) \u003c \\infty$ (I did not understand why) This has some relations with the entropy, we can use some log properties and have the following result:\n$$ D(P \\mid \\mid Q) = - H(P) - \\sum P(x) \\log Q(x) $$ The second addendum can be called cross-entropy.\nIn modo praticamente equivalente possiamo definire una versione condizionata. e si può applicare anche in questo caso una chain rule\n$$ DL(P(x, y) \\mid\\mid Q(x, y) = DL(P(x) \\mid\\mid Q(x)) + DL(P(x|y) \\mid\\mid Q(x|y)) $$Relative entropy is not a distance not a metric, so its incorrect to say it is a distance, but for practical purposes it seems to work well: if the Relative entropy is small also the probability vectors are small.\nKL is positive or null $$ DL(P \\mid\\mid Q) \\geq 0 $$ Con uguaglianza se hanno esattamente la stessa distribuzione. We have that $D(P \\mid \\mid Q) = 0 \\iff P = Q$ .\nE ricordandoci che $\\log$ è una funzione concava, quindi si può utilizzare Jensen. Lo dimostriamo ora in breve. Sappiamo che la funzione $-\\log(x) = \\log\\left( \\frac{1}{x} \\right)$ è una funzione convessa, perché il negativo di una funzione concava, che è il logaritmo.\nAllora consideriamo $\\frac{1}{u} = \\frac{Q(x)}{P(x)}$ che è la parte dentro al logaritmo perché così possiamo usare Jensen Allora comunque abbiamo\n$$ \\sum_{x} P(x) \\log\\left( \\frac{Q(x)}{P(x)} \\right) \\geq \\log\\left( \\sum_{x} P(x) \\cdot \\frac{Q(x)}{P(x)} \\right) = \\log(1) = 0 $$$$ D_{KL}(P \\mid \\mid Q) \\geq 0 $$ In modo facile.\nKL is convex $DL(p\\mid\\mid q)$ è convesso sulla coppia $(p, q)$, 2.7.2 di (Cover \u0026 Thomas 2012). Anche sula 2.26 di McKay è buono, anche se non esattamente parla di questo.\nMutual information Questa nozione definisce quanta informazione hanno in comune due variabili aleatorie\nDefinizione $$ I(X;Y) = \\sum_{x}\\sum_{y} p(x, y) \\log\\left( \\frac{p(x, y)}{p(x)p(y)} \\right) = H(X) - H(X|Y) $$ Si può fare dopo un po’ di calcoli che qui ho omesso, ma non dovrebbe essere difficile farlo.888\nSi può intendere la mutual information anche come KL fra le distribuzioni $p(x, y)$ e $p(x)p(y)$ si può notare che queste due sono uguali quando le due sono indipendenti, che è coerente con la nostra nozione che abbiamo dell’indipendenza.\nProprietà Another property: $$ I(X; Y\\mid Z) = I(X ; Y, Z) - I(X ; Z) $$ Redundancy and Synergy $$ I(X;Y;Z) = I(X;Y) - I(X;Y\\mid Z) $$ Then synergy between $Y$ and $Z$ is present when the interaction information is negative, and redundant when its positive. This is because, intuitively, if it is negative it means $Z$ can provide more information with $Y$ about $Z$, else, they both have some information about $X$ which is probably not enough.\nExample: Independent Variable in MI We see that if $Y \\mid X \\perp Z$ then $I(X;Z) = I(X, Y; Z)$ this means then that the relation between $Y$ and $Z$ is neither redundant or synergic\n$$ \\begin{align} I(X, Y; Z) \u0026= \\mathbb{E}\\left[ \\log \\frac{P(X, Y, Z)}{P(X, Y) P(Z)} \\right] \\\\ \u0026= \\mathbb{E}\\left[ \\log \\frac{P(X, Z)}{P(X) P(Z)} \\right] + \\mathbb{E}\\left[ \\log \\frac{P(Y \\mid X, Z)}{P(Y \\mid X)} \\right] \\\\ \u0026= I(X; Z) + \\mathbb{E}\\left[ \\log \\frac{P(Y \\mid X, Z)}{P(Y \\mid X)} \\right] \\\\ \u0026= I(X; Z) \\end{align} $$ Where in the last step we used the fact that $Y \\perp Z \\mid X \\implies P(Y \\mid X, Z) = P(Y \\mid X)$.\nThis means that if the independence condition is satified, then we can add random variables as we like without changing the mutual information.\nSufficient Statistics Possiamo rappresentare il sampling da una certa famiglia di distribuzioni $f_{\\theta}(x)$ , rappresentato da $X$, e una sua statistica a caso (media varianza etc, che credo basti una funzione sul valore) come T, allora possiamo rappresentarlo come una Markov Chains#Catena di 3 variabili $\\theta \\to X \\to T(X)$ E vale il teorema di information processing\n$$ I(\\theta; T(X)) \\leq I(\\theta; X) $$ Si può chiamare una statistica per $\\theta$ sufficiente se $X$ contiene tutta l’informazione di $\\theta$. Non so bene cosa significhi. La cosa importante è che la statistica sufficiente preserva la mutua informazione ossia si ha una uguaglianza in quella relazione di sopra. Vedere 2.9 di (Cover \u0026 Thomas 2012) per esempi .\nQuesta cosa potrebbe permettere di dire che usando quella statistica io posso dimenticarmi del parametro, perché riesco a ricavarmelo senza problemi credo….\nThe purpose of sufficiency is to demonstrate that statistics that satisfy this property do not discard information about the parameter, and as such, estimators that might be based on a sufficient statistic are in a sense “good” ones to choose.\nDa https://math.stackexchange.com/questions/1186645/understanding-sufficient-statistic. Sufficient statistics also have a quite nice relationship with The Exponential Family.\nFano’s inequality L\"idea principale è utilizzare una variabile aleatoria per stimarne una altra, usando l’entropia condizionale fra le due.\nEnunciato fano $$ H(P_{e}) + P_{e}\\log \\lvert \\mathcal{X} \\rvert \\geq H(X|\\hat{X}) \\geq H(X|Y) $$$$ 1 + P_{e} \\log \\lvert \\mathcal{X} \\rvert \\geq H(X|Y) $$Dimostrazione Fano Questa è una bomba da fare. Poi però ha un sacco di conseguenze non applicabili in modo immediato (cioè non ci arrivi subito se non le fai un po’ prima).\nMaximum Distribution entropy Un problema classico nella teoria dell’informazione è trovare la distribuzione che massimizzi l’entropia (quindi l’informazione contenuta credo) dati certe conoscenze a priori, Ossia data una funzione $f$ e certe condizioni che deve rispettare, massimizzare l’entropia.\nSI può dimostrare (lo si può vedere da una reference di sopra) che la distribuzione che massimizza l’entropia, avendo solamente la condizione di probabilità, ossia che $\\sum_{x}p(x) = 1$ è la distribuzione uniforme. Mentre se assumo anche media $\\mu$ e varianza $\\sigma^{2}$ allora è la gaussiana (dimostrato in Maximum Entropy Principle. In un certo senso possiamo dire che queste distribuzioni sono molto ricche di informazioni.\nCodewords Jensen’s Inequality $$ f\\left( \\sum_{i} \\lambda_{i} x_{i} \\right) \\leq \\sum_{i}\\lambda_{i}f(x_{i}) $$ Con $\\sum_{i}\\lambda_{i} = 1$. Questa cosa si estende in modo molto semplice a variabili aleatorie e $E$ quando al posto di $\\lambda_{i}$ mettiamo una probabilità in un punto.\nLa dimostrazione non dovrebbe essere molto difficile. La strategia è utilizzare l’induzione in modo abbastanza classico. Non so in che modo si estende su funzioni continue, ma quelle sono cose tecniche matematiche non interessantissime.\nLog sum inequality $$ \\sum_{i=1}^{n}a_{i} \\log\\left( \\frac{a_{i}}{b_{i}} \\right) \\geq \\left( \\sum_{i=1}^{n}a_{i} \\right)\\log \\frac{\\left( \\sum_{i=1}^{n} a_{i} \\right)}{\\sum_{i=1}^{n}b_{i}} $$ Con uguaglianza se vale che $\\forall i, \\frac{a_{i}}{b_{i}}= const$\nKrafts Inequality https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality Questo teorema interessa cose dei codewords, perché ci interessano dei set di prefixfree che sono molto più gestibili probabilmente dal punto di vista dell’interpretazione. La cosa interessante è:\n$$ \\sum_{x} 2^{-l(x)} \\leq 1 $$Il motivo è abbastanza semplice, questo si spiega in modo grafico in maniera praticamente immediata quando facciamo il disegno. Si può vedere dall’albero binario corrispondente di un insieme di set binari con prefissi che se un parente è scelto (colorato nel disegno), allora nessun discendente può essere scelto perché altrimenti avresti un prefisso. Inoltre se colori quelli sopra, significa che al massimo se sommi tutti quei valori otterrai 1 sse hai utilizzato tutti i rami a tua disposizione (meaning, che non puoi scegliere altri code-work, altrimenti perdi la prefix property). Source coding theorem for symbol codes $$ H(P) \\leq L \\leq H(P) + 1 $$ Ossia la lunghezza migliore possibile è boundata da valori di entropia. Che è una cosa abbastanza forte perché relaziona come deve essere fatto il code-words, con la complessità dell’informazione che vogliamo andare a utilizzare. La dimostrazione non la facciamo qui, ma è fattibile con le tue conoscenze credo, ti serve la Gibbs inequality qui sotto per una freccia\n$$ L = \\sum_{x} p(x) l(x) = \\sum_{x} \\left( p(x) \\log\\left( \\frac{1}{q(x)} \\right) \\right) \\geq \\sum_{x}p(x) \\log\\left( \\frac{1}{p(x)} \\right) =H(x) $$Dove abbiamo usato anche l’ineguaglianza di Gibbs #Gibbs Inequality e il fatto che vale #Krafts Inequality.\n$$ l_{i} = \\lceil -\\log_{2}(p_{i}) \\rceil $$$$ L = \\sum_{x} p(x) l(x) = \\sum_{x} p(x) \\lceil -\\log_{2}(p_{x}) \\rceil \\leq \\sum_{x}p(x) \\left( \\log_{2}\\left( \\frac{1}{p(x)}\\right) +1 \\right) = H(x) + 1 $$ Una nota interessante è questo teorema ci permette di definire un concetto di efficienza di rappresentazione. Tutto quanto dato da KL divergence è una specie di inefficienza.\n$$ L = H(x) + D_{KL}(P \\mid \\mid Q) $$ Con $Q$ la probabilità associata alle singole codewords, assumendo che siano uniformi e simili per dire.\nGibbs Inequality $$ \\sum_{x} P(x) \\log\\left( \\frac{1}{P(x)} \\right) \\leq \\sum_{x} P(x) \\log\\left( \\frac{1}{Q(x)} \\right) $$ Qualunque sia l’altra distribuzione. Si può dimostrare in modo abbastanza diretto utilizzando il fatto che la Kullback Leibler divergence, presentato in Neural Networks, è sempre positiva o uguale a 0. Infatti la parte di sopra si può riscrivere come\n$$ -\\sum_{x} P(x) \\log\\left( \\frac{P(x)}{Q(x)} \\right) = D_{KL}(P \\mid \\mid Q) $$References [1] Shannon “A Mathematical Theory of Communication” The Bell System Technical Journal Vol. 27, pp. 379--423, 623--656 1948 [2] Cover \u0026 Thomas “Elements of Information Theory” John Wiley \\\u0026 Sons 2012 [3] Li \u0026 Vit{\\'a}nyi “An Introduction to Kolmogorov Complexity and Its Applications” Springer International Publishing 2019 ",
  "wordCount" : "2402",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "2024-09-20T00:00:00Z",
  "dateModified": "2024-09-20T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang Angelo Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/entropy/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Entropy
    </h1>
    <div class="post-meta"><span title='2024-09-20 00:00:00 +0000 UTC'>September 20, 2024</span>&nbsp;·&nbsp;Reading Time: 12 minutes&nbsp;·&nbsp;
By Xuanqiang Angelo Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#introduction-to-entropy" aria-label="Introduction to Entropy">Introduction to Entropy</a><ul>
                        
                <li>
                    <a href="#the-shannon-information-content" aria-label="The Shannon Information Content">The Shannon Information Content</a></li>
                <li>
                    <a href="#axiomatic-approach-to-entropy" aria-label="Axiomatic Approach to Entropy">Axiomatic Approach to Entropy</a></li></ul>
                </li>
                <li>
                    <a href="#properties-of-the-entropy" aria-label="Properties of the entropy">Properties of the entropy</a><ul>
                        
                <li>
                    <a href="#axiomatic-approach" aria-label="Axiomatic approach">Axiomatic approach</a></li>
                <li>
                    <a href="#chain-rule" aria-label="Chain Rule">Chain Rule</a></li>
                <li>
                    <a href="#upper-bound" aria-label="Upper bound">Upper bound</a></li>
                <li>
                    <a href="#entropy-is-concave" aria-label="Entropy is concave">Entropy is concave</a></li>
                <li>
                    <a href="#functional-dependency" aria-label="Functional dependency">Functional dependency</a></li>
                <li>
                    <a href="#monotonicity-of-entropy" aria-label="Monotonicity of Entropy">Monotonicity of Entropy</a></li></ul>
                </li>
                <li>
                    <a href="#types-of-entropy" aria-label="Types of entropy">Types of entropy</a><ul>
                        
                <li>
                    <a href="#conditional-entropy" aria-label="Conditional Entropy">Conditional Entropy</a></li>
                <li>
                    <a href="#joint-entropy" aria-label="Joint Entropy">Joint Entropy</a></li></ul>
                </li>
                <li>
                    <a href="#relative-entropy-or-kullback-leibler" aria-label="Relative Entropy or Kullback-Leibler">Relative Entropy or Kullback-Leibler</a><ul>
                        
                <li>
                    <a href="#kl-is-positive-or-null" aria-label="KL is positive or null">KL is positive or null</a></li>
                <li>
                    <a href="#kl-is-convex" aria-label="KL is convex">KL is convex</a></li></ul>
                </li>
                <li>
                    <a href="#mutual-information" aria-label="Mutual information">Mutual information</a><ul>
                        
                <li>
                    <a href="#definizione" aria-label="Definizione">Definizione</a></li>
                <li>
                    <a href="#propriet%c3%a0" aria-label="Proprietà">Proprietà</a></li>
                <li>
                    <a href="#redundancy-and-synergy" aria-label="Redundancy and Synergy">Redundancy and Synergy</a></li>
                <li>
                    <a href="#example-independent-variable-in-mi" aria-label="Example: Independent Variable in MI">Example: Independent Variable in MI</a></li></ul>
                </li>
                <li>
                    <a href="#sufficient-statistics" aria-label="Sufficient Statistics">Sufficient Statistics</a></li>
                <li>
                    <a href="#fanos-inequality" aria-label="Fano&rsquo;s inequality">Fano&rsquo;s inequality</a><ul>
                        
                <li>
                    <a href="#enunciato-fano" aria-label="Enunciato fano">Enunciato fano</a></li>
                <li>
                    <a href="#dimostrazione-fano" aria-label="Dimostrazione Fano">Dimostrazione Fano</a></li></ul>
                </li>
                <li>
                    <a href="#maximum-distribution-entropy" aria-label="Maximum Distribution entropy">Maximum Distribution entropy</a></li>
                <li>
                    <a href="#codewords" aria-label="Codewords">Codewords</a><ul>
                        
                <li>
                    <a href="#jensens-inequality" aria-label="Jensen&rsquo;s Inequality">Jensen&rsquo;s Inequality</a></li>
                <li>
                    <a href="#log-sum-inequality" aria-label="Log sum inequality">Log sum inequality</a></li>
                <li>
                    <a href="#krafts-inequality" aria-label="Krafts Inequality">Krafts Inequality</a></li>
                <li>
                    <a href="#source-coding-theorem-for-symbol-codes" aria-label="Source coding theorem for symbol codes">Source coding theorem for symbol codes</a></li>
                <li>
                    <a href="#gibbs-inequality" aria-label="Gibbs Inequality">Gibbs Inequality</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Questo è stato creato da 1948 Shannon in <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">(Shannon 1948)</a>. Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso.</p>
<h3 id="introduction-to-entropy">Introduction to Entropy<a hidden class="anchor" aria-hidden="true" href="#introduction-to-entropy">#</a></h3>
<h4 id="the-shannon-information-content">The Shannon Information Content<a hidden class="anchor" aria-hidden="true" href="#the-shannon-information-content">#</a></h4>
$$
h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})}
$$<p>
We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.</p>
<p><a href="/notes/kolmogorov-complexity">Kolmogorov complexity</a> è un modo diverso per definire la complessità.
Legato è <a href="/notes/neural-networks#kullback-leibler-divergence">Neural Networks#Kullback-Leibler Divergence</a>.</p>
<p>We can model the classical view of entropy as the from [^1]</p>
<blockquote>
<p>Expected value of <strong>Surprisal</strong> which is the uncertainty of a random variable $X$ taking a certain value which is $p(X = x) = P(x)$, but we want to measure it using log-likelihood.</p></blockquote>
$$
H(\mathcal{X}) = - \sum_{x \in \mathcal{X}, P_{\mathcal{X}}(x) > 0} p(x)\log(p(x)) \tag{1.1}
$$<p>
Ossia, possiamo dire in modo intuitivo quanto sarebbe sorprendente vedere che si avverasse quell&rsquo;evento.</p>
<img src="/images/notes/Entropy-20240918151806980.webp" style="width: 100%" class="center" alt="Entropy-20240918151806980">
This is the graph for the binary case:
$$
H(\mathcal{X}) = p\log \frac{1}{p} + (1- p) \log \frac{1}{1 - p}
$$
$$
\begin{align*}
H(X) := E[I(X)] &= \sum_{i=1}^n P(x_i)I(x_i) \\
&= \sum_{i=1}^n p_i \log(1/p_i) \\
&= -\sum_{i=1}^n p_i \log(p_i) \tag{1.2}
\end{align*}
$$<h4 id="axiomatic-approach-to-entropy">Axiomatic Approach to Entropy<a hidden class="anchor" aria-hidden="true" href="#axiomatic-approach-to-entropy">#</a></h4>
<p>We can define the entropy as the only function that satisfies the following properties (the function is commonly called <strong>surprise</strong>):</p>
<ol>
<li>$H(X) \geq 0$ for every <strong>discrete</strong> r.v. which is related to the code length, which we don&rsquo;t want to be negative</li>
<li>$H(X, Y) = H(X) + H(Y)$ when $X, Y$ are independent random variables.</li>
<li>$H(X) \text{ is maximal }$ when $X \sim Unif(0, 1)$ this concept is related to <strong>hard to guess</strong> the results.</li>
</ol>
$$
H(X) = - \sum_{x \in \mathcal{X}} p(x) \log p(x)
$$<h3 id="properties-of-the-entropy">Properties of the entropy<a hidden class="anchor" aria-hidden="true" href="#properties-of-the-entropy">#</a></h3>
<p>One observation is that <strong>labels don&rsquo;t matter</strong>, we just need the <em>probability vector</em> and don&rsquo;t care about what it represents.</p>
<p>The entropy is <strong>always positive</strong>. It&rsquo;s easy to prove because all probabilities are $0 < p \leq 1$ so every term in the sum is positive because log in that interval is also positive.</p>
<h4 id="axiomatic-approach">Axiomatic approach<a hidden class="anchor" aria-hidden="true" href="#axiomatic-approach">#</a></h4>
<p>One could derive the entropy from an axiomatic point of view, with the idea of <strong>Surprise</strong>
We just need three requirements:
Given a probability space $\Omega, \mathcal{A}, \mathbb{P}$ , then the surprise of an even $E \subseteq \mathcal{A}$ is equal to a function $S([\mathbb{P}(E)])$ which satisfies the following properties:</p>
<ol>
<li>$S(1) = 0$</li>
<li>$S$ is continuous</li>
<li>$S$ is monotonic decreasing, meaning that if $p > q$ then $S(p) < S(q)$</li>
<li>$S$ is additive, meaning that $S(pq) = S(p) + S(q)$ when $p, q$ are independent.</li>
</ol>
$$
S(p) = - \log(p)
$$<p>
The entropy is just the expected value of the surprise.</p>
<h4 id="chain-rule">Chain Rule<a hidden class="anchor" aria-hidden="true" href="#chain-rule">#</a></h4>
$$
H(X, Y) = H(X) + H(X|Y)
$$$$
H(X_{0}, X_{1}, \dots, X_{i}) = \sum_{i}H(X_{i}|X_{i-1}\dots X_{0})
$$<h4 id="upper-bound">Upper bound<a hidden class="anchor" aria-hidden="true" href="#upper-bound">#</a></h4>
$$
H(X) \leq \log \lvert \mathcal{X} \rvert 
$$<p>
Con $\mathcal{X}$ l&rsquo;insieme immagine della variabile aleatoria <strong>discreta</strong> $X$. Importante in questo caso che la nostra variabile sia discreta, altrimenti il teorema provvisto in <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">(Cover &amp; Thomas 2012)</a> 2.6.4 non funziona.
Non è molto banale l&rsquo;idea di utilizzare la uniforme per modellare il numero di elementi. e usare la positività di KL per finire l&rsquo;upper bound.</p>
$$
P_{X}(x) = \frac{1}{\lvert \mathcal{X} \rvert }
$$$$
\sum P_{X}(x) \log \frac{1}{P_{X}(x)} = \log \frac{1}{P_{X}(x)} = \log \lvert X \rvert 
$$$$
\sum P_{X}(x) \log \frac{P_{X}(x)}{\frac{1}{\lvert \mathcal{X} \rvert }}
= \sum P_{X} \log P(x) + \sum P_{X}(x) \log \lvert \mathcal{X} \rvert 
= \log \lvert \mathcal{X} \rvert  - H(X)
$$$$
\log \lvert \mathcal{X} \rvert  - H(X) \geq 0
$$<p>
Which ends the proof.</p>
<h4 id="entropy-is-concave">Entropy is concave<a hidden class="anchor" aria-hidden="true" href="#entropy-is-concave">#</a></h4>
<p>Uso l&rsquo;upper bound e il fatto che KL è convesso per dimostrare questa cosa.</p>
<h4 id="functional-dependency">Functional dependency<a hidden class="anchor" aria-hidden="true" href="#functional-dependency">#</a></h4>
<p>Non fare
Se $Y = f(X)$ per qualche funzione, allora $H(Y|X) = H(X|Y) = 0$ si può risolvere con qualche ragionamento sul supporto di entropia.
Interessante vedere che ha una piccola relazione con <a href="/notes/normalizzazione-dei-database#dipendenze-funzionali">Normalizzazione dei database#Dipendenze funzionali</a>.</p>
<h4 id="monotonicity-of-entropy">Monotonicity of Entropy<a hidden class="anchor" aria-hidden="true" href="#monotonicity-of-entropy">#</a></h4>
$$
H(X) \geq H(X \mid Y)
$$<p>
It is also called the principle <em>Information never hurts</em> meaning you are never more uncertain about a random variable when you have more information about it.</p>
$$
H(X) - H(X \mid Y) = \sum P(x) \log P(x) - \sum P(x, y) \log P(x \mid y) = \sum P(x, y) \log \frac{P(x, y)}{P(x)} \underbrace{\geq}_{\text{Jensen}} 0
$$<p>
This is also the reason why mutual information is always positive.</p>
<h3 id="types-of-entropy">Types of entropy<a hidden class="anchor" aria-hidden="true" href="#types-of-entropy">#</a></h3>
<h4 id="conditional-entropy">Conditional Entropy<a hidden class="anchor" aria-hidden="true" href="#conditional-entropy">#</a></h4>
$$
H(Y|X) = \sum_{x \in \mathcal{X}}p(x) H(Y|X=x)
= \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log \frac{1}{P(y|x)}
= \mathbf{E}\left[ \log\frac{1}{p(Y|X)} \right] 
$$<p>
La nozione con il valore atteso è la più semplice anche in questo caso.</p>
<blockquote>
<p>conditional entropy corresponds to the expected remaining uncertainty in $Y$ after we observe $X$</p></blockquote>
$$
H(Y \mid X) = \mathbb{E}_{(x, y) \sim p(x, y)}\left[ \log \frac{1}{p(y \mid x)} \right]
$$<h4 id="joint-entropy">Joint Entropy<a hidden class="anchor" aria-hidden="true" href="#joint-entropy">#</a></h4>
$$
H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y)
$$$$
H(X, Y) = H(X) + H(Y \mid X) = H(Y) + H(X \mid Y)
$$<h3 id="relative-entropy-or-kullback-leibler">Relative Entropy or Kullback-Leibler<a hidden class="anchor" aria-hidden="true" href="#relative-entropy-or-kullback-leibler">#</a></h3>
<p>Let&rsquo;s take
$\lvert \mathcal{X} \rvert < \infty$, and take distributions P, Q, $\mathcal{X} \to \mathbb{R}$ such that  $p(x) \geq 0 \forall x \in \mathcal{X}$ and the sum is 1, same thing for $Q$, then we define the Kullback-Leibler Divergence between those distributions to be</p>
$$
D(P \mid \mid Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}
$$<p>
We need to define some corner cases:</p>
<ul>
<li>If $P(x) = 0$ and $Q$ is anything then its 0</li>
<li>If $\exists \xi \in \mathcal{X}$ such that $P(\xi) > 0$ and $Q(\xi) = 0$ =&gt; $D(P \mid \mid Q) = + \infty$.</li>
<li>If $P \ll Q$ then $D(P \mid \mid Q) < \infty$ (I did not understand why)</li>
</ul>
<p>This has some relations with the entropy, we can use some log properties and have the following result:</p>
$$
D(P \mid \mid Q) = - H(P) - \sum P(x) \log Q(x)
$$<p>
The second addendum can be called <em>cross-entropy</em>.</p>
<p>In modo praticamente equivalente possiamo definire una versione condizionata. e si può applicare anche in questo caso una chain rule</p>
$$
DL(P(x, y) \mid\mid Q(x, y) = DL(P(x) \mid\mid Q(x)) + DL(P(x|y) \mid\mid Q(x|y))
$$<p>Relative entropy <strong>is not a distance</strong> not a metric, so its incorrect to say it is a distance, but for practical purposes it seems to work well: if the Relative entropy is small also the probability vectors are small.</p>
<h4 id="kl-is-positive-or-null">KL is positive or null<a hidden class="anchor" aria-hidden="true" href="#kl-is-positive-or-null">#</a></h4>
$$
DL(P \mid\mid Q) \geq 0
$$<p>
Con uguaglianza se hanno esattamente la stessa distribuzione.
We have that $D(P \mid \mid Q) = 0 \iff P = Q$ .</p>
<p>E ricordandoci che $\log$ è una funzione concava, quindi si può utilizzare <a href="/notes/analisi-di-convessità#jensen">Jensen</a>.
Lo dimostriamo ora in breve.
Sappiamo che la funzione $-\log(x) = \log\left( \frac{1}{x} \right)$ è una funzione convessa, perché il negativo di una funzione concava, che è  il logaritmo.</p>
<p>Allora consideriamo $\frac{1}{u} = \frac{Q(x)}{P(x)}$ che è la parte dentro al logaritmo perché così possiamo usare Jensen
Allora comunque abbiamo</p>
$$
\sum_{x} P(x) \log\left( \frac{Q(x)}{P(x)} \right) \geq \log\left( \sum_{x} P(x) \cdot \frac{Q(x)}{P(x)} \right) = \log(1) = 0
$$$$
D_{KL}(P \mid \mid Q) \geq 0
$$<p>
In modo facile.</p>
<h4 id="kl-is-convex">KL is convex<a hidden class="anchor" aria-hidden="true" href="#kl-is-convex">#</a></h4>
<p>$DL(p\mid\mid q)$ è convesso sulla coppia $(p, q)$, 2.7.2 di <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">(Cover &amp; Thomas 2012)</a>.
Anche sula 2.26 di McKay è buono, anche se non esattamente parla di questo.</p>
<h3 id="mutual-information">Mutual information<a hidden class="anchor" aria-hidden="true" href="#mutual-information">#</a></h3>
<p>Questa nozione definisce quanta informazione hanno in comune due variabili aleatorie</p>
<h4 id="definizione">Definizione<a hidden class="anchor" aria-hidden="true" href="#definizione">#</a></h4>
$$
I(X;Y) = \sum_{x}\sum_{y} p(x, y) \log\left(  \frac{p(x, y)}{p(x)p(y)} \right)
= H(X) - H(X|Y)
$$<p>
Si può fare dopo un po&rsquo; di calcoli che qui ho omesso, ma non dovrebbe essere difficile farlo.888</p>
<p>Si può intendere la mutual information anche come KL fra le distribuzioni $p(x, y)$ e $p(x)p(y)$ si può notare che queste due sono uguali quando le due sono indipendenti, che è coerente con la nostra nozione che abbiamo dell&rsquo;indipendenza.</p>
<h4 id="proprietà">Proprietà<a hidden class="anchor" aria-hidden="true" href="#proprietà">#</a></h4>
<img src="/images/notes/Entropy-20240229150751912.webp" style="width: 100%" class="center" alt="Entropy-20240229150751912">
<img src="/images/notes/Entropy-20240229150807093.webp" style="width: 100%" class="center" alt="Entropy-20240229150807093">
Another property:
$$
I(X; Y\mid Z) = I(X ; Y, Z) - I(X ; Z)
$$
<h4 id="redundancy-and-synergy">Redundancy and Synergy<a hidden class="anchor" aria-hidden="true" href="#redundancy-and-synergy">#</a></h4>
$$
I(X;Y;Z) = I(X;Y) - I(X;Y\mid Z)
$$<p>
Then <strong>synergy</strong> between $Y$ and $Z$ is present when the interaction information is negative, and <strong>redundant</strong> when its positive.
This is because, intuitively, if it is negative it means $Z$ can provide more information with $Y$ about $Z$, else, they both have some information about $X$ which is probably not enough.</p>
<h4 id="example-independent-variable-in-mi">Example: Independent Variable in MI<a hidden class="anchor" aria-hidden="true" href="#example-independent-variable-in-mi">#</a></h4>
<p>We see that if $Y \mid X \perp Z$ then $I(X;Z) = I(X, Y; Z)$ this means then that the relation between $Y$ and $Z$ is neither redundant or synergic</p>
$$
\begin{align}
I(X, Y; Z) &= \mathbb{E}\left[ \log \frac{P(X, Y, Z)}{P(X, Y) P(Z)} \right] \\
&= \mathbb{E}\left[ \log \frac{P(X, Z)}{P(X) P(Z)} \right] + \mathbb{E}\left[ \log \frac{P(Y \mid X, Z)}{P(Y \mid X)} \right] \\
&= I(X; Z) + \mathbb{E}\left[ \log \frac{P(Y \mid X, Z)}{P(Y \mid X)} \right] \\
&= I(X; Z)
\end{align}
$$<p>
Where in the last step we used the fact that $Y \perp Z \mid X \implies P(Y \mid X, Z) = P(Y \mid X)$.</p>
<p>This means that if the independence condition is satified, then we can add random variables as we like without changing the mutual information.</p>
<h3 id="sufficient-statistics">Sufficient Statistics<a hidden class="anchor" aria-hidden="true" href="#sufficient-statistics">#</a></h3>
<p>Possiamo rappresentare il sampling da una certa famiglia di distribuzioni $f_{\theta}(x)$ , rappresentato da $X$, e una sua statistica a caso (media varianza etc, che credo basti una funzione sul valore) come T, allora possiamo rappresentarlo come una <a href="/notes/markov-chains#catena-di-3-variabili">Markov Chains#Catena di 3 variabili</a> $\theta \to X \to T(X)$
E vale il teorema di information processing</p>
$$
I(\theta; T(X)) \leq I(\theta; X)
$$<p>
Si può chiamare una statistica per $\theta$ sufficiente se $X$ contiene tutta l&rsquo;informazione di $\theta$. Non so bene cosa significhi.
La cosa importante è che la statistica sufficiente <strong>preserva la mutua informazione</strong> ossia si ha una uguaglianza in quella relazione di sopra. Vedere 2.9 di <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">(Cover &amp; Thomas 2012)</a> per esempi .</p>
<p>Questa cosa potrebbe permettere di dire che usando quella statistica io posso dimenticarmi del parametro, perché riesco a ricavarmelo senza problemi credo&hellip;.</p>
<blockquote>
<p>The purpose of sufficiency is to demonstrate that statistics that satisfy this property do not discard information about the parameter, and as such, estimators that might be based on a sufficient statistic are in a sense &ldquo;good&rdquo; ones to choose.</p></blockquote>
<p>Da <a href="https://math.stackexchange.com/questions/1186645/understanding-sufficient-statistic."><a href="https://math.stackexchange.com/questions/1186645/understanding-sufficient-statistic">https://math.stackexchange.com/questions/1186645/understanding-sufficient-statistic</a>.</a>
Sufficient statistics also have a quite nice relationship with <a href="/notes/the-exponential-family">The Exponential Family</a>.</p>
<h3 id="fanos-inequality">Fano&rsquo;s inequality<a hidden class="anchor" aria-hidden="true" href="#fanos-inequality">#</a></h3>
<p>L&quot;idea principale è utilizzare una variabile aleatoria per stimarne una altra, usando l&rsquo;entropia condizionale fra le due.</p>
<h4 id="enunciato-fano">Enunciato fano<a hidden class="anchor" aria-hidden="true" href="#enunciato-fano">#</a></h4>
$$
H(P_{e}) + P_{e}\log \lvert \mathcal{X} \rvert \geq H(X|\hat{X}) \geq H(X|Y)
$$$$
1 + P_{e} \log \lvert \mathcal{X} \rvert  \geq H(X|Y)
$$<h4 id="dimostrazione-fano">Dimostrazione Fano<a hidden class="anchor" aria-hidden="true" href="#dimostrazione-fano">#</a></h4>
<p>Questa è una bomba da fare. Poi però ha un sacco di conseguenze non applicabili in modo immediato (cioè non ci arrivi subito se non le fai un po&rsquo; prima).</p>
<h3 id="maximum-distribution-entropy">Maximum Distribution entropy<a hidden class="anchor" aria-hidden="true" href="#maximum-distribution-entropy">#</a></h3>
<p>Un problema classico nella teoria dell&rsquo;informazione è trovare la distribuzione che massimizzi l&rsquo;entropia (quindi l&rsquo;informazione contenuta credo) dati certe conoscenze a priori,
Ossia data una funzione $f$  e certe condizioni che deve rispettare, massimizzare l&rsquo;entropia.</p>
<p>SI può dimostrare (lo si può vedere da una reference di sopra) che la distribuzione che massimizza l&rsquo;entropia, avendo solamente la condizione di probabilità, ossia che $\sum_{x}p(x) = 1$ è la distribuzione uniforme.
Mentre se assumo anche media $\mu$ e varianza $\sigma^{2}$ allora è la gaussiana (dimostrato in <a href="/notes/maximum-entropy-principle">Maximum Entropy Principle</a>.
In un certo senso possiamo dire che queste distribuzioni sono molto ricche di informazioni.</p>
<h3 id="codewords">Codewords<a hidden class="anchor" aria-hidden="true" href="#codewords">#</a></h3>
<h4 id="jensens-inequality">Jensen&rsquo;s Inequality<a hidden class="anchor" aria-hidden="true" href="#jensens-inequality">#</a></h4>
$$
f\left( \sum_{i} \lambda_{i} x_{i} \right) \leq \sum_{i}\lambda_{i}f(x_{i})
$$<p>
Con $\sum_{i}\lambda_{i} = 1$. Questa cosa si estende in modo molto semplice a variabili aleatorie e $E$ quando al posto di $\lambda_{i}$ mettiamo una probabilità in un punto.</p>
<p>La dimostrazione non dovrebbe essere molto difficile. La strategia è utilizzare l&rsquo;induzione in modo abbastanza classico. Non so in che modo si estende su funzioni continue, ma quelle sono cose tecniche matematiche non interessantissime.</p>
<h4 id="log-sum-inequality">Log sum inequality<a hidden class="anchor" aria-hidden="true" href="#log-sum-inequality">#</a></h4>
$$
\sum_{i=1}^{n}a_{i} \log\left( \frac{a_{i}}{b_{i}} \right) \geq \left( \sum_{i=1}^{n}a_{i} \right)\log \frac{\left( \sum_{i=1}^{n} a_{i} \right)}{\sum_{i=1}^{n}b_{i}}
$$<p>
Con uguaglianza se vale che $\forall i, \frac{a_{i}}{b_{i}}= const$</p>
<h4 id="krafts-inequality">Krafts Inequality<a hidden class="anchor" aria-hidden="true" href="#krafts-inequality">#</a></h4>
<p><a href="https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality"><a href="https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality">https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality</a></a>
Questo teorema interessa cose dei codewords, perché ci interessano dei <strong>set</strong> di prefixfree che sono molto più gestibili probabilmente dal punto di vista dell&rsquo;interpretazione.
La cosa interessante è:</p>
$$
\sum_{x} 2^{-l(x)} \leq 1
$$<p>Il motivo è abbastanza semplice, questo si spiega in modo grafico in maniera praticamente immediata quando facciamo il disegno.
Si può vedere dall&rsquo;albero binario corrispondente di un insieme di set binari con prefissi che se un parente è scelto (colorato nel disegno), allora nessun discendente può essere scelto perché altrimenti avresti un prefisso. Inoltre se colori quelli sopra, significa che al massimo se sommi tutti quei valori otterrai 1 sse hai utilizzato tutti i rami a tua disposizione (meaning, che non puoi scegliere altri code-work, altrimenti perdi la prefix property).
<img src="/images/notes/Entropy-20240212162407876.webp" style="width: 100%" class="center" alt="Entropy-20240212162407876"></p>
<h4 id="source-coding-theorem-for-symbol-codes">Source coding theorem for symbol codes<a hidden class="anchor" aria-hidden="true" href="#source-coding-theorem-for-symbol-codes">#</a></h4>
$$
H(P) \leq L \leq H(P) + 1
$$<p>
Ossia la lunghezza migliore possibile è boundata da valori di entropia. Che è una cosa abbastanza forte perché relaziona come deve essere fatto il code-words, con la complessità dell&rsquo;informazione che vogliamo andare a utilizzare.
La dimostrazione non la facciamo qui, ma è fattibile con le tue conoscenze credo, ti serve la Gibbs inequality qui sotto per una freccia</p>
$$
L = \sum_{x} p(x) l(x) = \sum_{x} \left( p(x) \log\left( \frac{1}{q(x)} \right) \right)
\geq  \sum_{x}p(x) \log\left( \frac{1}{p(x)} \right) =H(x)
$$<p>Dove abbiamo usato anche l&rsquo;ineguaglianza di Gibbs <a href="/notes#gibbs-inequality">#Gibbs Inequality</a> e il fatto che vale <a href="/notes#krafts-inequality">#Krafts Inequality</a>.</p>
$$
l_{i} = \lceil -\log_{2}(p_{i}) \rceil 
$$$$
L = \sum_{x} p(x) l(x) = \sum_{x} p(x) \lceil -\log_{2}(p_{x}) \rceil \leq \sum_{x}p(x) \left( \log_{2}\left( \frac{1}{p(x)}\right) +1 \right) = H(x) + 1
$$<p>
Una nota interessante è questo teorema ci permette di definire un concetto di <em>efficienza di rappresentazione</em>. Tutto quanto dato da KL divergence è una specie di inefficienza.</p>
$$
L = H(x) + D_{KL}(P \mid \mid Q)
$$<p>
Con $Q$ la probabilità associata alle singole codewords, assumendo che siano uniformi e simili per dire.</p>
<h4 id="gibbs-inequality">Gibbs Inequality<a hidden class="anchor" aria-hidden="true" href="#gibbs-inequality">#</a></h4>
$$
\sum_{x} P(x) \log\left(  \frac{1}{P(x)} \right) \leq \sum_{x} P(x) \log\left( \frac{1}{Q(x)} \right)
$$<p>
Qualunque sia l&rsquo;altra distribuzione.
Si può dimostrare in modo abbastanza diretto utilizzando il fatto che la Kullback Leibler divergence, presentato in <a href="/notes/neural-networks">Neural Networks</a>, è sempre positiva o uguale a 0.
Infatti la parte di sopra si può riscrivere come</p>
$$
-\sum_{x} P(x) \log\left( \frac{P(x)}{Q(x)} \right) = D_{KL}(P \mid \mid Q)
$$<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=shannonMathematicalTheoryCommunication1948>[1] Shannon <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">“A Mathematical Theory of Communication”</a> The Bell System Technical Journal Vol. 27, pp. 379--423, 623--656 1948
 </p>
<p id=coverElementsInformationTheory2012>[2] Cover & Thomas <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">“Elements of Information Theory”</a> John Wiley \& Sons 2012
 </p>
<p id=liIntroductionKolmogorovComplexity2019>[3] Li & Vit{\'a}nyi <a href="http://link.springer.com/10.1007/978-3-030-11298-1">“An Introduction to Kolmogorov Complexity and Its Applications”</a> Springer International Publishing 2019
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/information-theory/">✏Information-Theory</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on x"
            href="https://x.com/intent/tweet/?text=Entropy&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f&amp;hashtags=%e2%9c%8finformation-theory">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f&amp;title=Entropy&amp;summary=Entropy&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f&title=Entropy">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on whatsapp"
            href="https://api.whatsapp.com/send?text=Entropy%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on telegram"
            href="https://telegram.me/share/url?text=Entropy&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Entropy&u=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
