<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Entropy | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="✏information-theory">
<meta name="description" content="Entropy Questo è stato creato da 1948 Shannon in (Shannon 1948). Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso. This is dependent on the notion of the Shannon information content defined as $$ h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/entropy/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/entropy/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Entropy" />
<meta property="og:description" content="Entropy Questo è stato creato da 1948 Shannon in (Shannon 1948). Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso. This is dependent on the notion of the Shannon information content defined as $$ h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/entropy/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Entropy"/>
<meta name="twitter:description" content="Entropy Questo è stato creato da 1948 Shannon in (Shannon 1948). Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso. This is dependent on the notion of the Shannon information content defined as $$ h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Entropy",
      "item": "https://flecart.github.io/notes/entropy/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Entropy",
  "name": "Entropy",
  "description": "Entropy Questo è stato creato da 1948 Shannon in (Shannon 1948). Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso. This is dependent on the notion of the Shannon information content defined as $$ h(x = a_{i}) = \\log_{2}\\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution.",
  "keywords": [
    "✏information-theory"
  ],
  "articleBody": "Entropy Questo è stato creato da 1948 Shannon in (Shannon 1948). Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso. This is dependent on the notion of the Shannon information content defined as $$ h(x = a_{i}) = \\log_{2}\\frac{1}{P(x = a_{i})} $$ We will see that the entropy is a weighted average of the information, so the expected information content in a distribution. TODO: define this better.\nKolmogorov complexity è un modo diverso per definire la complessità. Legato è Neural Networks#Kullback-Leibler Divergence.\nWe can model the classical view of entropy as the from [^1]\nExpected value of Surprisal which is the uncertainty of a random variable $X$ taking a certain value which is $p(X = x) = P(x)$, but we want to measure it using log-likelihood.\nWith $\\lvert \\mathcal{X} \\rvert \u003c \\infty$, $P_{X}(\\cdot)$. $$ H(\\mathcal{X}) = - \\sum_{x \\in \\mathcal{X}, P_{\\mathcal{X}}(x) \u003e 0} p(x)\\log(p(x)) \\tag{1.1} $$ Ossia, possiamo dire in modo intuitivo quanto sarebbe sorprendente vedere che si avverasse quell’evento.\nThis is the graph for the binary case: $$ H(\\mathcal{X}) = p\\log \\frac{1}{p} + (1- p) \\log \\frac{1}{1 - p} $$ Oppure come descritto da [^2] $$ \\begin{align*} H(X) := E[I(X)] \u0026= \\sum_{i=1}^n P(x_i)I(x_i) \\\\ \u0026= \\sum_{i=1}^n p_i \\log(1/p_i) \\\\ \u0026= -\\sum_{i=1}^n p_i \\log(p_i) \\tag{1.2} \\end{align*} $$ Properties of the entropy One observation is that labels don’t matter, we just need the probability vector and don’t care about what it represents.\nThe entropy is always positive. It’s easy to prove because all probabilities are $0 \u003c p \\leq 1$ so every term in the sum is positive because log in that interval is also positive.\nAxiomatic approach One could derive the entropy from an axiomatic point of view. We just need three requirements:\n$H(X) \\geq 0$ for every discrete r.v. which is related to the code length, which we don’t want to be negative $H(X, Y) = H(X) + H(Y)$ when $X, Y$ are independent random variables. $H(X) \\text{ is maximal }$ when $X \\sim Unif(0, 1)$ this concept is related to hard to guess the results. The professor in AML course has made examples of guessing what object has been seen, when both parties know the probabilities of every object. One can prove that only a single function satisfies these functions. Conditional Entropy $$ H(Y|X) = \\sum_{x \\in \\mathcal{X}}p(x) H(Y|X=x) = \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x, y) \\log \\frac{1}{P(y|x)} = \\mathbf{E}\\left[ \\log\\frac{1}{p(Y|X)} \\right] $$ La nozione con il valore atteso è la più semplice anche in questo caso.\nChain Rule non fare. Una proprietà random è $$ H(X, Y) = H(X) + H(X|Y) $$ La dimostrazione è abbastanza banale una volta che si conoscono le definizioni…. La cosa interessante è che si può generale per qualunque numero di variabili aleatorie: $$ H(X_{0}, X_{1}, \\dots, X_{i}) = \\sum_{i}H(X_{i}|X_{i-1}\\dots X_{0}) $$ Upper bound $$ H(X) \\leq \\log \\lvert \\mathcal{X} \\rvert $$ Con $\\mathcal{X}$ l’insieme immagine della variabile aleatoria discreta $X$. Importante in questo caso che la nostra variabile sia discreta, altrimenti il teorema provvisto in (Cover \u0026 Thomas 2012) 2.6.4 non funziona. Non è molto banale l’idea di utilizzare la uniforme per modellare il numero di elementi. e usare la positività di KL per finire l’upper bound.\nWe can prove the equality by having an uniform distribution. The computation is easy: If $\\mathcal{X}$ is uniform then: $$ P_{X}(x) = \\frac{1}{\\lvert \\mathcal{X} \\rvert } $$ And so we have $$ \\sum P_{X}(x) \\log \\frac{1}{P_{X}(x)} = \\log \\frac{1}{P_{X}(x)} = \\log \\lvert X \\rvert $$ Proving the upper bound Let’s take $D(P_{X} \\mid \\mid U)$ into consideration, then we have that this value is $$ \\sum P_{X}(x) \\log \\frac{P_{X}(x)}{\\frac{1}{\\lvert \\mathcal{X} \\rvert }} = \\sum P_{X} \\log P(x) + \\sum P_{X}(x) \\log \\lvert \\mathcal{X} \\rvert = \\log \\lvert \\mathcal{X} \\rvert - H(X) $$ And by knowing that the Kullback Leibler divergence is positive we know that $$ \\log \\lvert \\mathcal{X} \\rvert - H(X) \\geq 0 $$ Which ends the proof.\nEntropy is concave Uso l’upper bound e il fatto che KL è convesso per dimostrare questa cosa.\nFunctional dependency Non fare Se $Y = f(X)$ per qualche funzione, allora $H(Y|X) = H(X|Y) = 0$ si può risolvere con qualche ragionamento sul supporto di entropia. Interessante vedere che ha una piccola relazione con Normalizzazione dei database#Dipendenze funzionali.\nRelative Entropy or Kullback-Leibler Let’s take $\\lvert \\mathcal{X} \\rvert \u003c \\infty$, and take distributions P, Q, $\\mathcal{X} \\to \\mathbb{R}$ such that $p(x) \\geq 0 \\forall x \\in \\mathcal{X}$ and the sum is 1, same thing for $Q$, then we define the Kullback-Leibler Divergence between those distributions to be\n$$ D(P \\mid \\mid Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)} $$ We need to define some corner cases:\nIf $P(x) = 0$ and $Q$ is anything then its 0 If $\\exists \\xi \\in \\mathcal{X}$ such that $P(\\xi) \u003e 0$ and $Q(\\xi) = 0$ =\u003e $D(P \\mid \\mid Q) = + \\infty$. If $P \\ll Q$ then $D(P \\mid \\mid Q) \u003c \\infty$ (I did not understand why) This has some relations with the entropy, we can use some log properties and have the following result:\n$$ D(P \\mid \\mid Q) = - H(P) - \\sum P(x) \\log Q(x) $$ The second addendum can be called cross-entropy.\nIn modo praticamente equivalente possiamo definire una versione condizionata. e si può applicare anche in questo caso una chain rule\n$$ DL(P(x, y) \\mid\\mid Q(x, y) = DL(P(x) \\mid\\mid Q(x)) + DL(P(x|y) \\mid\\mid Q(x|y)) $$ Relative entropy is not a distance not a metric, so its incorrect to say it is a distance, but for practical purposes it seems to work well: if the Relative entropy is small also the probability vectors are small.\nKL is positive or null Ossia per ogni distribuzione $p$ o $q$ si ha che $$ DL(P \\mid\\mid Q) \\geq 0 $$ Con uguaglianza se hanno esattamente la stessa distribuzione. We have that $D(P \\mid \\mid Q) = 0 \\iff P = Q$ .\nE ricordandoci che $\\log$ è una funzione concava, quindi si può utilizzare Jensen. Lo dimostriamo ora in breve. Sappiamo che la funzione $-\\log(x) = \\log\\left( \\frac{1}{x} \\right)$ è una funzione convessa, perché il negativo di una funzione concava, che è il logaritmo.\nAllora consideriamo $\\frac{1}{u} = \\frac{Q(x)}{P(x)}$ che è la parte dentro al logaritmo perché così possiamo usare Jensen Allora comunque abbiamo\n$$ \\sum_{x} P(x) \\log\\left( \\frac{Q(x)}{P(x)} \\right) \\geq \\log\\left( \\sum_{x} P(x) \\cdot \\frac{Q(x)}{P(x)} \\right) = \\log(1) = 0 $$ Che conclude che $$ D_{KL}(P \\mid \\mid Q) \\geq 0 $$ In modo facile.\nKL is convex $DL(p\\mid\\mid q)$ è convesso sulla coppia $(p, q)$, 2.7.2 di (Cover \u0026 Thomas 2012). Anche sula 2.26 di McKay è buono, anche se non esattamente parla di questo.\nMutual information Questa nozione definisce quanta informazione hanno in comune due variabili aleatorie\nDefinizione $$ I(X;Y) = \\sum_{x}\\sum_{y} p(x, y) \\log\\left( \\frac{p(x, y)}{p(x)p(y)} \\right) = H(X) - H(X|Y) $$ Si può fare dopo un po’ di calcoli che qui ho omesso, ma non dovrebbe essere difficile farlo.888\nSi può intendere la mutual information anche come KL fra le distribuzioni $p(x, y)$ e $p(x)p(y)$ si può notare che queste due sono uguali quando le due sono indipendenti, che è coerente con la nostra nozione che abbiamo dell’indipendenza.\nProprietà Sufficient Statistics Possiamo rappresentare il sampling da una certa famiglia di distribuzioni $f_{\\theta}(x)$ , rappresentato da $X$, e una sua statistica a caso (media varianza etc, che credo basti una funzione sul valore) come T, allora possiamo rappresentarlo come una Markov Chains#Catena di 3 variabili $\\theta \\to X \\to T(X)$ E vale il teorema di information processing\n$$ I(\\theta; T(X)) \\leq I(\\theta; X) $$ Si può chiamare una statistica per $\\theta$ sufficiente se $X$ contiene tutta l’informazione di $\\theta$. Non so bene cosa significhi. La cosa importante è che la statistica sufficiente preserva la mutua informazione ossia si ha una uguaglianza in quella relazione di sopra. Vedere 2.9 di (Cover \u0026 Thomas 2012) per esempi .\nQuesta cosa potrebbe permettere di dire che usando quella statistica io posso dimenticarmi del parametro, perché riesco a ricavarmelo senza problemi credo….\nThe purpose of sufficiency is to demonstrate that statistics that satisfy this property do not discard information about the parameter, and as such, estimators that might be based on a sufficient statistic are in a sense “good” ones to choose.\nDa https://math.stackexchange.com/questions/1186645/understanding-sufficient-statistic.\nFano’s inequality L\"idea principale è utilizzare una variabile aleatoria per stimarne una altra, usando l’entropia condizionale fra le due.\nEnunciato fano Per ogni estimantore $\\hat{X}$ tale per cui $X \\to Y \\to \\hat{X}$ sia una catena di markov, e con $P_{e} = Pr(X \\not= \\hat{X})$ ossia la probabilità di errore, abbiamo che vale $$ H(P_{e}) + P_{e}\\log \\lvert \\mathcal{X} \\rvert \\geq H(X|\\hat{X}) \\geq H(X|Y) $$ Ci sono forme più deboli che possiamo considerare in un certo senso corollari, ossia che $$ 1 + P_{e} \\log \\lvert \\mathcal{X} \\rvert \\geq H(X|Y) $$ Dimostrazione Fano Questa è una bomba da fare. Poi però ha un sacco di conseguenze non applicabili in modo immediato (cioè non ci arrivi subito se non le fai un po’ prima).\nMaximum Distribution entropy Un problema classico nella teoria dell’informazione è trovare la distribuzione che massimizzi l’entropia (quindi l’informazione contenuta credo) dati certe conoscenze a priori, Ossia data una funzione $f$ e certe condizioni che deve rispettare, massimizzare l’entropia.\nSI può dimostrare (lo si può vedere da una reference di sopra) che la distribuzione che massimizza l’entropia, avendo solamente la condizione di probabilità, ossia che $\\sum_{x}p(x) = 1$ è la distribuzione uniforme. Mentre se assumo anche media $\\mu$ e varianza $\\sigma^{2}$ allora è la gaussiana. In un certo senso possiamo dire che queste distribuzioni sono molto ricche di informazioni.\nCodewords Jensen’s Inequality Questo è un teorema fondamentale per moltissime cose, e da un certo punto di vista è una cosa banale per le cose convesse/concave. Allora, sia data una funzione in $[a, b]$ tale che sia convessa (concava) in questo intervallo, allora vale che $$ f\\left( \\sum_{i} \\lambda_{i} x_{i} \\right) \\leq \\sum_{i}\\lambda_{i}f(x_{i}) $$ Con $\\sum_{i}\\lambda_{i} = 1$. Questa cosa si estende in modo molto semplice a variabili aleatorie e $E$ quando al posto di $\\lambda_{i}$ mettiamo una probabilità in un punto.\nLa dimostrazione non dovrebbe essere molto difficile. La strategia è utilizzare l’induzione in modo abbastanza classico. Non so in che modo si estende su funzioni continue, ma quelle sono cose tecniche matematiche non interessantissime.\nLog sum inequality Siano $a_{1}, a_{2}, \\dots a_{n}$ e $b_{1}, b_{2}, \\dots, b_{n}$ numeri non negativi, allora vale che $$ \\sum_{i=1}^{n}a_{i} \\log\\left( \\frac{a_{i}}{b_{i}} \\right) \\geq \\left( \\sum_{i=1}^{n}a_{i} \\right)\\log \\frac{\\left( \\sum_{i=1}^{n} a_{i} \\right)}{\\sum_{i=1}^{n}b_{i}} $$ Con uguaglianza se vale che $\\forall i, \\frac{a_{i}}{b_{i}}= const$\nKrafts Inequality https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality Questo teorema interessa cose dei codewords, perché ci interessano dei set di prefixfree che sono molto più gestibili probabilmente dal punto di vista dell’interpretazione. La cosa interessante è:\nSiano $l_{1}, l_{2}, l_{3}, \\dots, l_{n}$ lunghezze di code-words all’interno del nostro alfabeto, allora vale che esistono dei code-words (stringhe binarie) che hanno quelle lunghezze se e solo se viene soddisfatta la proprietà $$ \\sum_{x} 2^{-l(x)} \\leq 1 $$ Il motivo è abbastanza semplice, questo si spiega in modo grafico in maniera praticamente immediata quando facciamo il disegno. Si può vedere dall’albero binario corrispondente di un insieme di set binari con prefissi che se un parente è scelto (colorato nel disegno), allora nessun discendente può essere scelto perché altrimenti avresti un prefisso. Inoltre se colori quelli sopra, significa che al massimo se sommi tutti quei valori otterrai 1 sse hai utilizzato tutti i rami a tua disposizione (meaning, che non puoi scegliere altri code-work, altrimenti perdi la prefix property). Source coding theorem for symbol codes Chiamata anche come la relazione con Kolmogorov. 1.11.3 di (Li \u0026 Vitányi 2019), allora se prendiamo un set di code-words con $L$ il minimo prefix code che possiamo mai avere Ossia $L = \\sum_{x} P(x)l(x)$, con $l$ scelto il minimo possibile. Allora vale che $$ H(P) \\leq L \\leq H(P) + 1 $$ Ossia la lunghezza migliore possibile è boundata da valori di entropia. Che è una cosa abbastanza forte perché relaziona come deve essere fatto il code-words, con la complessità dell’informazione che vogliamo andare a utilizzare. La dimostrazione non la facciamo qui, ma è fattibile con le tue conoscenze credo, ti serve la Gibbs inequality qui sotto per una freccia\nDimostrazione Prendiamo $q_{i} = 2^{-l_{i}}$, allora abbiamo $l_{i} = \\log\\left( \\frac{1}{q_{i}} \\right)$ vale $$ L = \\sum_{x} p(x) l(x) = \\sum_{x} \\left( p(x) \\log\\left( \\frac{1}{q(x)} \\right) \\right) \\geq \\sum_{x}p(x) \\log\\left( \\frac{1}{p(x)} \\right) =H(x) $$ Dove abbiamo usato anche l’ineguaglianza di Gibbs #Gibbs Inequality e il fatto che vale #Krafts Inequality.\nProvando a dimostrare l’altro bound, supponiamo che $$ l_{i} = \\lceil -\\log_{2}(p_{i}) \\rceil $$ Si può dimostrare che questo è un prefix code, perché soddisfa Kraft. Allora si può notare come $$ L = \\sum_{x} p(x) l(x) = \\sum_{x} p(x) \\lceil -\\log_{2}(p_{x}) \\rceil \\leq \\sum_{x}p(x) \\left( \\log_{2}\\left( \\frac{1}{p(x)}\\right) +1 \\right) = H(x) + 1 $$ Una nota interessante è questo teorema ci permette di definire un concetto di efficienza di rappresentazione. Tutto quanto dato da KL divergence è una specie di inefficienza.\nInfatti possiamo scrivere $$ L = H(x) + D_{KL}(P \\mid \\mid Q) $$ Con $Q$ la probabilità associata alle singole codewords, assumendo che siano uniformi e simili per dire.\nGibbs Inequality Afferma che l’entropia è minore rispetto alla cross-entropy di qualunque cosa, ossia $$ \\sum_{x} P(x) \\log\\left( \\frac{1}{P(x)} \\right) \\leq \\sum_{x} P(x) \\log\\left( \\frac{1}{Q(x)} \\right) $$ Qualunque sia l’altra distribuzione. Si può dimostrare in modo abbastanza diretto utilizzando il fatto che la Kullback Leibler divergence, presentato in Neural Networks, è sempre positiva o uguale a 0. Infatti la parte di sopra si può riscrivere come\n$$ -\\sum_{x} P(x) \\log\\left( \\frac{P(x)}{Q(x)} \\right) = D_{KL}(P \\mid \\mid Q) $$ References [1] Li \u0026 Vitányi “An Introduction to Kolmogorov Complexity and Its Applications” Springer International Publishing 2019\n[2] Shannon “A Mathematical Theory of Communication” The Bell System Technical Journal Vol. 27, pp. 379–423, 623–656 1948\n[3] Cover \u0026 Thomas “Elements of Information Theory” John Wiley \u0026 Sons 2012\n",
  "wordCount" : "2324",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/entropy/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Entropy
    </h1>
    <div class="post-meta">11 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#entropy" aria-label="Entropy">Entropy</a><ul>
                        
                <li>
                    <a href="#properties-of-the-entropy" aria-label="Properties of the entropy">Properties of the entropy</a><ul>
                        
                <li>
                    <a href="#axiomatic-approach" aria-label="Axiomatic approach">Axiomatic approach</a></li></ul>
                </li>
                <li>
                    <a href="#conditional-entropy" aria-label="Conditional Entropy">Conditional Entropy</a></li>
                <li>
                    <a href="#chain-rule" aria-label="Chain Rule">Chain Rule</a></li>
                <li>
                    <a href="#upper-bound" aria-label="Upper bound">Upper bound</a></li>
                <li>
                    <a href="#entropy-is-concave" aria-label="Entropy is concave">Entropy is concave</a></li>
                <li>
                    <a href="#functional-dependency" aria-label="Functional dependency">Functional dependency</a></li></ul>
                </li>
                <li>
                    <a href="#relative-entropy-or-kullback-leibler" aria-label="Relative Entropy or Kullback-Leibler">Relative Entropy or Kullback-Leibler</a><ul>
                        
                <li>
                    <a href="#kl-is-positive-or-null" aria-label="KL is positive or null">KL is positive or null</a></li>
                <li>
                    <a href="#kl-is-convex" aria-label="KL is convex">KL is convex</a></li></ul>
                </li>
                <li>
                    <a href="#mutual-information" aria-label="Mutual information">Mutual information</a><ul>
                        
                <li>
                    <a href="#definizione" aria-label="Definizione">Definizione</a></li>
                <li>
                    <a href="#propriet%c3%a0" aria-label="Proprietà">Proprietà</a></li></ul>
                </li>
                <li>
                    <a href="#sufficient-statistics" aria-label="Sufficient Statistics">Sufficient Statistics</a></li>
                <li>
                    <a href="#fanos-inequality" aria-label="Fano&rsquo;s inequality">Fano&rsquo;s inequality</a><ul>
                        
                <li>
                    <a href="#enunciato-fano" aria-label="Enunciato fano">Enunciato fano</a></li>
                <li>
                    <a href="#dimostrazione-fano" aria-label="Dimostrazione Fano">Dimostrazione Fano</a></li></ul>
                </li>
                <li>
                    <a href="#maximum-distribution-entropy" aria-label="Maximum Distribution entropy">Maximum Distribution entropy</a></li>
                <li>
                    <a href="#codewords" aria-label="Codewords">Codewords</a><ul>
                        
                <li>
                    <a href="#jensens-inequality" aria-label="Jensen&rsquo;s Inequality">Jensen&rsquo;s Inequality</a></li>
                <li>
                    <a href="#log-sum-inequality" aria-label="Log sum inequality">Log sum inequality</a></li>
                <li>
                    <a href="#krafts-inequality" aria-label="Krafts Inequality">Krafts Inequality</a></li>
                <li>
                    <a href="#source-coding-theorem-for-symbol-codes" aria-label="Source coding theorem for symbol codes">Source coding theorem for symbol codes</a></li>
                <li>
                    <a href="#gibbs-inequality" aria-label="Gibbs Inequality">Gibbs Inequality</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="entropy">Entropy<a hidden class="anchor" aria-hidden="true" href="#entropy">#</a></h3>
<p>Questo è stato creato da 1948 Shannon in <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">(Shannon 1948)</a>. Questa nozione è basata sulla nozione di probabilità, perché le cose rare sono più informative rispetto a qualcosa che accade spesso.
This is dependent on the notion of the <strong>Shannon information content</strong> defined as
</p>
$$
h(x = a_{i}) = \log_{2}\frac{1}{P(x = a_{i})}
$$
<p>
We will see that the entropy is a weighted average of the information, so the expected information content in a distribution. TODO: define this better.</p>
<p><a href="/notes/kolmogorov-complexity/">Kolmogorov complexity</a> è un modo diverso per definire la complessità.
Legato è <a href="/notes/neural-networks/#kullback-leibler-divergence">Neural Networks#Kullback-Leibler Divergence</a>.</p>
<p>We can model the classical view of entropy as the from [^1]</p>
<blockquote>
<p>Expected value of <strong>Surprisal</strong> which is the uncertainty of a random variable $X$ taking a certain value which is $p(X = x) = P(x)$, but we want to measure it using log-likelihood.</p>
</blockquote>
<p>With $\lvert \mathcal{X} \rvert < \infty$, $P_{X}(\cdot)$.
</p>
$$
H(\mathcal{X}) = - \sum_{x \in \mathcal{X}, P_{\mathcal{X}}(x) > 0} p(x)\log(p(x)) \tag{1.1}
$$
<p>
Ossia, possiamo dire in modo intuitivo quanto sarebbe sorprendente vedere che si avverasse quell&rsquo;evento.</p>
<img src="/images/notes/Entropy-20240918151806980.webp" alt="Entropy-20240918151806980">
This is the graph for the binary case:
$$
H(\mathcal{X}) = p\log \frac{1}{p} + (1- p) \log \frac{1}{1 - p}
$$
<p>Oppure come descritto da [^2]
</p>
$$
\begin{align*}
H(X) := E[I(X)] &= \sum_{i=1}^n P(x_i)I(x_i) \\
&= \sum_{i=1}^n p_i \log(1/p_i) \\
&= -\sum_{i=1}^n p_i \log(p_i) \tag{1.2}
\end{align*}
$$
<h4 id="properties-of-the-entropy">Properties of the entropy<a hidden class="anchor" aria-hidden="true" href="#properties-of-the-entropy">#</a></h4>
<p>One observation is that <strong>labels don&rsquo;t matter</strong>, we just need the <em>probability vector</em> and don&rsquo;t care about what it represents.</p>
<p>The entropy is <strong>always positive</strong>. It&rsquo;s easy to prove because all probabilities are $0 < p \leq 1$ so every term in the sum is positive because log in that interval is also positive.</p>
<h5 id="axiomatic-approach">Axiomatic approach<a hidden class="anchor" aria-hidden="true" href="#axiomatic-approach">#</a></h5>
<p>One could derive the entropy from an axiomatic point of view.
We just need three requirements:</p>
<ol>
<li>$H(X) \geq 0$ for every <strong>discrete</strong> r.v. which is related to the code length, which we don&rsquo;t want to be negative</li>
<li>$H(X, Y) = H(X) + H(Y)$ when $X, Y$ are independent random variables.</li>
<li>$H(X) \text{ is maximal }$ when $X \sim Unif(0, 1)$ this concept is related to <strong>hard to guess</strong> the results.
The professor in AML course has made examples of guessing what object has been seen, when both parties know the probabilities of every object.
One can prove that only a single function satisfies these functions.</li>
</ol>
<h4 id="conditional-entropy">Conditional Entropy<a hidden class="anchor" aria-hidden="true" href="#conditional-entropy">#</a></h4>
$$
H(Y|X) = \sum_{x \in \mathcal{X}}p(x) H(Y|X=x)
= \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log \frac{1}{P(y|x)}
= \mathbf{E}\left[ \log\frac{1}{p(Y|X)} \right] 
$$
<p>
La nozione con il valore atteso è la più semplice anche in questo caso.</p>
<h4 id="chain-rule">Chain Rule<a hidden class="anchor" aria-hidden="true" href="#chain-rule">#</a></h4>
<p>non fare.
Una proprietà random è
</p>
$$
H(X, Y) = H(X) + H(X|Y)
$$
<p>
La dimostrazione è abbastanza banale una volta che si conoscono le definizioni&hellip;.
La cosa interessante è che si può generale per qualunque numero di variabili aleatorie:
</p>
$$
H(X_{0}, X_{1}, \dots, X_{i}) = \sum_{i}H(X_{i}|X_{i-1}\dots X_{0})
$$
<h4 id="upper-bound">Upper bound<a hidden class="anchor" aria-hidden="true" href="#upper-bound">#</a></h4>
$$
H(X) \leq \log \lvert \mathcal{X} \rvert 
$$
<p>
Con $\mathcal{X}$ l&rsquo;insieme immagine della variabile aleatoria <strong>discreta</strong> $X$. Importante in questo caso che la nostra variabile sia discreta, altrimenti il teorema provvisto in <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">(Cover &amp; Thomas 2012)</a> 2.6.4 non funziona.
Non è molto banale l&rsquo;idea di utilizzare la uniforme per modellare il numero di elementi. e usare la positività di KL per finire l&rsquo;upper bound.</p>
<p>We can prove the equality by having an uniform distribution. The computation is easy:
If $\mathcal{X}$ is uniform then:
</p>
$$
P_{X}(x) = \frac{1}{\lvert \mathcal{X} \rvert }
$$
<p>
And so we have
</p>
$$
\sum P_{X}(x) \log \frac{1}{P_{X}(x)} = \log \frac{1}{P_{X}(x)} = \log \lvert X \rvert 
$$
<p><strong>Proving the upper bound</strong>
Let&rsquo;s take $D(P_{X} \mid \mid U)$ into consideration, then we have that this value is
</p>
$$
\sum P_{X}(x) \log \frac{P_{X}(x)}{\frac{1}{\lvert \mathcal{X} \rvert }}
= \sum P_{X} \log P(x) + \sum P_{X}(x) \log \lvert \mathcal{X} \rvert 
= \log \lvert \mathcal{X} \rvert  - H(X)
$$
<p>
And by knowing that the Kullback Leibler divergence is positive we know that
</p>
$$
\log \lvert \mathcal{X} \rvert  - H(X) \geq 0
$$
<p>
Which ends the proof.</p>
<h4 id="entropy-is-concave">Entropy is concave<a hidden class="anchor" aria-hidden="true" href="#entropy-is-concave">#</a></h4>
<p>Uso l&rsquo;upper bound e il fatto che KL è convesso per dimostrare questa cosa.</p>
<h4 id="functional-dependency">Functional dependency<a hidden class="anchor" aria-hidden="true" href="#functional-dependency">#</a></h4>
<p>Non fare
Se $Y = f(X)$ per qualche funzione, allora $H(Y|X) = H(X|Y) = 0$ si può risolvere con qualche ragionamento sul supporto di entropia.
Interessante vedere che ha una piccola relazione con <a href="/notes/normalizzazione-dei-database/#dipendenze-funzionali">Normalizzazione dei database#Dipendenze funzionali</a>.</p>
<h3 id="relative-entropy-or-kullback-leibler">Relative Entropy or Kullback-Leibler<a hidden class="anchor" aria-hidden="true" href="#relative-entropy-or-kullback-leibler">#</a></h3>
<p>Let&rsquo;s take
$\lvert \mathcal{X} \rvert < \infty$, and take distributions P, Q, $\mathcal{X} \to \mathbb{R}$ such that  $p(x) \geq 0 \forall x \in \mathcal{X}$ and the sum is 1, same thing for $Q$, then we define the Kullback-Leibler Divergence between those distributions to be</p>
$$
D(P \mid \mid Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}
$$
<p>
We need to define some corner cases:</p>
<ul>
<li>If $P(x) = 0$ and $Q$ is anything then its 0</li>
<li>If $\exists \xi \in \mathcal{X}$ such that $P(\xi) > 0$ and $Q(\xi) = 0$ =&gt; $D(P \mid \mid Q) = + \infty$.</li>
<li>If $P \ll Q$ then $D(P \mid \mid Q) < \infty$ (I did not understand why)</li>
</ul>
<p>This has some relations with the entropy, we can use some log properties and have the following result:</p>
$$
D(P \mid \mid Q) = - H(P) - \sum P(x) \log Q(x)
$$
<p>
The second addendum can be called <em>cross-entropy</em>.</p>
<p>In modo praticamente equivalente possiamo definire una versione condizionata. e si può applicare anche in questo caso una chain rule</p>
$$
DL(P(x, y) \mid\mid Q(x, y) = DL(P(x) \mid\mid Q(x)) + DL(P(x|y) \mid\mid Q(x|y))
$$
<p>Relative entropy <strong>is not a distance</strong> not a metric, so its incorrect to say it is a distance, but for practical purposes it seems to work well: if the Relative entropy is small also the probability vectors are small.</p>
<h4 id="kl-is-positive-or-null">KL is positive or null<a hidden class="anchor" aria-hidden="true" href="#kl-is-positive-or-null">#</a></h4>
<p>Ossia per ogni distribuzione $p$ o $q$ si ha che
</p>
$$
DL(P \mid\mid Q) \geq 0
$$
<p>
Con uguaglianza se hanno esattamente la stessa distribuzione.
We have that $D(P \mid \mid Q) = 0 \iff P = Q$ .</p>
<p>E ricordandoci che $\log$ è una funzione concava, quindi si può utilizzare <a href="/notes/analisi-di-convessit%C3%A0/#jensen">Jensen</a>.
Lo dimostriamo ora in breve.
Sappiamo che la funzione $-\log(x) = \log\left( \frac{1}{x} \right)$ è una funzione convessa, perché il negativo di una funzione concava, che è  il logaritmo.</p>
<p>Allora consideriamo $\frac{1}{u} = \frac{Q(x)}{P(x)}$ che è la parte dentro al logaritmo perché così possiamo usare Jensen
Allora comunque abbiamo</p>
$$
\sum_{x} P(x) \log\left( \frac{Q(x)}{P(x)} \right) \geq \log\left( \sum_{x} P(x) \cdot \frac{Q(x)}{P(x)} \right) = \log(1) = 0
$$
<p>Che conclude che
</p>
$$
D_{KL}(P \mid \mid Q) \geq 0
$$
<p>
In modo facile.</p>
<h4 id="kl-is-convex">KL is convex<a hidden class="anchor" aria-hidden="true" href="#kl-is-convex">#</a></h4>
<p>$DL(p\mid\mid q)$ è convesso sulla coppia $(p, q)$, 2.7.2 di <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">(Cover &amp; Thomas 2012)</a>.
Anche sula 2.26 di McKay è buono, anche se non esattamente parla di questo.</p>
<h3 id="mutual-information">Mutual information<a hidden class="anchor" aria-hidden="true" href="#mutual-information">#</a></h3>
<p>Questa nozione definisce quanta informazione hanno in comune due variabili aleatorie</p>
<h4 id="definizione">Definizione<a hidden class="anchor" aria-hidden="true" href="#definizione">#</a></h4>
$$
I(X;Y) = \sum_{x}\sum_{y} p(x, y) \log\left(  \frac{p(x, y)}{p(x)p(y)} \right)
= H(X) - H(X|Y)
$$
<p>
Si può fare dopo un po&rsquo; di calcoli che qui ho omesso, ma non dovrebbe essere difficile farlo.888</p>
<p>Si può intendere la mutual information anche come KL fra le distribuzioni $p(x, y)$ e $p(x)p(y)$ si può notare che queste due sono uguali quando le due sono indipendenti, che è coerente con la nostra nozione che abbiamo dell&rsquo;indipendenza.</p>
<h4 id="proprietà">Proprietà<a hidden class="anchor" aria-hidden="true" href="#proprietà">#</a></h4>
<img src="/images/notes/Entropy-20240229150751912.webp" alt="Entropy-20240229150751912">
<img src="/images/notes/Entropy-20240229150807093.webp" alt="Entropy-20240229150807093">
<h3 id="sufficient-statistics">Sufficient Statistics<a hidden class="anchor" aria-hidden="true" href="#sufficient-statistics">#</a></h3>
<p>Possiamo rappresentare il sampling da una certa famiglia di distribuzioni $f_{\theta}(x)$ , rappresentato da $X$, e una sua statistica a caso (media varianza etc, che credo basti una funzione sul valore) come T, allora possiamo rappresentarlo come una <a href="/notes/markov-chains/#catena-di-3-variabili">Markov Chains#Catena di 3 variabili</a> $\theta \to X \to T(X)$
E vale il teorema di information processing</p>
$$
I(\theta; T(X)) \leq I(\theta; X)
$$
<p>
Si può chiamare una statistica per $\theta$ sufficiente se $X$ contiene tutta l&rsquo;informazione di $\theta$. Non so bene cosa significhi.
La cosa importante è che la statistica sufficiente <strong>preserva la mutua informazione</strong> ossia si ha una uguaglianza in quella relazione di sopra. Vedere 2.9 di <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">(Cover &amp; Thomas 2012)</a> per esempi .</p>
<p>Questa cosa potrebbe permettere di dire che usando quella statistica io posso dimenticarmi del parametro, perché riesco a ricavarmelo senza problemi credo&hellip;.</p>
<blockquote>
<p>The purpose of sufficiency is to demonstrate that statistics that satisfy this property do not discard information about the parameter, and as such, estimators that might be based on a sufficient statistic are in a sense &ldquo;good&rdquo; ones to choose.</p>
</blockquote>
<p>Da <a href="https://math.stackexchange.com/questions/1186645/understanding-sufficient-statistic.">https://math.stackexchange.com/questions/1186645/understanding-sufficient-statistic.</a></p>
<h3 id="fanos-inequality">Fano&rsquo;s inequality<a hidden class="anchor" aria-hidden="true" href="#fanos-inequality">#</a></h3>
<p>L&quot;idea principale è utilizzare una variabile aleatoria per stimarne una altra, usando l&rsquo;entropia condizionale fra le due.</p>
<h4 id="enunciato-fano">Enunciato fano<a hidden class="anchor" aria-hidden="true" href="#enunciato-fano">#</a></h4>
<p>Per ogni estimantore $\hat{X}$ tale per cui $X \to Y \to \hat{X}$ sia una catena di markov, e con $P_{e} = Pr(X \not= \hat{X})$ ossia la probabilità di errore, abbiamo che vale
</p>
$$
H(P_{e}) + P_{e}\log \lvert \mathcal{X} \rvert \geq H(X|\hat{X}) \geq H(X|Y)
$$
<p>
Ci sono forme più deboli che possiamo considerare in un certo senso corollari, ossia che
</p>
$$
1 + P_{e} \log \lvert \mathcal{X} \rvert  \geq H(X|Y)
$$
<h4 id="dimostrazione-fano">Dimostrazione Fano<a hidden class="anchor" aria-hidden="true" href="#dimostrazione-fano">#</a></h4>
<p>Questa è una bomba da fare. Poi però ha un sacco di conseguenze non applicabili in modo immediato (cioè non ci arrivi subito se non le fai un po&rsquo; prima).</p>
<h3 id="maximum-distribution-entropy">Maximum Distribution entropy<a hidden class="anchor" aria-hidden="true" href="#maximum-distribution-entropy">#</a></h3>
<p>Un problema classico nella teoria dell&rsquo;informazione è trovare la distribuzione che massimizzi l&rsquo;entropia (quindi l&rsquo;informazione contenuta credo) dati certe conoscenze a priori,
Ossia data una funzione $f$  e certe condizioni che deve rispettare, massimizzare l&rsquo;entropia.</p>
<p>SI può dimostrare (lo si può vedere da una reference di sopra) che la distribuzione che massimizza l&rsquo;entropia, avendo solamente la condizione di probabilità, ossia che $\sum_{x}p(x) = 1$ è la distribuzione uniforme.
Mentre se assumo anche media $\mu$ e varianza $\sigma^{2}$ allora è la gaussiana.
In un certo senso possiamo dire che queste distribuzioni sono molto ricche di informazioni.</p>
<h3 id="codewords">Codewords<a hidden class="anchor" aria-hidden="true" href="#codewords">#</a></h3>
<h4 id="jensens-inequality">Jensen&rsquo;s Inequality<a hidden class="anchor" aria-hidden="true" href="#jensens-inequality">#</a></h4>
<p>Questo è un teorema fondamentale per moltissime cose, e da un certo punto di vista è una cosa banale per le cose convesse/concave.
Allora, sia data una funzione in $[a, b]$ tale che sia convessa (concava) in questo intervallo, allora vale che
</p>
$$
f\left( \sum_{i} \lambda_{i} x_{i} \right) \leq \sum_{i}\lambda_{i}f(x_{i})
$$
<p>
Con $\sum_{i}\lambda_{i} = 1$. Questa cosa si estende in modo molto semplice a variabili aleatorie e $E$ quando al posto di $\lambda_{i}$ mettiamo una probabilità in un punto.</p>
<p>La dimostrazione non dovrebbe essere molto difficile. La strategia è utilizzare l&rsquo;induzione in modo abbastanza classico. Non so in che modo si estende su funzioni continue, ma quelle sono cose tecniche matematiche non interessantissime.</p>
<h4 id="log-sum-inequality">Log sum inequality<a hidden class="anchor" aria-hidden="true" href="#log-sum-inequality">#</a></h4>
<p>Siano $a_{1}, a_{2}, \dots a_{n}$ e $b_{1}, b_{2}, \dots, b_{n}$ numeri non negativi, allora vale che
</p>
$$
\sum_{i=1}^{n}a_{i} \log\left( \frac{a_{i}}{b_{i}} \right) \geq \left( \sum_{i=1}^{n}a_{i} \right)\log \frac{\left( \sum_{i=1}^{n} a_{i} \right)}{\sum_{i=1}^{n}b_{i}}
$$
<p>
Con uguaglianza se vale che $\forall i, \frac{a_{i}}{b_{i}}= const$</p>
<h4 id="krafts-inequality">Krafts Inequality<a hidden class="anchor" aria-hidden="true" href="#krafts-inequality">#</a></h4>
<p><a href="https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality">https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality</a>
Questo teorema interessa cose dei codewords, perché ci interessano dei <strong>set</strong> di prefixfree che sono molto più gestibili probabilmente dal punto di vista dell&rsquo;interpretazione.
La cosa interessante è:</p>
<p>Siano $l_{1}, l_{2}, l_{3}, \dots, l_{n}$ lunghezze di code-words all&rsquo;interno del nostro alfabeto, allora vale che esistono dei code-words (stringhe binarie) che hanno quelle lunghezze <strong>se e solo se</strong> viene soddisfatta la proprietà
</p>
$$
\sum_{x} 2^{-l(x)} \leq 1
$$
<p>Il motivo è abbastanza semplice, questo si spiega in modo grafico in maniera praticamente immediata quando facciamo il disegno.
Si può vedere dall&rsquo;albero binario corrispondente di un insieme di set binari con prefissi che se un parente è scelto (colorato nel disegno), allora nessun discendente può essere scelto perché altrimenti avresti un prefisso. Inoltre se colori quelli sopra, significa che al massimo se sommi tutti quei valori otterrai 1 sse hai utilizzato tutti i rami a tua disposizione (meaning, che non puoi scegliere altri code-work, altrimenti perdi la prefix property).
<img src="/images/notes/Entropy-20240212162407876.webp" alt="Entropy-20240212162407876"></p>
<h4 id="source-coding-theorem-for-symbol-codes">Source coding theorem for symbol codes<a hidden class="anchor" aria-hidden="true" href="#source-coding-theorem-for-symbol-codes">#</a></h4>
<p>Chiamata anche come la <strong>relazione con Kolmogorov</strong>.
1.11.3 di <a href="http://link.springer.com/10.1007/978-3-030-11298-1">(Li &amp; Vitányi 2019)</a>, allora se prendiamo un set di code-words con $L$ il minimo prefix code che possiamo mai avere
Ossia $L = \sum_{x} P(x)l(x)$, con $l$ scelto il minimo possibile. Allora vale che
</p>
$$
H(P) \leq L \leq H(P) + 1
$$
<p>
Ossia la lunghezza migliore possibile è boundata da valori di entropia. Che è una cosa abbastanza forte perché relaziona come deve essere fatto il code-words, con la complessità dell&rsquo;informazione che vogliamo andare a utilizzare.
La dimostrazione non la facciamo qui, ma è fattibile con le tue conoscenze credo, ti serve la Gibbs inequality qui sotto per una freccia</p>
<p><strong>Dimostrazione</strong>
Prendiamo $q_{i} = 2^{-l_{i}}$, allora abbiamo $l_{i} = \log\left( \frac{1}{q_{i}} \right)$ vale
</p>
$$
L = \sum_{x} p(x) l(x) = \sum_{x} \left( p(x) \log\left( \frac{1}{q(x)} \right) \right)
\geq  \sum_{x}p(x) \log\left( \frac{1}{p(x)} \right) =H(x)
$$
<p>Dove abbiamo usato anche l&rsquo;ineguaglianza di Gibbs <a href="/notes/entropy/#gibbs-inequality">#Gibbs Inequality</a> e il fatto che vale <a href="/notes/entropy/#krafts-inequality">#Krafts Inequality</a>.</p>
<p>Provando a dimostrare l&rsquo;altro bound, supponiamo che
</p>
$$
l_{i} = \lceil -\log_{2}(p_{i}) \rceil 
$$
<p>
Si può dimostrare che questo è un prefix code, perché soddisfa Kraft.
Allora si può notare come
</p>
$$
L = \sum_{x} p(x) l(x) = \sum_{x} p(x) \lceil -\log_{2}(p_{x}) \rceil \leq \sum_{x}p(x) \left( \log_{2}\left( \frac{1}{p(x)}\right) +1 \right) = H(x) + 1
$$
<p>
Una nota interessante è questo teorema ci permette di definire un concetto di <em>efficienza di rappresentazione</em>. Tutto quanto dato da KL divergence è una specie di inefficienza.</p>
<p>Infatti possiamo scrivere
</p>
$$
L = H(x) + D_{KL}(P \mid \mid Q)
$$
<p>
Con $Q$ la probabilità associata alle singole codewords, assumendo che siano uniformi e simili per dire.</p>
<h4 id="gibbs-inequality">Gibbs Inequality<a hidden class="anchor" aria-hidden="true" href="#gibbs-inequality">#</a></h4>
<p>Afferma che l&rsquo;entropia è minore rispetto alla cross-entropy di qualunque cosa, ossia
</p>
$$
\sum_{x} P(x) \log\left(  \frac{1}{P(x)} \right) \leq \sum_{x} P(x) \log\left( \frac{1}{Q(x)} \right)
$$
<p>
Qualunque sia l&rsquo;altra distribuzione.
Si può dimostrare in modo abbastanza diretto utilizzando il fatto che la Kullback Leibler divergence, presentato in <a href="/notes/neural-networks/">Neural Networks</a>, è sempre positiva o uguale a 0.
Infatti la parte di sopra si può riscrivere come</p>
$$
-\sum_{x} P(x) \log\left( \frac{P(x)}{Q(x)} \right) = D_{KL}(P \mid \mid Q)
$$
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Li &amp; Vitányi <a href="http://link.springer.com/10.1007/978-3-030-11298-1">“An Introduction to Kolmogorov Complexity and Its Applications”</a> Springer International Publishing 2019</p>
<p>[2] Shannon <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">“A Mathematical Theory of Communication”</a> The Bell System Technical Journal Vol. 27, pp. 379&ndash;423, 623&ndash;656 1948</p>
<p>[3] Cover &amp; Thomas <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">“Elements of Information Theory”</a> John Wiley &amp; Sons 2012</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/information-theory/">✏Information-Theory</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on x"
            href="https://x.com/intent/tweet/?text=Entropy&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f&amp;hashtags=%e2%9c%8finformation-theory">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f&amp;title=Entropy&amp;summary=Entropy&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f&title=Entropy">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on whatsapp"
            href="https://api.whatsapp.com/send?text=Entropy%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on telegram"
            href="https://telegram.me/share/url?text=Entropy&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Entropy&u=https%3a%2f%2fflecart.github.io%2fnotes%2fentropy%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
