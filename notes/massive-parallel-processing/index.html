<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Massive Parallel Processing | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="📓big-data">
<meta name="description" content="We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.
Introduction Common input formats You need to know well what
Shards Textual input binary, parquet and similars CSV and similars General framework We can see in the image taken from the course book [({fourny} 2024)](https://ghislainfourny.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/massive-parallel-processing/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/massive-parallel-processing/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Massive Parallel Processing" />
<meta property="og:description" content="We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.
Introduction Common input formats You need to know well what
Shards Textual input binary, parquet and similars CSV and similars General framework We can see in the image taken from the course book [({fourny} 2024)](https://ghislainfourny." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/massive-parallel-processing/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Massive Parallel Processing"/>
<meta name="twitter:description" content="We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.
Introduction Common input formats You need to know well what
Shards Textual input binary, parquet and similars CSV and similars General framework We can see in the image taken from the course book [({fourny} 2024)](https://ghislainfourny."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Massive Parallel Processing",
      "item": "https://flecart.github.io/notes/massive-parallel-processing/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Massive Parallel Processing",
  "name": "Massive Parallel Processing",
  "description": "We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.\nIntroduction Common input formats You need to know well what\nShards Textual input binary, parquet and similars CSV and similars General framework We can see in the image taken from the course book [({fourny} 2024)](https://ghislainfourny.",
  "keywords": [
    "📓big-data"
  ],
  "articleBody": "We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.\nIntroduction Common input formats You need to know well what\nShards Textual input binary, parquet and similars CSV and similars General framework We can see in the image taken from the course book [({fourny} 2024)](https://ghislainfourny.github.io/big-data-textbook/), that there are two general phases for the map reduce framework: 1. First phase is mapping, where the data is divided into chunks and processed by the mappers, then these are delivered to the designated reducers. 2. The reduce process connects to all other machines to get the their own mapped value. This is usually the bottleneck, the part that runs in $\\mathcal{O}(n^{2})$. 3. Reducers then processed the partitioned, probably homogeneous data and return a result, which is often a file in HDFS. MapReduce Architecture We can process terabytes of data with thousands of nodes with this architecture.\nCentralized Architecture Hadoop MapReduce builds upon a centralized architecture compatible to Distributed file systems and HBase. In this case there is a Jobtracker, often in the same node as the NameNode; and Tasktrackers that are in the same node as the DataNodes. We have now three processes on the same machine. When MapReduce finishes, usually every TaskTracker produces a file in the disk (usually sharded, numerated increasingly). If the task fails for a node, the Jobtracker usually re-assigns the node to another.\nImpendance Mismatch MapReduce needs to have key-values to process the files. For text, we usually use the lines and its character offset to index them. We could also use a special character to separate different parts of the text. Often this part part of preprocessing the dataset so that it is understandable by mapreduce is called impendance mismatch\nCombining This is a form of optimization. When we are mapping, we can do part of the reducing step. Often the combining step is the same as the reducing step. But this is not often possible:\nType must be the same for reduce function. We need to have commutative and associative functions for reduce. After the combining step we have the following diagram: And there will be less overload on the network, which makes the whole thing faster.\nRule of thumb: tasks are 3x the number of nodes that are mapping and reducing.\nOn Terminology You need to know what maps, reducer and combiners are. You also need to know what tasks, slots and phases, are.\nSlots are bundles of CPU cores, memory resources, and network bandwidth. Usually, a single node could have more than one slot, which helps to handle the different needs for resources of different slots (some could need more memory, while others on the same node could need less memory, this better balances it). TODO: write this section better.\nSplits and Blocks Splits are map or reduce partition sets of the whole data that needs to be processed. This is physically stored in HDFS blocks. These blocks are exactly of size 128MB, which implies the first and last key-values could be splitted in different blocks. This is an inconvenience as one needs to fetch also the preceding and successive blocks. This is why in HDFS’s api one can read blocks partially. The underlying details on how these optimizations are done are not covered here.\nResource Management Bottlenecks of the original version Jobtrackers were the bottleneck and have many responsibilities.\nSome responsibilities of the Jobtracker\nResource Management Monitoring every task’s progress Job scheduling This is usually a bottleneck in this model. Clusters of about 4,000 become usually very very slow and about 10,000 tasks. Another drawback is that many reducers are idle during the map phase. Containers are virtual collections of slots (one container could have more than one map or reduce slot), each with some cpu and some amount of memory.\nApplicationMaster This is an innovation introduced by the YARN architecture. With this new architecture we have containers and a resource manager that allocates resources to the tasks that need to be processed. When a user starts a task, the resource manager elevates a container to be the application master by communicating with a proc. This container then takes part of the responsibilities of the jobtracker in the old version; it now tracks task state, re allocates other containers when one fails, it asks or releases resources to the resource manager.\nSetting up a task ApplicationMasters can request and release containers at any time, dynamically. A container request is typically made by the Application- Masters with a specific demand (e.g., “10 containers with each 2 cores and 16 GB of RAM”). If the request is granted by the ResourceMan- ager fully or partially, this is done indirectly by signing and issuing a container token to the ApplicationMaster that acts as proof that the resource was granted. The ApplicationMaster can then connect to the allocated Node- Manager and send the token. The NodeManager will then check the validity of the token and provide the memory and CPU granted by the ResourceManager. The ApplicationMaster ships the code (e.g., as a jar file) as well as parameters, which then runs as a process with exclusive use of this memory and CPU. ~From the Book ({fourny} 2024)\nPerformance This new version supports 10,000 nodes and 100,000 tasks, a big improvement over the previous version.\nScheduling methods Queue scheduler For example, the queue scheduler where a job takes the whole cluster one after the other. But it’s probably not efficient as it doesn’t allow for multiple-tenancy of the cluster.\nCapacity Scheduler Another way could be the hierarchical scheduler where the amount of resource is allocated a priori. This is also called capacity scheduling because the whole cluster is divided into multiple sub-clusters corresponding to a specific department in a university or parts of a company\nFair Schedule It’s difficult to orchestrate the allocation of different resources. But how to allocate it correctly when the need of resources is different for other processes? Some processes could need more memory than cpu, Disk and Network Input and Output and others other resources. Some solution is dominant resource fairness, but we won’t delve into this topic here. 🟥 TODO: get the missing parts for dominant resource fairness.\nReferences [1] {fourny} “The Big Data Textbook” 2024\n",
  "wordCount" : "1066",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/massive-parallel-processing/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Massive Parallel Processing
    </h1>
    <div class="post-meta">6 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a><ul>
                        
                <li>
                    <a href="#common-input-formats" aria-label="Common input formats">Common input formats</a></li>
                <li>
                    <a href="#general-framework" aria-label="General framework">General framework</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#mapreduce-architecture" aria-label="MapReduce Architecture">MapReduce Architecture</a><ul>
                        <ul>
                        
                <li>
                    <a href="#centralized-architecture" aria-label="Centralized Architecture">Centralized Architecture</a></li>
                <li>
                    <a href="#impendance-mismatch" aria-label="Impendance Mismatch">Impendance Mismatch</a></li>
                <li>
                    <a href="#combining" aria-label="Combining">Combining</a></li>
                <li>
                    <a href="#on-terminology" aria-label="On Terminology">On Terminology</a></li>
                <li>
                    <a href="#splits-and-blocks" aria-label="Splits and Blocks">Splits and Blocks</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#resource-management" aria-label="Resource Management">Resource Management</a><ul>
                        <ul>
                        
                <li>
                    <a href="#bottlenecks-of-the-original-version" aria-label="Bottlenecks of the original version">Bottlenecks of the original version</a></li></ul>
                    
                <li>
                    <a href="#applicationmaster" aria-label="ApplicationMaster">ApplicationMaster</a><ul>
                        
                <li>
                    <a href="#setting-up-a-task" aria-label="Setting up a task">Setting up a task</a></li>
                <li>
                    <a href="#performance" aria-label="Performance">Performance</a></li></ul>
                </li>
                <li>
                    <a href="#scheduling-methods" aria-label="Scheduling methods">Scheduling methods</a><ul>
                        
                <li>
                    <a href="#queue-scheduler" aria-label="Queue scheduler">Queue scheduler</a></li>
                <li>
                    <a href="#capacity-scheduler" aria-label="Capacity Scheduler">Capacity Scheduler</a></li>
                <li>
                    <a href="#fair-schedule" aria-label="Fair Schedule">Fair Schedule</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.</p>
<h3 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h3>
<h4 id="common-input-formats">Common input formats<a hidden class="anchor" aria-hidden="true" href="#common-input-formats">#</a></h4>
<p>You need to know well what</p>
<ul>
<li>Shards</li>
<li>Textual input</li>
<li>binary, parquet and similars</li>
<li>CSV and similars</li>
</ul>
<h4 id="general-framework">General framework<a hidden class="anchor" aria-hidden="true" href="#general-framework">#</a></h4>
<img src="/images/notes/Massive Parallel Processing-20241030093405625.webp" alt="Massive Parallel Processing-20241030093405625">
We can see in the image taken from the course book [({fourny} 2024)](https://ghislainfourny.github.io/big-data-textbook/), that there are two general phases for the map reduce framework:
1. First phase is mapping, where the data is divided into chunks and processed by the mappers, then these are delivered to the designated reducers.
2. The reduce process connects to all other machines to get the their own mapped value. This is usually the bottleneck, the part that runs in $\mathcal{O}(n^{2})$.
3. Reducers then processed the partitioned, probably homogeneous data and return a result, which is often a file in HDFS.
<h2 id="mapreduce-architecture">MapReduce Architecture<a hidden class="anchor" aria-hidden="true" href="#mapreduce-architecture">#</a></h2>
<p>We can process terabytes of data with thousands of nodes with this architecture.</p>
<h4 id="centralized-architecture">Centralized Architecture<a hidden class="anchor" aria-hidden="true" href="#centralized-architecture">#</a></h4>
<p>Hadoop MapReduce builds upon a centralized architecture compatible to <a href="/notes/distributed-file-systems/">Distributed file systems</a> and <a href="/notes/wide-column-storage/">HBase</a>. In this case there is a <strong>Jobtracker</strong>, often in the same node as the <strong>NameNode</strong>; and <strong>Tasktrackers</strong> that are in the same node as the <strong>DataNodes</strong>. We have now three processes on the same machine.
When MapReduce finishes, usually every TaskTracker produces a file in the disk (usually sharded, numerated increasingly).
If the task fails for a node, the Jobtracker usually re-assigns the node to another.</p>
<h4 id="impendance-mismatch">Impendance Mismatch<a hidden class="anchor" aria-hidden="true" href="#impendance-mismatch">#</a></h4>
<p>MapReduce needs to have key-values to process the files.
For text, we usually use the <strong>lines</strong> and its character offset to index them. We could also use a special character to separate different parts of the text.
Often this part part of preprocessing the dataset so that it is understandable by mapreduce is called impendance mismatch</p>
<h4 id="combining">Combining<a hidden class="anchor" aria-hidden="true" href="#combining">#</a></h4>
<p>This is a form of optimization. When we are mapping, we can do part of the reducing step. Often the combining step is the same as the reducing step. But this is not often possible:</p>
<ul>
<li>Type must be the same for reduce function.</li>
<li>We need to have commutative and associative functions for reduce.</li>
</ul>
<p>After the combining step we have the following diagram:
<img src="/images/notes/Massive Parallel Processing-20241103195030645.webp" alt="Massive Parallel Processing-20241103195030645">
And there will be less overload on the network, which makes the whole thing faster.</p>
<p>Rule of thumb: tasks are 3x the number of nodes that are mapping and reducing.</p>
<h4 id="on-terminology">On Terminology<a hidden class="anchor" aria-hidden="true" href="#on-terminology">#</a></h4>
<p>You need to know what maps, reducer and combiners are. You also need to know what tasks, slots and phases, are.</p>
<p>Slots are bundles of CPU cores, memory resources, and network bandwidth. Usually, a single node could have more than one slot, which helps to handle the different needs for resources of different slots (some could need more memory, while others on the same node could need less memory, this better balances it).
TODO: write this section better.</p>
<h4 id="splits-and-blocks">Splits and Blocks<a hidden class="anchor" aria-hidden="true" href="#splits-and-blocks">#</a></h4>
<p>Splits are map or reduce partition sets of the whole data that needs to be processed. This is physically stored in HDFS blocks.
These blocks are exactly of size 128MB, which implies the first and last key-values could be splitted in different blocks. This is an inconvenience as one needs to fetch also the preceding and successive blocks.
This is why in HDFS&rsquo;s api one can read blocks partially. The underlying details on how these optimizations are done are not covered here.</p>
<h2 id="resource-management">Resource Management<a hidden class="anchor" aria-hidden="true" href="#resource-management">#</a></h2>
<h4 id="bottlenecks-of-the-original-version">Bottlenecks of the original version<a hidden class="anchor" aria-hidden="true" href="#bottlenecks-of-the-original-version">#</a></h4>
<p>Jobtrackers were the bottleneck and have many responsibilities.</p>
<p>Some responsibilities of the Jobtracker</p>
<ul>
<li>Resource Management</li>
<li>Monitoring every task&rsquo;s progress</li>
<li>Job scheduling
This is usually a bottleneck in this model. Clusters of about 4,000 become usually very very slow and about 10,000 tasks.
Another drawback is that many reducers are idle during the map phase.</li>
</ul>
<p>Containers are virtual collections of slots (one container could have more than one map or reduce slot), each with some cpu and some amount of memory.</p>
<h3 id="applicationmaster">ApplicationMaster<a hidden class="anchor" aria-hidden="true" href="#applicationmaster">#</a></h3>
<p>This is an innovation introduced by the YARN architecture. With this new architecture we have <strong>containers</strong> and a resource manager that allocates resources to the tasks that need to be processed.
When a user starts a task, the resource manager elevates a container to be the <strong>application master</strong> by communicating with a proc. This container then takes part of the responsibilities of the jobtracker in the old version; it now tracks task state, re allocates other containers when one fails, it asks or releases resources to the resource manager.</p>
<h4 id="setting-up-a-task">Setting up a task<a hidden class="anchor" aria-hidden="true" href="#setting-up-a-task">#</a></h4>
<blockquote>
<p>ApplicationMasters can request and release containers at any time,
dynamically. A container request is typically made by the Application-
Masters with a specific demand (e.g., “10 containers with each 2 cores
and 16 GB of RAM”). If the request is granted by the ResourceMan-
ager fully or partially, this is done indirectly by signing and issuing a
container token to the ApplicationMaster that acts as proof that the
resource was granted.
The ApplicationMaster can then connect to the allocated Node-
Manager and send the token. The NodeManager will then check the
validity of the token and provide the memory and CPU granted by the
ResourceManager. The ApplicationMaster ships the code (e.g., as a jar
file) as well as parameters, which then runs as a process with exclusive
use of this memory and CPU. <em>~From the Book <a href="https://ghislainfourny.github.io/big-data-textbook/">({fourny} 2024)</a></em></p>
</blockquote>
<h4 id="performance">Performance<a hidden class="anchor" aria-hidden="true" href="#performance">#</a></h4>
<p>This new version supports 10,000 nodes and 100,000 tasks, a big improvement over the previous version.</p>
<h3 id="scheduling-methods">Scheduling methods<a hidden class="anchor" aria-hidden="true" href="#scheduling-methods">#</a></h3>
<h4 id="queue-scheduler">Queue scheduler<a hidden class="anchor" aria-hidden="true" href="#queue-scheduler">#</a></h4>
<p>For example, the <strong>queue scheduler</strong> where a job takes the whole cluster one after the other. But it&rsquo;s probably not efficient as it doesn&rsquo;t allow for multiple-tenancy of the cluster.</p>
<h4 id="capacity-scheduler">Capacity Scheduler<a hidden class="anchor" aria-hidden="true" href="#capacity-scheduler">#</a></h4>
<p>Another way could be the <strong>hierarchical scheduler</strong> where the amount of resource is allocated a priori. This is also called <strong>capacity scheduling</strong> because the whole cluster is divided into multiple sub-clusters corresponding to a specific department in a university or parts of a company</p>
<h4 id="fair-schedule">Fair Schedule<a hidden class="anchor" aria-hidden="true" href="#fair-schedule">#</a></h4>
<p>It&rsquo;s difficult to orchestrate the allocation of different resources.
But how to allocate it correctly when the need of resources is different for other processes? Some processes could need more memory than cpu, Disk and Network Input and Output and others other resources. Some solution is <strong>dominant resource fairness</strong>, but we won&rsquo;t delve into this topic here.
🟥 TODO: get the missing parts for dominant resource fairness.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] {fourny} <a href="https://ghislainfourny.github.io/big-data-textbook/">“The Big Data Textbook”</a>  2024</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/big-data/">📓Big-Data</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on x"
            href="https://x.com/intent/tweet/?text=Massive%20Parallel%20Processing&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f&amp;hashtags=%f0%9f%93%93big-data">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f&amp;title=Massive%20Parallel%20Processing&amp;summary=Massive%20Parallel%20Processing&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f&title=Massive%20Parallel%20Processing">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on whatsapp"
            href="https://api.whatsapp.com/send?text=Massive%20Parallel%20Processing%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on telegram"
            href="https://telegram.me/share/url?text=Massive%20Parallel%20Processing&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Massive%20Parallel%20Processing&u=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
