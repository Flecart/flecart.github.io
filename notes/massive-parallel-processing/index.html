<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Massive Parallel Processing | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="üììbig-data">
<meta name="description" content="We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.
Introduction
Common input formats
You need to know well what

Shards
Textual input
binary, parquet and similars
CSV and similars

Sharding
It is a common practice to divide a big dataset into chunks (or shards), smaller parts which recomposed give the original dataset.
For example, in Cloud Storage settings we often divide big files into chunks, while in Distributed file systems the system automatically divides big files into native files of maximum 10 GB size.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/massive-parallel-processing/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/massive-parallel-processing/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/massive-parallel-processing/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Massive Parallel Processing">
  <meta property="og:description" content="We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.
Introduction Common input formats You need to know well what
Shards Textual input binary, parquet and similars CSV and similars Sharding It is a common practice to divide a big dataset into chunks (or shards), smaller parts which recomposed give the original dataset. For example, in Cloud Storage settings we often divide big files into chunks, while in Distributed file systems the system automatically divides big files into native files of maximum 10 GB size.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="üììBig-Data">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Massive Parallel Processing">
<meta name="twitter:description" content="We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.
Introduction
Common input formats
You need to know well what

Shards
Textual input
binary, parquet and similars
CSV and similars

Sharding
It is a common practice to divide a big dataset into chunks (or shards), smaller parts which recomposed give the original dataset.
For example, in Cloud Storage settings we often divide big files into chunks, while in Distributed file systems the system automatically divides big files into native files of maximum 10 GB size.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Massive Parallel Processing",
      "item": "https://flecart.github.io/notes/massive-parallel-processing/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Massive Parallel Processing",
  "name": "Massive Parallel Processing",
  "description": "We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.\nIntroduction Common input formats You need to know well what\nShards Textual input binary, parquet and similars CSV and similars Sharding It is a common practice to divide a big dataset into chunks (or shards), smaller parts which recomposed give the original dataset. For example, in Cloud Storage settings we often divide big files into chunks, while in Distributed file systems the system automatically divides big files into native files of maximum 10 GB size.\n",
  "keywords": [
    "üììbig-data"
  ],
  "articleBody": "We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.\nIntroduction Common input formats You need to know well what\nShards Textual input binary, parquet and similars CSV and similars Sharding It is a common practice to divide a big dataset into chunks (or shards), smaller parts which recomposed give the original dataset. For example, in Cloud Storage settings we often divide big files into chunks, while in Distributed file systems the system automatically divides big files into native files of maximum 10 GB size.\nThis has two main advantages:\nWe can use the MapReduce framework to process the chunk in a highly parallelized manner. We are more robust to network communication . MapReduce We can process terabytes of data with thousands of nodes with this architecture.\nLogical Model General framework We can see in the image taken from the course book ({fourny} 2024), that there are two general phases for the map reduce framework: 1. First phase is mapping, where the data is divided into chunks and processed by the mappers, then these are delivered to the designated reducers. This is usually the bottleneck, the part that runs in $\\mathcal{O}(n^{2})$. 2. The reduce process connects to all other machines to get the their own mapped value. 3. Reducers then processed the partitioned, probably homogeneous data and return a result, which is often a file in HDFS. Key Value Pairs We have seen that MapReduce is composed of mainly two parts:\nOne part where we are extracting the data from various sources (Could be cloud, distributed files systems, wide column storages etc‚Ä¶) We map this data to the correct reducer This part often produces some spaghetti like data flows in it. Which has some weird shapes of arrows in all directions (some parts could be more linear, while others could be more spaghetti like) The final part is the reduce where the actual value is produced and stored. The three parts in principle can have different key types, but they need to have some key type known at compile time. But often the output key and the key input to the reducer have the same type.\nThe Architecture Centralized Architecture Hadoop MapReduce builds upon a centralized architecture compatible to Distributed file systems and HBase. In this case there is a Jobtracker, often in the same node as the NameNode; and Tasktrackers that are in the same node as the DataNodes. We have now three processes on the same machine. When MapReduce finishes, usually every TaskTracker produces a file in the disk (usually sharded, numerated increasingly). If the task fails for a node, the Jobtracker usually re-assigns the node to another.\nJobtracker‚Äôs responsibilities The Jobtracker has many responsibilities, see (Dean \u0026 Ghemawat 2008):\nTracking the state of each map and reduce task (idle, executed, on execution). Actively pinging the Tasktrackers to see if they are still active or dead. If a worker has died, then re-issue the task to another machine. In the case a map machine has failed, it should issue all reducers to read from the machine of the newly-assigned worker. Scheduling the tasks to account for locality information (meaning usually the map task needs a split that is in the same node, or on node close by it). Impendance Mismatch MapReduce needs to have key-values to process the files. For text, we usually use the lines and its character offset to index them. We could also use a special character to separate different parts of the text. Often this part part of preprocessing the dataset so that it is understandable by mapreduce is called impendance mismatch\nTask Granularity We subdivide the map phase into M pieces and the reduce phase into R pieces, as described above. Ideally, $M$ and $R$ should be much larger than the number of worker machines. From (Dean \u0026 Ghemawat 2008).\nThe reason why $M$ and $R$ should be larger is that in this manner we have better load balancing. Usual numbers are: $M$ knowing the size of the original dataset, the aim is to keep the blocks of data around 64MB, the size of a Google GFS. $R$ is usually a multiple of the number of reduce nodes. The paper reports usual numbers like: $M = 200'000$ and $R = 5'000$, using 2,000 worker machines.\nCombining This is a form of optimization. Recall that a big bottleneck of this system is the network communication, when we are mapping, we can do part of the reducing step so that later we have fewer key-value pairs to transmit. Often the combining step is the same as the reducing step. But this is not often possible:\nType must be the same for reduce function. We need to have commutative and associative functions for reduce. After the combining step we have the following diagram: And there will be less overload on the network, which makes the whole thing faster.\nRule of thumb: tasks are 3x the number of nodes that are mapping and reducing.\nOn Terminology You need to know what maps, reducer and combiners are. You also need to know what tasks, slots and phases, are.\nFunctions The only difference between a map and reduce function, is that the former takes a single key-value pair as input, while the second can take one or more. Both of them are functions that can produce zero, one, or more output key-value pairs. The combine function has the same definition as the reduce function.\nTasks Tasks are sequential calls to the respective functions. If we consider map-tasks for example, then the number of map function calls within the task is the the number of key-value pairs within an input task. The input split is so divided into many mapping tasks, and each task calls the mapping function individually for each key-value pair.\nSame thing could be said for reduce tasks: they are sequential calls to the reduce function on a subset of intermediate key-value pairs.\nBut, there are no combine tasks, as they are immediately executed after the map function, if any.\nSlots Slots are bundles of resources like CPU cores, memory resources, and network bandwidth. Usually, a single node could have more than one slot, which helps to handle the different needs for resources of different slots (some could need more memory, while others on the same node could need less memory, this better balances it).\nA Map Slot is usually a single CPU core with some memory. As we have a single core, the slot processes a single map task at a time, but several map tasks after one other.\nSame thing is said for Reduce Slots, but in this case we are talking about reduce functions.\nPhase The map phase is several map slots processing several map tasks in parallel. Same thing is said for the reduce phase. With the standard MapReduce framework, slots are allocated statically at the beginning, which could hinder performance. Innovation like YARN ease this problem, we will talk about this in the following sections.\nSplits and Blocks the data belonging to the split that is the input of a map task resides on the same machine as the map slot processing this map task.\nSplits are map or reduce partition sets of the whole data that needs to be processed. This is physically stored in HDFS blocks. These blocks are exactly of size 128MB, which implies the first and last key-values could be splitted in different blocks. This is an inconvenience as one needs to fetch also the preceding and successive blocks. This is why in HDFS‚Äôs api one can read blocks partially. The underlying details on how these optimizations are done are not covered here.\nResource Management We now introduce ResourceManagers, ApplicationManagers and NodeManagers. See here. For a not checked explanation.\nBottlenecks of the original version Jobtrackers were the bottleneck and have many responsibilities.\nJobtracker‚Äôs single point of failure With the above traditional MapReduce function, the Jobtracker is just assumed not to die: (Dean \u0026 Ghemawat 2008). This is clearly a single point of failure. Here the idea is to shift the responsibility of managing the job to some specific slot (container) within the tasktracker.\nSome responsibilities of the Jobtracker\nResource Management Monitoring every task‚Äôs progress Job scheduling This point of failure is still not solved! The Resource Manager becomes the next point of failure within the system!\nScalability issues : This is usually a bottleneck in this model. Clusters of about 4,000 become usually very very slow and about 10,000 tasks. Another drawback is that many reducers are idle during the map phase.\nContainers are virtual collections of slots (one container could have more than one map or reduce slot), each with some cpu and some amount of memory.\nOther bottlenecks Scalability : MapReduce has limited scalability, while YARN can scale to 10,000 nodes and 100,000 tasks, one order of magnitute higher compared to classical MapReduce! Rigidity : MapReduce v1 only supports MapReduce specific jobs. There is a need, however, for scheduling non-MapReduce workloads. For instance, we would like the ability to share cluster with MPI, graph processing, and any user code. Resource utilization : in MapReduce v1, the reducers wait on the mappers to finish (and vice-versa), leaving large fractions of time when either the reducers or the mappers are idle. Ideally all resources should be used at any given time. Flexibility : mapper and reducer roles are decided at configuration time, and cannot be reconfigured, this is also called lack of fungibility. Spark makes use of in-memory data too, making the access to data much faster.\nAdvantages of MapReduce Nonetheless, there could be cases where MapReduce is better than Spark, for example when the data is too big to fit in memory.\nUltra-large datasets that do not fit in memory: For massive datasets that don‚Äôt fit even across a large Spark cluster‚Äôs memory, MapReduce‚Äôs disk-based processing can be more manageable. Strict fault tolerance requirements: MapReduce‚Äôs fault tolerance mechanism is very robust, as it writes intermediate results to HDFS, which may be preferred in environments where data recovery is critical. Low hardware resources: Spark requires more memory and computational resources than MapReduce, so in resource-constrained environments, MapReduce may be a more practical choice. ApplicationMaster and ResourceMaster YARN decouples the scheduling of the jobs, the allocation of resource of the jobs, and the monitoring of the jobs, the first is done by the ResourceMaster, the last by the ApplicationMaster, while the middle one is done in collaboration.\nThis is an innovation introduced by the YARN architecture. With this new architecture we have containers and a resource manager that allocates resources to the tasks that need to be processed.\nResourceMaster The ResourceMaster‚Äôs main job is to grant:\nleases for resources when the ApplicationManager requests it. Schedule the jobs, and track only the state (on queue, executed, done). Accept requests by clients for jobs. Track the resources requested by each application. Disaster recovery The RM recovers from its own failures by restoring its state from a persistent store on initialization. Once the recovery process is complete, it kills all the containers running in the cluster, including live ApplicationMasters. It then launches new instances of each AM. From (Vavilapalli et al. 2013).\nApplicationMaster When a user starts a task, the resource manager elevates a container to be the application master by communicating with a process. This container then takes part of the responsibilities of the Jobtracker in the old version;\nit now tracks task state, as done with the Jobtracker, re-allocates other containers when one fails (expects liveliness by the worker machines) it asks or releases resources to the resource manager, at any time. (they are quite good with pre-emption, see Gestione delle risorse). Send HeartBeats to the resource-manager to signal he is still alive. NodeManagers These processes run on everynode in the cluster, and are responsible for:\nMonitoring the resources on the node (CPU, memory, disk, network) Reporting to the ResourceManager (using heartbeats) Managing the containers on the node. If a NodeManager dies, the ResourceManager assumes all the containers on the node are gone, and reports failure to all ApplicationManagers, who then will re-request resources and update their state tracking part. Setting up a task ApplicationMasters can request and release containers at any time, dynamically. A container request is typically made by the ApplicationMasters with a specific demand (e.g., ‚Äú10 containers with each 2 cores and 16 GB of RAM‚Äù). If the request is granted by the ResourceManager fully or partially, this is done indirectly by signing and issuing a container token to the ApplicationMaster that acts as proof that the resource was granted. The ApplicationMaster can then connect to the allocated NodeManager and send the token. The NodeManager will then check the validity of the token and provide the memory and CPU granted by the ResourceManager. The ApplicationMaster ships the code (e.g., as a jar file) as well as parameters, which then runs as a process with exclusive use of this memory and CPU. ~From the Book ({fourny} 2024)\nThe main facts to keep in mind:\nApplicationMasters can set up and release resources dynamically at any time ApplicationMasters can request resources in a fine-grained manner, asking for exactly a specific amount of resources to the ResourceManager The Granting procedure uses authorization tokens. Code is usually shipped by the ApplicationMaster to the worker container. ApplicationMasters can be pre-empted by the ResourceManager if the resources are needed by another application. All the information to create a container is called Container Launch Context or CLC for short. Containers usually contain many slots, in fact they can execute several tasks in parallel.\nResource tracking Also in this case, as has been done similarly for Distributed file systems and others, we track the available resources of each machine using heartbeats sent by NodeManagers to the ResourceManager. In this way, the ResourceManager knows exacly what is the load of each node.\nScheduling methods We deepen this in Cluster Management Policies.\nQueue scheduler For example, the queue scheduler where a job takes the whole cluster one after the other. But it‚Äôs probably not efficient as it doesn‚Äôt allow for multiple-tenancy of the cluster.\nCapacity Scheduler Another way could be the hierarchical scheduler where the amount of resource is allocated a priori. This is also called capacity scheduling because the whole cluster is divided into multiple sub-clusters corresponding to a specific department in a university or parts of a company. Then, each department can use a specific local scheduler, which could be a standard fair queue.\nFair Schedule Fair scheduling involves more complex algorithms that attempt to allocate resources in a way fair to all users of the cluster and based on the share they are normally entitled to.\nIt‚Äôs difficult to orchestrate the allocation of different resources. But how to allocate it correctly when the need of resources is different for other processes? Some processes could need more memory than cpu, Disk and Network Input and Output and others other resources. If some department is not using the resource, then it could be allocated to another department. There are different types of fair scheduling, the next sections briefly examine the main methods of scheduling\nSteady Fair Share This is the share of the cluster officially allocated to each department. The various departments agree upon this with each other in advance. This number is thus static and rarely changes.\nInstantaneous Fair Share this is the fair share that a department should ideally be allocated (according to economic and game theory considerations) at any point in time. This is a dynamic number that changes constantly, based on departments being idle: if a department is idle, then the instantaneous fair share of others department becomes higher than their steady fair shares.\nCurrent Share this is the actual share of the cluster that a department effectively uses at any point in time. This is highly dynamic. The current share does not necessarily match the instantaneous fair share because there is some inertia in the process: a department might be using more resources while another is idle. When the other department later stops being idle, these resources are not immediately withdrawn from the first department; rather, the first department will stop getting more resources, and the second department will gradually recover these resources as they get released by the first department.\nThree types of schedule sharing TODO: fill this in a later occasion (See page 273 of the book)\nSteady fair share: Instantaneous fair share: Current share: References [1] Dean \u0026 Ghemawat ‚ÄúMapReduce: Simplified Data Processing on Large Clusters‚Äù Vol. 51(1), pp. 107--113 2008 [2] {fourny} ‚ÄúThe Big Data Textbook‚Äù Self Published 2024 [3] Vavilapalli et al. ‚ÄúApache Hadoop YARN: Yet Another Resource Negotiator‚Äù ACM 2013 ",
  "wordCount" : "2806",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/massive-parallel-processing/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Massive Parallel Processing
    </h1>
    <div class="post-meta">14 min&nbsp;¬∑&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a><ul>
                        
                <li>
                    <a href="#common-input-formats" aria-label="Common input formats">Common input formats</a></li>
                <li>
                    <a href="#sharding" aria-label="Sharding">Sharding</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#mapreduce" aria-label="MapReduce">MapReduce</a><ul>
                        
                <li>
                    <a href="#logical-model" aria-label="Logical Model">Logical Model</a><ul>
                        
                <li>
                    <a href="#general-framework" aria-label="General framework">General framework</a></li>
                <li>
                    <a href="#key-value-pairs" aria-label="Key Value Pairs">Key Value Pairs</a></li></ul>
                </li>
                <li>
                    <a href="#the-architecture" aria-label="The Architecture">The Architecture</a><ul>
                        
                <li>
                    <a href="#centralized-architecture" aria-label="Centralized Architecture">Centralized Architecture</a></li>
                <li>
                    <a href="#jobtrackers-responsibilities" aria-label="Jobtracker&rsquo;s responsibilities">Jobtracker&rsquo;s responsibilities</a></li>
                <li>
                    <a href="#impendance-mismatch" aria-label="Impendance Mismatch">Impendance Mismatch</a></li>
                <li>
                    <a href="#task-granularity" aria-label="Task Granularity">Task Granularity</a></li>
                <li>
                    <a href="#combining" aria-label="Combining">Combining</a></li></ul>
                </li>
                <li>
                    <a href="#on-terminology" aria-label="On Terminology">On Terminology</a><ul>
                        
                <li>
                    <a href="#functions" aria-label="Functions">Functions</a></li>
                <li>
                    <a href="#tasks" aria-label="Tasks">Tasks</a></li>
                <li>
                    <a href="#slots" aria-label="Slots">Slots</a></li>
                <li>
                    <a href="#phase" aria-label="Phase">Phase</a></li>
                <li>
                    <a href="#splits-and-blocks" aria-label="Splits and Blocks">Splits and Blocks</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#resource-management" aria-label="Resource Management">Resource Management</a><ul>
                        
                <li>
                    <a href="#bottlenecks-of-the-original-version" aria-label="Bottlenecks of the original version">Bottlenecks of the original version</a><ul>
                        
                <li>
                    <a href="#jobtrackers-single-point-of-failure" aria-label="Jobtracker&rsquo;s single point of failure">Jobtracker&rsquo;s single point of failure</a></li>
                <li>
                    <a href="#scalability-issues-" aria-label="Scalability issues :">Scalability issues :</a></li>
                <li>
                    <a href="#other-bottlenecks" aria-label="Other bottlenecks">Other bottlenecks</a></li>
                <li>
                    <a href="#advantages-of-mapreduce" aria-label="Advantages of MapReduce">Advantages of MapReduce</a></li></ul>
                </li>
                <li>
                    <a href="#applicationmaster-and-resourcemaster" aria-label="ApplicationMaster and ResourceMaster">ApplicationMaster and ResourceMaster</a><ul>
                        
                <li>
                    <a href="#resourcemaster" aria-label="ResourceMaster">ResourceMaster</a></li>
                <li>
                    <a href="#disaster-recovery" aria-label="Disaster recovery">Disaster recovery</a></li>
                <li>
                    <a href="#applicationmaster" aria-label="ApplicationMaster">ApplicationMaster</a></li>
                <li>
                    <a href="#nodemanagers" aria-label="NodeManagers">NodeManagers</a></li>
                <li>
                    <a href="#setting-up-a-task" aria-label="Setting up a task">Setting up a task</a></li>
                <li>
                    <a href="#resource-tracking" aria-label="Resource tracking">Resource tracking</a></li></ul>
                </li>
                <li>
                    <a href="#scheduling-methods" aria-label="Scheduling methods">Scheduling methods</a><ul>
                        
                <li>
                    <a href="#queue-scheduler" aria-label="Queue scheduler">Queue scheduler</a></li>
                <li>
                    <a href="#capacity-scheduler" aria-label="Capacity Scheduler">Capacity Scheduler</a></li>
                <li>
                    <a href="#fair-schedule" aria-label="Fair Schedule">Fair Schedule</a></li>
                <li>
                    <a href="#steady-fair-share" aria-label="Steady Fair Share">Steady Fair Share</a></li>
                <li>
                    <a href="#instantaneous-fair-share" aria-label="Instantaneous Fair Share">Instantaneous Fair Share</a></li>
                <li>
                    <a href="#current-share" aria-label="Current Share">Current Share</a></li>
                <li>
                    <a href="#three-types-of-schedule-sharing" aria-label="Three types of schedule sharing">Three types of schedule sharing</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>We have a group of mappers that work on dividing the keys for some reducers that actually work on that same group of data. The bottleneck is the assigning part: when mappers finish and need to handle the data to the reducers.</p>
<h3 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h3>
<h4 id="common-input-formats">Common input formats<a hidden class="anchor" aria-hidden="true" href="#common-input-formats">#</a></h4>
<p>You need to know well what</p>
<ul>
<li>Shards</li>
<li>Textual input</li>
<li>binary, parquet and similars</li>
<li>CSV and similars</li>
</ul>
<h4 id="sharding">Sharding<a hidden class="anchor" aria-hidden="true" href="#sharding">#</a></h4>
<p>It is a common practice to divide a big dataset into <em>chunks</em> (or shards), smaller parts which recomposed give the original dataset.
For example, in <a href="/notes/cloud-storage">Cloud Storage</a> settings we often divide big files into chunks, while in <a href="/notes/distributed-file-systems">Distributed file systems</a> the system automatically divides big files into native files of maximum 10 GB size.</p>
<p>This has two main advantages:</p>
<ol>
<li>We can use the MapReduce framework to process the chunk in a highly parallelized manner.</li>
<li>We are more robust to network communication .</li>
</ol>
<h2 id="mapreduce">MapReduce<a hidden class="anchor" aria-hidden="true" href="#mapreduce">#</a></h2>
<p>We can process terabytes of data with thousands of nodes with this architecture.</p>
<h3 id="logical-model">Logical Model<a hidden class="anchor" aria-hidden="true" href="#logical-model">#</a></h3>
<h4 id="general-framework">General framework<a hidden class="anchor" aria-hidden="true" href="#general-framework">#</a></h4>
<img src="/images/notes/Massive Parallel Processing-20241030093405625.webp" style="width: 100%" class="center" alt="Massive Parallel Processing-20241030093405625">
We can see in the image taken from the course book <a href="https://ghislainfourny.github.io/big-data-textbook/">({fourny} 2024)</a>, that there are two general phases for the map reduce framework:
1. First phase is mapping, where the data is divided into chunks and processed by the mappers, then these are delivered to the designated reducers. This is usually the bottleneck, the part that runs in $\mathcal{O}(n^{2})$.
2. The reduce process connects to all other machines to get the their own mapped value. 
3. Reducers then processed the partitioned, probably homogeneous data and return a result, which is often a file in HDFS.
<h4 id="key-value-pairs">Key Value Pairs<a hidden class="anchor" aria-hidden="true" href="#key-value-pairs">#</a></h4>
<p>We have seen that MapReduce is composed of mainly two parts:</p>
<ol>
<li>One part where we are extracting the data from various sources (Could be cloud, distributed files systems, wide column storages etc&hellip;)</li>
<li>We map this data to the correct reducer
<ol>
<li>This part often produces some <em>spaghetti</em> like data flows in it. Which has some weird shapes of arrows in all directions (some parts could be more linear, while others could be more spaghetti like)</li>
</ol>
</li>
<li>The final part is the reduce where the actual value is produced and stored.</li>
</ol>
<p>The three parts in principle can have <strong>different key</strong> types, but they need to have some key type <em>known at compile time</em>.
But often the output key and the key input to the reducer have the same type.</p>
<h3 id="the-architecture">The Architecture<a hidden class="anchor" aria-hidden="true" href="#the-architecture">#</a></h3>
<h4 id="centralized-architecture">Centralized Architecture<a hidden class="anchor" aria-hidden="true" href="#centralized-architecture">#</a></h4>
<p>Hadoop MapReduce builds upon a centralized architecture compatible to <a href="/notes/distributed-file-systems">Distributed file systems</a> and <a href="/notes/wide-column-storage">HBase</a>. In this case there is a <strong>Jobtracker</strong>, often in the same node as the <strong>NameNode</strong>; and <strong>Tasktrackers</strong> that are in the same node as the <strong>DataNodes</strong>. We have now three processes on the same machine.
When MapReduce finishes, usually every TaskTracker produces a file in the disk (usually sharded, numerated increasingly).
If the task fails for a node, the Jobtracker usually re-assigns the node to another.</p>
<h4 id="jobtrackers-responsibilities">Jobtracker&rsquo;s responsibilities<a hidden class="anchor" aria-hidden="true" href="#jobtrackers-responsibilities">#</a></h4>
<p>The Jobtracker has many responsibilities, see <a href="https://dl.acm.org/doi/10.1145/1327452.1327492">(Dean &amp; Ghemawat 2008)</a>:</p>
<ul>
<li>Tracking the state of each map and reduce task (idle, executed, on execution).</li>
<li>Actively pinging the Tasktrackers to see if they are still active or dead.</li>
<li>If a worker has died, then re-issue the task to another machine. In the case a map machine has failed, it should issue all reducers to read from the machine of the newly-assigned worker.</li>
<li>Scheduling the tasks to account for <strong>locality information</strong> (meaning usually the map task needs a split that is in the same node, or on node close by it).</li>
</ul>
<h4 id="impendance-mismatch">Impendance Mismatch<a hidden class="anchor" aria-hidden="true" href="#impendance-mismatch">#</a></h4>
<p>MapReduce needs to have key-values to process the files.
For text, we usually use the <strong>lines</strong> and its character offset to index them. We could also use a special character to separate different parts of the text.
Often this part part of preprocessing the dataset so that it is understandable by mapreduce is called impendance mismatch</p>
<h4 id="task-granularity">Task Granularity<a hidden class="anchor" aria-hidden="true" href="#task-granularity">#</a></h4>
<blockquote>
<p>We subdivide the map phase into M pieces and the reduce phase into R pieces, as described above. Ideally, $M$ and $R$ should be much larger than the number of worker machines. From <a href="https://dl.acm.org/doi/10.1145/1327452.1327492">(Dean &amp; Ghemawat 2008)</a>.</p></blockquote>
<p>The reason why $M$ and $R$ should be larger is that in this manner we have better load balancing.
Usual numbers are: $M$ knowing the size of the original dataset, the aim is to keep the blocks of data around 64MB, the size of a Google GFS.
$R$ is usually a multiple of the number of reduce nodes. The paper reports usual numbers like: $M = 200'000$ and $R = 5'000$, using 2,000 worker machines.</p>
<h4 id="combining">Combining<a hidden class="anchor" aria-hidden="true" href="#combining">#</a></h4>
<p>This is a form of optimization. Recall that a big bottleneck of this system is the network communication, when we are mapping, we can do part of the reducing step so that later we have fewer key-value pairs to transmit. Often the combining step is the same as the reducing step.
But this is not often possible:</p>
<ul>
<li>Type must be the same for reduce function.</li>
<li>We need to have commutative and associative functions for reduce.</li>
</ul>
<p>After the combining step we have the following diagram:
<img src="/images/notes/Massive Parallel Processing-20241103195030645.webp" style="width: 100%" class="center" alt="Massive Parallel Processing-20241103195030645">
And there will be less overload on the network, which makes the whole thing faster.</p>
<p>Rule of thumb: tasks are 3x the number of nodes that are mapping and reducing.</p>
<h3 id="on-terminology">On Terminology<a hidden class="anchor" aria-hidden="true" href="#on-terminology">#</a></h3>
<p>You need to know what maps, reducer and combiners are. You also need to know what tasks, slots and phases, are.</p>
<h4 id="functions">Functions<a hidden class="anchor" aria-hidden="true" href="#functions">#</a></h4>
<p>The only difference between a map and reduce function, is that the former takes a single key-value pair as input, while the second can take one or more.
Both of them are functions that can produce zero, one, or more output key-value pairs.
The combine function has the same definition as the reduce function.</p>
<h4 id="tasks">Tasks<a hidden class="anchor" aria-hidden="true" href="#tasks">#</a></h4>
<p>Tasks are sequential calls to the respective functions.
If we consider map-tasks for example, then the number of map function calls within the task is the the number of key-value pairs within an input task.
The input split is so divided into many mapping tasks, and each task calls the mapping function individually for each key-value pair.</p>
<p>Same thing could be said for <strong>reduce tasks</strong>: they are <em>sequential</em> calls to the reduce function on a subset of intermediate key-value pairs.</p>
<p>But, there are no combine tasks, as they are immediately executed after the map function, if any.</p>
<h4 id="slots">Slots<a hidden class="anchor" aria-hidden="true" href="#slots">#</a></h4>
<p>Slots are bundles of resources like <em>CPU cores, memory resources, and network bandwidth</em>. Usually, a single node could have more than one slot, which helps to handle the different needs for resources of different slots (some could need more memory, while others on the same node could need less memory, this better balances it).</p>
<p>A <strong>Map Slot</strong> is usually a single CPU core with some memory. As we have a single core, the slot processes a single map task at a time, but several map tasks after one other.</p>
<p>Same thing is said for <strong>Reduce Slots</strong>, but in this case we are talking about reduce functions.</p>
<h4 id="phase">Phase<a hidden class="anchor" aria-hidden="true" href="#phase">#</a></h4>
<p>The map phase is several map slots processing several map tasks in parallel. Same thing is said for the <strong>reduce phase</strong>.
With the standard MapReduce framework, slots are allocated statically at the beginning, which could hinder performance. Innovation like YARN ease this problem, we will talk about this in the following sections.</p>
<h4 id="splits-and-blocks">Splits and Blocks<a hidden class="anchor" aria-hidden="true" href="#splits-and-blocks">#</a></h4>
<blockquote>
<p>the data belonging to the split that is the input of a map task resides on the same machine as the map slot processing this map task.</p></blockquote>
<p>Splits are map or reduce partition sets of the whole data that needs to be processed. This is physically stored in HDFS blocks.
These blocks are exactly of size 128MB, which implies the first and last key-values could be splitted in different blocks. This is an inconvenience as one needs to fetch also the preceding and successive blocks.
This is why in HDFS&rsquo;s api one can <strong>read blocks partially</strong>.
The underlying details on how these optimizations are done are not covered here.</p>
<h2 id="resource-management">Resource Management<a hidden class="anchor" aria-hidden="true" href="#resource-management">#</a></h2>
<p>We now introduce ResourceManagers, ApplicationManagers and NodeManagers. See <a href="https://chatgpt.com/share/673da14d-5254-8009-ac60-f08762c15b0a">here</a>. For a not checked explanation.</p>
<h3 id="bottlenecks-of-the-original-version">Bottlenecks of the original version<a hidden class="anchor" aria-hidden="true" href="#bottlenecks-of-the-original-version">#</a></h3>
<p>Jobtrackers were the bottleneck and have many responsibilities.</p>
<h4 id="jobtrackers-single-point-of-failure">Jobtracker&rsquo;s single point of failure<a hidden class="anchor" aria-hidden="true" href="#jobtrackers-single-point-of-failure">#</a></h4>
<p>With the above traditional MapReduce function, the Jobtracker is just assumed not to die: <a href="https://dl.acm.org/doi/10.1145/1327452.1327492">(Dean &amp; Ghemawat 2008)</a>. This is clearly a single point of failure.
Here the idea is to shift the responsibility of managing the job to some specific slot (container) within the tasktracker.</p>
<p>Some responsibilities of the Jobtracker</p>
<ul>
<li>Resource Management</li>
<li>Monitoring every task&rsquo;s progress</li>
<li>Job scheduling</li>
</ul>
<p>This point of failure is still not solved! The Resource Manager becomes the next point of failure within the system!</p>
<h4 id="scalability-issues-">Scalability issues :<a hidden class="anchor" aria-hidden="true" href="#scalability-issues-">#</a></h4>
<p>This is usually a bottleneck in this model. Clusters of about 4,000 become usually very very slow and about 10,000 tasks.
Another drawback is that many reducers are idle during the map phase.</p>
<p>Containers are virtual collections of slots (one container could have more than one map or reduce slot), each with some cpu and some amount of memory.</p>
<h4 id="other-bottlenecks">Other bottlenecks<a hidden class="anchor" aria-hidden="true" href="#other-bottlenecks">#</a></h4>
<ol>
<li><strong>Scalability</strong> : MapReduce has limited scalability, while YARN can scale to 10,000 nodes and 100,000 tasks, one order of magnitute higher compared to classical MapReduce!</li>
<li><strong>Rigidity</strong> : MapReduce v1 only supports MapReduce specific jobs. There is a need, however, for scheduling non-MapReduce workloads. For instance, we would like the ability to share cluster with MPI, graph processing, and any user code.</li>
<li><strong>Resource utilization</strong> : in MapReduce v1, the reducers wait on the mappers to finish (and vice-versa), leaving large fractions of time when either the reducers or the mappers are idle. Ideally all resources should be used at any given time.</li>
<li><strong>Flexibility</strong> : mapper and reducer roles are decided at configuration time, and cannot be reconfigured, this is also called <em>lack of fungibility</em>.</li>
</ol>
<p>Spark makes use of in-memory data too, making the access to data much faster.</p>
<h4 id="advantages-of-mapreduce">Advantages of MapReduce<a hidden class="anchor" aria-hidden="true" href="#advantages-of-mapreduce">#</a></h4>
<p>Nonetheless, there could be cases where MapReduce is better than Spark, for example when the data is too big to fit in memory.</p>
<ul>
<li>Ultra-large datasets that do not fit in memory: For massive datasets that don‚Äôt fit even across a large Spark cluster‚Äôs memory, MapReduce‚Äôs disk-based processing can be more manageable.</li>
<li>Strict fault tolerance requirements: MapReduce‚Äôs fault tolerance mechanism is very robust, as it writes intermediate results to HDFS, which may be preferred in environments where data recovery is critical.</li>
<li>Low hardware resources: Spark requires more memory and computational resources than MapReduce, so in resource-constrained environments, MapReduce may be a more practical choice.</li>
</ul>
<h3 id="applicationmaster-and-resourcemaster">ApplicationMaster and ResourceMaster<a hidden class="anchor" aria-hidden="true" href="#applicationmaster-and-resourcemaster">#</a></h3>
<p>YARN decouples the scheduling of the jobs, the allocation of resource of the jobs, and the monitoring of the jobs, the first is done by the ResourceMaster, the last by the ApplicationMaster, while the middle one is done in collaboration.</p>
<p>This is an innovation introduced by the YARN architecture. With this new architecture we have <strong>containers</strong> and a resource manager that allocates resources to the tasks that need to be processed.</p>
<h4 id="resourcemaster">ResourceMaster<a hidden class="anchor" aria-hidden="true" href="#resourcemaster">#</a></h4>
<p>The ResourceMaster&rsquo;s main job is to grant:</p>
<ul>
<li><em>leases</em> for resources when the ApplicationManager requests it.</li>
<li>Schedule the jobs, and track only the state (on queue, executed, done).</li>
<li>Accept requests by clients for jobs.</li>
<li>Track the resources requested by each application.</li>
</ul>
<h4 id="disaster-recovery">Disaster recovery<a hidden class="anchor" aria-hidden="true" href="#disaster-recovery">#</a></h4>
<blockquote>
<p>The RM recovers from its own failures by restoring its state from a persistent store on initialization. Once the recovery process is complete, it kills all the containers running in the cluster, including live ApplicationMasters. It then launches new instances of each AM. From <a href="https://dl.acm.org/doi/10.1145/2523616.2523633">(Vavilapalli et al. 2013)</a>.</p></blockquote>
<h4 id="applicationmaster">ApplicationMaster<a hidden class="anchor" aria-hidden="true" href="#applicationmaster">#</a></h4>
<p>When a user starts a task, the resource manager elevates a container to be the <strong>application master</strong> by communicating with a process. This container then takes part of the responsibilities of the Jobtracker in the old version;</p>
<ul>
<li>it now tracks task state, as done with the Jobtracker,</li>
<li>re-allocates other containers when one fails (expects liveliness by the worker machines)</li>
<li>it asks or releases resources to the resource manager, at <em>any time</em>. (they are quite good with pre-emption, see <a href="/notes/gestione-delle-risorse">Gestione delle risorse</a>).</li>
<li>Send HeartBeats to the resource-manager to signal he is still alive.</li>
</ul>
<h4 id="nodemanagers">NodeManagers<a hidden class="anchor" aria-hidden="true" href="#nodemanagers">#</a></h4>
<p>These processes run on everynode in the cluster, and are responsible for:</p>
<ul>
<li>Monitoring the resources on the node (CPU, memory, disk, network)</li>
<li>Reporting to the ResourceManager (using heartbeats)</li>
<li>Managing the containers on the node.
If a NodeManager dies, the ResourceManager assumes all the containers on the node are gone, and reports failure to all ApplicationManagers, who then will re-request resources and update their state tracking part.</li>
</ul>
<h4 id="setting-up-a-task">Setting up a task<a hidden class="anchor" aria-hidden="true" href="#setting-up-a-task">#</a></h4>
<blockquote>
<p>ApplicationMasters can request and release containers at <strong>any time</strong>, dynamically. A container request is typically made by the ApplicationMasters with a specific demand (e.g., ‚Äú10 containers with each 2 cores and 16 GB of RAM‚Äù). If the request is granted by the ResourceManager fully or partially, this is done indirectly by signing and issuing a <strong>container token</strong> to the ApplicationMaster that acts as proof that the resource was granted.
The ApplicationMaster can then connect to the allocated NodeManager and send the token. The NodeManager will then check the validity of the token and provide the memory and CPU granted by the
ResourceManager. The ApplicationMaster ships the code (e.g., as a jar file) as well as parameters, which then runs as a process with exclusive use of this memory and CPU.
<em>~From the Book <a href="https://ghislainfourny.github.io/big-data-textbook/">({fourny} 2024)</a></em></p></blockquote>
<p>The main facts to keep in mind:</p>
<ol>
<li>ApplicationMasters can set up and release resources <strong>dynamically</strong> at any time</li>
<li>ApplicationMasters can request resources in a <strong>fine-grained</strong> manner, asking for exactly a specific amount of resources to the ResourceManager</li>
<li>The Granting procedure uses <strong>authorization tokens</strong>.</li>
<li>Code is usually <strong>shipped</strong> by the ApplicationMaster to the worker container.</li>
<li>ApplicationMasters can be <strong>pre-empted</strong> by the ResourceManager if the resources are needed by another application.</li>
</ol>
<p>All the information to create a container is called <em>Container Launch Context</em> or CLC for short.
Containers usually contain <em>many</em> slots, in fact they can execute several tasks in parallel.</p>
<h4 id="resource-tracking">Resource tracking<a hidden class="anchor" aria-hidden="true" href="#resource-tracking">#</a></h4>
<p>Also in this case, as has been done similarly for <a href="/notes/distributed-file-systems">Distributed file systems</a> and others, we track the available resources of each machine using <strong>heartbeats</strong> sent by NodeManagers to the ResourceManager. In this way, the ResourceManager knows exacly what is the load of each node.</p>
<h3 id="scheduling-methods">Scheduling methods<a hidden class="anchor" aria-hidden="true" href="#scheduling-methods">#</a></h3>
<p>We deepen this in <a href="/notes/cluster-management-policies">Cluster Management Policies</a>.</p>
<h4 id="queue-scheduler">Queue scheduler<a hidden class="anchor" aria-hidden="true" href="#queue-scheduler">#</a></h4>
<p>For example, the <strong>queue scheduler</strong> where a job takes the whole cluster one after the other. But it&rsquo;s probably not efficient as it doesn&rsquo;t allow for multiple-tenancy of the cluster.</p>
<h4 id="capacity-scheduler">Capacity Scheduler<a hidden class="anchor" aria-hidden="true" href="#capacity-scheduler">#</a></h4>
<p>Another way could be the <strong>hierarchical scheduler</strong> where the amount of resource is allocated a priori. This is also called <strong>capacity scheduling</strong> because the whole cluster is divided into multiple sub-clusters corresponding to a specific department in a university or parts of a company.
Then, each department can use a specific local scheduler, which could be a standard fair queue.</p>
<h4 id="fair-schedule">Fair Schedule<a hidden class="anchor" aria-hidden="true" href="#fair-schedule">#</a></h4>
<blockquote>
<p>Fair scheduling involves more complex algorithms that attempt to allocate resources in a way fair to all users of the cluster and based on the share they are normally entitled to.</p></blockquote>
<p>It&rsquo;s difficult to orchestrate the allocation of different resources. But how to allocate it correctly when the need of resources is different for other processes? Some processes could need more memory than cpu, Disk and Network Input and Output and others other resources. If some department is not using the resource, then it could be allocated to another department.
There are different types of fair scheduling, the next sections briefly examine the main methods of scheduling</p>
<h4 id="steady-fair-share">Steady Fair Share<a hidden class="anchor" aria-hidden="true" href="#steady-fair-share">#</a></h4>
<blockquote>
<p>This is the share of the cluster officially allocated to each department. The various departments agree upon this with each other <strong>in advance</strong>. This number is thus <strong>static</strong> and rarely changes.</p></blockquote>
<h4 id="instantaneous-fair-share">Instantaneous Fair Share<a hidden class="anchor" aria-hidden="true" href="#instantaneous-fair-share">#</a></h4>
<blockquote>
<p>this is the fair share that a department should <em>ideally</em> be allocated (according to economic and game theory considerations) at any point in time. This is a dynamic number that changes constantly, based on departments being idle: if a department is idle, then the instantaneous fair share of others department becomes higher than their steady fair shares.</p></blockquote>
<h4 id="current-share">Current Share<a hidden class="anchor" aria-hidden="true" href="#current-share">#</a></h4>
<blockquote>
<p>this is the actual share of the cluster that a department effectively uses at any point in time. This is highly dynamic. The current share does not necessarily match the instantaneous fair share because there is some inertia in the process: a department might be using more resources while another is idle. When the other department later stops being idle, these resources are not immediately withdrawn from the first department; rather, the first department will stop getting more resources, and the second
department will gradually recover these resources as they get released by the first department.</p></blockquote>
<h4 id="three-types-of-schedule-sharing">Three types of schedule sharing<a hidden class="anchor" aria-hidden="true" href="#three-types-of-schedule-sharing">#</a></h4>
<p>TODO: fill this in a later occasion (See page 273 of the book)</p>
<ul>
<li>Steady fair share:</li>
<li>Instantaneous fair share:</li>
<li>Current share:</li>
</ul>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=deanMapReduceSimplifiedData2008>[1] Dean & Ghemawat <a href="https://dl.acm.org/doi/10.1145/1327452.1327492">‚ÄúMapReduce: Simplified Data Processing on Large Clusters‚Äù</a>  Vol. 51(1), pp. 107--113 2008
 </p>
<p id=fournyBigDataTextbook2024>[2] {fourny} <a href="https://ghislainfourny.github.io/big-data-textbook/">‚ÄúThe Big Data Textbook‚Äù</a> Self Published 2024
 </p>
<p id=vavilapalliApacheHadoopYARN2013>[3] Vavilapalli et al. <a href="https://dl.acm.org/doi/10.1145/2523616.2523633">‚ÄúApache Hadoop YARN: Yet Another Resource Negotiator‚Äù</a> ACM  2013
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/big-data/">üììBig-Data</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on x"
            href="https://x.com/intent/tweet/?text=Massive%20Parallel%20Processing&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f&amp;hashtags=%f0%9f%93%93big-data">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f&amp;title=Massive%20Parallel%20Processing&amp;summary=Massive%20Parallel%20Processing&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f&title=Massive%20Parallel%20Processing">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on whatsapp"
            href="https://api.whatsapp.com/send?text=Massive%20Parallel%20Processing%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on telegram"
            href="https://telegram.me/share/url?text=Massive%20Parallel%20Processing&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Massive Parallel Processing on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Massive%20Parallel%20Processing&u=https%3a%2f%2fflecart.github.io%2fnotes%2fmassive-parallel-processing%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
