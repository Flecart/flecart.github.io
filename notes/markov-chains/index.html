<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Markov Chains | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="➕probabilistic-artificial-intelligence">
<meta name="description" content="Introduzione alle catene di Markov
La proprietà di Markov
Una sequenza di variabili aleatorie $X_{1}, X_{2}, X_{3}, \dots$ gode della proprietà di Markov se vale:
$$
P(X_{n}| X_{n - 1}, X_{n - 2}, \dots, X_{1}) = P(X_{n}|X_{n-1})
$$
Ossia posso scordarmi tutta la storia precedente, mi interessa solamente lo stato precedente per sapere la probabilità attuale.
Da un punto di vista filosofico/fisico, ha senso perché mi sta dicendo che posso predire lo stato successivo se ho una conoscenza (completa, (lo dico io completo, originariamente non esiste)) del presente.">
<meta name="author" content="
By Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/markov-chains/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f790d9af969c56c079c1ce2d5972a04486bf3d6144295d5fba319830e1e55a7a.css" integrity="sha256-95DZr5acVsB5wc4tWXKgRIa/PWFEKV1fujGYMOHlWno=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/markov-chains/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/markov-chains/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Markov Chains">
  <meta property="og:description" content="Introduzione alle catene di Markov La proprietà di Markov Una sequenza di variabili aleatorie $X_{1}, X_{2}, X_{3}, \dots$ gode della proprietà di Markov se vale:
$$ P(X_{n}| X_{n - 1}, X_{n - 2}, \dots, X_{1}) = P(X_{n}|X_{n-1}) $$ Ossia posso scordarmi tutta la storia precedente, mi interessa solamente lo stato precedente per sapere la probabilità attuale.
Da un punto di vista filosofico/fisico, ha senso perché mi sta dicendo che posso predire lo stato successivo se ho una conoscenza (completa, (lo dico io completo, originariamente non esiste)) del presente.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="➕Probabilistic-Artificial-Intelligence">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Markov Chains">
<meta name="twitter:description" content="Introduzione alle catene di Markov
La proprietà di Markov
Una sequenza di variabili aleatorie $X_{1}, X_{2}, X_{3}, \dots$ gode della proprietà di Markov se vale:
$$
P(X_{n}| X_{n - 1}, X_{n - 2}, \dots, X_{1}) = P(X_{n}|X_{n-1})
$$
Ossia posso scordarmi tutta la storia precedente, mi interessa solamente lo stato precedente per sapere la probabilità attuale.
Da un punto di vista filosofico/fisico, ha senso perché mi sta dicendo che posso predire lo stato successivo se ho una conoscenza (completa, (lo dico io completo, originariamente non esiste)) del presente.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Markov Chains",
      "item": "https://flecart.github.io/notes/markov-chains/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Markov Chains",
  "name": "Markov Chains",
  "description": "Introduzione alle catene di Markov La proprietà di Markov Una sequenza di variabili aleatorie $X_{1}, X_{2}, X_{3}, \\dots$ gode della proprietà di Markov se vale:\n$$ P(X_{n}| X_{n - 1}, X_{n - 2}, \\dots, X_{1}) = P(X_{n}|X_{n-1}) $$ Ossia posso scordarmi tutta la storia precedente, mi interessa solamente lo stato precedente per sapere la probabilità attuale.\nDa un punto di vista filosofico/fisico, ha senso perché mi sta dicendo che posso predire lo stato successivo se ho una conoscenza (completa, (lo dico io completo, originariamente non esiste)) del presente.\n",
  "keywords": [
    "➕probabilistic-artificial-intelligence"
  ],
  "articleBody": "Introduzione alle catene di Markov La proprietà di Markov Una sequenza di variabili aleatorie $X_{1}, X_{2}, X_{3}, \\dots$ gode della proprietà di Markov se vale:\n$$ P(X_{n}| X_{n - 1}, X_{n - 2}, \\dots, X_{1}) = P(X_{n}|X_{n-1}) $$ Ossia posso scordarmi tutta la storia precedente, mi interessa solamente lo stato precedente per sapere la probabilità attuale.\nDa un punto di vista filosofico/fisico, ha senso perché mi sta dicendo che posso predire lo stato successivo se ho una conoscenza (completa, (lo dico io completo, originariamente non esiste)) del presente.\nLa catena di Markov $$ \\mathbb{P}(X_{t+1} = j \\mid X_{0} = i_{0}, \\dots, X_{t} = i_{t}) = \\mathbb{P}(X_{t + 1} = j \\mid X_{t} = i_{t}) = P_{ij} $$ Queste catene sono dette time-homogeneus. Con la probabilità di Markov si può dimostrare che se $X_{0} \\sim \\mu_{0}$ con $\\mu_{0}$ il vettore riga, allora la probabilità della distribuzione $X_{t}$ sarà $\\mu_{t} = \\mu_{0}P^{t}$. We have that $\\mu$ is a stationary distribution if $\\mu = \\mu P$ is valid. A study of this property is sometimes interesting.\nCatena di 3 variabili $$ p(x, y, z) = p(x)p(y|x)p(z|y) $$$$ p(x, y, z) = p(z)p(y|z)p(x|y) $$$$ P(x, z|y) = P(x|y)P(z|y) $$ Che dovrebbe essere una conseguenza diretta della parte di sopra. Una altra osservazione è che se vale quella catena, vale anche l’inversa, ossia $Z \\to Y \\to X$.\nAbbiamo analizzato molte catene di questo genere quando abbiamo parlato di d-separabilità in Counterfactual Invariance.\nData processing inequality $$ I(X ; Y) \\geq I(X; Z) $$ Perché una parte di computazione è possibile modellarlo con la catena di Markov. E mi sta dicendo che l’informazione comune all’input $X$ con l’output $Y$ o output $Z$ dopo seguente computazione viene sempre meno con più computazione, e anche che non aggiungo informazione con più computazione.\n$$ X \\to Y \\to Z $$$$ I(X; Y) \\geq I(X; Z) $$ If the equality is satisfied then $Z$ is the sufficient statistic for $Y$. This has some relation with The Exponential Family.\n$$ \\begin{align} I(X;Z) \u0026= I(X; Y, Z) - I(X; Y \\mid Z) \\\\ \u0026= I(X;Y) - I(X; Z \\mid Y)- I(X; Y \\mid Z) \\\\ \u0026\\implies I(X;Y) \\geq I(X;Z) \\end{align} $$Definizioni Comuni Raggiungibilità $$ P_{ij}^{m} \u003e 0 $$ Molto più facile vedere sta cosa se lo rappresentiamo come un comunissimo grafo.\nClasse di stati Sono un insieme di stati tutti raggiungibili fra di loro (comunque presi due stati all’interno della classe, esiste un percorso che parte da uno e finisce sull’altro per dire).\nRecurrent vs Transient È recurrent se per ogni nodo, tutti i nodi raggiungibili da un nodo $i$ raggiungono anche il nodo $i$ stesso. Transient se non è recurrent. Alcuni chiamano la recurrent come irreducible come in (Cover \u0026 Thomas 2012).\nPeriodic vs Aperiodic Sia $d$ il massimo comune divisore per tutti gli $m$ tali per cui vale $P_{ii}^{m} \u003e 0$ (ossia può raggiungere sé stesso con probabilità non nulla), allora è periodico se $d \u003e 1$ altrimenti è aperiodico. Questo implica che non esistono cicli di passi che siano divisibili per un numero $d$, e abbiamo una definizione un poco più intuitiva di periodicità. Una catena di Markov è aperiodica se tutti i nodi sono aperiodici.\nErgodic Markov Chain Una catena di Markov si dice Ergodico se è recurrent e aperiodico. Si può anche definire come se esista un $t$ finito tale per cui tutti gli stati siano raggiungibili da tutti gli altri in esattamente $t$ steps, questa è una definizione molto più intuitiva, che è anche giustificata dal teorema seguente.\n$$ P^{(M - 1)^{2} + 1}_{ij} \u003e 0 $$ Con $M$ il numero totale di stati, e $ij$ qualunque stato iniziale o finale. Dimostrazione è un esercizio. Può essere molto utile il Chicken McNugget Theorem per dimostrare questo, e fare ragionamenti sul massimo risultato ottenibile. Comunque possiamo dire che esiste un $t$ tale per cui tutti gli stati sono raggiungibili da tutti gli altri in $t$ steps, la dimostrazione si può trovare nel teorema 1.7 di questo Originariamente preso da qui al corso di discrete stochastic processes.\nUnichain Una catena che contiene una singola classe recurrent più alcuni stati transienti\nChapman-Kolmogorov Equation $$ P^{n + m}_{ij} = \\sum_{k = 0}^{N} P_{ik}^{n} P_{kj}^{m} $$ Ossia posso moltiplicare matrici di transizione assieme per avere la probabilità di muovermi da uno stato $i$ a uno stato $j$ in $n + m$ passi.\nPossiamo scrivere questa equazione nella forma continua, che è utile per l’analisi di processi di diffusione: Quando scritto è preso da qui, pagina 30: in questo caso $t$ sono i passi di tempo\nConvergenza Ha senso pensare che una catena di Markov converga nel proseguire delle transazioni.\nTeorema di convergenza per catene ergodiche Questo è un teorema importante. Fatto sta che esiste una distribuzione stazionaria una volta che ho fatto abbastanza passi. Da fare.\nConvergenza per unichains ergodiche. Stationary distributions Una distribuzione $\\pi$ è detta stazionaria se vale che $\\pi = \\pi P$. ossia la probabilità di finire in uno stato $x$ dopo aver fatto una altra mossa seguendo la distribuzione $\\pi$ è uguale a $\\pi(x)$.\nThe Ergodic Theorem Questa è una generalizzazione della Legge dei grandi numeri (guarda qui Central Limit Theorem and Law of Large Numbers), per catene di Markov ergodiche. In generale ci permette di utilizzare catene di markov per fare stime di probabilità, elementi che risultano molto utili per fare Markov Chain Monte Carlo.\n$$ \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{t = 0}^{n}f(X_{t}) = \\sum_{x \\in S}\\pi(x)f(x) \\approx \\mathbb{E}_{x \\sim \\pi}[f(x)] $$Questo teorema ci permetterà di fare sampling, utilizzando catene di Markov.\nSee appendix C of “Markov chains and mixing times” (Levin and Peres, 2017) for a proof.\nQuesto ci dice che l’estimatore che abbiamo è unbiased.\nInoltre abbiamo un burn-in time prima di iniziare a fare sampling in modo corretto, quindi i primi samples vengono scartati.\nDetailed Balance Equation Intuitivamente questa assunzione ci dice che per catene di Markov Ergodiche tale che per cui probabilità di andare da $x \\to x'$ e da $x' \\to x$ è esattamente uguale, cosa non ovvia per catene di Markov qualunque, ammettono una distribuzione stazionaria $Q(x)$ per ogni stato $Q$. Questo ci da un modo per costruire Markov Chains in modo che producano seguendo una distribuzione $Q$ data a priori.\n$$ \\pi(x)P(x \\mid y) = \\pi(y)P(y\\mid x) $$ Queste catene di Markov si dicono anche reversible per l’osservazione di sopra.\nSe una catena di Markov è reversible per una certa distribuzione $\\pi$ allora quella è la distribuzione stazionaria della catena.\n$$ \\begin{align} P(X_{t + 1} = x') = \\\\ \\sum_{x}\\pi(X_{t} = x)P(X_{t + 1} = x' \\mid x) = \u0026\u0026 \\text{ using marginalization and product}\\\\ \\sum_{x}\\pi(X_{t} = x')P(X_{t+1} =x \\mid x') = \u0026\u0026 \\text{reversibility}\\\\ \\pi(X_{t} = x')\\sum_{x}P(X_{t+1} =x \\mid x') = \\pi( x') \\\\ \\end{align} $$Che è la distribuzione stazionaria che volevamo. Si vede che è stazionaria perché facendo un update della catena, la probabilità resta ancora la stessa, per ogni stato di partenza.\nCon rewards Vogliamo associare a ogni stato $i$ un reward $r_{i}$ Si può creare allora una altra variabile aleatoria che prende la variabile aleatoria di Markov $X_{i}$ e lo mappa a un reward. Quello che ci interessano di più sono le expectation dei rewards.\nNoi vogliamo il valore\n$$ E[R(X_{n})| X_{0} = i] = \\sum_{j}r_{j}P_{ij}^{n} $$ E per la proprietà di Markov credo sia la stessa cosa quando non parto da step 0.\nAggregate reward function Questo è definito anche come value function in Reinforcement Learning, a introduction.\n$v_{i}(n) = E[R(X_{m}) + \\dots + R(X_{m + n - 1}) | X_{m} = i]$\nSe la catena è convergente, abbiamo che anche il value function è convergente a un valore preciso, ed è:\n$$ g = \\sum \\pi_{j}r_{j} = \\vec{\\pi} \\cdot \\vec{r} $$ Indipendentemente allo stato iniziale (che stupisce molto).\nReferences [1] Cover \u0026 Thomas “Elements of Information Theory” John Wiley \\\u0026 Sons 2012 ",
  "wordCount" : "1298",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/markov-chains/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Markov Chains
    </h1>
    <div class="post-meta">Reading Time: 7 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#introduzione-alle-catene-di-markov" aria-label="Introduzione alle catene di Markov">Introduzione alle catene di Markov</a><ul>
                        
                <li>
                    <a href="#la-propriet%c3%a0-di-markov" aria-label="La proprietà di Markov">La proprietà di Markov</a></li>
                <li>
                    <a href="#la-catena-di-markov" aria-label="La catena di Markov">La catena di Markov</a></li>
                <li>
                    <a href="#catena-di-3-variabili" aria-label="Catena di 3 variabili">Catena di 3 variabili</a></li>
                <li>
                    <a href="#data-processing-inequality" aria-label="Data processing inequality">Data processing inequality</a></li></ul>
                </li>
                <li>
                    <a href="#definizioni-comuni" aria-label="Definizioni Comuni">Definizioni Comuni</a><ul>
                        
                <li>
                    <a href="#raggiungibilit%c3%a0" aria-label="Raggiungibilità">Raggiungibilità</a></li>
                <li>
                    <a href="#classe-di-stati" aria-label="Classe di stati">Classe di stati</a></li>
                <li>
                    <a href="#recurrent-vs-transient" aria-label="Recurrent vs Transient">Recurrent vs Transient</a></li>
                <li>
                    <a href="#periodic-vs-aperiodic" aria-label="Periodic vs Aperiodic">Periodic vs Aperiodic</a></li>
                <li>
                    <a href="#ergodic-markov-chain" aria-label="Ergodic Markov Chain">Ergodic Markov Chain</a></li>
                <li>
                    <a href="#unichain" aria-label="Unichain">Unichain</a></li>
                <li>
                    <a href="#chapman-kolmogorov-equation" aria-label="Chapman-Kolmogorov Equation">Chapman-Kolmogorov Equation</a></li></ul>
                </li>
                <li>
                    <a href="#convergenza" aria-label="Convergenza">Convergenza</a><ul>
                        
                <li>
                    <a href="#teorema-di-convergenza-per-catene-ergodiche" aria-label="Teorema di convergenza per catene ergodiche">Teorema di convergenza per catene ergodiche</a></li>
                <li>
                    <a href="#convergenza-per-unichains-ergodiche" aria-label="Convergenza per unichains ergodiche.">Convergenza per unichains ergodiche.</a></li>
                <li>
                    <a href="#stationary-distributions" aria-label="Stationary distributions">Stationary distributions</a></li>
                <li>
                    <a href="#the-ergodic-theorem" aria-label="The Ergodic Theorem">The Ergodic Theorem</a></li>
                <li>
                    <a href="#detailed-balance-equation" aria-label="Detailed Balance Equation">Detailed Balance Equation</a></li></ul>
                </li>
                <li>
                    <a href="#con-rewards" aria-label="Con rewards">Con rewards</a><ul>
                        
                <li>
                    <a href="#aggregate-reward-function" aria-label="Aggregate reward function">Aggregate reward function</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="introduzione-alle-catene-di-markov">Introduzione alle catene di Markov<a hidden class="anchor" aria-hidden="true" href="#introduzione-alle-catene-di-markov">#</a></h3>
<h4 id="la-proprietà-di-markov">La proprietà di Markov<a hidden class="anchor" aria-hidden="true" href="#la-proprietà-di-markov">#</a></h4>
<p>Una sequenza di variabili aleatorie $X_{1}, X_{2}, X_{3}, \dots$ gode della proprietà di Markov se vale:</p>
$$
P(X_{n}| X_{n - 1}, X_{n - 2}, \dots, X_{1}) = P(X_{n}|X_{n-1})
$$<p>
Ossia posso scordarmi tutta la <strong>storia precedente</strong>, mi interessa solamente lo stato precedente per sapere la probabilità attuale.</p>
<p>Da un punto di vista filosofico/fisico, ha senso perché mi sta dicendo che posso predire lo stato successivo se ho una conoscenza (completa, (lo dico io completo, originariamente non esiste)) del presente.</p>
<h4 id="la-catena-di-markov">La catena di Markov<a hidden class="anchor" aria-hidden="true" href="#la-catena-di-markov">#</a></h4>
$$
\mathbb{P}(X_{t+1} = j \mid X_{0} = i_{0}, \dots, X_{t} = i_{t}) = \mathbb{P}(X_{t + 1} = j \mid X_{t} = i_{t}) = P_{ij}
$$<p>
Queste catene sono dette <strong>time-homogeneus</strong>.
Con la probabilità di Markov si può dimostrare che se $X_{0} \sim \mu_{0}$ con $\mu_{0}$ il vettore riga, allora la probabilità della distribuzione $X_{t}$ sarà $\mu_{t} = \mu_{0}P^{t}$.
We have that $\mu$ is a stationary distribution if $\mu = \mu P$ is valid. A study of this property is sometimes interesting.</p>
<h4 id="catena-di-3-variabili">Catena di 3 variabili<a hidden class="anchor" aria-hidden="true" href="#catena-di-3-variabili">#</a></h4>
$$
p(x, y, z) = p(x)p(y|x)p(z|y)
$$$$
p(x, y, z) = p(z)p(y|z)p(x|y)
$$$$
P(x, z|y) = P(x|y)P(z|y)
$$<p>
Che dovrebbe essere una conseguenza diretta della parte di sopra.
Una altra osservazione è che se vale quella catena, vale anche l&rsquo;inversa, ossia $Z \to Y \to X$.</p>
<p>Abbiamo analizzato molte catene di questo genere quando abbiamo parlato di d-separabilità in <a href="/notes/counterfactual-invariance">Counterfactual Invariance</a>.</p>
<h4 id="data-processing-inequality">Data processing inequality<a hidden class="anchor" aria-hidden="true" href="#data-processing-inequality">#</a></h4>
$$
I(X ; Y) \geq I(X; Z)
$$<p>
Perché una parte di computazione è possibile modellarlo con la catena di Markov. E mi sta dicendo che l&rsquo;informazione comune all&rsquo;input $X$ con  l&rsquo;output $Y$ o output $Z$ dopo seguente computazione viene sempre meno con più computazione, e anche che non aggiungo informazione con più computazione.</p>
$$
X \to Y \to Z
$$$$
I(X; Y) \geq I(X; Z)
$$<p>
If the equality is satisfied then $Z$ is the sufficient statistic for $Y$. This has some relation with <a href="/notes/the-exponential-family">The Exponential Family</a>.</p>
$$
\begin{align}
I(X;Z) &= I(X; Y, Z) - I(X; Y \mid Z)  \\
&= I(X;Y) - I(X; Z \mid Y)- I(X; Y \mid Z) \\
&\implies I(X;Y) \geq I(X;Z)
\end{align}
$$<h3 id="definizioni-comuni">Definizioni Comuni<a hidden class="anchor" aria-hidden="true" href="#definizioni-comuni">#</a></h3>
<h4 id="raggiungibilità">Raggiungibilità<a hidden class="anchor" aria-hidden="true" href="#raggiungibilità">#</a></h4>
$$
P_{ij}^{m} > 0
$$<p>
Molto più facile vedere sta cosa se lo rappresentiamo come un comunissimo <a href="/notes/grafi">grafo</a>.</p>
<h4 id="classe-di-stati">Classe di stati<a hidden class="anchor" aria-hidden="true" href="#classe-di-stati">#</a></h4>
<p>Sono un insieme di stati tutti raggiungibili fra di loro (comunque presi due stati all&rsquo;interno della classe, esiste un percorso che parte da uno e finisce sull&rsquo;altro per dire).</p>
<h4 id="recurrent-vs-transient">Recurrent vs Transient<a hidden class="anchor" aria-hidden="true" href="#recurrent-vs-transient">#</a></h4>
<p>È <em>recurrent</em> se per ogni nodo, tutti i nodi raggiungibili da un nodo $i$ raggiungono anche il nodo $i$ stesso.
<em>Transient</em> se non è <em>recurrent.</em>
Alcuni chiamano la <em>recurrent</em> come <em>irreducible</em> come in <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">(Cover &amp; Thomas 2012)</a>.</p>
<h4 id="periodic-vs-aperiodic">Periodic vs Aperiodic<a hidden class="anchor" aria-hidden="true" href="#periodic-vs-aperiodic">#</a></h4>
<p>Sia $d$ il massimo comune divisore per tutti gli $m$ tali per cui vale $P_{ii}^{m} > 0$ (ossia può raggiungere sé stesso con probabilità non nulla), allora è periodico se $d > 1$ altrimenti è aperiodico.
Questo implica che non esistono cicli di passi che siano divisibili per un numero $d$, e abbiamo una definizione un poco più intuitiva di periodicità.
Una catena di Markov è aperiodica se tutti i nodi sono aperiodici.</p>
<h4 id="ergodic-markov-chain">Ergodic Markov Chain<a hidden class="anchor" aria-hidden="true" href="#ergodic-markov-chain">#</a></h4>
<p>Una catena di Markov si dice Ergodico se è <em>recurrent</em> e <em>aperiodico</em>. Si può anche definire come se esista un $t$ finito tale per cui tutti gli stati siano raggiungibili da tutti gli altri in esattamente $t$ steps, questa è una definizione molto più intuitiva, che è anche giustificata dal teorema seguente.</p>
$$
P^{(M - 1)^{2} + 1}_{ij} > 0
$$<p>
Con $M$ il numero totale di stati, e $ij$ qualunque stato iniziale o finale. Dimostrazione è un esercizio. Può essere molto utile il <a href="https://artofproblemsolving.com/wiki/index.php/Chicken_McNugget_Theorem">Chicken McNugget Theorem</a> per dimostrare questo, e fare ragionamenti sul massimo risultato ottenibile. Comunque possiamo dire che esiste un $t$ tale per cui tutti gli stati sono raggiungibili da tutti gli altri in $t$ steps, la dimostrazione si può trovare nel teorema 1.7 di <a href="https://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf">questo</a>
Originariamente preso da <a href="https://ocw.mit.edu/courses/6-262-discrete-stochastic-processes-spring-2011/resources/mit6_262s11_chap03/">qui</a> al corso di discrete stochastic processes.</p>
<h4 id="unichain">Unichain<a hidden class="anchor" aria-hidden="true" href="#unichain">#</a></h4>
<p>Una catena che contiene una <strong>singola</strong> classe <em>recurrent</em> più alcuni stati transienti</p>
<h4 id="chapman-kolmogorov-equation">Chapman-Kolmogorov Equation<a hidden class="anchor" aria-hidden="true" href="#chapman-kolmogorov-equation">#</a></h4>
$$
P^{n + m}_{ij} = \sum_{k = 0}^{N} P_{ik}^{n} P_{kj}^{m}
$$<p>
Ossia posso moltiplicare matrici di transizione assieme per avere la probabilità di muovermi da uno stato $i$ a uno stato $j$ in $n + m$ passi.</p>
<p>Possiamo scrivere questa equazione nella forma continua, che è utile per l&rsquo;analisi di processi di diffusione:
Quando scritto è preso da <a href="https://arxiv.org/pdf/cond-mat/0701242">qui</a>, pagina 30: in questo caso $t$ sono i passi di tempo</p>
<h3 id="convergenza">Convergenza<a hidden class="anchor" aria-hidden="true" href="#convergenza">#</a></h3>
<p>Ha senso pensare che una catena di Markov converga nel proseguire delle transazioni.</p>
<h4 id="teorema-di-convergenza-per-catene-ergodiche">Teorema di convergenza per catene ergodiche<a hidden class="anchor" aria-hidden="true" href="#teorema-di-convergenza-per-catene-ergodiche">#</a></h4>
<p>Questo è un teorema importante. Fatto sta che esiste una distribuzione stazionaria una volta che ho fatto abbastanza passi.
Da fare.</p>
<h4 id="convergenza-per-unichains-ergodiche">Convergenza per unichains ergodiche.<a hidden class="anchor" aria-hidden="true" href="#convergenza-per-unichains-ergodiche">#</a></h4>
<h4 id="stationary-distributions">Stationary distributions<a hidden class="anchor" aria-hidden="true" href="#stationary-distributions">#</a></h4>
<p>Una distribuzione $\pi$ è detta stazionaria se vale che $\pi = \pi P$. ossia la probabilità di finire in uno stato $x$ dopo aver fatto una altra mossa seguendo la distribuzione $\pi$ è uguale a $\pi(x)$.</p>
<h4 id="the-ergodic-theorem">The Ergodic Theorem<a hidden class="anchor" aria-hidden="true" href="#the-ergodic-theorem">#</a></h4>
<p>Questa è una generalizzazione della Legge dei grandi numeri (guarda qui <a href="/notes/central-limit-theorem-and-law-of-large-numbers">Central Limit Theorem and Law of Large Numbers</a>), per catene di Markov ergodiche. In generale ci permette di utilizzare catene di markov per fare stime di probabilità, elementi che risultano molto utili per fare Markov Chain Monte Carlo.</p>
$$
\lim_{n \to \infty} \frac{1}{n}\sum_{t = 0}^{n}f(X_{t}) = \sum_{x \in S}\pi(x)f(x) \approx \mathbb{E}_{x \sim \pi}[f(x)]
$$<p>Questo teorema ci permetterà di fare sampling, utilizzando catene di Markov.</p>
<blockquote>
<p>See appendix C of “Markov chains and mixing times” <a href="https://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf">(Levin and Peres, 2017)</a> for a proof.</p></blockquote>
<p>Questo ci dice che l&rsquo;estimatore che abbiamo è <strong>unbiased</strong>.</p>
<p>Inoltre abbiamo un <em>burn-in</em> time prima di iniziare a fare sampling in modo corretto, quindi i primi samples vengono scartati.</p>
<h4 id="detailed-balance-equation">Detailed Balance Equation<a hidden class="anchor" aria-hidden="true" href="#detailed-balance-equation">#</a></h4>
<p>Intuitivamente questa assunzione ci dice che per catene di Markov Ergodiche tale che per cui probabilità di andare da $x \to x'$  e da $x' \to x$ è esattamente uguale, cosa non ovvia per catene di Markov qualunque,  ammettono una distribuzione stazionaria $Q(x)$ per ogni stato $Q$.
Questo ci da un modo per costruire Markov Chains in modo che producano seguendo una distribuzione $Q$ data a priori.</p>
$$
\pi(x)P(x \mid y) = \pi(y)P(y\mid x)
$$<p>
Queste catene di Markov si dicono anche <em>reversible</em> per l&rsquo;osservazione di sopra.</p>
<blockquote>
<p>Se una catena di Markov è <em>reversible</em> per una certa distribuzione $\pi$ allora quella è la distribuzione stazionaria della catena.</p></blockquote>
$$
\begin{align}
P(X_{t + 1} = x') = \\
 \sum_{x}\pi(X_{t} = x)P(X_{t + 1} = x' \mid x) = && \text{ using marginalization and product}\\ 
\sum_{x}\pi(X_{t} = x')P(X_{t+1} =x \mid x') =  && \text{reversibility}\\
\pi(X_{t} = x')\sum_{x}P(X_{t+1} =x \mid x') = \pi( x') \\
\end{align}
$$<p>Che è la distribuzione stazionaria che volevamo.
Si vede che è stazionaria perché facendo un update della catena, la probabilità resta ancora la stessa, per ogni stato di partenza.</p>
<h3 id="con-rewards">Con rewards<a hidden class="anchor" aria-hidden="true" href="#con-rewards">#</a></h3>
<p>Vogliamo associare a ogni stato $i$ un reward $r_{i}$
Si può creare allora una altra variabile aleatoria che prende la variabile aleatoria di Markov $X_{i}$ e lo mappa a un reward.
Quello che ci interessano di più sono le <strong>expectation dei rewards</strong>.</p>
<p>Noi vogliamo il valore</p>
$$
E[R(X_{n})| X_{0} = i] = \sum_{j}r_{j}P_{ij}^{n}
$$<p>
E per la proprietà di Markov credo sia la stessa cosa quando non parto da step 0.</p>
<h4 id="aggregate-reward-function">Aggregate reward function<a hidden class="anchor" aria-hidden="true" href="#aggregate-reward-function">#</a></h4>
<p>Questo è definito anche come <strong>value function</strong> in <a href="/notes/reinforcement-learning,-a-introduction">Reinforcement Learning, a introduction</a>.</p>
<p>$v_{i}(n) = E[R(X_{m}) + \dots + R(X_{m + n - 1}) | X_{m} = i]$</p>
<p>Se la catena è convergente, abbiamo che anche il value function è convergente a un valore preciso, ed è:</p>
$$
g = \sum \pi_{j}r_{j} = \vec{\pi} \cdot \vec{r}
$$<p>
Indipendentemente allo stato iniziale (che stupisce molto).</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=coverElementsInformationTheory2012>[1] Cover & Thomas <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">“Elements of Information Theory”</a> John Wiley \& Sons 2012
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">➕Probabilistic-Artificial-Intelligence</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Markov Chains on x"
            href="https://x.com/intent/tweet/?text=Markov%20Chains&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmarkov-chains%2f&amp;hashtags=%e2%9e%95probabilistic-artificial-intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Markov Chains on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmarkov-chains%2f&amp;title=Markov%20Chains&amp;summary=Markov%20Chains&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fmarkov-chains%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Markov Chains on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fmarkov-chains%2f&title=Markov%20Chains">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Markov Chains on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fmarkov-chains%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Markov Chains on whatsapp"
            href="https://api.whatsapp.com/send?text=Markov%20Chains%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fmarkov-chains%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Markov Chains on telegram"
            href="https://telegram.me/share/url?text=Markov%20Chains&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmarkov-chains%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Markov Chains on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Markov%20Chains&u=https%3a%2f%2fflecart.github.io%2fnotes%2fmarkov-chains%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
