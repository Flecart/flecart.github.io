<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Normalizing Flows | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machine-perception">
<meta name="description" content="Normalizing flows have both  latent space and can produce tractable explicit probability distributions (closer to Autoregressive Modelling, they have tractable distributions, but not a latent space). This means we are able to get the likelihoods of a certain sample.

This approach to modelling a flexible distribution is called a normalizing flow because the transformation of a probability distribution through a sequence of mappings is somewhat analogous to the flow of a fluid. From (Bishop &amp; Bishop 2024)">
<meta name="author" content="
By Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/normalizing-flows/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f790d9af969c56c079c1ce2d5972a04486bf3d6144295d5fba319830e1e55a7a.css" integrity="sha256-95DZr5acVsB5wc4tWXKgRIa/PWFEKV1fujGYMOHlWno=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/normalizing-flows/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/normalizing-flows/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Normalizing Flows">
  <meta property="og:description" content="Normalizing flows have both latent space and can produce tractable explicit probability distributions (closer to Autoregressive Modelling, they have tractable distributions, but not a latent space). This means we are able to get the likelihoods of a certain sample.
This approach to modelling a flexible distribution is called a normalizing flow because the transformation of a probability distribution through a sequence of mappings is somewhat analogous to the flow of a fluid. From (Bishop &amp; Bishop 2024)">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="Machine-Perception">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Normalizing Flows">
<meta name="twitter:description" content="Normalizing flows have both  latent space and can produce tractable explicit probability distributions (closer to Autoregressive Modelling, they have tractable distributions, but not a latent space). This means we are able to get the likelihoods of a certain sample.

This approach to modelling a flexible distribution is called a normalizing flow because the transformation of a probability distribution through a sequence of mappings is somewhat analogous to the flow of a fluid. From (Bishop &amp; Bishop 2024)">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Normalizing Flows",
      "item": "https://flecart.github.io/notes/normalizing-flows/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Normalizing Flows",
  "name": "Normalizing Flows",
  "description": "Normalizing flows have both latent space and can produce tractable explicit probability distributions (closer to Autoregressive Modelling, they have tractable distributions, but not a latent space). This means we are able to get the likelihoods of a certain sample.\nThis approach to modelling a flexible distribution is called a normalizing flow because the transformation of a probability distribution through a sequence of mappings is somewhat analogous to the flow of a fluid. From (Bishop \u0026amp; Bishop 2024)\n",
  "keywords": [
    "machine-perception"
  ],
  "articleBody": "Normalizing flows have both latent space and can produce tractable explicit probability distributions (closer to Autoregressive Modelling, they have tractable distributions, but not a latent space). This means we are able to get the likelihoods of a certain sample.\nThis approach to modelling a flexible distribution is called a normalizing flow because the transformation of a probability distribution through a sequence of mappings is somewhat analogous to the flow of a fluid. From (Bishop \u0026 Bishop 2024)\nThe main idea The intuition here is that we can both have a latent space and some sort of autoregressive modelling. We want\nAn analytical model that is easy to sample from. We want this distribution to be able to represent complex data. The idea is to use many change of variables, $$\\iint_R f(x,y) \\, dx \\, dy = \\iint_G f(g(u,v),h(u,v)) \\, J(u,v) \\, du \\, dv$$$$ dx = \\left\\lvert \\det \\left( \\frac{ \\partial f^{-1}(z) }{ \\partial z } \\right) \\right\\rvert dz $$$$ dx = dz \\left\\lvert \\det \\left( \\frac{ \\partial f(z) }{ \\partial z } \\right) \\right\\rvert ^{-1} $$ And you can remember that if we have an invertible matrix $A$ then you can take out the inverse thing.\n$$ \\begin{align*} p_{x}(x) = p_{z}(A^{-1}x) \\lvert \\det(A^{-1}) \\rvert \\\\ p_{x}(x) = p_{z}(f^{-1}(x)) \\left\\lvert \\det\\left( \\frac{ \\partial f^{-1}(x) }{ \\partial x } \\right) \\right\\rvert = p_{z}(f^{-1}(x)) \\left\\lvert \\det\\left( \\frac{ \\partial f(z) }{ \\partial z } \\right) \\right\\rvert^{-1} \\\\ \\end{align*} $$Normalizing Flows Normalizing flows are direct application of the change of variables formula. we have three desiderata:\nInvertible Differentiable Preserve dimensionality The model $$ f_{\\theta}: \\mathbb{R}^{d} \\to \\mathbb{R}^{d}, \\text{s.t., } X = f_{\\theta}(Z) \\cap Z = f_{\\theta}^{-1}(X) $$ It is important that the transformation is invertible (but this raises also expressivity concerns, because it limits possible functions).\n$$ p_{X}(x ; \\theta) = p_{z} (f^{-1}_{\\theta}(x)) \\cdot \\left\\lvert \\det \\left( \\frac{ \\partial f^{-1}_{\\theta}(x) }{ \\partial x } \\right) \\right\\rvert $$Parameterizing the transformation We want to find a way to learn the function $f$. The difficulty is having invertible neural networks here, and that it preserves the dimensionality. (Some activations like ReLU are not invertible). And we want to be able to compute the Jacobian efficiently, it is possible to compute it in $O(d)$ if we have a triangular matrix (the determinant is just the diagonal).\n$$ x = f(z) = f_{k} \\circ f_{k - 1} \\circ \\ldots \\circ f_{1}(z) $$$$ p_{X}(x ; \\theta) = p_{Z}(z) \\cdot \\prod_{i = 1}^{k} \\left\\lvert \\det \\left( \\frac{ \\partial f_{i}^{-1}(x) }{ \\partial x } \\right) \\right\\rvert $$ Taking the log we have that our final density is:\n$$ \\log p_{X}(x ; \\theta) = \\log p_{Z}(f^{-1}(x)) + \\sum_{i = 1}^{k} \\log \\left\\lvert \\det \\left( \\frac{ \\partial f_{i}^{-1}(x) }{ \\partial x } \\right) \\right\\rvert $$ And we can do for all points in the dataset. A nice thing about invertibility is:\nWe can compute the probability of a true sample We can also generate new samples. The Coupling Layer The main idea of a normalizing flow is to couple some invertible transformation to be able to get both the likelihood of generated data and the generation itself. Simply stacking linear layers, though invertible, does not work (stack of linear layers is still a linear layer)\nHere we use a similar idea present in cryptography for the Feistel network, see Block Ciphers, the idea to partition the space of parameters and apply a transformation to only part of the parameters, this technique has been named real NVP (non-volume preserving). $\\beta$ can be an arbitrarily complex function, since it structure allows to preserve that value.\n$$ \\begin{pmatrix} y_{A} \\\\ y_{B} \\end{pmatrix} = \\begin{pmatrix} h(x_{A}, \\beta_{\\phi}(x_{B}); \\theta) \\\\ x_{B} \\end{pmatrix} $$$$ \\begin{pmatrix} x_{A} \\\\ x_{B} \\end{pmatrix} = \\begin{pmatrix} h^{-1}(y_{A}, \\beta_{\\phi}(y_{B}); \\theta) \\\\ y_{B} \\end{pmatrix} $$$$ h(x_{A}, x_{B}; \\theta) = \\exp(\\theta(x_{A})) \\odot x_{B} + \\mu(x_{A}, w) $$ Where $\\odot$ is the element-wise product, and $\\mu$ is a function of $x_{A}$ and $w$ (the parameters). You can see that the inverse is quite easy do get.\n$$ \\frac{\\partial y}{\\partial x} = \\begin{pmatrix} \\frac{\\partial h}{\\partial x_{A}} \u0026 \\frac{\\partial h}{\\partial x_{B}} \\\\ 0 \u0026 I \\end{pmatrix} $$ And this determinant is very easy to compute since it’s a diagonal matrix. This design makes it good!\nTraining the flow of transformations $$ \\log p_{x}(x) = \\log p_{z}(f^{-1}_{\\theta}(x)) + \\sum_{i=1}^{k} \\log \\left\\lvert \\det \\left( \\frac{ \\partial f_{i}^{-1}(x) }{ \\partial x } \\right) \\right\\rvert $$ And summing for the whole dataset. Because of the structure, we can explicitly evaluate how probable is the sampled sample.\nContinuous Normalizing Flows We don’t know how many layers we would need, the same problem we had in Clustering. We use similar idea of Dirichlet Processes, going into continuous mode, or meta parametrization, and you have continuous normalizing flows, which is something close to (Chen et al. 2019), so that you do not need to set up a specific number of layers. This was the main selling point for these kind of models. Flow matching and diffusion models made them quite famous.\nResolution is quite slow because it preserves dimensions (a lot of computational time).\nPlanar Normalizing Flows Given a latent variable $z \\in \\mathbb{R}^{d}$ and $x \\in \\mathbb{R}^{d}$ the mapping is a function in the form:\n$$ x = f(z) = z + u \\cdot h (w^{T}z + b) $$ Where $f$ is continuously differentiable and $h(\\cdot)$ is a non-linear activation others are learnable parameters.\n$$ p_{z}(z) = \\prod_{i} \\frac{1}{\\sqrt{ 2\\pi }} \\exp(-z^{2}_{i} / 2) $$ Then one can prove that the arg min of this model has the form:\n$$ \\begin{align*} \\arg \\min \\mathcal{L} \u0026= \\arg \\max \\log p_{x}(x) \\\\ \u0026= \\arg \\max \\log p_{z}(z) + \\log \\lvert \\det(J^{-1}) \\rvert \\\\ \u0026= \\arg \\max \\log - \\lvert z \\rvert ^{2} / 2 - \\log \\lvert 1 + h'^{T}u^{T}w \\rvert + \\text{ constant} \\end{align*} $$ Where we have substituted the definition of the Gaussian latent seen that the Jacobian is:\n$$ J = \\frac{\\partial x}{\\partial z} = I + h' \\cdot u^{T}w $$The Architecture Squeeze We want to reshape from $4 \\times 4 \\times 1$ to $2 \\times 2\\times 4$ dimensionality thing. One drawback is that at each coupling layer we process only half of the data, shuffling helps plain the processing. Using usually some checked structure to split it into many parts so that we can use flow in parallel So they just change the spatial resolution of the layers.\nFlow step Act norm is for example a batch norm see section in Convolutional Neural Network.\n1x1 Convolutions 1x1 convolutions are generalizations of permutations (i.e. they contain permutations, if the matrix is a permutation matrix), introduced in (Kingma \u0026 Dhariwal 2018).\n$$ W = PL(U + \\text{diag}(s)) $$ Where $L$ is lower triangular, $P$ is a permutation matrix, and $U$ is upper triangular (with 0 on diagonal). One can observe that the log determinant of this matrix is just the sum of $s$, which is quite quick to compute. The log determinant of this matrix would just be $\\sum s_{i}$ because of the above properties.\n$$ \\begin{pmatrix} y_{A} \\\\ y_{B} \\end{pmatrix} = \\begin{pmatrix} h(x_{A}, \\beta(x_{B}, W)) \\\\ x_{B} \\end{pmatrix} $$Small History of Early Flows NICE (Dinh et al. 2015), Nice, introduces a first working version for NF, but just does swapping for the variables, not quite efficient, and uses additive coupling layer. RealNVP (Dinh et al. 2017): introduces affine coupling layer (adds scale) and a checkboard pattern to better mix the values. GLOW: (Kingma \u0026 Dhariwal 2018): adds the invertible 1x1 convolutions, first paper that generates something high quality, community now sees NFs as architectures that could actually work. Applications Common applications are:\nSuper-resolution You learn a distribution of possible super resoluted images. Disentanglement of fetures (akin to what they do in stylegans, see Generative Adversarial Networks). Multimodal modeling (related to 3D vision applications). 3D shape modelling (see Parametric Human Body Models). 3D pose estimation SRFlow Paper -\u003e (Lugmayr et al. 2020). SRFlow modifies the beta by conditioning on the higher resolution image, achieving a super resolution with flow methods. Since it needs to keep the resolution, the input is conditioned in the beta layer. after n encoding step.\nArchitecture from the paper.\nWe have and encoder $u = g_{\\theta}(x)$ that maps the low resolution image to $u$, then activation maps are modulated with this $u$ value, in a manner somewhat similar to StyleGANs, with feature modulation.\nForward: $h^{n+1} = \\exp(\\beta^{n}_{\\theta, s}(u)) \\cdot h^{n} + \\beta_{\\theta, b}(u)$ Inverse: $h^{n} = (h^{n+1} - \\beta_{\\theta, b}(u)) \\odot \\exp(-\\beta^{n}_{\\theta, s}(u))$ Log determinant is: $\\sum \\beta^{n}_{\\theta,s}(u)$\nThe encoding of the features, is somewhat similar to what they do in Log Linear Models. After training, we can have multiple variants of the same image, in plausible versions.\nStyleFlow This is the paper -\u003e (Abdal et al. 2021). Other works are for example StyleFlow, extension of styleGAN, described in Generative Adversarial Networks, where you can interpolate between identities, or make other continuous disentangled modifications.\nThey use Normalizing Flows to create encoded weights of the style (conditioned with some attribute, e.g. lighting, head pose etc). You can control some nice features such as:\nHead pose Lighting What they mainly do is changing the mapping between the latent space to the mapped features that they then feed into the synthesis model. They found that the $W$ space of the style features where highly expressive, we want to control the features of this space so that we can better choose exactly the shapes that you need. They use attribute conditioned continuous normalized flows.\nGenerate synthetic data, and collect pairs of latent styles and images. They use a pretrained GAN model to do this part. Probably thousands of samples. From the images, they estimate a set of attributes (light, pose etc.) They use the attributes to condition the NF to generate the latent styles. And they use it for many many blocks. Image from the paper | Conditional continuous normalizing flow (CNF) function block realized as a neural network block. Note that the learned function, conditioned on attribute vector at , can be used for both forward and backward inference.\nThe difference is that we have time input since it is a continuous normalizing flow. You can reverse back, change features and forward to change some features of the face, which is quite cool.\nC-Flow for multi-modal data The idea here is to condition one flow with another flow, and use it for more complex part. This section needs further study, because I did not understand.\nHere they introduce the translation between image and point cloud, flow A conditioning, flow B the flow to be conditioned.\nImage from the paper | The two flows can work in different modes of data, but in this way they can share some, not well defined, information\nIn the paper they show results for style transfers in shows. It needs a lot of compute for low resolution (which is the main problem for normalizing flows). And usually they are not so high dimensional.\nOther applications Pose estimation: You have some poses, and you can find the probability of some poses (so you can take just the highest probability pose in a maximum likelihood fashion).\nPointFlow: From Gaussian point cloud to chairs and similar stuff.\nReferences [1] Bishop \u0026 Bishop “Deep Learning: Foundations and Concepts” Springer International Publishing 2024 [2] Chen et al. “Neural Ordinary Differential Equations” arXiv preprint arXiv:1806.07366 2019 [3] Kingma \u0026 Dhariwal “Glow: Generative Flow with Invertible 1x1 Convolutions” arXiv preprint arXiv:1807.03039 2018 [4] Dinh et al. “Density Estimation Using Real NVP” arXiv preprint arXiv:1605.08803 2017 [5] Dinh et al. “NICE: Non-linear Independent Components Estimation” arXiv preprint arXiv:1410.8516 2015 [6] Abdal et al. “StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images Using Conditional Continuous Normalizing Flows” ACM Trans. Graph. Vol. 40(3), pp. 21:1--21:21 2021 [7] Lugmayr et al. “SRFlow: Learning the Super-Resolution Space with Normalizing Flow” ECCV 2020 ",
  "wordCount" : "1957",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/normalizing-flows/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Normalizing Flows
    </h1>
    <div class="post-meta">Reading Time: 10 minutes&nbsp;·&nbsp;
By Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul><ul>
                <li>
                    <a href="#the-main-idea" aria-label="The main idea">The main idea</a></li></ul>
                    
                <li>
                    <a href="#normalizing-flows" aria-label="Normalizing Flows">Normalizing Flows</a><ul>
                        
                <li>
                    <a href="#the-model" aria-label="The model">The model</a></li>
                <li>
                    <a href="#parameterizing-the-transformation" aria-label="Parameterizing the transformation">Parameterizing the transformation</a></li>
                <li>
                    <a href="#the-coupling-layer" aria-label="The Coupling Layer">The Coupling Layer</a></li>
                <li>
                    <a href="#training-the-flow-of-transformations" aria-label="Training the flow of transformations">Training the flow of transformations</a></li>
                <li>
                    <a href="#continuous-normalizing-flows" aria-label="Continuous Normalizing Flows">Continuous Normalizing Flows</a></li>
                <li>
                    <a href="#planar-normalizing-flows" aria-label="Planar Normalizing Flows">Planar Normalizing Flows</a></li></ul>
                </li>
                <li>
                    <a href="#the-architecture" aria-label="The Architecture">The Architecture</a><ul>
                        
                <li>
                    <a href="#squeeze" aria-label="Squeeze">Squeeze</a></li>
                <li>
                    <a href="#flow-step" aria-label="Flow step">Flow step</a></li>
                <li>
                    <a href="#1x1-convolutions" aria-label="1x1 Convolutions">1x1 Convolutions</a></li>
                <li>
                    <a href="#small-history-of-early-flows" aria-label="Small History of Early Flows">Small History of Early Flows</a></li></ul>
                </li>
                <li>
                    <a href="#applications" aria-label="Applications">Applications</a><ul>
                        
                <li>
                    <a href="#srflow" aria-label="SRFlow">SRFlow</a></li>
                <li>
                    <a href="#styleflow" aria-label="StyleFlow">StyleFlow</a></li>
                <li>
                    <a href="#c-flow-for-multi-modal-data" aria-label="C-Flow for multi-modal data">C-Flow for multi-modal data</a></li>
                <li>
                    <a href="#other-applications" aria-label="Other applications">Other applications</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Normalizing flows have both  latent space and can produce tractable <strong>explicit</strong> probability distributions (closer to <a href="/notes/autoregressive-modelling">Autoregressive Modelling</a>, they have tractable distributions, but not a latent space). This means we are able to get the likelihoods of a certain sample.</p>
<blockquote>
<p>This approach to modelling a flexible distribution is called a normalizing flow because the transformation of a probability distribution through a sequence of mappings is somewhat analogous to the flow of a fluid. From <a href="https://link.springer.com/10.1007/978-3-031-45468-4">(Bishop &amp; Bishop 2024)</a></p></blockquote>
<h4 id="the-main-idea">The main idea<a hidden class="anchor" aria-hidden="true" href="#the-main-idea">#</a></h4>
<p>The intuition here is that we can both have a latent space and some sort of autoregressive modelling.
We want</p>
<ul>
<li>An analytical model that is easy to sample from.</li>
<li>We want this distribution to be able to represent complex data.</li>
<li>The idea is to use many change of variables,</li>
</ul>
$$\iint_R f(x,y) \, dx \, dy = \iint_G f(g(u,v),h(u,v)) \, J(u,v) \, du \, dv$$$$
dx = \left\lvert  \det \left( \frac{ \partial f^{-1}(z) }{ \partial z }  \right)   \right\rvert dz
$$$$
dx = dz \left\lvert   \det \left( \frac{ \partial f(z) }{ \partial z }  \right)   \right\rvert ^{-1}
$$<p>
And you can remember that if we have an invertible matrix $A$ then you can take out the inverse thing.</p>
$$
\begin{align*}
p_{x}(x) = p_{z}(A^{-1}x) \lvert \det(A^{-1}) \rvert \\
p_{x}(x) = p_{z}(f^{-1}(x)) \left\lvert  \det\left( \frac{ \partial f^{-1}(x) }{ \partial x }  \right)  \right\rvert = p_{z}(f^{-1}(x)) \left\lvert  \det\left( \frac{ \partial f(z) }{ \partial z }  \right)  \right\rvert^{-1} \\
\end{align*}
$$<h3 id="normalizing-flows">Normalizing Flows<a hidden class="anchor" aria-hidden="true" href="#normalizing-flows">#</a></h3>
<p>Normalizing flows are <strong>direct</strong> application of the change of variables formula.
we have three desiderata:</p>
<ul>
<li>Invertible</li>
<li>Differentiable</li>
<li>Preserve dimensionality</li>
</ul>
<h4 id="the-model">The model<a hidden class="anchor" aria-hidden="true" href="#the-model">#</a></h4>
$$
f_{\theta}: \mathbb{R}^{d} \to \mathbb{R}^{d}, \text{s.t., } X = f_{\theta}(Z) \cap Z = f_{\theta}^{-1}(X)
$$<p>
It is important that the transformation is invertible (but this raises also expressivity concerns, because it limits possible functions).</p>
$$
p_{X}(x ; \theta) = p_{z} (f^{-1}_{\theta}(x)) \cdot \left\lvert  \det \left( \frac{ \partial f^{-1}_{\theta}(x) }{ \partial x }  \right)   \right\rvert
$$<h4 id="parameterizing-the-transformation">Parameterizing the transformation<a hidden class="anchor" aria-hidden="true" href="#parameterizing-the-transformation">#</a></h4>
<p>We want to find a way to learn the function $f$. The difficulty is having <em>invertible neural networks</em> here, and that it preserves the dimensionality. (Some activations like ReLU are not invertible).
And we want to be able to compute the Jacobian efficiently, it is possible to compute it in $O(d)$ if we have a <strong>triangular matrix</strong> (the determinant is just the diagonal).</p>
$$
x  = f(z) = f_{k} \circ f_{k - 1} \circ \ldots \circ f_{1}(z)
$$$$
p_{X}(x ; \theta) = p_{Z}(z) \cdot \prod_{i = 1}^{k} \left\lvert  \det \left( \frac{ \partial f_{i}^{-1}(x) }{ \partial x }  \right)   \right\rvert
$$<p>
Taking the log we have that our final density is:</p>
$$
\log p_{X}(x ; \theta) = \log p_{Z}(f^{-1}(x)) + \sum_{i = 1}^{k} \log \left\lvert  \det \left( \frac{ \partial f_{i}^{-1}(x) }{ \partial x }  \right)   \right\rvert
$$<p>
And we can do for all points in the dataset. A nice thing about invertibility is:</p>
<ul>
<li>We can compute the probability of a true sample</li>
<li>We can also generate new samples.</li>
</ul>
<h4 id="the-coupling-layer">The Coupling Layer<a hidden class="anchor" aria-hidden="true" href="#the-coupling-layer">#</a></h4>
<p>The main idea of a normalizing flow is to couple some <strong>invertible</strong> transformation to be able to get both the likelihood of generated data and the generation itself. Simply stacking linear layers, though invertible, does not work (stack of linear layers is still a linear layer)</p>
<p>Here we use a similar idea present in cryptography for the Feistel network, see <a href="/notes/block-ciphers">Block Ciphers</a>, the idea to partition the space of parameters and apply a transformation to only part of the parameters, this technique has been named real NVP (non-volume preserving).  $\beta$ can be an arbitrarily complex function, since it structure allows to preserve that value.</p>
$$
\begin{pmatrix}
y_{A} \\
y_{B}
\end{pmatrix} = \begin{pmatrix}
h(x_{A}, \beta_{\phi}(x_{B}); \theta) \\
x_{B}
\end{pmatrix}
$$$$
\begin{pmatrix}
x_{A} \\
x_{B}
\end{pmatrix} = \begin{pmatrix}
h^{-1}(y_{A}, \beta_{\phi}(y_{B}); \theta) \\
y_{B}
\end{pmatrix}
$$$$
h(x_{A}, x_{B}; \theta) = \exp(\theta(x_{A})) \odot x_{B} + \mu(x_{A}, w)
$$<p>
Where $\odot$ is the element-wise product, and $\mu$ is a function of $x_{A}$ and $w$ (the parameters). You can see that the inverse is quite easy do get.</p>
$$
\frac{\partial y}{\partial x} = \begin{pmatrix}
\frac{\partial h}{\partial x_{A}} & \frac{\partial h}{\partial x_{B}} \\
0 & I
\end{pmatrix}
$$<p>
And this determinant is very easy to compute since it&rsquo;s a diagonal matrix. This design makes it good!</p>
<h4 id="training-the-flow-of-transformations">Training the flow of transformations<a hidden class="anchor" aria-hidden="true" href="#training-the-flow-of-transformations">#</a></h4>
$$
\log p_{x}(x) = \log p_{z}(f^{-1}_{\theta}(x)) + \sum_{i=1}^{k} \log \left\lvert  \det \left( \frac{ \partial f_{i}^{-1}(x) }{ \partial x }  \right)   \right\rvert
$$<p>
And summing for the whole dataset.
Because of the structure, we can explicitly evaluate how probable is the sampled sample.</p>
<h4 id="continuous-normalizing-flows">Continuous Normalizing Flows<a hidden class="anchor" aria-hidden="true" href="#continuous-normalizing-flows">#</a></h4>
<p>We don&rsquo;t know how many layers we would need, the same problem we had in <a href="/notes/clustering">Clustering</a>.
We use similar idea of <a href="/notes/dirichlet-processes">Dirichlet Processes</a>, going into continuous mode, or meta parametrization, and you have continuous normalizing flows, which is something close to <a href="http://arxiv.org/abs/1806.07366">(Chen et al. 2019)</a>, so that you do not need to set up a specific number of layers. This was the main selling point for these kind of models. Flow matching and diffusion models made them quite famous.</p>
<p>Resolution is quite slow because it preserves dimensions (a lot of computational time).</p>
<h4 id="planar-normalizing-flows">Planar Normalizing Flows<a hidden class="anchor" aria-hidden="true" href="#planar-normalizing-flows">#</a></h4>
<p>Given a latent variable $z \in \mathbb{R}^{d}$ and $x \in \mathbb{R}^{d}$ the mapping is a function in the form:</p>
$$
x = f(z) = z + u \cdot h (w^{T}z + b)
$$<p>
Where $f$ is continuously differentiable and $h(\cdot)$ is a non-linear activation others are learnable parameters.</p>
$$
p_{z}(z) = \prod_{i} \frac{1}{\sqrt{ 2\pi }} \exp(-z^{2}_{i} /  2)
$$<p>
Then one can prove that the arg min of this model has the form:</p>
$$
\begin{align*}
\arg \min \mathcal{L} &= \arg \max \log p_{x}(x) \\
&= \arg \max \log p_{z}(z) + \log \lvert \det(J^{-1}) \rvert    \\
&= \arg \max \log - \lvert z \rvert ^{2} / 2 - \log \lvert 1 + h'^{T}u^{T}w \rvert  + \text{ constant}
\end{align*}
$$<p>
Where we have substituted the definition of the Gaussian latent seen that the Jacobian is:</p>
$$
J = \frac{\partial x}{\partial z} = I + h'  \cdot u^{T}w
$$<h3 id="the-architecture">The Architecture<a hidden class="anchor" aria-hidden="true" href="#the-architecture">#</a></h3>
<h4 id="squeeze">Squeeze<a hidden class="anchor" aria-hidden="true" href="#squeeze">#</a></h4>
<p>We want to reshape from $4 \times 4 \times 1$ to $2 \times 2\times 4$ dimensionality thing. One drawback is that at each coupling layer we process only half of the data, shuffling helps plain the processing.
Using usually some checked structure to split it into many parts so that we can use flow in parallel
So they just change the <strong>spatial resolution</strong> of the layers.</p>
<h4 id="flow-step">Flow step<a hidden class="anchor" aria-hidden="true" href="#flow-step">#</a></h4>
<img src="/images/notes/Normalizing Flows-20250420114254149.webp" style="width: 100%" class="center" alt="Normalizing Flows-20250420114254149">
<p>Act norm is for example a batch norm see section in <a href="/notes/convolutional-neural-network">Convolutional Neural Network</a>.</p>
<h4 id="1x1-convolutions">1x1 Convolutions<a hidden class="anchor" aria-hidden="true" href="#1x1-convolutions">#</a></h4>
<p>1x1 convolutions are generalizations of permutations (i.e. they contain permutations, if the matrix is a permutation matrix), introduced in <a href="http://arxiv.org/abs/1807.03039">(Kingma &amp; Dhariwal 2018)</a>.</p>
$$
W = PL(U + \text{diag}(s))
$$<p>
Where $L$ is lower triangular, $P$ is a permutation matrix, and $U$ is upper triangular (with 0 on diagonal).
One can observe that the log determinant of this matrix is just the sum of $s$, which is quite quick to compute.
The log determinant of this matrix would just be $\sum s_{i}$ because of the above properties.</p>
$$
\begin{pmatrix}
y_{A} \\
y_{B}
\end{pmatrix} = \begin{pmatrix}
h(x_{A}, \beta(x_{B}, W)) \\
x_{B}
\end{pmatrix}
$$<h4 id="small-history-of-early-flows">Small History of Early Flows<a hidden class="anchor" aria-hidden="true" href="#small-history-of-early-flows">#</a></h4>
<ul>
<li>NICE <a href="http://arxiv.org/abs/1410.8516">(Dinh et al. 2015)</a>, Nice, introduces a first working version for NF, but just does swapping for the variables, not quite efficient, and uses additive coupling layer.</li>
<li>RealNVP <a href="http://arxiv.org/abs/1605.08803">(Dinh et al. 2017)</a>: introduces affine coupling layer (adds scale) and a checkboard pattern to better mix the values.</li>
<li>GLOW: <a href="http://arxiv.org/abs/1807.03039">(Kingma &amp; Dhariwal 2018)</a>: adds the invertible 1x1 convolutions, first paper that generates something high quality, community now sees NFs as architectures that could actually work.</li>
</ul>
<h3 id="applications">Applications<a hidden class="anchor" aria-hidden="true" href="#applications">#</a></h3>
<p>Common applications are:</p>
<ul>
<li>Super-resolution
<ul>
<li>You learn a distribution of possible super resoluted images.</li>
</ul>
</li>
<li>Disentanglement of fetures (akin to what they do in stylegans, see <a href="/notes/generative-adversarial-networks">Generative Adversarial Networks</a>).</li>
<li>Multimodal modeling (related to 3D vision applications).</li>
<li>3D shape modelling (see <a href="/notes/parametric-human-body-models">Parametric Human Body Models</a>).</li>
<li>3D pose estimation</li>
</ul>
<h4 id="srflow">SRFlow<a hidden class="anchor" aria-hidden="true" href="#srflow">#</a></h4>
<p>Paper -&gt; <a href="http://ecva.net/papers/eccv_2020/papers_ECCV/papers/123500698.pdf">(Lugmayr et al. 2020)</a>.
SRFlow modifies the beta by conditioning on the higher resolution image, achieving a super resolution with flow methods. Since it needs to keep the resolution, the input is conditioned in the beta layer. after n encoding step.</p>
<figure class="center">
<img src="/images/notes/Normalizing Flows-20250524114413996.webp" style="width: 100%"   alt="Normalizing Flows-20250524114413996" title="Normalizing Flows-20250524114413996"/>
<figcaption><p style="text-align:center;">Architecture from the paper.</p></figcaption>
</figure>
<p>We have and encoder $u = g_{\theta}(x)$ that maps the low resolution image to $u$, then activation maps are modulated with this $u$ value, in a manner somewhat similar to <a href="/notes/generative-adversarial-networks">StyleGANs</a>, with <strong>feature modulation</strong>.</p>
<p>Forward: $h^{n+1} = \exp(\beta^{n}_{\theta, s}(u)) \cdot h^{n} + \beta_{\theta, b}(u)$
Inverse: $h^{n} = (h^{n+1} - \beta_{\theta, b}(u)) \odot \exp(-\beta^{n}_{\theta, s}(u))$
Log determinant is: $\sum \beta^{n}_{\theta,s}(u)$</p>
<p>The encoding of the features, is somewhat similar to what they do in <a href="/notes/log-linear-models">Log Linear Models</a>.
After training, we can have multiple variants of the same image, in plausible versions.</p>
<h4 id="styleflow">StyleFlow<a hidden class="anchor" aria-hidden="true" href="#styleflow">#</a></h4>
<p>This is the paper -&gt; <a href="https://dl.acm.org/doi/10.1145/3447648">(Abdal et al. 2021)</a>.
Other works are for example <strong>StyleFlow</strong>, extension of styleGAN, described in <a href="/notes/generative-adversarial-networks">Generative Adversarial Networks</a>, where you can <em>interpolate</em> between identities, or make other continuous <em>disentangled</em> modifications.</p>
<p>They use Normalizing Flows to create encoded weights of the style (conditioned with some attribute, e.g. lighting, head pose etc). You can control some nice features such as:</p>
<ol>
<li>Head pose</li>
<li>Lighting</li>
</ol>
<p>What they mainly do is changing the mapping between the latent space to the mapped features that they then feed into the synthesis model.
They found that the $W$ space of the style features where highly expressive, we want to control the features of this space so that we can better choose exactly the shapes that you need.
They use <strong>attribute conditioned continuous normalized flows</strong>.</p>
<ul>
<li>Generate synthetic data, and collect pairs of latent styles and images. They use a pretrained GAN model to do this part. Probably thousands of samples.</li>
<li>From the images, they estimate a set of attributes (light, pose etc.)</li>
<li>They use the attributes to condition the NF to generate the latent styles. And they use it for many many blocks.</li>
</ul>
<figure class="center">
<img src="/images/notes/Normalizing Flows-20250524115608914.webp" style="width: 100%"   alt="Normalizing Flows-20250524115608914" title="Normalizing Flows-20250524115608914"/>
<figcaption><p style="text-align:center;">Image from the paper | Conditional continuous normalizing flow (CNF) function block realized as a neural network block. Note that the learned function, conditioned on attribute vector at , can be used for both forward and backward inference.</p></figcaption>
</figure>
<p>The difference is that we have <em>time input</em> since it is a continuous normalizing flow.
You can reverse back, change features and forward to change some features of the face, which is quite cool.</p>
<h4 id="c-flow-for-multi-modal-data">C-Flow for multi-modal data<a hidden class="anchor" aria-hidden="true" href="#c-flow-for-multi-modal-data">#</a></h4>
<p>The idea here is to condition <em>one flow with another flow</em>, and use it for more complex part. This section needs further study, because I did not understand.</p>
<p>Here they introduce the translation between image and point cloud, flow A conditioning, flow B the flow to be conditioned.</p>
<figure class="center">
<img src="/images/notes/Normalizing Flows-20250524150212839.webp" style="width: 100%"   alt="Normalizing Flows-20250524150212839" title="Normalizing Flows-20250524150212839"/>
<figcaption><p style="text-align:center;">Image from the paper | The two flows can work in different modes of data, but in this way they can share some, not well defined, information</p></figcaption>
</figure>
<p>In the paper they show results for style transfers in shows. It needs a lot of compute for low resolution (which is the main problem for normalizing flows). And usually they are not so high dimensional.</p>
<h4 id="other-applications">Other applications<a hidden class="anchor" aria-hidden="true" href="#other-applications">#</a></h4>
<p><strong>Pose estimation:</strong>
You have some poses, and you can find the probability of some poses (so you can take just the highest probability pose in a maximum likelihood fashion).</p>
<p><strong>PointFlow</strong>:
From Gaussian point cloud to chairs and similar stuff.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=bishopDeepLearningFoundations2024>[1] Bishop & Bishop <a href="https://link.springer.com/10.1007/978-3-031-45468-4">“Deep Learning: Foundations and Concepts”</a> Springer International Publishing 2024
 </p>
<p id=chenNeuralOrdinaryDifferential2019>[2] Chen et al. <a href="http://arxiv.org/abs/1806.07366">“Neural Ordinary Differential Equations”</a> arXiv preprint arXiv:1806.07366 2019
 </p>
<p id=kingmaGlowGenerativeFlow2018>[3] Kingma & Dhariwal <a href="http://arxiv.org/abs/1807.03039">“Glow: Generative Flow with Invertible 1x1 Convolutions”</a> arXiv preprint arXiv:1807.03039 2018
 </p>
<p id=dinhDensityEstimationUsing2017>[4] Dinh et al. <a href="http://arxiv.org/abs/1605.08803">“Density Estimation Using Real NVP”</a> arXiv preprint arXiv:1605.08803 2017
 </p>
<p id=dinhNICENonlinearIndependent2015>[5] Dinh et al. <a href="http://arxiv.org/abs/1410.8516">“NICE: Non-linear Independent Components Estimation”</a> arXiv preprint arXiv:1410.8516 2015
 </p>
<p id=abdalStyleFlowAttributeconditionedExploration2021>[6] Abdal et al. <a href="https://dl.acm.org/doi/10.1145/3447648">“StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images Using Conditional Continuous Normalizing Flows”</a> ACM Trans. Graph. Vol. 40(3), pp. 21:1--21:21 2021
 </p>
<p id=lugmayrSRFlowLearningSuperResolution2020>[7] Lugmayr et al. <a href="http://ecva.net/papers/eccv_2020/papers_ECCV/papers/123500698.pdf">“SRFlow: Learning the Super-Resolution Space with Normalizing Flow”</a> ECCV  2020
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machine-perception/">Machine-Perception</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Normalizing Flows on x"
            href="https://x.com/intent/tweet/?text=Normalizing%20Flows&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fnormalizing-flows%2f&amp;hashtags=machine-perception">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Normalizing Flows on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fnormalizing-flows%2f&amp;title=Normalizing%20Flows&amp;summary=Normalizing%20Flows&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fnormalizing-flows%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Normalizing Flows on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fnormalizing-flows%2f&title=Normalizing%20Flows">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Normalizing Flows on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fnormalizing-flows%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Normalizing Flows on whatsapp"
            href="https://api.whatsapp.com/send?text=Normalizing%20Flows%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fnormalizing-flows%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Normalizing Flows on telegram"
            href="https://telegram.me/share/url?text=Normalizing%20Flows&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fnormalizing-flows%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Normalizing Flows on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Normalizing%20Flows&u=https%3a%2f%2fflecart.github.io%2fnotes%2fnormalizing-flows%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
