<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Clustering | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning">
<meta name="description" content="Gaussian Mixture Models This set takes inspiration from chapter 9.2 of (Bishop 2006). We assume that the reader already knows quite well what is a Gaussian mixture model and we will just restate the models here. We will discuss the problem of estimating the best possible parameters (so, this is a density estimation problem) when the data is generated by a mixture of Gaussians.
Remember that the standard multivariate Gaussian has this format: $$ \mathcal{N}(x \mid \mu, \Sigma) = \frac{1}{\sqrt{ 2\pi }} \frac{1}{\lvert \Sigma \rvert^{1/2} } \exp \left( -\frac{1}{2} (x - \mu)^{T} \Sigma^{-1}(x - \mu) \right) $$ Problem statement üü© Given a set of data points $x_{1}, \dots, x_{n}$ in $\mathbb{R}^{d}$ sampled by $k$ Gaussian each with responsibility $\pi_{k}$ the objective of this problem is to estimate the best $\pi_{k}$ for each Gaussian and the relative mean and covariance matrix.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/clustering/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/clustering/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Clustering" />
<meta property="og:description" content="Gaussian Mixture Models This set takes inspiration from chapter 9.2 of (Bishop 2006). We assume that the reader already knows quite well what is a Gaussian mixture model and we will just restate the models here. We will discuss the problem of estimating the best possible parameters (so, this is a density estimation problem) when the data is generated by a mixture of Gaussians.
Remember that the standard multivariate Gaussian has this format: $$ \mathcal{N}(x \mid \mu, \Sigma) = \frac{1}{\sqrt{ 2\pi }} \frac{1}{\lvert \Sigma \rvert^{1/2} } \exp \left( -\frac{1}{2} (x - \mu)^{T} \Sigma^{-1}(x - \mu) \right) $$ Problem statement üü© Given a set of data points $x_{1}, \dots, x_{n}$ in $\mathbb{R}^{d}$ sampled by $k$ Gaussian each with responsibility $\pi_{k}$ the objective of this problem is to estimate the best $\pi_{k}$ for each Gaussian and the relative mean and covariance matrix." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/clustering/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Clustering"/>
<meta name="twitter:description" content="Gaussian Mixture Models This set takes inspiration from chapter 9.2 of (Bishop 2006). We assume that the reader already knows quite well what is a Gaussian mixture model and we will just restate the models here. We will discuss the problem of estimating the best possible parameters (so, this is a density estimation problem) when the data is generated by a mixture of Gaussians.
Remember that the standard multivariate Gaussian has this format: $$ \mathcal{N}(x \mid \mu, \Sigma) = \frac{1}{\sqrt{ 2\pi }} \frac{1}{\lvert \Sigma \rvert^{1/2} } \exp \left( -\frac{1}{2} (x - \mu)^{T} \Sigma^{-1}(x - \mu) \right) $$ Problem statement üü© Given a set of data points $x_{1}, \dots, x_{n}$ in $\mathbb{R}^{d}$ sampled by $k$ Gaussian each with responsibility $\pi_{k}$ the objective of this problem is to estimate the best $\pi_{k}$ for each Gaussian and the relative mean and covariance matrix."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Clustering",
      "item": "https://flecart.github.io/notes/clustering/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Clustering",
  "name": "Clustering",
  "description": "Gaussian Mixture Models This set takes inspiration from chapter 9.2 of (Bishop 2006). We assume that the reader already knows quite well what is a Gaussian mixture model and we will just restate the models here. We will discuss the problem of estimating the best possible parameters (so, this is a density estimation problem) when the data is generated by a mixture of Gaussians.\nRemember that the standard multivariate Gaussian has this format: $$ \\mathcal{N}(x \\mid \\mu, \\Sigma) = \\frac{1}{\\sqrt{ 2\\pi }} \\frac{1}{\\lvert \\Sigma \\rvert^{1/2} } \\exp \\left( -\\frac{1}{2} (x - \\mu)^{T} \\Sigma^{-1}(x - \\mu) \\right) $$ Problem statement üü© Given a set of data points $x_{1}, \\dots, x_{n}$ in $\\mathbb{R}^{d}$ sampled by $k$ Gaussian each with responsibility $\\pi_{k}$ the objective of this problem is to estimate the best $\\pi_{k}$ for each Gaussian and the relative mean and covariance matrix.",
  "keywords": [
    "machinelearning"
  ],
  "articleBody": "Gaussian Mixture Models This set takes inspiration from chapter 9.2 of (Bishop 2006). We assume that the reader already knows quite well what is a Gaussian mixture model and we will just restate the models here. We will discuss the problem of estimating the best possible parameters (so, this is a density estimation problem) when the data is generated by a mixture of Gaussians.\nRemember that the standard multivariate Gaussian has this format: $$ \\mathcal{N}(x \\mid \\mu, \\Sigma) = \\frac{1}{\\sqrt{ 2\\pi }} \\frac{1}{\\lvert \\Sigma \\rvert^{1/2} } \\exp \\left( -\\frac{1}{2} (x - \\mu)^{T} \\Sigma^{-1}(x - \\mu) \\right) $$ Problem statement üü© Given a set of data points $x_{1}, \\dots, x_{n}$ in $\\mathbb{R}^{d}$ sampled by $k$ Gaussian each with responsibility $\\pi_{k}$ the objective of this problem is to estimate the best $\\pi_{k}$ for each Gaussian and the relative mean and covariance matrix. We will assume a latent model with a variable $z$ which represents which Gaussian has been chosen for a specific sample. We have this prior: $$ p(z) = \\prod_{i = 1}^{k} \\pi_{i}^{z_{i}} $$ Because we know that $z$ is a $k$ dimensional vector that has a single digit indicating which Gaussian was chosen.\nThe Maximum Likelihood problem üü©‚Äì The frequentist approach with Maximum likelihood is quite probable to give rise to particular edge-cases that make this method difficult to apply for this density estimation problem. Let‚Äôs remember that in the case of Gaussian mixture models, our loss function is the following: $$ \\min_{\\pi, \\mu, \\Sigma} \\log p(X \\mid \\pi, \\mu, \\Sigma) = \\sum_{n = 1}^{N} \\log \\left\\{ \\sum_{i = 1}^{K} \\pi_{i} \\mathcal{N}(x_{n} \\mid \\mu_{k}, \\Sigma_{k}) \\right\\} $$ So our parameter space is the following $$ \\Theta = \\left\\{ \\pi_{k}, \\mu_{k}, \\Sigma_{k} : k \\leq K \\right\\} $$ Let‚Äôs see now a case where this function is not well behaved. Let‚Äôs consider the covariance matrix to be $\\sigma_{i}^{2}I$ and let‚Äôs say we have sampled a single point that is exactly $\\mu_{i}$ then we have that the contribution of this particular Gaussian to our loss function is $$ \\mathcal{N}(x_{n} \\mid x_{n}, \\mu_{i}, \\sigma_{i}) = \\frac{1}{\\sqrt{ 2\\pi } \\sigma_{_{i}}} $$ If we have a single point, and $\\sigma_{i} \\to 0$ which is reasonable because we have a single point on the mean, then this value explodes and makes the whole log-likelihood to go to infinity. This is a case we don‚Äôt want to explore. There are some methods that try to solve this problem. But in this setting we don‚Äôt want to explore this, and focus on the expectation maximization algorithm.\nIdeas for the solution üü® Let‚Äôs consider for instance the value $p(X, Z)$ this is a known value and it‚Äôs equal to $$ \\log p(X, Z) = \\sum_{n = 1}^{N}\\log \\left\\{ \\pi_{z} \\mathcal{N}(x_{n} \\mid \\mu_{z}, \\Sigma_{z}) \\right\\} = \\sum_{n = 1}^{N} (\\log \\pi_{z} + \\log\\mathcal{N}(x_{n} \\mid \\mu_{z}, \\Sigma_{z})) $$ That decomposition is quite nice. Having had this observation we can write our objective as $$ \\log p(X) = \\mathbb{E}_{Z \\sim q} \\left[ \\log \\frac{P(X, Z)}{P(Z \\mid X)} \\right] $$ Using product rule and using the expectation to get rid of the $Z$.\nThe interesting part comes when we multiply and divide by $q(Z)$ then we can decompose it further into two parts:\n$$ \\log p(X) = \\mathbb{E}_{Z \\sim q} \\left[ \\log \\frac{P(X, Z)}{q(Z)} \\right] + \\mathbb{E}_{Z \\sim q} \\left[ \\log \\frac{q(Z)}{P(Z \\mid X)} \\right] = M(q, \\theta) + E(q, \\theta) $$ We note that $E$ is a Kullback-Leibler divergence so it‚Äôs always positive, and we have $\\log p(X) \\geq M(q, \\theta)$.\nAnother fundamental operation is that we can find the parameters of $q$ such that $E$ is null, because we know that if two distributions are the same then the divergence is null. We can compute this because we know the values of the posterior.\nThe expectation-maximization algorithm üü©‚Äì Dempster et al., 1977; McLachlan and Krishnan, 1997 are useful references for this method.\nThe algorithm in brief goes as follows:\nSet an initial value $\\theta^{(0)}$ for values $t=1,2,\\dots$ Set $q^{*}$ such that $E(q, \\theta^{t - 1}) = 0$, which is just minimizing this value. Set $\\theta$ to the max of $M(q^{*}, \\theta)$. By adequately changing parameters for $p(X, Z)$ which is tractable. From a more high level view:\nCompute the posterior $\\gamma$ Compute the best mean, variance and priors with the formula above and update them Repeat until convergence. It is guaranteed that the likelihood is increasing, but we might be stuck on local maxima and similar things.\nConvergence of EM This is just a bounded optimization problem, after which you use the convergence theorem in Successioni which asserts that the limit for bounded monotone sequences always exists and is Unique.\nWe know that $\\log p(X) = M(q, \\theta) + E(q, \\theta)$, after the $E$ step, the corresponding Kullback-Leibler divergence is 0, so we have $\\log p(X) = M(q', \\theta)$ where $q'$ is the updated variational estimator.\nThen, if we set $\\theta^{'} = \\arg\\max_{\\theta} \\log p(X) = \\arg\\max_{\\theta} M(q', \\theta)$, we have the following equations:\n$$ \\log p_{\\theta'}(X) \\geq M(q', \\theta') \\geq M(q', \\theta) = \\log p_{\\theta}(X) $$ Which is a increasing sequence. The upper bound is trivial, by axiomatic definition of $p$.\nThe importance of the class If you assume to know the class for which the point $x$ is part of, then the problems becomes actually quite easy. This is the original problem that concerns k-means too! We don‚Äôt know a priori which class has been used to generate the point $x$, so taking the expected value accounting for each possibility makes this usually quite hard.\nThis part corresponds to the E-step of the algorithm. In the case of Gaussian Mixture Models, this just corresponds setting $q(z) \\sim p(z \\mid x)$ Which is just: $$ q(z = i) = p(z \\mid x) = \\frac{p(x, z)}{p(x)} = \\frac{\\pi_{i}\\mathcal{N}(x \\mid \\mu_{i}, \\Sigma_{i})}{\\sum_{j} \\pi_{j}\\mathcal{N}(x \\mid \\mu_{j}, \\Sigma_{j})} $$ The Loss Function It is possible to define a loss function with respect to the parameters $\\pi, \\Sigma, \\mu$ after the variational posterior has been fitted in the $E$ step.\nWe denote $$ \\gamma(z_{nk}) = p(z_{j} \\mid x_{n}) = \\frac{p (x_{n} \\mid z_{j}) p(z_{j})}{\\sum_{i} p(x_{n} \\mid z_{i}) p(z_{i})} = \\frac{\\pi_{j} \\mathcal{N}(x_{n} \\mid \\mu_{j}, \\Sigma_{j})}{\\sum_{i} \\pi_{i} \\mathcal{N}(x_{n} \\mid \\mu_{i}, \\Sigma_{i})} $$ Then we can write the loss function as $$ \\begin{align} \\mathcal{L}(\\pi, \\Sigma, \\mu) = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} \\gamma(z_{nk}) (\\log \\lvert \\Sigma_{k} \\rvert + \\frac{1}{2} (x_{n} - \\mu_{k})^{T} \\Sigma^{-1}_{k} (x_{n} - \\mu_{k}) - \\log \\pi_{k}) \\end{align} $$ Then you can derive this loss to get the best mean, Sigma and $\\pi$. This follows (Murphy 2012), but I am not sure I modified it correctly.\nDeriving the expected mean üü®++ Then we continue with the maximization step, which is finding the best variables under this new variational family.\nFirst we want to do some multivariable analysis in order to derive some conditions of the minima, for this reason we take the derivative with respect to $\\mu_{k}$ of the loss equation, and we derive that\n$$ \\begin{align} \\frac{ \\partial \\log p(x) }{ \\partial \\mu_{k} }\u0026= \\frac{\\pi_{k}\\mathcal{N}(x_{n} \\mid \\mu_{k}, \\Sigma_{k})}{\\sum_{j} \\pi_{j} \\mathcal{N} (x_{n} \\mid \\mu_{j}, \\Sigma_{j})} \\frac{ \\partial (x_{n} - \\mu_{k})^{T} \\Sigma^{-1}{k}(x{n} - \\mu_{k}) }{ \\partial \\mu_{k} } \\ \u0026\\implies- \\sum_{n = 1}^{N} \\frac{\\pi_{k}\\mathcal{N}(x_{n} \\mid \\mu_{k}, \\Sigma_{k})}{\\sum_{j} \\pi_{j} \\mathcal{N} (x_{n} \\mid \\mu_{j}, \\Sigma_{j})} \\Sigma^{-1}{k} (x{n} - \\mu_{k}) = 0\\ \u0026\\implies - \\sum_{n = 1}^{N} \\gamma(z_{nk})\\Sigma^{-1}{k}(x{n} - \\mu_{k}) = 0 \\ \u0026\\implies \\sum_{n = 1}^{N} \\gamma(z_{nk})(x_{n} - \\mu_{k}) = 0 \\ \u0026\\implies \\mu_{k} = \\frac{1}{N_{k}} \\sum_{n = 1}^{N} \\gamma(z_{nk}) x_{n}\n\\end{align} $$\nWhere $N_{k} = \\sum_{n = 1}^{N} \\gamma(z_{nk})$\nwe can interpret $N_{k}$ to be the number of points generated by the Gaussian $k$, and the internal part is just the weighted average of the points generated by $k$! This gives an easy interpretation of the mean of the expectation part of this algorithm.\nDeriving the expected deviation This one is harder, and I still have not understood how exactly this matrix derivative is done, but the end results is very similar to the above, we have $$ \\Sigma_{k} = \\frac{1}{N_{k}} \\sum_{n = 1}^{N} \\gamma(z_{nk})(x_{n} - \\mu_{k})(x_{n} -\\mu_{k})^{T} $$ Selecting the number of Clusters You can check this in chapter 25.2 There is a problem at the beginning, even before you can apply the EM algorithm to estimate the probability, we need to choose the hyperparameter $k$ for the number of the classes that we are assuming to exist. We need a way to find a solution to find $k$ that could be more principled than just searching over the possible $k$ in a bruteforce manner.\nWith the stick breaking idea, we assume to have and infinite number of clusters. Then we will have some realizations of the clusters. We have the result that with $N \\to \\infty$ we will have a realization of every cluster. Having a realization means we have one member of $x$ that is part of this class.\nWe discover how to select this with the following section, where we delve into Dirichlet processes.\nK-Means The problem üü© Let‚Äôs say we have a set of $d$ dimensional points $\\mathcal{X} = \\left\\{ x_{1}, \\dots, x_{n} \\right\\}$ We would like to learn a function $$ c : \\mathbb{R}^{d} \\to \\left\\{ 1, \\dots, k \\right\\} $$ That assigns each point some unique label.\nWe consider the prototype which is a representative of one class. In classical k-means we would like to minimize the the squared distance (or other distance function) for each example of the row. We can write: $$ R(c, \\mathcal{Y}) = \\sum_{i = 1}^{N} \\lVert x_{i} - \\mu_{c(x)} \\rVert ^{2} $$ The Algorithm üü© References [1] Murphy ‚ÄúMachine Learning: A Probabilistic Perspective‚Äù 2012\n[2] Bishop ‚ÄúPattern Recognition and Machine Learning‚Äù Springer 2006\n",
  "wordCount" : "1598",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/clustering/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Clustering
    </h1>
    <div class="post-meta">8 min&nbsp;¬∑&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#gaussian-mixture-models" aria-label="Gaussian Mixture Models">Gaussian Mixture Models</a><ul>
                        
                <li>
                    <a href="#problem-statement-" aria-label="Problem statement üü©">Problem statement üü©</a></li>
                <li>
                    <a href="#the-maximum-likelihood-problem---" aria-label="The Maximum Likelihood problem üü©&ndash;">The Maximum Likelihood problem üü©&ndash;</a></li>
                <li>
                    <a href="#ideas-for-the-solution-" aria-label="Ideas for the solution üü®">Ideas for the solution üü®</a></li></ul>
                </li>
                <li>
                    <a href="#the-expectation-maximization-algorithm---" aria-label="The expectation-maximization algorithm üü©&ndash;">The expectation-maximization algorithm üü©&ndash;</a><ul>
                        
                <li>
                    <a href="#convergence-of-em" aria-label="Convergence of EM">Convergence of EM</a></li>
                <li>
                    <a href="#the-importance-of-the-class" aria-label="The importance of the class">The importance of the class</a></li>
                <li>
                    <a href="#the-loss-function" aria-label="The Loss Function">The Loss Function</a></li>
                <li>
                    <a href="#deriving-the-expected-mean-" aria-label="Deriving the expected mean üü®&#43;&#43;">Deriving the expected mean üü®++</a></li>
                <li>
                    <a href="#deriving-the-expected-deviation" aria-label="Deriving the expected deviation">Deriving the expected deviation</a></li>
                <li>
                    <a href="#selecting-the-number-of-clusters" aria-label="Selecting the number of Clusters">Selecting the number of Clusters</a></li></ul>
                </li>
                <li>
                    <a href="#k-means" aria-label="K-Means">K-Means</a><ul>
                        
                <li>
                    <a href="#the-problem-" aria-label="The problem üü©">The problem üü©</a></li>
                <li>
                    <a href="#the-algorithm-" aria-label="The Algorithm üü©">The Algorithm üü©</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="gaussian-mixture-models">Gaussian Mixture Models<a hidden class="anchor" aria-hidden="true" href="#gaussian-mixture-models">#</a></h3>
<p>This set takes inspiration from chapter 9.2 of (Bishop 2006).
We assume that the reader already knows quite well what is a <a href="/notes/gaussian-mixture-models">Gaussian mixture model</a> and we will just restate the models here. We will discuss the problem of estimating the best possible parameters (so, this is a density estimation problem) when the data is generated by a mixture of Gaussians.</p>
<p>Remember that the standard multivariate Gaussian has this format:
</p>
$$
\mathcal{N}(x \mid \mu, \Sigma) = \frac{1}{\sqrt{ 2\pi }} \frac{1}{\lvert \Sigma \rvert^{1/2}  } \exp \left( -\frac{1}{2} (x - \mu)^{T} \Sigma^{-1}(x - \mu) \right) 
$$
<h4 id="problem-statement-">Problem statement üü©<a hidden class="anchor" aria-hidden="true" href="#problem-statement-">#</a></h4>
<p>Given a set of data points $x_{1}, \dots, x_{n}$ in $\mathbb{R}^{d}$ sampled by $k$ Gaussian each with responsibility $\pi_{k}$ the objective of this problem is to estimate the best $\pi_{k}$ for each Gaussian and the relative mean and covariance matrix.
We will assume a <strong>latent</strong> model with a variable $z$ which represents which Gaussian has been chosen for a specific sample.
We have this prior:
</p>
$$
p(z) = \prod_{i = 1}^{k} \pi_{i}^{z_{i}}
$$
<p>
Because we know that $z$ is a $k$ dimensional vector that has a single digit indicating which Gaussian was chosen.</p>
<h4 id="the-maximum-likelihood-problem---">The Maximum Likelihood problem üü©&ndash;<a hidden class="anchor" aria-hidden="true" href="#the-maximum-likelihood-problem---">#</a></h4>
<p>The frequentist approach with Maximum likelihood is quite probable to give rise to particular edge-cases that make this method difficult to apply for this density estimation problem.
Let&rsquo;s remember that in the case of Gaussian mixture models, our loss function is the following:
</p>
$$
\min_{\pi, \mu, \Sigma} \log p(X \mid \pi, \mu, \Sigma) = \sum_{n = 1}^{N} \log \left\{ \sum_{i = 1}^{K} \pi_{i} \mathcal{N}(x_{n} \mid \mu_{k}, \Sigma_{k}) \right\} 
$$
<p>
So our parameter space is the following
</p>
$$
\Theta = \left\{ \pi_{k}, \mu_{k}, \Sigma_{k} : k \leq K \right\} 
$$
<p>Let&rsquo;s see now a case where this function is not well behaved. Let&rsquo;s consider the covariance matrix to be $\sigma_{i}^{2}I$ and  let&rsquo;s say we have sampled a single point that is exactly $\mu_{i}$ then we have that the contribution of this particular Gaussian to our loss function is
</p>
$$
\mathcal{N}(x_{n} \mid x_{n}, \mu_{i}, \sigma_{i})
= \frac{1}{\sqrt{ 2\pi } \sigma_{_{i}}}
$$
<p>
If we have a single point, and $\sigma_{i} \to 0$ which is reasonable because we have a single point on the mean, then this value explodes and makes the whole log-likelihood to go to infinity. This is a case we don&rsquo;t want to explore. There are some methods that try to solve this problem. But in this setting we don&rsquo;t want to explore this, and focus on the expectation maximization algorithm.</p>
<h4 id="ideas-for-the-solution-">Ideas for the solution üü®<a hidden class="anchor" aria-hidden="true" href="#ideas-for-the-solution-">#</a></h4>
<p>Let&rsquo;s consider for instance the value $p(X, Z)$ this is a known value and it&rsquo;s equal to
</p>
$$
\log p(X, Z) = \sum_{n = 1}^{N}\log \left\{ \pi_{z} \mathcal{N}(x_{n} \mid \mu_{z}, \Sigma_{z}) \right\}
= \sum_{n = 1}^{N} (\log \pi_{z} + \log\mathcal{N}(x_{n} \mid \mu_{z}, \Sigma_{z}))
$$
<p>
That decomposition is quite nice.
Having had this observation we can write our objective as
</p>
$$
\log p(X) = \mathbb{E}_{Z \sim q} \left[  \log \frac{P(X, Z)}{P(Z \mid X)} \right]
$$
<p>
Using product rule and using the expectation to get rid of the $Z$.</p>
<p>The interesting part comes when we multiply and divide by $q(Z)$ then we can decompose it further into two parts:</p>
$$
\log p(X) = \mathbb{E}_{Z \sim q} \left[  \log \frac{P(X, Z)}{q(Z)} \right] + \mathbb{E}_{Z \sim q} \left[ \log \frac{q(Z)}{P(Z \mid X)} \right] = M(q, \theta) + E(q, \theta)
$$
<p>
We note that $E$ is a <a href="/notes/entropy/#relative-entropy-or-kullback-leibler">Kullback-Leibler divergence</a> so it&rsquo;s always positive, and we have $\log p(X) \geq M(q, \theta)$.</p>
<p>Another fundamental operation is that we can find the parameters of $q$ such that $E$ is null, because we know that if two distributions are the same then the divergence is null. We can compute this because we know the values of the posterior.</p>
<h3 id="the-expectation-maximization-algorithm---">The expectation-maximization algorithm üü©&ndash;<a hidden class="anchor" aria-hidden="true" href="#the-expectation-maximization-algorithm---">#</a></h3>
<p>Dempster et al., 1977; McLachlan and Krishnan, 1997 are useful references for this method.</p>
<p>The algorithm in brief goes as follows:</p>
<ol>
<li>Set an initial value $\theta^{(0)}$</li>
<li>for values $t=1,2,\dots$
<ol>
<li>Set $q^{*}$ such that $E(q, \theta^{t - 1}) = 0$, which is just minimizing this value.</li>
<li>Set $\theta$ to the max of $M(q^{*}, \theta)$. By adequately changing parameters for $p(X, Z)$ which is tractable.</li>
</ol>
</li>
</ol>
<p>From a more high level view:</p>
<ol>
<li>Compute the posterior $\gamma$</li>
<li>Compute the best mean, variance and priors with the formula above and update them</li>
<li>Repeat until convergence.</li>
</ol>
<p>It is guaranteed that the likelihood is increasing, but we might be stuck on local maxima and similar things.</p>
<h4 id="convergence-of-em">Convergence of EM<a hidden class="anchor" aria-hidden="true" href="#convergence-of-em">#</a></h4>
<p>This is just a bounded optimization problem, after which you use the convergence theorem in <a href="/notes/successioni/">Successioni</a> which asserts that the limit for bounded monotone sequences always exists and is Unique.</p>
<p>We know that $\log p(X) = M(q, \theta) + E(q, \theta)$, after the $E$ step, the corresponding Kullback-Leibler divergence is 0, so we have $\log p(X) = M(q', \theta)$ where $q'$ is the updated variational estimator.</p>
<p>Then, if we set $\theta^{'} = \arg\max_{\theta} \log p(X) = \arg\max_{\theta} M(q', \theta)$, we have the following equations:</p>
$$
\log p_{\theta'}(X) \geq M(q', \theta') \geq M(q', \theta) = \log p_{\theta}(X)
$$
<p>
Which is a increasing sequence. The upper bound is trivial, by axiomatic definition of $p$.</p>
<h4 id="the-importance-of-the-class">The importance of the class<a hidden class="anchor" aria-hidden="true" href="#the-importance-of-the-class">#</a></h4>
<p>If you assume to know the class for which the point $x$ is part of, then the problems becomes actually quite easy. This is the original problem that concerns k-means too! We don&rsquo;t know a priori which class has been used to generate the point $x$, so taking the expected value accounting for each possibility makes this usually quite hard.</p>
<p>This part corresponds to the E-step of the algorithm.
In the case of Gaussian Mixture Models, this just corresponds setting $q(z) \sim p(z \mid x)$
Which is just:
</p>
$$
q(z = i) = p(z \mid x) = \frac{p(x, z)}{p(x)} = \frac{\pi_{i}\mathcal{N}(x \mid \mu_{i}, \Sigma_{i})}{\sum_{j} \pi_{j}\mathcal{N}(x \mid \mu_{j}, \Sigma_{j})}
$$
<h4 id="the-loss-function">The Loss Function<a hidden class="anchor" aria-hidden="true" href="#the-loss-function">#</a></h4>
<p>It is possible to define a loss function with respect to the parameters $\pi, \Sigma, \mu$ after the variational posterior has been fitted in the $E$ step.</p>
<p>We denote
</p>
$$
\gamma(z_{nk}) = p(z_{j} \mid x_{n}) = \frac{p (x_{n} \mid z_{j}) p(z_{j})}{\sum_{i} p(x_{n} \mid z_{i}) p(z_{i})} = \frac{\pi_{j} \mathcal{N}(x_{n} \mid \mu_{j}, \Sigma_{j})}{\sum_{i} \pi_{i} \mathcal{N}(x_{n} \mid \mu_{i}, \Sigma_{i})}
$$
<p>Then we can write the loss function as
</p>
$$
\begin{align}
\mathcal{L}(\pi, \Sigma, \mu) = \sum_{n = 1}^{N} \sum_{k = 1}^{K} \gamma(z_{nk}) (\log \lvert \Sigma_{k} \rvert + \frac{1}{2}  (x_{n} - \mu_{k})^{T} \Sigma^{-1}_{k} (x_{n} - \mu_{k}) - \log \pi_{k})
\end{align}
$$
<p>Then you can derive this loss to get the best mean, Sigma and $\pi$.
This follows <a href="https://mitpress.mit.edu/9780262018029/machine-learning/">(Murphy 2012)</a>, but I am not sure I modified it correctly.</p>
<h4 id="deriving-the-expected-mean-">Deriving the expected mean üü®++<a hidden class="anchor" aria-hidden="true" href="#deriving-the-expected-mean-">#</a></h4>
<p>Then we continue with the maximization step, which is finding the best variables under this new variational family.</p>
<p>First we want to do some <a href="/notes/massimi-minimi-multi-variabile/">multivariable analysis</a> in order to derive some conditions of the minima, for this reason we take the derivative with respect to $\mu_{k}$ of the loss equation, and we derive that</p>
<p>$$
\begin{align}
\frac{ \partial \log p(x) }{ \partial \mu_{k} }&amp;= \frac{\pi_{k}\mathcal{N}(x_{n} \mid \mu_{k}, \Sigma_{k})}{\sum_{j} \pi_{j} \mathcal{N} (x_{n} \mid \mu_{j}, \Sigma_{j})} \frac{ \partial (x_{n} - \mu_{k})^{T} \Sigma^{-1}<em>{k}(x</em>{n} - \mu_{k}) }{ \partial \mu_{k} }  \
&amp;\implies- \sum_{n = 1}^{N} \frac{\pi_{k}\mathcal{N}(x_{n} \mid \mu_{k}, \Sigma_{k})}{\sum_{j} \pi_{j} \mathcal{N} (x_{n} \mid \mu_{j}, \Sigma_{j})} \Sigma^{-1}<em>{k} (x</em>{n} - \mu_{k}) = 0\
&amp;\implies - \sum_{n = 1}^{N} \gamma(z_{nk})\Sigma^{-1}<em>{k}(x</em>{n} - \mu_{k}) = 0 \
&amp;\implies \sum_{n = 1}^{N} \gamma(z_{nk})(x_{n} - \mu_{k}) = 0  \
&amp;\implies \mu_{k} = \frac{1}{N_{k}} \sum_{n = 1}^{N} \gamma(z_{nk}) x_{n}</p>
<p>\end{align}
$$</p>
<p>Where $N_{k} = \sum_{n = 1}^{N} \gamma(z_{nk})$</p>
<p>we can interpret $N_{k}$ to be the number of points generated by the Gaussian $k$, and the internal part is just the <strong>weighted average</strong> of the points generated by $k$! This gives an easy interpretation of the mean of the expectation part of this algorithm.</p>
<h4 id="deriving-the-expected-deviation">Deriving the expected deviation<a hidden class="anchor" aria-hidden="true" href="#deriving-the-expected-deviation">#</a></h4>
<p>This one is harder, and I still have not understood how exactly this matrix derivative is done, but the end results is very similar to the above, we have
</p>
$$
\Sigma_{k} = \frac{1}{N_{k}} \sum_{n = 1}^{N} \gamma(z_{nk})(x_{n} - \mu_{k})(x_{n} -\mu_{k})^{T}
$$
<h4 id="selecting-the-number-of-clusters">Selecting the number of Clusters<a hidden class="anchor" aria-hidden="true" href="#selecting-the-number-of-clusters">#</a></h4>
<p>You can check this in chapter 25.2
There is a problem at the beginning, even before you can apply the EM algorithm to estimate the probability, we need to choose the hyperparameter $k$ for the number of the classes that we are assuming to exist.
We need a way to find a solution to find $k$ that could be more principled than just searching over the possible $k$ in a bruteforce manner.</p>
<p>With the stick breaking idea, we assume to have and infinite number of clusters. Then we will have some realizations of the clusters. We have the result that with $N \to \infty$ we will have a realization of every cluster. Having a realization means we have one member of $x$ that is part of this class.</p>
<p>We discover how to select this with the following section, where we delve into Dirichlet processes.</p>
<h3 id="k-means">K-Means<a hidden class="anchor" aria-hidden="true" href="#k-means">#</a></h3>
<h4 id="the-problem-">The problem üü©<a hidden class="anchor" aria-hidden="true" href="#the-problem-">#</a></h4>
<p>Let&rsquo;s say we have a set of $d$ dimensional points $\mathcal{X} = \left\{ x_{1}, \dots, x_{n} \right\}$
We would like to learn a function
</p>
$$
c : \mathbb{R}^{d} \to \left\{ 1, \dots, k \right\} 
$$
<p>
That assigns each point some <em>unique</em> label.</p>
<p>We consider the <strong>prototype</strong> which is a representative of one class.
In classical k-means we would like to minimize the the squared distance (or other distance function) for each example of the row.
We can write:
</p>
$$
R(c, \mathcal{Y}) = \sum_{i  = 1}^{N} \lVert x_{i} - \mu_{c(x)}  \rVert ^{2}
$$
<h4 id="the-algorithm-">The Algorithm üü©<a hidden class="anchor" aria-hidden="true" href="#the-algorithm-">#</a></h4>
<img src="/images/notes/Non-parametric Modeling-20241205152331188.webp" alt="Non-parametric Modeling-20241205152331188">
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Murphy <a href="https://mitpress.mit.edu/9780262018029/machine-learning/">‚ÄúMachine Learning: A Probabilistic Perspective‚Äù</a>  2012</p>
<p>[2] Bishop ‚ÄúPattern Recognition and Machine Learning‚Äù Springer 2006</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Clustering on x"
            href="https://x.com/intent/tweet/?text=Clustering&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fclustering%2f&amp;hashtags=machinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Clustering on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fclustering%2f&amp;title=Clustering&amp;summary=Clustering&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fclustering%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Clustering on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fclustering%2f&title=Clustering">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Clustering on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fclustering%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Clustering on whatsapp"
            href="https://api.whatsapp.com/send?text=Clustering%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fclustering%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Clustering on telegram"
            href="https://telegram.me/share/url?text=Clustering&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fclustering%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Clustering on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Clustering&u=https%3a%2f%2fflecart.github.io%2fnotes%2fclustering%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
