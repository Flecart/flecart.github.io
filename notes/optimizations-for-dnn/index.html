<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Optimizations for DNN | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="☁cloud-computing">
<meta name="description" content="Mixture of Experts
There is a gate that opens a subset of the experts, and the output is the weighted sum of the outputs of the experts. The weights are computed by a gating network.

One problem is load balancing, non uniform assignment. And there is a lot of communication overhead when you place them in different devices.
LoRA: Low-Rank Adaptation
We only finetune a part of the network, called lora adapters, not the whole thing.
There are two matrices here, a matrix A and B, they are some sort of an Autoencoders, done for every Q nd V matrices in the LLM attention layer. The nice thing is that there are not many inference costs if adapters are merged post training:">
<meta name="author" content="
By Xuanqiang Angelo Huang">
<link rel="canonical" href="https://flecart.github.io/notes/optimizations-for-dnn/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f790d9af969c56c079c1ce2d5972a04486bf3d6144295d5fba319830e1e55a7a.css" integrity="sha256-95DZr5acVsB5wc4tWXKgRIa/PWFEKV1fujGYMOHlWno=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/optimizations-for-dnn/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/optimizations-for-dnn/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Optimizations for DNN">
  <meta property="og:description" content="Mixture of Experts There is a gate that opens a subset of the experts, and the output is the weighted sum of the outputs of the experts. The weights are computed by a gating network. One problem is load balancing, non uniform assignment. And there is a lot of communication overhead when you place them in different devices.
LoRA: Low-Rank Adaptation We only finetune a part of the network, called lora adapters, not the whole thing. There are two matrices here, a matrix A and B, they are some sort of an Autoencoders, done for every Q nd V matrices in the LLM attention layer. The nice thing is that there are not many inference costs if adapters are merged post training:">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2025-06-03T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-06-03T00:00:00+00:00">
    <meta property="article:tag" content="☁Cloud-Computing">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Optimizations for DNN">
<meta name="twitter:description" content="Mixture of Experts
There is a gate that opens a subset of the experts, and the output is the weighted sum of the outputs of the experts. The weights are computed by a gating network.

One problem is load balancing, non uniform assignment. And there is a lot of communication overhead when you place them in different devices.
LoRA: Low-Rank Adaptation
We only finetune a part of the network, called lora adapters, not the whole thing.
There are two matrices here, a matrix A and B, they are some sort of an Autoencoders, done for every Q nd V matrices in the LLM attention layer. The nice thing is that there are not many inference costs if adapters are merged post training:">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Optimizations for DNN",
      "item": "https://flecart.github.io/notes/optimizations-for-dnn/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Optimizations for DNN",
  "name": "Optimizations for DNN",
  "description": "Mixture of Experts There is a gate that opens a subset of the experts, and the output is the weighted sum of the outputs of the experts. The weights are computed by a gating network. One problem is load balancing, non uniform assignment. And there is a lot of communication overhead when you place them in different devices.\nLoRA: Low-Rank Adaptation We only finetune a part of the network, called lora adapters, not the whole thing. There are two matrices here, a matrix A and B, they are some sort of an Autoencoders, done for every Q nd V matrices in the LLM attention layer. The nice thing is that there are not many inference costs if adapters are merged post training:\n",
  "keywords": [
    "☁cloud-computing"
  ],
  "articleBody": "Mixture of Experts There is a gate that opens a subset of the experts, and the output is the weighted sum of the outputs of the experts. The weights are computed by a gating network. One problem is load balancing, non uniform assignment. And there is a lot of communication overhead when you place them in different devices.\nLoRA: Low-Rank Adaptation We only finetune a part of the network, called lora adapters, not the whole thing. There are two matrices here, a matrix A and B, they are some sort of an Autoencoders, done for every Q nd V matrices in the LLM attention layer. The nice thing is that there are not many inference costs if adapters are merged post training:\n$$ h = Wx + BAx = (W + BA)x = W'x $$ And it is very easy to switch them.\nLLM Optimizations Quadratic Bottleneck The attention matrix scales quadratically with respect to the sequence length, which makes it difficult to store. For example, if you have a GPT2 context window of 1024 tokens, which is about 340 words, this would make your attention matrix very large with not many words.\nModern LLMs make tokens longer by having large vocabularies, and they have larger context windows (in the order of the millions for Gemini 1.5 Pro (2 Millions)).\nFlashAttention This is a systems paper which is important for computing introduced in (Dao et al. 2022). This enables exact attention computation but longer context lengths. The compute the attention matrix in a careful way to tile it inside the SRAM, this is some sort of optimization that is close to what you do in Fast Linear Algebra, they basically reduce number of memory reads and writes between GPU high bandwidth memory and on-chip SRAM (similar optimizations are done between RAM and cache, same idea basically).\nImage from the paper: FlashAttention uses tiling to prevent materialization of the large $N \\times N$ attention matrix (dotted box) on (relatively) slow GPU HBM.\nKV Caching There is some redundant computation which is done in the attention matrix due to the autoregressive nature of the inference process. This means that instead of recomputing, we can cache some old results and not compute them again. However, this can grow a lot.\nMultiple parallel requests Complex decoding trategies. This was introduced in (Pope et al. 2023). This motivates to cache Key and Values from the computation. At every step we just need to compute a single vector instead of recomputing the old values.\nPagedAttention A lot of memory of the GPU is filled by the KV Cache. PagedAttention introduces virtual pages, inspired by OS memory access in Memoria virtuale. The System assumes the KV cache is continuous, but it is actually managed in some particular manner. This brings fragmentation. Adding this virtual layer makes better use of the actual resources. The authors claim to have near zero wasted memory in the KV cache memory due to fragmentation.\nSpeculative Decoding You have a small model that runs faster and attempts to predict a group of next tokens. Then you feed it to a large model and verify the tokens are correct (probable enough to have produced these tokens). This is basically Accept Reject algorithm for sampling the tokens. The easy example is with greedy decoding: you just accept all the tokens that match the argmax. But LLM decoding is stochastic, so for this reason it is better to have anohter method that keeps the locality. We use Accept Reject with acceptance probability:\n$$ A(y \\mid x) = \\min\\left( 1, \\frac{L(y\\mid x)}{k \\cdot S(y \\mid x)} \\right) $$ Where $y$ is the new token, $x$ is the prompt, $S$ is the draft fast model and $L$ is the slow model, and we assume: $L(y \\mid x) \\leq k \\cdot S(y \\mid x)$.\nBatching behaviours (He et al. 2025) is a good paper that explains most of the common optimizations that are made for LLMs. In this particular example, we have\nClassical LLM serving infrastructure Retrieval-Augmented Generation (RAG) Pretraining costs a lot, and updating weight knowledge using more training rounds and finetuning costs a lot. RAG addresses the problem of adding more knowledge for the model without any training, but just the so called in-context learning.\nSlowly, RAG is becoming industry standard to serve LLM in a reliable manner. Prompting is becoming a normal thing.\nRAG with internet-scale retrieval The pipeline is very easy: just use websearch to get many many documents, add it as a prefix to the LLM and use standard decoding.\nRAG for long-context processing In many occasions, there are very very long documents that are prefixed to the LLM input. In these cases, it can be very costly to process these kinds of documents.\nIn this case, we just want to add relevant passages, not whole documents and use those for LLM inference. Beginning and end parts of the prompt are better processed by the model, this is a phenomenon known as lost in the middle.\nRAG with iterative retrievals In this case the model can generate a question to generate another query to retrieve other documents. This is usually used with CoT, see (Wei et al. 2023), and sequential information retrievals.\nRAG with rewriter and reranker The idea is to break down the user query into more, or rewrite the query into a format more specific for the dataset.\nVector Search One of the classical methods are TF-IDF with document word frequency counts. The general problem is a Clustering, with nearest neighbors search.\nThe mathematical definition is as follows: Sure, here are the contents of the image as clean, copiable Markdown notes:\nProblem statement Given:\nA query vector $q \\in \\mathbb{R}^d$ A collection of database vectors $\\mathcal{D} = \\{ x_1, x_2, \\dots, x_N \\} \\subset \\mathbb{R}^d$ A similarity (or distance) function $\\text{sim} : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ Vector search aims to find the top-$k$ nearest neighbors of $q$ in $\\mathcal{D}$, defined as:\n$$ \\arg \\, \\text{top-}k \\, \\text{sim}(q, x) \\quad \\text{for } x \\in \\mathcal{D} $$Common Metrics Choosing the metric is usually dependent on the model and application.\nCosine Similarity $\\cos(x, y) = \\frac{x \\cdot y}{||x|| ||y||}$ Euclidean Distance $d(x, y) = ||x - y||$ Manhattan Distance $d(x, y) = \\sum_i |x_i - y_i|$ One big problem with the metrics in high dimensionality is the classical Curse of Dimensionality, discussed in Kernel Methods. You have far sparse sparse points in high dimensional data. Indeed, for single dimensional data, tree searches are a nice and efficient method to solve this problem.\nIVF-PQ Algorithm This is some sort of an approximate nearest neighbor search.\nThis has two steps the first is: Inverted-file index: used to prune the search space to have a smaller set of candidates. This is done by clustering the data and creating an inverted index for each cluster.\nThis step is basically Clustering the data into $n$ predefined clusters, and use their center as some sort of representative vector. The centroids are called inverted lists/index. The important thing here is to store the residual with respect to that center. Product Quantization: used to lossy-compress the vectors we have. This is done by quantizing the vectors into a smaller set of representative vectors. The original $D$ dimensional vectors are first turned into $m$ subvectors, then each subvector is reclustered and approximated into original cluster centroids, then you just use the cluster id of the centroid. Now we have integers, which are very nice, called pico code which saves lots of representation space. Searching is done in a similar manner, and you also build distance lookup tables with these.\nThe idea is to divide the original part into $m$ subvectors and use their relative centroids to give a sort of representation, that is then integer, which is far more memory efficient.\nWhen you search you only scan a subset of the IVF. During search you build a distance loop-up table based on the centroids and use that to compare to the quantized vectors.\nPseudo code for search To give an idea of the search, this is some pseudocode for the search process\ndef IVF_PQ_SEARCH(query_vector q, index, top_k): # Step 1: Coarse quantization coarse_centroids = index.coarse_centroids probe_centroids = find_nearest_centroids(q, coarse_centroids, n_probe) # n_probe is the default number of centroids to return. candidate_vectors = [] for centroid in probe_centroids: # Step 2: Compute residual vector for query residual_q = q - centroid # Step 3: Prepare distance lookup tables for PQ pq_lookup_table = build_lookup_table(residual_q, index.pq_codebooks) # Step 4: For each encoded vector in the selected inverted list for code in index.inverted_list[centroid]: # Step 5: Compute approximate distance using lookup table distance = compute_adc_distance(code, pq_lookup_table) candidate_vectors.append((distance, code.original_id)) # Step 6: Select top-k closest vectors return top_k_smallest(candidate_vectors, k=top_k) Asymmetric Distance Computation (ADC) This is some metric that is used for the residuals usually. In this case, the database is quantized, but the query is not, we need to handle this kind of asymmetries.\nWe want to estimate the squared Euclidean distance between the real-valued query vector $q \\in \\mathbb{R}^d$ and a PQ-compressed vector $\\hat{x}$.\nSplit query vector $q$ into $m$ subvectors (this is the codebook part): $q = [q^{(1)}, q^{(2)}, \\dots, q^{(m)}]$ Each subvector $q^{(i)} \\in \\mathbb{R}^{d/m}$ For each subspace $i = 1, \\dots, m$, precompute a lookup table: $D_i[j] = \\| q^{(i)} - c^{(i)}_j \\|^2$ where $c^{(i)}_j$ is the $j$-th centroid in the codebook for subspace $i$, and $j = 0, \\dots, 255$. So each $D_i$ is a vector of 256 distances For a compressed database vector $\\hat{x}$, represented by codes: $$ \\text{code} = [c_1, c_2, \\dots, c_m] \\quad \\text{with } c_i \\in \\{0, \\dots, 255\\} $$ The total approximate distance is: $$ \\| q - \\hat{x} \\|^2 \\approx \\sum_{i=1}^m D_i[c_i] $$ → Just m table look-ups and m additions, which is easily parallelizable. KNN Graphs Examples of graph databases where studied in Graph Databases in the big data course the first time.\nThe problem here is to build a similarity graph with this kind of representation. One possibility is to build a KNN graph and doing pruning using some heuristics. With this method we have usually high recall, but:\nit is not very flexible with data updates it is quite memory intensive. Incremental insertion Graphs Option 2: Incremental Insertion (Online Construction)\nFor each new point, perform a search Connect to the top nearest nodes Possibly prune to maintain diversity • Pros: Can be done online (supports dynamic updates) More memory-efficient; no need to compute full KNN graph • Cons: Quality depends on insertion order and search quality You don’t have the possibility of getting the same graph every single time. Graph-based vector search The algorithm goes in the following way:\nWe select an entry point in the graph. We keep two queues High quality candidates to visit Existing nearest neighbors We pop from A and add to B We end if there are not enough high-quality candidates to visit or all nodes in A are too far from queue B. Return the best results so far. One of the modern algorithms that solves some problems in vector search that we have now is Hierarchical Navigable Small World, which does some kind of hierarchical search.\nHierarchical Navigable Small World NSW (Navigable Small World) Builds a small-world graph where nodes are connected to their close and some faraway neighbors. Search is greedy: jump to closer nodes until you can’t get closer. HNSW (Hierarchical NSW) Adds multiple layers: Top layers are sparse (longer-range links). Bottom layers are dense (short-range neighbors). Search starts at the top layer and drills down layer by layer, refining the search. References [1] He et al. “PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System” ACM 2025 [2] Pope et al. “Efficiently Scaling Transformer Inference” Curan 2023 [3] Wei et al. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models” arXiv preprint arXiv:2201.11903 2023 [4] Dao et al. “FLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-awareness” Curran Associates Inc. 2022 ",
  "wordCount" : "1990",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "2025-06-03T00:00:00Z",
  "dateModified": "2025-06-03T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang Angelo Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/optimizations-for-dnn/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Optimizations for DNN
    </h1>
    <div class="post-meta"><span title='2025-06-03 00:00:00 +0000 UTC'>June 3, 2025</span>&nbsp;·&nbsp;Reading Time: 10 minutes&nbsp;·&nbsp;
By Xuanqiang Angelo Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#mixture-of-experts" aria-label="Mixture of Experts">Mixture of Experts</a><ul>
                        
                <li>
                    <a href="#lora-low-rank-adaptation" aria-label="LoRA: Low-Rank Adaptation">LoRA: Low-Rank Adaptation</a></li></ul>
                </li>
                <li>
                    <a href="#llm-optimizations" aria-label="LLM Optimizations">LLM Optimizations</a><ul>
                        
                <li>
                    <a href="#quadratic-bottleneck" aria-label="Quadratic Bottleneck">Quadratic Bottleneck</a></li>
                <li>
                    <a href="#flashattention" aria-label="FlashAttention">FlashAttention</a></li>
                <li>
                    <a href="#kv-caching" aria-label="KV Caching">KV Caching</a></li>
                <li>
                    <a href="#pagedattention" aria-label="PagedAttention">PagedAttention</a></li>
                <li>
                    <a href="#speculative-decoding" aria-label="Speculative Decoding">Speculative Decoding</a></li>
                <li>
                    <a href="#batching-behaviours" aria-label="Batching behaviours">Batching behaviours</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#classical-llm-serving-infrastructure" aria-label="Classical LLM serving infrastructure">Classical LLM serving infrastructure</a><ul>
                        
                <li>
                    <a href="#retrieval-augmented-generation-rag" aria-label="Retrieval-Augmented Generation (RAG)">Retrieval-Augmented Generation (RAG)</a><ul>
                        
                <li>
                    <a href="#rag-with-internet-scale-retrieval" aria-label="RAG with internet-scale retrieval">RAG with internet-scale retrieval</a></li>
                <li>
                    <a href="#rag-for-long-context-processing" aria-label="RAG for long-context processing">RAG for long-context processing</a></li>
                <li>
                    <a href="#rag-with-iterative-retrievals" aria-label="RAG with iterative retrievals">RAG with iterative retrievals</a></li>
                <li>
                    <a href="#rag-with-rewriter-and-reranker" aria-label="RAG with rewriter and reranker">RAG with rewriter and reranker</a></li></ul>
                </li>
                <li>
                    <a href="#vector-search" aria-label="Vector Search">Vector Search</a><ul>
                        
                <li>
                    <a href="#problem-statement" aria-label="Problem statement">Problem statement</a></li>
                <li>
                    <a href="#common-metrics" aria-label="Common Metrics">Common Metrics</a></li>
                <li>
                    <a href="#ivf-pq-algorithm" aria-label="IVF-PQ Algorithm">IVF-PQ Algorithm</a></li>
                <li>
                    <a href="#pseudo-code-for-search" aria-label="Pseudo code for search">Pseudo code for search</a></li>
                <li>
                    <a href="#asymmetric-distance-computation-adc" aria-label="Asymmetric Distance Computation (ADC)">Asymmetric Distance Computation (ADC)</a></li>
                <li>
                    <a href="#knn-graphs" aria-label="KNN Graphs">KNN Graphs</a></li>
                <li>
                    <a href="#incremental-insertion-graphs" aria-label="Incremental insertion Graphs">Incremental insertion Graphs</a></li>
                <li>
                    <a href="#graph-based-vector-search" aria-label="Graph-based vector search">Graph-based vector search</a></li>
                <li>
                    <a href="#hierarchical-navigable-small-world" aria-label="Hierarchical Navigable Small World">Hierarchical Navigable Small World</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="mixture-of-experts">Mixture of Experts<a hidden class="anchor" aria-hidden="true" href="#mixture-of-experts">#</a></h3>
<p>There is a gate that opens a subset of the experts, and the output is the weighted sum of the outputs of the experts. The weights are computed by a gating network.
<img src="/images/notes/Optimizations for DNN-20250510123829347.webp" style="width: 100%" class="center" alt="Optimizations for DNN-20250510123829347"></p>
<p>One problem is load balancing, non uniform assignment. And there is a lot of communication overhead when you place them in different devices.</p>
<h4 id="lora-low-rank-adaptation">LoRA: Low-Rank Adaptation<a hidden class="anchor" aria-hidden="true" href="#lora-low-rank-adaptation">#</a></h4>
<p>We only finetune a part of the network, called <strong>lora adapters</strong>, not the whole thing.
There are two matrices here, a matrix A and B, they are some sort of an <a href="/notes/autoencoders">Autoencoders</a>, done for every Q nd V matrices in the LLM attention layer. The nice thing is that there are not many inference costs if adapters are merged post training:</p>
$$
h = Wx + BAx = (W + BA)x = W'x
$$<p>
And it is very easy to switch them.</p>
<h3 id="llm-optimizations">LLM Optimizations<a hidden class="anchor" aria-hidden="true" href="#llm-optimizations">#</a></h3>
<h4 id="quadratic-bottleneck">Quadratic Bottleneck<a hidden class="anchor" aria-hidden="true" href="#quadratic-bottleneck">#</a></h4>
<p>The attention matrix scales quadratically with respect to the sequence length, which makes it difficult to store. For example, if you have a GPT2 context window of 1024 tokens, which is about 340 words, this would make your attention matrix very large with not many words.</p>
<p>Modern LLMs make tokens longer by having large vocabularies, and they have larger context windows (in the order of the millions for Gemini 1.5 Pro (2 Millions)).</p>
<h4 id="flashattention">FlashAttention<a hidden class="anchor" aria-hidden="true" href="#flashattention">#</a></h4>
<p>This is a systems paper which is important for computing introduced in <a href="/notes/optimizations-for-dnn#daoFLASHATTENTIONFastMemoryefficient2022">(Dao et al. 2022)</a>. This enables <strong>exact</strong> attention computation but longer context lengths. The compute the attention matrix in a careful way to tile it inside the SRAM, this is some sort of optimization that is close to what you do in <a href="/notes/fast-linear-algebra">Fast Linear Algebra</a>, they basically reduce number of memory reads and writes between GPU high bandwidth memory and on-chip SRAM (similar optimizations are done between RAM and cache, same idea basically).</p>
<figure class="center">
<img src="/images/notes/Optimizations for DNN-20250517184200201.webp" style="width: 100%"   alt="Optimizations for DNN-20250517184200201" title="Optimizations for DNN-20250517184200201"/>
<figcaption><p style="text-align:center;"> Image from the paper: FlashAttention uses tiling to prevent materialization of the large $N \times N$ attention matrix (dotted box) on (relatively) slow GPU HBM.</p></figcaption>
</figure>
<h4 id="kv-caching">KV Caching<a hidden class="anchor" aria-hidden="true" href="#kv-caching">#</a></h4>
<p>There is some redundant computation which is done in the attention matrix due to the autoregressive nature of the inference process. This means that instead of recomputing, we can <strong>cache</strong> some old results and not compute them again. However, this can grow a lot.</p>
<ul>
<li>Multiple parallel requests</li>
<li>Complex decoding trategies.
This was introduced in <a href="https://proceedings.mlsys.org/paper_files/paper/2023/file/c4be71ab8d24cdfb45e3d06dbfca2780-Paper-mlsys2023.pdf">(Pope et al. 2023)</a>.</li>
</ul>
<p>This motivates to cache Key and Values from the computation. At every step we just need to compute a single vector instead of recomputing the old values.</p>
<h4 id="pagedattention">PagedAttention<a hidden class="anchor" aria-hidden="true" href="#pagedattention">#</a></h4>
<p>A lot of memory of the GPU is filled by the KV Cache. PagedAttention introduces virtual pages, inspired by OS memory access in <a href="/notes/memoria-virtuale">Memoria virtuale</a>.
The System assumes the KV cache is continuous, but it is actually managed in some particular manner.
This brings fragmentation. Adding this virtual layer makes better use of the actual resources. The authors claim to have near zero wasted memory in the KV cache memory due to fragmentation.</p>
<h4 id="speculative-decoding">Speculative Decoding<a hidden class="anchor" aria-hidden="true" href="#speculative-decoding">#</a></h4>
<p>You have a small model that runs faster and attempts to predict a group of next tokens. Then you feed it to a large model and verify the tokens are correct (probable enough to have produced these tokens). This is basically <a href="/notes/accept-reject-algorithm">Accept Reject algorithm</a> for sampling the tokens.
<img src="/images/notes/Optimizations for DNN-20250517185146160.webp" style="width: 100%" class="center" alt="Optimizations for DNN-20250517185146160">
The easy example is with greedy decoding: you just accept all the tokens that match the argmax.
But LLM decoding is stochastic, so for this reason it is better to have anohter method that keeps the locality.
We use Accept Reject with acceptance probability:</p>
$$
A(y \mid x) = \min\left( 1, \frac{L(y\mid x)}{k \cdot S(y \mid x)} \right)
$$<p>
Where $y$ is the new token, $x$ is the prompt, $S$ is the draft fast model and $L$ is the slow model, and we assume: $L(y \mid x) \leq k \cdot S(y \mid x)$.</p>
<h4 id="batching-behaviours">Batching behaviours<a hidden class="anchor" aria-hidden="true" href="#batching-behaviours">#</a></h4>
<p><a href="https://dl.acm.org/doi/10.1145/3676641.3716009">(He et al. 2025)</a> is a good paper that explains most of the common optimizations that are made for LLMs.
In this particular example, we have</p>
<h2 id="classical-llm-serving-infrastructure">Classical LLM serving infrastructure<a hidden class="anchor" aria-hidden="true" href="#classical-llm-serving-infrastructure">#</a></h2>
<h3 id="retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)<a hidden class="anchor" aria-hidden="true" href="#retrieval-augmented-generation-rag">#</a></h3>
<p>Pretraining costs a lot, and updating <em>weight</em> knowledge using more training rounds and finetuning costs a lot. RAG addresses the problem of adding more knowledge for the model without any training, but just the so called <em>in-context learning</em>.</p>
<img src="/images/notes/Optimizations for DNN-20250518110750716.webp" style="width: 100%" class="center" alt="Optimizations for DNN-20250518110750716">
<p>Slowly, RAG is becoming <strong>industry standard</strong> to serve LLM in a reliable manner. Prompting is becoming a normal thing.</p>
<h4 id="rag-with-internet-scale-retrieval">RAG with internet-scale retrieval<a hidden class="anchor" aria-hidden="true" href="#rag-with-internet-scale-retrieval">#</a></h4>
<p>The pipeline is very easy: just use websearch to get many many documents, add it as a prefix to the LLM and use standard decoding.</p>
<img src="/images/notes/Optimizations for DNN-20250518111258679.webp" style="width: 100%" class="center" alt="Optimizations for DNN-20250518111258679">
<h4 id="rag-for-long-context-processing">RAG for long-context processing<a hidden class="anchor" aria-hidden="true" href="#rag-for-long-context-processing">#</a></h4>
<p>In many occasions, there are very very long documents that are prefixed to the LLM input. In these cases, it can be very costly to process these kinds of documents.</p>
<p>In this case, we just want to add relevant passages, not whole documents and use those for LLM inference.
Beginning and end parts of the prompt are better processed by the model, this is a phenomenon known as <strong>lost in the middle</strong>.</p>
<img src="/images/notes/Optimizations for DNN-20250518111608439.webp" style="width: 100%" class="center" alt="Optimizations for DNN-20250518111608439">
<h4 id="rag-with-iterative-retrievals">RAG with iterative retrievals<a hidden class="anchor" aria-hidden="true" href="#rag-with-iterative-retrievals">#</a></h4>
<p>In this case the model can generate a question to generate another query to retrieve other documents. This is usually used with CoT, see <a href="http://arxiv.org/abs/2201.11903">(Wei et al. 2023)</a>, and sequential information retrievals.</p>
<h4 id="rag-with-rewriter-and-reranker">RAG with rewriter and reranker<a hidden class="anchor" aria-hidden="true" href="#rag-with-rewriter-and-reranker">#</a></h4>
<p>The idea is to break down the user query into more, or rewrite the query into a format more specific for the dataset.</p>
<img src="/images/notes/Optimizations for DNN-20250518111845240.webp" style="width: 100%" class="center" alt="Optimizations for DNN-20250518111845240">
<h3 id="vector-search">Vector Search<a hidden class="anchor" aria-hidden="true" href="#vector-search">#</a></h3>
<p>One of the classical methods are TF-IDF with document word frequency counts.
The general problem is a <a href="/notes/clustering">Clustering</a>, with nearest neighbors search.</p>
<p>The mathematical definition is as follows:
Sure, here are the contents of the image as clean, copiable Markdown notes:</p>
<h4 id="problem-statement">Problem statement<a hidden class="anchor" aria-hidden="true" href="#problem-statement">#</a></h4>
<p>Given:</p>
<ul>
<li>A <strong>query vector</strong> $q \in \mathbb{R}^d$</li>
<li>A <strong>collection of database vectors</strong>
$\mathcal{D} = \{ x_1, x_2, \dots, x_N \} \subset \mathbb{R}^d$</li>
<li>A <strong>similarity (or distance) function</strong>
$\text{sim} : \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$</li>
</ul>
<p><strong>Vector search</strong> aims to find the <strong>top-</strong>$k$ <strong>nearest neighbors</strong> of $q$ in $\mathcal{D}$, defined as:</p>
$$
\arg \, \text{top-}k \, \text{sim}(q, x) \quad \text{for } x \in \mathcal{D}
$$<h4 id="common-metrics">Common Metrics<a hidden class="anchor" aria-hidden="true" href="#common-metrics">#</a></h4>
<p>Choosing the metric is usually dependent on the model and application.</p>
<ul>
<li>Cosine Similarity $\cos(x, y) = \frac{x \cdot y}{||x|| ||y||}$</li>
<li>Euclidean Distance $d(x, y) = ||x - y||$</li>
<li>Manhattan Distance $d(x, y) = \sum_i |x_i - y_i|$</li>
</ul>
<p>One big problem with the metrics in high dimensionality is the classical Curse of Dimensionality, discussed in <a href="/notes/kernel-methods">Kernel Methods</a>. You have far sparse sparse points in high dimensional data. Indeed, for single dimensional data, tree searches are a nice and efficient method to solve this problem.</p>
<h4 id="ivf-pq-algorithm">IVF-PQ Algorithm<a hidden class="anchor" aria-hidden="true" href="#ivf-pq-algorithm">#</a></h4>
<p>This is some sort of an approximate nearest neighbor search.</p>
<p>This has two steps the first is:
<strong>Inverted-file index:</strong> used to <em>prune</em> the search space to have a smaller set of candidates. This is done by clustering the data and creating an inverted index for each cluster.</p>
<ul>
<li>This step is basically <a href="/notes/clustering">Clustering</a> the data into $n$ predefined clusters, and use their center as some sort of representative vector. The centroids are called <strong>inverted lists/index</strong>.</li>
<li>The important thing here is to <strong>store the residual</strong> with respect to that center.</li>
</ul>
<p><strong>Product Quantization</strong>: used to <em>lossy-compress</em> the vectors we have. This is done by quantizing the vectors into a smaller set of representative vectors. The original $D$ dimensional vectors are first turned into $m$ subvectors, then each subvector is reclustered and approximated into original cluster centroids, then you just use the cluster id of the centroid. Now we have integers, which are very nice, called <strong>pico code</strong> which saves lots of representation space.
Searching is done in a similar manner, and you also build distance lookup tables with these.</p>
<figure class="center">
<img src="/images/notes/Optimizations for DNN-20250519103210621.webp" style="width: 100%"   alt="Optimizations for DNN-20250519103210621" title="Optimizations for DNN-20250519103210621"/>
<figcaption><p style="text-align:center;">The idea is to divide the original part into $m$ subvectors and use their relative centroids to give a sort of representation, that is then integer, which is far more memory efficient.</p></figcaption>
</figure>
<p>When you search you only scan a subset of the IVF.
During search you build a <strong>distance loop-up table</strong> based on the centroids and use that to compare to the quantized vectors.</p>
<h4 id="pseudo-code-for-search">Pseudo code for search<a hidden class="anchor" aria-hidden="true" href="#pseudo-code-for-search">#</a></h4>
<p>To give an idea of the search, this is some pseudocode for the search process</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">IVF_PQ_SEARCH</span><span class="p">(</span><span class="n">query_vector</span> <span class="n">q</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">top_k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Step 1: Coarse quantization</span>
</span></span><span class="line"><span class="cl">    <span class="n">coarse_centroids</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">coarse_centroids</span>
</span></span><span class="line"><span class="cl">    <span class="n">probe_centroids</span> <span class="o">=</span> <span class="n">find_nearest_centroids</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">coarse_centroids</span><span class="p">,</span> <span class="n">n_probe</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="c1"># n_probe is the default number of centroids to return.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">candidate_vectors</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">centroid</span> <span class="ow">in</span> <span class="n">probe_centroids</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Step 2: Compute residual vector for query</span>
</span></span><span class="line"><span class="cl">        <span class="n">residual_q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">-</span> <span class="n">centroid</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Step 3: Prepare distance lookup tables for PQ</span>
</span></span><span class="line"><span class="cl">        <span class="n">pq_lookup_table</span> <span class="o">=</span> <span class="n">build_lookup_table</span><span class="p">(</span><span class="n">residual_q</span><span class="p">,</span> <span class="n">index</span><span class="o">.</span><span class="n">pq_codebooks</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Step 4: For each encoded vector in the selected inverted list</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">code</span> <span class="ow">in</span> <span class="n">index</span><span class="o">.</span><span class="n">inverted_list</span><span class="p">[</span><span class="n">centroid</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Step 5: Compute approximate distance using lookup table</span>
</span></span><span class="line"><span class="cl">            <span class="n">distance</span> <span class="o">=</span> <span class="n">compute_adc_distance</span><span class="p">(</span><span class="n">code</span><span class="p">,</span> <span class="n">pq_lookup_table</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">candidate_vectors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">distance</span><span class="p">,</span> <span class="n">code</span><span class="o">.</span><span class="n">original_id</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Step 6: Select top-k closest vectors</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">top_k_smallest</span><span class="p">(</span><span class="n">candidate_vectors</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="asymmetric-distance-computation-adc">Asymmetric Distance Computation (ADC)<a hidden class="anchor" aria-hidden="true" href="#asymmetric-distance-computation-adc">#</a></h4>
<p>This is some metric that is used for the residuals usually. In this case, the database is quantized, but the query is not, we need to handle this kind of asymmetries.</p>
<p>We want to estimate the squared Euclidean distance between the <strong>real-valued query vector $q \in \mathbb{R}^d$</strong> and a <strong>PQ-compressed vector $\hat{x}$</strong>.</p>
<ol>
<li><strong>Split query vector $q$</strong> into $m$ subvectors (this is the codebook part): $q = [q^{(1)}, q^{(2)}, \dots, q^{(m)}]$ Each subvector $q^{(i)} \in \mathbb{R}^{d/m}$</li>
<li>For each subspace $i = 1, \dots, m$, precompute a <strong>lookup table</strong>: $D_i[j] = \| q^{(i)} - c^{(i)}_j \|^2$
where $c^{(i)}_j$ is the $j$-th centroid in the codebook for subspace $i$, and $j = 0, \dots, 255$. So each $D_i$ is a vector of 256 distances</li>
<li>For a compressed database vector $\hat{x}$, represented by codes:
$$
   \text{code} = [c_1, c_2, \dots, c_m] \quad \text{with } c_i \in \{0, \dots, 255\}
   $$</li>
<li>The total approximate distance is:
$$
   \| q - \hat{x} \|^2 \approx \sum_{i=1}^m D_i[c_i]
   $$
→ Just m table look-ups and m additions, which is <strong>easily parallelizable</strong>.</li>
</ol>
<h4 id="knn-graphs">KNN Graphs<a hidden class="anchor" aria-hidden="true" href="#knn-graphs">#</a></h4>
<p>Examples of graph databases where studied in <a href="/notes/graph-databases">Graph Databases</a> in the big data course the first time.</p>
<p>The problem here is to <strong>build a similarity graph</strong> with this kind of representation.
One possibility is to build a KNN graph and doing pruning using some heuristics. With this method we have usually <strong>high recall</strong>, but:</p>
<ul>
<li>it is not very flexible with data updates</li>
<li>it is quite memory intensive.</li>
</ul>
<h4 id="incremental-insertion-graphs">Incremental insertion Graphs<a hidden class="anchor" aria-hidden="true" href="#incremental-insertion-graphs">#</a></h4>
<p>Option 2: Incremental Insertion (Online Construction)</p>
<ul>
<li>For each new point, perform a search</li>
<li>Connect to the top nearest nodes</li>
<li>Possibly prune to maintain diversity
• Pros:</li>
<li>Can be done online (supports dynamic updates)</li>
<li>More memory-efficient; no need to compute full KNN graph
• Cons:</li>
<li>Quality depends on insertion order and search quality</li>
<li>You don&rsquo;t have the possibility of getting the same graph every single time.</li>
</ul>
<h4 id="graph-based-vector-search">Graph-based vector search<a hidden class="anchor" aria-hidden="true" href="#graph-based-vector-search">#</a></h4>
<p>The algorithm goes in the following way:</p>
<ul>
<li>We select an entry point in the graph.</li>
<li>We keep two queues
<ul>
<li>High quality candidates to visit</li>
<li>Existing nearest neighbors</li>
<li>We pop from A and add to B</li>
</ul>
</li>
<li>We end if there are not enough high-quality candidates to visit or all nodes in A are too far from queue B.</li>
<li>Return the best results so far.</li>
</ul>
<p>One of the modern algorithms that solves some problems in vector search that we have now is <strong>Hierarchical Navigable Small World</strong>, which does some kind of hierarchical search.</p>
<h4 id="hierarchical-navigable-small-world">Hierarchical Navigable Small World<a hidden class="anchor" aria-hidden="true" href="#hierarchical-navigable-small-world">#</a></h4>
<ol>
<li><strong>NSW (Navigable Small World)</strong></li>
</ol>
<ul>
<li>Builds a small-world graph where nodes are connected to their close and some faraway neighbors.</li>
<li>Search is greedy: jump to closer nodes until you can’t get closer.</li>
</ul>
<ol start="2">
<li><strong>HNSW (Hierarchical NSW)</strong></li>
</ol>
<ul>
<li>Adds multiple layers:
<ul>
<li>Top layers are sparse (longer-range links).</li>
<li>Bottom layers are dense (short-range neighbors).</li>
</ul>
</li>
<li>Search starts at the top layer and drills down layer by layer, refining the search.</li>
</ul>
<img src="/images/notes/Optimizations for DNN-20250519104742775.webp" style="width: 100%" class="center" alt="Optimizations for DNN-20250519104742775">
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=hePAPIExploitingDynamic2025>[1] He et al. <a href="https://dl.acm.org/doi/10.1145/3676641.3716009">“PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System”</a> ACM  2025
 </p>
<p id=MLSYS2023_c4be71ab>[2] Pope et al. <a href="https://proceedings.mlsys.org/paper_files/paper/2023/file/c4be71ab8d24cdfb45e3d06dbfca2780-Paper-mlsys2023.pdf">“Efficiently Scaling Transformer Inference”</a> Curan  2023
 </p>
<p id=weiChainofThoughtPromptingElicits2023>[3] Wei et al. <a href="http://arxiv.org/abs/2201.11903">“Chain-of-Thought Prompting Elicits Reasoning in Large Language Models”</a> arXiv preprint arXiv:2201.11903 2023
 </p>
<p id=daoFLASHATTENTIONFastMemoryefficient2022>[4] Dao et al. “FLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-awareness” Curran Associates Inc.  2022
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/cloud-computing/">☁Cloud-Computing</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Optimizations for DNN on x"
            href="https://x.com/intent/tweet/?text=Optimizations%20for%20DNN&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2foptimizations-for-dnn%2f&amp;hashtags=%e2%98%81cloud-computing">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Optimizations for DNN on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2foptimizations-for-dnn%2f&amp;title=Optimizations%20for%20DNN&amp;summary=Optimizations%20for%20DNN&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2foptimizations-for-dnn%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Optimizations for DNN on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2foptimizations-for-dnn%2f&title=Optimizations%20for%20DNN">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Optimizations for DNN on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2foptimizations-for-dnn%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Optimizations for DNN on whatsapp"
            href="https://api.whatsapp.com/send?text=Optimizations%20for%20DNN%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2foptimizations-for-dnn%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Optimizations for DNN on telegram"
            href="https://telegram.me/share/url?text=Optimizations%20for%20DNN&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2foptimizations-for-dnn%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Optimizations for DNN on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Optimizations%20for%20DNN&u=https%3a%2f%2fflecart.github.io%2fnotes%2foptimizations-for-dnn%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
