<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bayesian Optimization | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="➕probabilistic-artificial-intelligence">
<meta name="description" content="While Active Learning looks for the most informative points to recover a true underlying function, Bayesian Optimization is just interested to find the maximum of that function. In Bayesian Optimization, we ask for the best way to find sequentially a set of points $x_{1}, \dots, x_{n}$ to find $\max_{x \in \mathcal{X}} f(x)$ for a certain unknown function $f$. This is what the whole thing is about.
Definitions First we will introduce some useful definitions in this context.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/bayesian-optimization/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/bayesian-optimization/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Bayesian Optimization" />
<meta property="og:description" content="While Active Learning looks for the most informative points to recover a true underlying function, Bayesian Optimization is just interested to find the maximum of that function. In Bayesian Optimization, we ask for the best way to find sequentially a set of points $x_{1}, \dots, x_{n}$ to find $\max_{x \in \mathcal{X}} f(x)$ for a certain unknown function $f$. This is what the whole thing is about.
Definitions First we will introduce some useful definitions in this context." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/bayesian-optimization/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Bayesian Optimization"/>
<meta name="twitter:description" content="While Active Learning looks for the most informative points to recover a true underlying function, Bayesian Optimization is just interested to find the maximum of that function. In Bayesian Optimization, we ask for the best way to find sequentially a set of points $x_{1}, \dots, x_{n}$ to find $\max_{x \in \mathcal{X}} f(x)$ for a certain unknown function $f$. This is what the whole thing is about.
Definitions First we will introduce some useful definitions in this context."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bayesian Optimization",
      "item": "https://flecart.github.io/notes/bayesian-optimization/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bayesian Optimization",
  "name": "Bayesian Optimization",
  "description": "While Active Learning looks for the most informative points to recover a true underlying function, Bayesian Optimization is just interested to find the maximum of that function. In Bayesian Optimization, we ask for the best way to find sequentially a set of points $x_{1}, \\dots, x_{n}$ to find $\\max_{x \\in \\mathcal{X}} f(x)$ for a certain unknown function $f$. This is what the whole thing is about.\nDefinitions First we will introduce some useful definitions in this context.",
  "keywords": [
    "➕probabilistic-artificial-intelligence"
  ],
  "articleBody": "While Active Learning looks for the most informative points to recover a true underlying function, Bayesian Optimization is just interested to find the maximum of that function. In Bayesian Optimization, we ask for the best way to find sequentially a set of points $x_{1}, \\dots, x_{n}$ to find $\\max_{x \\in \\mathcal{X}} f(x)$ for a certain unknown function $f$. This is what the whole thing is about.\nDefinitions First we will introduce some useful definitions in this context. These were also somewhat introduced in N-Bandit Problem, which is one of the classical optimization problems we can find in the literature.\nWe suppose there is an underlying function $f$, but we are only observing some noised estimates $y = f + \\varepsilon$ where $\\varepsilon$ is usually modeled as Gaussian noise.\nAt each time step we can choose some $x_{t}$, then the reward is $y_{t} = f(x_{t}) + \\varepsilon_{t}$. We would like to maximize the cumulative reward $\\sum_{t=1}^{n} y_{t}$.\nRegret We define regret to be: $$ R_{n} = \\sum_{t=1}^{n} f(x^{*}) - f(x_{t}) $$ Where $x^{*}$ is the best possible point we could have chosen for the underlying unknown function $f$. This regret is defined to be sub-linear if the following holds: $$ \\lim_{n \\to \\infty} \\frac{R_{n}}{n} = 0 $$ Intuitively, it says that if we explore enough, then we will always get the best possible choice.\nExploration and Exploitation This setting naturally leads to the exploration-exploitation tradeoff. We can either choose to explore new regions of the input space, or exploit the regions we have already visited. It’s easy to observe that both greedy strategies that only exploit the current best guess and strategies that only explore new regions are not optimal, meaning they will yield not sub-linear regrets.\nOptimization Techniques A standard Idea In the rest of the section, we will assume that the $f$ function is a Gaussian Process. Then one greedy way to solve this optimization problem is the following algorithm\nInitialize a $f \\sim GP(\\mu_{0}, k_{0})$. For $t = 1, \\dots, n$ do Find the point $x_{t} = \\arg\\max_{x \\in \\mathcal{X}} f(x)$. Observe the reward $y_{t} = f(x_{t}) + \\varepsilon_{t}$. Update the GP with the new observation. Entropy Search GP-UCB The idea Image taken from slides of the PAI course ETH 2024. Optimism in the face of uncertainty\nWe just want to pick the point that maximizes our upper confidence bound in the posterior of the Gaussian Processes. The green region are the possible regions that we would like to sample from, as their upper confidence is higher than our lower bound. This idea is recurring in this setting.\nIf the model is well calibrated, then the uncertainty regions are coherent with what actually returns the underlying function. This motivates why we can safely ignore the zones on our input where the maximum is less than the minimum value of the variance. We are sure, due to calibration, that those zones do not contain our maximum with high confidence.\nUCB Picking principle This is formalized as $$ x_{t} = \\arg\\max_{x \\in D} \\mu_{t - 1}(x) + \\beta_{t - 1} \\sigma_{t - 1}(x) $$ Where $\\mu_{t - 1}(x)$ is the mean of the GP at the point $x$ and $\\sigma_{t - 1}(x)$ is the standard deviation. $\\beta_{t - 1}$ is a parameter that we can tune to have more or less exploration. This should select the point that maximizes the upper confidence bound in the GP.\nWe observe that if $\\forall t,\\beta_{t - 1} = 0$ then the algorithm is purely exploitative, while if $\\beta_{t - 1} = \\infty$ then the algorithm is purely esplorative, and it’s equivalent to uncertainty sampling presented in Active Learning.\nConvergence Bound on GP-UCP The convergence bound on the GP-UCB, given we choose the $\\beta$ correctly, is $$ \\frac{1}{T} \\sum_{t = 1}^{T} (f(x^{*}) - f(x_{t})) = \\mathcal{O}\\left( \\sqrt{\\frac{\\gamma_{T}}{T}} \\right) $$ The maximum information gain determines the regret, often indicated as $\\gamma_{T}$, sometimes as $\\max_{S \\leq T} I(f; y_{S})$. This sets the rate at which we get information. Due to submodularity of mutual information we can show that $\\gamma_{T}$ can at most grow linearly with $T$.\nI have no idea why this is true. These are the bounds for different kernels in the GP The nice thing is that with all these kernels, we have guarantees that the regret is sub-linear, so we get the best possible solution.\nGrowth rate of information gain There is a clean intuition for the following graph: if the function is continuous, smooth so to say, then knowing the value of one point gives lots of insights to the value of the next point. Which means we need fewer point to get a good estimate of the function (this is why the slope of the function is low). If we have quite independent samples, then we need more points to get a good estimate of the function. This is why it has a linear growth rate: every point gives you about the same information. Note that this happens only for white noise, so probably it is not a much interesting case.\nChoosing the point Usually the acquisition function is non-convex, so we need some methods to compute that maximum.\nLow dimensionality: we can use lipschitz optimization. High dimensionality: we can use gradient descent. Probability Improvement The PI Idea Thompson Sampling Thompson Sampling Method With Thompson sampling , we first sample a function: $$ f_{t + 1} \\sim p( \\cdot \\mid x_{1:t}, y_{1:t}) $$ And the find the new point as the maximum of this function $x_{t + 1} = \\arg\\max_{x \\in \\mathcal{X}} f_{t + 1}(x)$. In this case, we are leveraging the randomness of $f$ to trade between exploitation and exploration. Similar bounds to UCB can be established also for Thompson.\nInformation Directed Sampling A common problem One common problem with these methods is the kernel. How could we choose the correct kernel and its hyper-parameters? We are using these kernels to select the data itself, which could be quite biased. Another problem is getting calibrated models in this setting where you continuously select points.\nCommon approaches to this problem are:\nImposing priors on the kernel parameters. Occasionally selecting random points. ",
  "wordCount" : "1020",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/bayesian-optimization/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Bayesian Optimization
    </h1>
    <div class="post-meta">5 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#definitions" aria-label="Definitions">Definitions</a><ul>
                        
                <li>
                    <a href="#regret" aria-label="Regret">Regret</a></li>
                <li>
                    <a href="#exploration-and-exploitation" aria-label="Exploration and Exploitation">Exploration and Exploitation</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#optimization-techniques" aria-label="Optimization Techniques">Optimization Techniques</a><ul>
                        
                <li>
                    <a href="#a-standard-idea" aria-label="A standard Idea">A standard Idea</a></li>
                <li>
                    <a href="#entropy-search" aria-label="Entropy Search">Entropy Search</a></li>
                <li>
                    <a href="#gp-ucb" aria-label="GP-UCB">GP-UCB</a><ul>
                        
                <li>
                    <a href="#the-idea" aria-label="The idea">The idea</a></li>
                <li>
                    <a href="#ucb-picking-principle" aria-label="UCB Picking principle">UCB Picking principle</a></li>
                <li>
                    <a href="#convergence-bound-on-gp-ucp" aria-label="Convergence Bound on GP-UCP">Convergence Bound on GP-UCP</a></li>
                <li>
                    <a href="#growth-rate-of-information-gain" aria-label="Growth rate of information gain">Growth rate of information gain</a></li>
                <li>
                    <a href="#choosing-the-point" aria-label="Choosing the point">Choosing the point</a></li></ul>
                </li>
                <li>
                    <a href="#probability-improvement" aria-label="Probability Improvement">Probability Improvement</a><ul>
                        
                <li>
                    <a href="#the-pi-idea" aria-label="The PI Idea">The PI Idea</a></li></ul>
                </li>
                <li>
                    <a href="#thompson-sampling" aria-label="Thompson Sampling">Thompson Sampling</a><ul>
                        
                <li>
                    <a href="#thompson-sampling-method" aria-label="Thompson Sampling Method">Thompson Sampling Method</a></li></ul>
                </li>
                <li>
                    <a href="#information-directed-sampling" aria-label="Information Directed Sampling">Information Directed Sampling</a></li>
                <li>
                    <a href="#a-common-problem" aria-label="A common problem">A common problem</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>While <a href="/notes/active-learning/">Active Learning</a> looks for the most informative points to recover a <em>true</em> underlying function, Bayesian Optimization is just interested to find the maximum of that function.
In Bayesian Optimization, we ask for the best way to find <em>sequentially</em> a set of points $x_{1}, \dots, x_{n}$ to find $\max_{x \in \mathcal{X}} f(x)$ for a certain unknown function $f$. This is what the whole thing is about.</p>
<h3 id="definitions">Definitions<a hidden class="anchor" aria-hidden="true" href="#definitions">#</a></h3>
<p>First we will introduce some useful definitions in this context. These were also somewhat introduced in N-Bandit Problem, which is one of the classical optimization problems we can find in the literature.</p>
<p>We suppose there is an underlying function $f$, but we are only observing some noised estimates $y = f + \varepsilon$ where $\varepsilon$ is usually modeled as Gaussian noise.</p>
<p>At each time step we can choose some $x_{t}$, then the reward is $y_{t} = f(x_{t}) + \varepsilon_{t}$.
We would like to maximize the cumulative reward $\sum_{t=1}^{n} y_{t}$.</p>
<h4 id="regret">Regret<a hidden class="anchor" aria-hidden="true" href="#regret">#</a></h4>
<p>We define regret to be:
</p>
$$
R_{n} = \sum_{t=1}^{n} f(x^{*}) - f(x_{t})
$$
<p>
Where $x^{*}$ is the best possible point we could have chosen for the underlying unknown function $f$.
This regret is defined to be <strong>sub-linear</strong> if the following holds:
</p>
$$
\lim_{n \to \infty} \frac{R_{n}}{n} = 0
$$
<p>Intuitively, it says that if we explore enough, then we will always get the best possible choice.</p>
<h4 id="exploration-and-exploitation">Exploration and Exploitation<a hidden class="anchor" aria-hidden="true" href="#exploration-and-exploitation">#</a></h4>
<p>This setting naturally leads to the exploration-exploitation tradeoff. We can either choose to explore new regions of the input space, or exploit the regions we have already visited.
It&rsquo;s easy to observe that both greedy strategies that only exploit the current best guess and strategies that only explore new regions are not optimal, meaning they will yield not sub-linear regrets.</p>
<h2 id="optimization-techniques">Optimization Techniques<a hidden class="anchor" aria-hidden="true" href="#optimization-techniques">#</a></h2>
<h3 id="a-standard-idea">A standard Idea<a hidden class="anchor" aria-hidden="true" href="#a-standard-idea">#</a></h3>
<p>In the rest of the section, we will assume that the $f$ function is a <a href="/notes/gaussian-processes/">Gaussian Process</a>.
Then one greedy way to solve this optimization problem is the following algorithm</p>
<ol>
<li>Initialize a $f \sim GP(\mu_{0}, k_{0})$.</li>
<li>For $t = 1, \dots, n$ do
<ol>
<li>Find the point $x_{t} = \arg\max_{x \in \mathcal{X}} f(x)$.</li>
<li>Observe the reward $y_{t} = f(x_{t}) + \varepsilon_{t}$.</li>
<li>Update the GP with the new observation.</li>
</ol>
</li>
</ol>
<h3 id="entropy-search">Entropy Search<a hidden class="anchor" aria-hidden="true" href="#entropy-search">#</a></h3>
<h3 id="gp-ucb">GP-UCB<a hidden class="anchor" aria-hidden="true" href="#gp-ucb">#</a></h3>
<h4 id="the-idea">The idea<a hidden class="anchor" aria-hidden="true" href="#the-idea">#</a></h4>
<img src="/images/notes/N-Bandit Problem-20241110182231356.webp" width="624" alt="N-Bandit Problem-20241110182231356">
Image taken from slides of the PAI course ETH 2024.
<blockquote>
<p>Optimism in the face of uncertainty</p>
</blockquote>
<p>We just want to pick the point that maximizes our upper confidence bound in the posterior of the <a href="/notes/gaussian-processes/">Gaussian Processes</a>.
The green region are the possible regions that we would like to sample from, as their upper confidence is higher than our lower bound. This idea is recurring in this setting.</p>
<p>If the model is well calibrated, then the uncertainty regions are coherent with what actually returns the underlying function. This motivates why we can safely ignore the zones on our input where the maximum is less than the minimum value of the variance. We are sure, due to calibration, that those zones do not contain our maximum with high confidence.</p>
<h4 id="ucb-picking-principle">UCB Picking principle<a hidden class="anchor" aria-hidden="true" href="#ucb-picking-principle">#</a></h4>
<p>This is formalized as
</p>
$$
x_{t} = \arg\max_{x \in D} \mu_{t - 1}(x) + \beta_{t - 1} \sigma_{t - 1}(x)
$$
<p>
Where $\mu_{t - 1}(x)$ is the mean of the GP at the point $x$ and $\sigma_{t - 1}(x)$ is the standard deviation. $\beta_{t - 1}$ is a parameter that we can tune to have more or less exploration. This should select the point that maximizes the upper confidence bound in the GP.</p>
<p>We observe that if $\forall t,\beta_{t - 1} = 0$ then the algorithm is <em>purely exploitative</em>, while if $\beta_{t - 1} = \infty$ then the algorithm is <em>purely esplorative</em>, and it&rsquo;s equivalent to uncertainty sampling presented in <a href="/notes/active-learning/">Active Learning</a>.</p>
<h4 id="convergence-bound-on-gp-ucp">Convergence Bound on GP-UCP<a hidden class="anchor" aria-hidden="true" href="#convergence-bound-on-gp-ucp">#</a></h4>
<p>The convergence bound on the GP-UCB, given we choose the $\beta$ correctly, is
</p>
$$
\frac{1}{T} \sum_{t = 1}^{T}  (f(x^{*}) - f(x_{t})) = \mathcal{O}\left( \sqrt{\frac{\gamma_{T}}{T}} \right)
$$
<p>
The maximum information gain determines the regret, often indicated as $\gamma_{T}$, sometimes as $\max_{S \leq T} I(f; y_{S})$. This sets the <em>rate</em> at which we get information. Due to submodularity of mutual information we can show that $\gamma_{T}$ can at most grow linearly with $T$.</p>
<p>I have no idea why this is true.
These are the bounds for different kernels in the GP
<img src="/images/notes/N-Bandit Problem-20241110183044028.webp" alt="N-Bandit Problem-20241110183044028">
The nice thing is that with all these kernels, we have guarantees that the regret is sub-linear, so we get the best possible solution.</p>
<h4 id="growth-rate-of-information-gain">Growth rate of information gain<a hidden class="anchor" aria-hidden="true" href="#growth-rate-of-information-gain">#</a></h4>
<p>There is a clean intuition for the following graph:
if the function is continuous, smooth so to say, then knowing the value of one point gives lots of insights to the value of the next point. Which means we need fewer point to get a good estimate of the function (this is why the slope of the function is low).
If we have quite independent samples, then we need more points to get a good estimate of the function.
This is why it has a linear growth rate: every point gives you about the same information. Note that this happens only for white noise, so probably it is not a much interesting case.</p>
<img src="/images/notes/Bayesian Optimization-20241220155951442.webp" alt="Bayesian Optimization-20241220155951442">
<h4 id="choosing-the-point">Choosing the point<a hidden class="anchor" aria-hidden="true" href="#choosing-the-point">#</a></h4>
<p>Usually the acquisition function is <em>non-convex</em>, so we need some methods to compute that maximum.</p>
<ul>
<li><em>Low dimensionality</em>: we can use lipschitz optimization.</li>
<li><em>High dimensionality</em>: we can use gradient descent.</li>
</ul>
<h3 id="probability-improvement">Probability Improvement<a hidden class="anchor" aria-hidden="true" href="#probability-improvement">#</a></h3>
<h4 id="the-pi-idea">The PI Idea<a hidden class="anchor" aria-hidden="true" href="#the-pi-idea">#</a></h4>
<h3 id="thompson-sampling">Thompson Sampling<a hidden class="anchor" aria-hidden="true" href="#thompson-sampling">#</a></h3>
<h4 id="thompson-sampling-method">Thompson Sampling Method<a hidden class="anchor" aria-hidden="true" href="#thompson-sampling-method">#</a></h4>
<p>With Thompson sampling , we first sample a function:
</p>
$$
f_{t + 1} \sim p( \cdot \mid x_{1:t}, y_{1:t})
$$
<p>
And the find the new point as the maximum of this function $x_{t + 1} = \arg\max_{x \in \mathcal{X}} f_{t + 1}(x)$.
In this case, we are leveraging the randomness of $f$ to trade between exploitation and exploration.
Similar bounds to UCB can be established also for Thompson.</p>
<h3 id="information-directed-sampling">Information Directed Sampling<a hidden class="anchor" aria-hidden="true" href="#information-directed-sampling">#</a></h3>
<h3 id="a-common-problem">A common problem<a hidden class="anchor" aria-hidden="true" href="#a-common-problem">#</a></h3>
<p>One common problem with these methods is the kernel. How could we choose the correct kernel and its hyper-parameters?
We are using these kernels to select the data itself, which could be quite biased. Another problem is getting calibrated models in this setting where you continuously select points.</p>
<p>Common approaches to this problem are:</p>
<ul>
<li>Imposing priors on the kernel parameters.</li>
<li>Occasionally selecting random points.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">➕Probabilistic-Artificial-Intelligence</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Optimization on x"
            href="https://x.com/intent/tweet/?text=Bayesian%20Optimization&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-optimization%2f&amp;hashtags=%e2%9e%95probabilistic-artificial-intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Optimization on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-optimization%2f&amp;title=Bayesian%20Optimization&amp;summary=Bayesian%20Optimization&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-optimization%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Optimization on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-optimization%2f&title=Bayesian%20Optimization">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Optimization on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-optimization%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Optimization on whatsapp"
            href="https://api.whatsapp.com/send?text=Bayesian%20Optimization%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-optimization%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Optimization on telegram"
            href="https://telegram.me/share/url?text=Bayesian%20Optimization&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-optimization%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian Optimization on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Bayesian%20Optimization&u=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-optimization%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
