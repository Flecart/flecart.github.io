<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Multi Variable Derivatives | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="⚗linear-algebra">
<meta name="description" content="Multi-variable derivative
To the people that are not used to matrix derivatives (like me) it could be useful to see how
$$
\frac{ \partial u^{T}Su }{ \partial u }  = 2Su
$$
First, we note that if you derive with respect to some matrix, the output will be of the same dimension of that matrix. That notation is just deriving every single component independently and then joining them together, so it will be better understood as as
$$
\frac{ \partial u^{T}Su }{ \partial u }  =
\begin{bmatrix}
\frac{ \partial u^{T}Su }{ \partial u_{1} }  \
\dots \
\frac{ \partial u^{T}Su }{ \partial u_{M} }  \
\end{bmatrix}
$$
So we can prove each derivative independently, it&#39;s just a lot of manual work!
We see that $u^{T}Su$ is just a quadratic form, studied in Massimi minimi multi-variabile#Forme quadratiche so it is just computing this:
$$
u^{T}Su = \sum_{i, j = 1, 1}^{M} u_{i}u_{j}S_{ij} \implies \frac{ \partial u^{T}Su }{ \partial u_{1} } =2u_{1}S_{11} &#43; \sum_{j \neq 1}^{M}(u_{j}S_{1j}  &#43; u_{j}S_{j1}) = 2\left( u_{1}S_{11} &#43; \sum_{j \neq 1}u_{j}S_{1j} \right) = 2(Su)_{1}
$$
Last equation is true because $S$ is a symmetric matrix, then we easily see that indeed it&rsquo;s true that indeed it&rsquo;s the first row of the $Su$ matrix multiplied by 2.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/multi-variable-derivatives/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/multi-variable-derivatives/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/multi-variable-derivatives/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Multi Variable Derivatives">
  <meta property="og:description" content="Multi-variable derivative To the people that are not used to matrix derivatives (like me) it could be useful to see how $$ \frac{ \partial u^{T}Su }{ \partial u } = 2Su $$ First, we note that if you derive with respect to some matrix, the output will be of the same dimension of that matrix. That notation is just deriving every single component independently and then joining them together, so it will be better understood as as $$ \frac{ \partial u^{T}Su }{ \partial u } = \begin{bmatrix} \frac{ \partial u^{T}Su }{ \partial u_{1} } \ \dots \ \frac{ \partial u^{T}Su }{ \partial u_{M} } \ \end{bmatrix} $$ So we can prove each derivative independently, it&#39;s just a lot of manual work! We see that $u^{T}Su$ is just a quadratic form, studied in Massimi minimi multi-variabile#Forme quadratiche so it is just computing this: $$ u^{T}Su = \sum_{i, j = 1, 1}^{M} u_{i}u_{j}S_{ij} \implies \frac{ \partial u^{T}Su }{ \partial u_{1} } =2u_{1}S_{11} &#43; \sum_{j \neq 1}^{M}(u_{j}S_{1j} &#43; u_{j}S_{j1}) = 2\left( u_{1}S_{11} &#43; \sum_{j \neq 1}u_{j}S_{1j} \right) = 2(Su)_{1} $$ Last equation is true because $S$ is a symmetric matrix, then we easily see that indeed it’s true that indeed it’s the first row of the $Su$ matrix multiplied by 2.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="⚗Linear-Algebra">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Multi Variable Derivatives">
<meta name="twitter:description" content="Multi-variable derivative
To the people that are not used to matrix derivatives (like me) it could be useful to see how
$$
\frac{ \partial u^{T}Su }{ \partial u }  = 2Su
$$
First, we note that if you derive with respect to some matrix, the output will be of the same dimension of that matrix. That notation is just deriving every single component independently and then joining them together, so it will be better understood as as
$$
\frac{ \partial u^{T}Su }{ \partial u }  =
\begin{bmatrix}
\frac{ \partial u^{T}Su }{ \partial u_{1} }  \
\dots \
\frac{ \partial u^{T}Su }{ \partial u_{M} }  \
\end{bmatrix}
$$
So we can prove each derivative independently, it&#39;s just a lot of manual work!
We see that $u^{T}Su$ is just a quadratic form, studied in Massimi minimi multi-variabile#Forme quadratiche so it is just computing this:
$$
u^{T}Su = \sum_{i, j = 1, 1}^{M} u_{i}u_{j}S_{ij} \implies \frac{ \partial u^{T}Su }{ \partial u_{1} } =2u_{1}S_{11} &#43; \sum_{j \neq 1}^{M}(u_{j}S_{1j}  &#43; u_{j}S_{j1}) = 2\left( u_{1}S_{11} &#43; \sum_{j \neq 1}u_{j}S_{1j} \right) = 2(Su)_{1}
$$
Last equation is true because $S$ is a symmetric matrix, then we easily see that indeed it&rsquo;s true that indeed it&rsquo;s the first row of the $Su$ matrix multiplied by 2.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Multi Variable Derivatives",
      "item": "https://flecart.github.io/notes/multi-variable-derivatives/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Multi Variable Derivatives",
  "name": "Multi Variable Derivatives",
  "description": "Multi-variable derivative To the people that are not used to matrix derivatives (like me) it could be useful to see how $$ \\frac{ \\partial u^{T}Su }{ \\partial u } = 2Su $$ First, we note that if you derive with respect to some matrix, the output will be of the same dimension of that matrix. That notation is just deriving every single component independently and then joining them together, so it will be better understood as as $$ \\frac{ \\partial u^{T}Su }{ \\partial u } = \\begin{bmatrix} \\frac{ \\partial u^{T}Su }{ \\partial u_{1} } \\ \\dots \\ \\frac{ \\partial u^{T}Su }{ \\partial u_{M} } \\ \\end{bmatrix} $$ So we can prove each derivative independently, it's just a lot of manual work! We see that $u^{T}Su$ is just a quadratic form, studied in Massimi minimi multi-variabile#Forme quadratiche so it is just computing this: $$ u^{T}Su = \\sum_{i, j = 1, 1}^{M} u_{i}u_{j}S_{ij} \\implies \\frac{ \\partial u^{T}Su }{ \\partial u_{1} } =2u_{1}S_{11} + \\sum_{j \\neq 1}^{M}(u_{j}S_{1j} + u_{j}S_{j1}) = 2\\left( u_{1}S_{11} + \\sum_{j \\neq 1}u_{j}S_{1j} \\right) = 2(Su)_{1} $$ Last equation is true because $S$ is a symmetric matrix, then we easily see that indeed it\u0026rsquo;s true that indeed it\u0026rsquo;s the first row of the $Su$ matrix multiplied by 2.\n",
  "keywords": [
    "⚗linear-algebra"
  ],
  "articleBody": "Multi-variable derivative To the people that are not used to matrix derivatives (like me) it could be useful to see how $$ \\frac{ \\partial u^{T}Su }{ \\partial u } = 2Su $$ First, we note that if you derive with respect to some matrix, the output will be of the same dimension of that matrix. That notation is just deriving every single component independently and then joining them together, so it will be better understood as as $$ \\frac{ \\partial u^{T}Su }{ \\partial u } = \\begin{bmatrix} \\frac{ \\partial u^{T}Su }{ \\partial u_{1} } \\ \\dots \\ \\frac{ \\partial u^{T}Su }{ \\partial u_{M} } \\ \\end{bmatrix} $$ So we can prove each derivative independently, it's just a lot of manual work! We see that $u^{T}Su$ is just a quadratic form, studied in Massimi minimi multi-variabile#Forme quadratiche so it is just computing this: $$ u^{T}Su = \\sum_{i, j = 1, 1}^{M} u_{i}u_{j}S_{ij} \\implies \\frac{ \\partial u^{T}Su }{ \\partial u_{1} } =2u_{1}S_{11} + \\sum_{j \\neq 1}^{M}(u_{j}S_{1j} + u_{j}S_{j1}) = 2\\left( u_{1}S_{11} + \\sum_{j \\neq 1}u_{j}S_{1j} \\right) = 2(Su)_{1} $$ Last equation is true because $S$ is a symmetric matrix, then we easily see that indeed it’s true that indeed it’s the first row of the $Su$ matrix multiplied by 2.\nKnown theorems The Multivariate Chain Rule Let $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$ be an $n$-dimensional vector, and let each $x_i$ depend on a scalar variable $t$, i.e.,\n$$ x_i = x_i(t), \\quad \\text{for } i = 1, 2, \\dots, n. $$Suppose we have a function $f$ that maps $\\mathbb{R}^n \\to \\mathbb{R}$, i.e.,\n$$ f: \\mathbb{R}^n \\to \\mathbb{R}, \\quad f = f(x_1, x_2, \\dots, x_n). $$Then, the total derivative of $f$ with respect to $t$ is given by:\n$$ \\frac{d f}{d t} = \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial x_i} \\frac{d x_i}{d t}. $$or, in vector notation:\n$$ \\frac{d f}{d t} = \\nabla f \\cdot \\frac{d \\mathbf{x}}{d t}, $$where:\n$\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)$ is the gradient of $f$. $\\frac{d \\mathbf{x}}{d t} = \\left( \\frac{d x_1}{d t}, \\frac{d x_2}{d t}, \\dots, \\frac{d x_n}{d t} \\right)$ is the time derivative of $\\mathbf{x}$. Proof\nBy definition, the total derivative of $f$ with respect to $t$ measures the rate of change of $f$ as $t$ varies:\n$$ \\frac{d f}{d t} = \\lim_{\\Delta t \\to 0} \\frac{f(\\mathbf{x}(t + \\Delta t)) - f(\\mathbf{x}(t))}{\\Delta t}. $$Since $f$ is a function of $\\mathbf{x}$, we perform a first-order Taylor expansion (see Hopital, Taylor, Peano) around $\\mathbf{x}(t)$:\n$$ f(\\mathbf{x}(t + \\Delta t)) \\approx f(\\mathbf{x}(t)) + \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial x_i} \\Big|_{\\mathbf{x}(t)} \\cdot \\Delta x_i. $$Dividing by $\\Delta t$ and taking the limit:\n$$ \\frac{d f}{d t} = \\lim_{\\Delta t \\to 0} \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial x_i} \\frac{\\Delta x_i}{\\Delta t} $$Since $\\lim_{\\Delta t \\to 0} \\frac{\\Delta x_i}{\\Delta t} = \\frac{d x_i}{d t}$, we obtain:\n$$ \\frac{d f}{d t} = \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial x_i} \\frac{d x_i}{d t}. $$$$ \\frac{d f}{d t} = \\nabla f \\cdot \\frac{d \\mathbf{x}}{d t}. $$This represents the directional derivative of $f$ along the trajectory $\\mathbf{x}(t)$, showing how $f$ evolves as $t$ changes.\nTotal Derivative Rule This is a simple extension of the multi-variable chain rule described above:\nLet $f(\\mathbf{w}, \\theta)$ be a function of:\nA vector $\\mathbf{w} \\in \\mathbb{R}^n$ which itself depends on $\\theta$, i.e., $\\mathbf{w} = t(\\theta, \\epsilon)$. A scalar parameter $\\theta$. The total derivative of $f(\\mathbf{w}, \\theta)$ with respect to $\\theta$ is given by:\n$$ \\frac{d}{d \\theta} f(\\mathbf{w}, \\theta) = \\frac{\\partial f}{\\partial \\mathbf{w}} \\cdot \\frac{d \\mathbf{w}}{d \\theta} + \\frac{\\partial f}{\\partial \\theta}. $$This result follows from the multivariate chain rule. For a function $f(x_1, x_2, \\dots, x_n, \\theta)$ where each $x_i$ depends on $\\theta$, the total derivative is:\n$$ \\frac{d f}{d \\theta} = \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial x_i} \\frac{d x_i}{d \\theta} + \\frac{\\partial f}{\\partial \\theta} \\frac{ \\partial \\theta }{ \\partial \\theta } $$In our case:\nThe variables $x_i$ correspond to the components of $\\mathbf{w}$. $\\mathbf{w}$ is a vector, so we sum over its components. Thus, applying the chain rule:\n$$ \\frac{d}{d \\theta} f(\\mathbf{w}, \\theta) = \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial w_i} \\frac{d w_i}{d \\theta} + \\frac{\\partial f}{\\partial \\theta}. $$We can rewrite the above in vector notation. Since $\\mathbf{w}$ is an $n$-dimensional vector, we rewrite the sum as a dot product:\n$$ \\frac{d}{d \\theta} f(\\mathbf{w}, \\theta) = \\nabla_{\\mathbf{w}}f\\cdot \\frac{d \\mathbf{w}}{d \\theta} + \\frac{\\partial f}{\\partial \\theta}=\\frac{\\partial f}{\\partial \\mathbf{w}} \\cdot \\frac{d \\mathbf{w}}{d \\theta} + \\frac{\\partial f}{\\partial \\theta} $$where:\n$\\frac{\\partial f}{\\partial \\mathbf{w}}$ is the gradient $\\left[ \\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2}, \\dots, \\frac{\\partial f}{\\partial w_n} \\right]$. $\\frac{d \\mathbf{w}}{d \\theta}$ is the Jacobian $\\left[ \\frac{d w_1}{d \\theta}, \\frac{d w_2}{d \\theta}, \\dots, \\frac{d w_n}{d \\theta} \\right]$. One application of this formalism can is the reparametrization trick in Variational Inference.\nCommon derivatives Determinant $$ \\frac{\\partial \\det(\\mathbf{A}(t))}{\\partial \\mathbf{t}} = \\det(\\mathbf{A}) \\cdot \\left( \\text{tr}(\\mathbf{A}^{-1}) \\cdot \\frac{ \\partial A(t) }{ \\partial x } \\right) $$$$ \\frac{\\partial \\det(\\mathbf{A})}{\\partial \\mathbf{A}} = \\det(\\mathbf{A}) \\cdot (\\mathbf{A}^{-1})^\\top $$$$ \\begin{align} \\frac{\\partial \\det(\\mathbf{A})}{\\partial \\mathbf{A}} \u0026= \\det(\\mathbf{A}) \\cdot \\frac{\\partial \\ln \\det(\\mathbf{A})}{\\partial \\mathbf{A}} \\\\ \u0026= \\det(\\mathbf{A}) \\cdot \\frac{\\partial \\text{tr} (\\ln A)}{\\partial \\mathbf{A}}\\\\ \\\\ \u0026= \\det(\\mathbf{A}) \\cdot (\\mathbf{A}^{-1})^\\top \\end{align} $$ I don’t think I have understood this thing quite well…\nMatrix Inverse $$ \\frac{\\partial \\mathbf{A}^{-1}}{\\partial \\mathbf{A}} = -\\mathbf{A}^{-1} \\otimes \\mathbf{A}^{-1}. $$$$ \\begin{align} \\frac{\\partial}{\\partial \\mathbf{A}} (\\mathbf{A} \\mathbf{A}^{-1}) \u0026= \\frac{\\partial \\mathbf{I}}{\\partial \\mathbf{A}} = 0 \\\\ \u0026\\implies \\frac{\\partial \\mathbf{A}}{\\partial \\mathbf{A}} \\cdot \\mathbf{A}^{-1} + \\mathbf{A} \\cdot \\frac{\\partial \\mathbf{A}^{-1}}{\\partial \\mathbf{A}} = 0 \\\\ \u0026\\implies\\mathbf{I} \\cdot \\mathbf{A}^{-1} + \\mathbf{A} \\cdot \\frac{\\partial \\mathbf{A}^{-1}}{\\partial \\mathbf{A}} = 0 \\\\ \u0026\\implies \\frac{\\partial \\mathbf{A}^{-1}}{\\partial \\mathbf{A}} = -\\mathbf{A}^{-1} \\cdot \\mathbf{A}^{-1}. \\end{align} $$Quadratic Form $$ \\frac{\\partial}{\\partial \\mathbf{A}} \\left( \\mathbf{v}^\\top \\mathbf{A} \\mathbf{v} \\right) = \\mathbf{v} \\mathbf{v}^\\top. $$This should be easy, and quite similar to the above case when we have derived $v$.\nQuadratic Inverse $$ \\frac{\\partial}{\\partial \\mathbf{A}} \\left( \\mathbf{v}^\\top \\mathbf{A}^{-1} \\mathbf{v} \\right) = -\\mathbf{A}^{-1} \\mathbf{v} \\mathbf{v}^\\top \\mathbf{A}^{-1}. $$You can interpret this as a function composition.\nVector Matrix derivative $$ \\frac{ \\partial Mv }{ \\partial M } = I_{m} \\otimes v^{T} $$$$ \\frac{ \\partial Mv }{ \\partial M } \\in \\mathbb{R}^{m \\times mh} $$",
  "wordCount" : "974",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/multi-variable-derivatives/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Multi Variable Derivatives
    </h1>
    <div class="post-meta">5 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#multi-variable-derivative" aria-label="Multi-variable derivative">Multi-variable derivative</a></li></ul>
                    
                <li>
                    <a href="#known-theorems" aria-label="Known theorems">Known theorems</a><ul>
                        
                <li>
                    <a href="#the-multivariate-chain-rule" aria-label="The Multivariate Chain Rule">The Multivariate Chain Rule</a></li>
                <li>
                    <a href="#total-derivative-rule" aria-label="Total Derivative Rule">Total Derivative Rule</a></li></ul>
                </li>
                <li>
                    <a href="#common-derivatives" aria-label="Common derivatives">Common derivatives</a><ul>
                        
                <li>
                    <a href="#determinant" aria-label="Determinant">Determinant</a></li>
                <li>
                    <a href="#matrix-inverse" aria-label="Matrix Inverse">Matrix Inverse</a></li>
                <li>
                    <a href="#quadratic-form" aria-label="Quadratic Form">Quadratic Form</a></li>
                <li>
                    <a href="#quadratic-inverse" aria-label="Quadratic Inverse">Quadratic Inverse</a></li>
                <li>
                    <a href="#vector-matrix-derivative" aria-label="Vector Matrix derivative">Vector Matrix derivative</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h4 id="multi-variable-derivative">Multi-variable derivative<a hidden class="anchor" aria-hidden="true" href="#multi-variable-derivative">#</a></h4>
<h1 id="endbmatrix">To the people that are not used to matrix derivatives (like me) it could be useful to see how
$$
\frac{ \partial u^{T}Su }{ \partial u }  = 2Su
$$
First, we note that if you derive with respect to some matrix, the output will be of the same dimension of that matrix. That notation is just deriving every single component independently and then joining them together, so it will be better understood as as
$$
\frac{ \partial u^{T}Su }{ \partial u }  =
\begin{bmatrix}
\frac{ \partial u^{T}Su }{ \partial u_{1} }  \
\dots \
\frac{ \partial u^{T}Su }{ \partial u_{M} }  \
\end{bmatrix}</h1>
$$
So we can prove each derivative independently, it's just a lot of manual work!
We see that $u^{T}Su$ is just a quadratic form, studied in <a href="/notes/massimi-minimi-multi-variabile#forme-quadratiche">Massimi minimi multi-variabile#Forme quadratiche</a> so it is just computing this:
$$<p>
u^{T}Su = \sum_{i, j = 1, 1}^{M} u_{i}u_{j}S_{ij} \implies \frac{ \partial u^{T}Su }{ \partial u_{1} } =2u_{1}S_{11} + \sum_{j \neq 1}^{M}(u_{j}S_{1j}  + u_{j}S_{j1}) = 2\left( u_{1}S_{11} + \sum_{j \neq 1}u_{j}S_{1j} \right) = 2(Su)_{1}
$$
Last equation is true because $S$ is a symmetric matrix, then we easily see that indeed it&rsquo;s true that indeed it&rsquo;s the first row of the $Su$ matrix multiplied by 2.</p>
<h3 id="known-theorems">Known theorems<a hidden class="anchor" aria-hidden="true" href="#known-theorems">#</a></h3>
<h4 id="the-multivariate-chain-rule">The Multivariate Chain Rule<a hidden class="anchor" aria-hidden="true" href="#the-multivariate-chain-rule">#</a></h4>
<p>Let $\mathbf{x} = (x_1, x_2, \dots, x_n)$ be an $n$-dimensional vector, and let each $x_i$ depend on a scalar variable $t$, i.e.,</p>
$$
x_i = x_i(t), \quad \text{for } i = 1, 2, \dots, n.
$$<p>Suppose we have a function $f$ that maps $\mathbb{R}^n \to \mathbb{R}$, i.e.,</p>
$$
f: \mathbb{R}^n \to \mathbb{R}, \quad f = f(x_1, x_2, \dots, x_n).
$$<p>Then, the total derivative of $f$ with respect to $t$ is given by:</p>
$$
\frac{d f}{d t} = \sum_{i=1}^{n} \frac{\partial f}{\partial x_i} \frac{d x_i}{d t}.
$$<p>or, in vector notation:</p>
$$
\frac{d f}{d t} = \nabla f \cdot \frac{d \mathbf{x}}{d t},
$$<p>where:</p>
<ul>
<li>$\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)$ is the <strong>gradient</strong> of $f$.</li>
<li>$\frac{d \mathbf{x}}{d t} = \left( \frac{d x_1}{d t}, \frac{d x_2}{d t}, \dots, \frac{d x_n}{d t} \right)$ is the <strong>time derivative of $\mathbf{x}$</strong>.</li>
</ul>
<p><strong>Proof</strong></p>
<p>By definition, the total derivative of $f$ with respect to $t$ measures the rate of change of $f$ as $t$ varies:</p>
$$
\frac{d f}{d t} = \lim_{\Delta t \to 0} \frac{f(\mathbf{x}(t + \Delta t)) - f(\mathbf{x}(t))}{\Delta t}.
$$<p>Since $f$ is a function of $\mathbf{x}$, we perform a first-order Taylor expansion (see <a href="/notes/hopital,-taylor,-peano">Hopital, Taylor, Peano</a>) around $\mathbf{x}(t)$:</p>
$$
f(\mathbf{x}(t + \Delta t)) \approx f(\mathbf{x}(t)) + \sum_{i=1}^{n} \frac{\partial f}{\partial x_i} \Big|_{\mathbf{x}(t)} \cdot \Delta x_i.
$$<p>Dividing by $\Delta t$ and taking the limit:</p>
$$
\frac{d f}{d t} = \lim_{\Delta t \to 0} \sum_{i=1}^{n} \frac{\partial f}{\partial x_i} \frac{\Delta x_i}{\Delta t}
$$<p>Since $\lim_{\Delta t \to 0} \frac{\Delta x_i}{\Delta t} = \frac{d x_i}{d t}$, we obtain:</p>
$$
\frac{d f}{d t} = \sum_{i=1}^{n} \frac{\partial f}{\partial x_i} \frac{d x_i}{d t}.
$$$$
\frac{d f}{d t} = \nabla f \cdot \frac{d \mathbf{x}}{d t}.
$$<p>This represents the <strong>directional derivative</strong> of $f$ along the trajectory $\mathbf{x}(t)$, showing how $f$ evolves as $t$ changes.</p>
<h4 id="total-derivative-rule">Total Derivative Rule<a hidden class="anchor" aria-hidden="true" href="#total-derivative-rule">#</a></h4>
<p>This is a simple extension of the multi-variable chain rule described above:</p>
<p>Let $f(\mathbf{w}, \theta)$ be a function of:</p>
<ul>
<li>A vector $\mathbf{w} \in \mathbb{R}^n$ which itself depends on $\theta$, i.e., $\mathbf{w} = t(\theta, \epsilon)$.</li>
<li>A scalar parameter $\theta$.</li>
</ul>
<p>The <strong>total derivative</strong> of $f(\mathbf{w}, \theta)$ with respect to $\theta$ is given by:</p>
$$
\frac{d}{d \theta} f(\mathbf{w}, \theta) = \frac{\partial f}{\partial \mathbf{w}} \cdot \frac{d \mathbf{w}}{d \theta} + \frac{\partial f}{\partial \theta}.
$$<p>This result follows from the multivariate chain rule.
For a function $f(x_1, x_2, \dots, x_n, \theta)$ where each $x_i$ depends on $\theta$, the total derivative is:</p>
$$
\frac{d f}{d \theta} = \sum_{i=1}^{n} \frac{\partial f}{\partial x_i} \frac{d x_i}{d \theta} + \frac{\partial f}{\partial \theta} \frac{ \partial \theta }{ \partial \theta } 
$$<p>In our case:</p>
<ul>
<li>The variables $x_i$ correspond to the components of $\mathbf{w}$.</li>
<li>$\mathbf{w}$ is a vector, so we sum over its components.</li>
</ul>
<p>Thus, applying the chain rule:</p>
$$
\frac{d}{d \theta} f(\mathbf{w}, \theta) = \sum_{i=1}^{n} \frac{\partial f}{\partial w_i} \frac{d w_i}{d \theta} + \frac{\partial f}{\partial \theta}.
$$<p>We can rewrite the above in vector notation.
Since $\mathbf{w}$ is an $n$-dimensional vector, we rewrite the sum as a dot product:</p>
$$
\frac{d}{d \theta} f(\mathbf{w}, \theta) =  \nabla_{\mathbf{w}}f\cdot \frac{d \mathbf{w}}{d \theta} + \frac{\partial f}{\partial \theta}=\frac{\partial f}{\partial \mathbf{w}} \cdot \frac{d \mathbf{w}}{d \theta} + \frac{\partial f}{\partial \theta}
$$<p>where:</p>
<ul>
<li>$\frac{\partial f}{\partial \mathbf{w}}$ is the gradient $\left[ \frac{\partial f}{\partial w_1}, \frac{\partial f}{\partial w_2}, \dots, \frac{\partial f}{\partial w_n} \right]$.</li>
<li>$\frac{d \mathbf{w}}{d \theta}$ is the Jacobian $\left[ \frac{d w_1}{d \theta}, \frac{d w_2}{d \theta}, \dots, \frac{d w_n}{d \theta} \right]$.</li>
</ul>
<p>One application of this formalism can is the reparametrization trick in <a href="/notes/variational-inference">Variational Inference</a>.</p>
<h3 id="common-derivatives">Common derivatives<a hidden class="anchor" aria-hidden="true" href="#common-derivatives">#</a></h3>
<h4 id="determinant">Determinant<a hidden class="anchor" aria-hidden="true" href="#determinant">#</a></h4>
$$
\frac{\partial \det(\mathbf{A}(t))}{\partial \mathbf{t}} = \det(\mathbf{A}) \cdot \left( \text{tr}(\mathbf{A}^{-1}) \cdot \frac{ \partial A(t) }{ \partial x }  \right)
$$$$
\frac{\partial \det(\mathbf{A})}{\partial \mathbf{A}} = \det(\mathbf{A}) \cdot (\mathbf{A}^{-1})^\top
$$$$
\begin{align}
\frac{\partial \det(\mathbf{A})}{\partial \mathbf{A}} &= \det(\mathbf{A}) \cdot \frac{\partial \ln \det(\mathbf{A})}{\partial \mathbf{A}} \\
&= \det(\mathbf{A}) \cdot \frac{\partial \text{tr} (\ln A)}{\partial \mathbf{A}}\\
 \\
&= \det(\mathbf{A}) \cdot (\mathbf{A}^{-1})^\top
\end{align}
$$<p>
I don&rsquo;t think I have understood this thing quite well&hellip;</p>
<h4 id="matrix-inverse">Matrix Inverse<a hidden class="anchor" aria-hidden="true" href="#matrix-inverse">#</a></h4>
$$
\frac{\partial \mathbf{A}^{-1}}{\partial \mathbf{A}} = -\mathbf{A}^{-1} \otimes \mathbf{A}^{-1}.
$$$$
\begin{align}
\frac{\partial}{\partial \mathbf{A}} (\mathbf{A} \mathbf{A}^{-1}) &= \frac{\partial \mathbf{I}}{\partial \mathbf{A}} = 0 \\
&\implies \frac{\partial \mathbf{A}}{\partial \mathbf{A}} \cdot \mathbf{A}^{-1} + \mathbf{A} \cdot \frac{\partial \mathbf{A}^{-1}}{\partial \mathbf{A}} = 0 \\
&\implies\mathbf{I} \cdot \mathbf{A}^{-1} + \mathbf{A} \cdot \frac{\partial \mathbf{A}^{-1}}{\partial \mathbf{A}} = 0 \\
&\implies \frac{\partial \mathbf{A}^{-1}}{\partial \mathbf{A}} = -\mathbf{A}^{-1} \cdot \mathbf{A}^{-1}.
\end{align}
$$<h4 id="quadratic-form">Quadratic Form<a hidden class="anchor" aria-hidden="true" href="#quadratic-form">#</a></h4>
$$
\frac{\partial}{\partial \mathbf{A}} \left( \mathbf{v}^\top \mathbf{A} \mathbf{v} \right) = \mathbf{v} \mathbf{v}^\top.
$$<p>This should be easy, and quite similar to the above case when we have derived $v$.</p>
<h4 id="quadratic-inverse">Quadratic Inverse<a hidden class="anchor" aria-hidden="true" href="#quadratic-inverse">#</a></h4>
$$
\frac{\partial}{\partial \mathbf{A}} \left( \mathbf{v}^\top \mathbf{A}^{-1} \mathbf{v} \right) = -\mathbf{A}^{-1} \mathbf{v} \mathbf{v}^\top \mathbf{A}^{-1}.
$$<p>You can interpret this as a function composition.</p>
<h4 id="vector-matrix-derivative">Vector Matrix derivative<a hidden class="anchor" aria-hidden="true" href="#vector-matrix-derivative">#</a></h4>
$$
\frac{ \partial Mv }{ \partial M }  = I_{m} \otimes v^{T}
$$$$
\frac{ \partial Mv }{ \partial M }  \in \mathbb{R}^{m \times mh}
$$

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/linear-algebra/">⚗Linear-Algebra</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi Variable Derivatives on x"
            href="https://x.com/intent/tweet/?text=Multi%20Variable%20Derivatives&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmulti-variable-derivatives%2f&amp;hashtags=%e2%9a%97linear-algebra">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi Variable Derivatives on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmulti-variable-derivatives%2f&amp;title=Multi%20Variable%20Derivatives&amp;summary=Multi%20Variable%20Derivatives&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fmulti-variable-derivatives%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi Variable Derivatives on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fmulti-variable-derivatives%2f&title=Multi%20Variable%20Derivatives">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi Variable Derivatives on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fmulti-variable-derivatives%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi Variable Derivatives on whatsapp"
            href="https://api.whatsapp.com/send?text=Multi%20Variable%20Derivatives%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fmulti-variable-derivatives%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi Variable Derivatives on telegram"
            href="https://telegram.me/share/url?text=Multi%20Variable%20Derivatives&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fmulti-variable-derivatives%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi Variable Derivatives on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Multi%20Variable%20Derivatives&u=https%3a%2f%2fflecart.github.io%2fnotes%2fmulti-variable-derivatives%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
