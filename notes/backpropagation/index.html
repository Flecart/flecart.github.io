<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Backpropagation | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning, 💬natural-language-processing, machine-perception">
<meta name="description" content="Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well.
An important observation is that this algorithm is linear: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover. See colah&rsquo;s blog.
Karpathy has a nice resource for this topic too!
Stanford lecture on backpropagation is another resource.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/backpropagation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/backpropagation/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/backpropagation/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Backpropagation">
  <meta property="og:description" content="Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well. An important observation is that this algorithm is linear: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover. See colah’s blog. Karpathy has a nice resource for this topic too! Stanford lecture on backpropagation is another resource.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="Machinelearning">
    <meta property="article:tag" content="💬Natural-Language-Processing">
    <meta property="article:tag" content="Machine-Perception">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Backpropagation">
<meta name="twitter:description" content="Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well.
An important observation is that this algorithm is linear: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover. See colah&rsquo;s blog.
Karpathy has a nice resource for this topic too!
Stanford lecture on backpropagation is another resource.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Backpropagation",
      "item": "https://flecart.github.io/notes/backpropagation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Backpropagation",
  "name": "Backpropagation",
  "description": "Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well. An important observation is that this algorithm is linear: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover. See colah\u0026rsquo;s blog. Karpathy has a nice resource for this topic too! Stanford lecture on backpropagation is another resource.\n",
  "keywords": [
    "machinelearning", "💬natural-language-processing", "machine-perception"
  ],
  "articleBody": "Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well. An important observation is that this algorithm is linear: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover. See colah’s blog. Karpathy has a nice resource for this topic too! Stanford lecture on backpropagation is another resource.\nA Brief History of Backpropagation Backpropagation builds on some backbone ideas in mathematics and computer science.\nBuilding blocks of backpropagation go back a long time\nThe chain rule (Leibniz, 1676; L’Hôpital, 1696) Dynamic Programming (DP, Bellman, 1957) Minimisation of errors through gradient descent (Cauchy 1847, Hadamard, 1908) in the parameter space of complex, nonlinear, differentiable, multi-stage, NN-related systems (Kelley 1960; Bryson, 1961; Bryson and Denham, 1961; Pontryagin et al., 1961, …)\nExplicit, efficient error backpropagation (BP) in arbitrary, discrete, possibly sparsely connected, NN-like networks apparently was first described in 1970 by Finnish master student Seppo Linnainmaa\nOne of the first NN-specific applications of efficient BP was described by Werbos (1982)\nRumelhart, Hinton and William, 1986 significantly contributed to the popularization of BP for NNs as computers became faster. We also list this resource, it is a quite nice view on backpropagation as an optimization problem. But if you just study for the exams, probably this is not necessary.\nThe problem setting $$ \\min_{\\theta} \\sum_{(x, y) \\in \\mathcal{D}} \\mathcal{L}(f(x ; \\theta), y) $$ A common tool is gradient descent but we need to compute the gradients for this algorithm. The astounding fact is that the speed of computing the gradients is same as a forward pass. In optimization classes computing the gradients is given for granted, while the algorithm itself is not trivial.\nIn the old times people would calculate the gradients by hand for every parameter. We need automatic differentiation and we will explain here how does it work.\nMultivector chain rule Suppose you have these vectors $z = f(\\mathbf{y}) \\in \\mathbb{R}, \\mathbf{y} = g(\\mathbf{x}) \\quad \\mathbf{x} \\in \\mathbb{R}^m, \\mathbf{y} \\in \\mathbb{R}^n$\n$$ \\frac{\\partial z}{\\partial \\mathbf{x}} = \\frac{\\partial z}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = (\\nabla_{\\mathbf{y}} z)^T J_{\\mathbf{x}}(\\mathbf{y}) = \\begin{bmatrix} \\frac{\\partial z}{\\partial y_1}, \\dots, \\frac{\\partial z}{\\partial y_n} \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} \u0026 \\dots \u0026 \\frac{\\partial y_1}{\\partial x_m} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial y_n}{\\partial x_1} \u0026 \\dots \u0026 \\frac{\\partial y_n}{\\partial x_m} \\end{bmatrix} $$ We need to efficiently being able to compute Jacobians.\n$$ \\frac{\\partial z}{\\partial x_i} = \\frac{\\partial z}{\\partial y_1} \\frac{\\partial y_1}{\\partial x_i} + \\frac{\\partial z}{\\partial y_2} \\frac{\\partial y_2}{\\partial x_i} + \\dots + = \\sum_{j=1}^{m} \\frac{\\partial z}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i} $$ Using the sum rule, but if you do it in graph form it helps to understand it better.\nLinear Layer’s Backprop $$ \\frac{\\partial z}{\\partial \\mathbf{x}} = \\frac{\\partial z }{\\partial \\mathbf{y} } \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\mathbf{J}_{\\mathbf{y}}(z) \\cdot \\mathbf{J}_{\\mathbf{x}}(\\mathbf{y}) = (\\nabla_{\\mathbf{y}} z)^T \\cdot \\mathbf{J}_{\\mathbf{x}}(\\mathbf{y})$$Usually, it is quite expensive to compute every single part in this manner, so you might need to use the tools developed with automatic differentiation explained in the next sections.\nAutomatic Differentiation We will use a specific type of automatic differentiation, called reverse-mode. This is not all automatic because we need to know\nHow to decompose the whole function in easy parts Need to know the differential of the simple primitive functions that we use compute the forward pass. We can summarize the whole thing with this theorem: Reverse-mode automatic differentiation can compute the gradient of $f$ in the same time complexity as computing $f$\nThis will be a constructive theorem. First refresh some basis of analysis, see for example Limiti, Derivate, Hopital, Taylor, Peano, Calcolo differenziale, e Jacobians, and some notes about Computer Science i.e. Notazione Asintotica.\nHypergraph TODO: see (Huang 2008) for discussion on hypergraphs in NLP.\nComputational graphs 🟨 A composite function is a function composed by a series of (nonlinear) functions. Hypergraph is a graph that allows hyperedges, which is a edge that branches into more than one node, or two edges that merge into one, so it allows to have more than one source or more than one target, in normal graphs we only have single source and single targets for an edge. In the graph, the squares are hyperedges, the order of the input nodes usually matters (for example for the division operator).\nFor example the $+$ is a hyper-edge with two parents. We need hyper edges because we need a kind of ordering.\n$$ \\frac{ \\partial y }{ \\partial x } = \\sum_{p \\in \\mathcal{P(i, j)}} \\prod_{(k, l) \\in p} \\frac{ \\partial z_{l} }{ \\partial z_{k} } $$ Meaning we need to find all paths from $i$ to $j$ and then use the chain rule to compute the gradient contribution for this path. And this algorithm is exponential in time complexity.\nThe forward pass 🟩 We can write the forward pass of this algorithm in the following way: The backward pass 🟩– The optimal accumulation problem is NP-hard. This problem has some similarities with the backward pass.\nWe just to the backward pass, this example is explanatory: You can see that the derivative of $g$ to intermediate point is stored when doing the backpropagation part, and this makes it easy to propagate back step by step.\nWe can write the algorithm mathematically Bauer Paths 🟨– Bauer’s formula is a chain rule but for a graph! Given a function $f: \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$ for input $x_{j}$ and output $y_{j}$ we have that\n$$ \\frac{ \\partial y_{i} }{ \\partial x_{j} } = \\sum_{p \\in \\mathcal{P}(j, i)} \\prod_{(k, l) \\in p} \\frac{ \\partial z_{l} }{ \\partial z_{k} } $$Where $\\mathcal{P(j, i)}$ is the set of all paths from $j \\to i$ and $(k, l)$ are the individual edges. Every path in this formula is called Bauer Path.\nThe following is an example: .\nWe have many of the paths are re-used! This is also why dynamic programming is important at this stage. Weird but interesting thing is that there is a link between factoring polynomials and dynamic programming, but I did not understand why.\nTypes of Differentiation There are other types of automatic differentiation. In this section we will briefly present two of the most known aside auto-grad.\nNumerical differentiation 🟩 $$ \\frac{ \\partial f(x, y, z) }{ \\partial z } \\approx \\frac{f(x, y, z + h) - f(x, y, z)}{h} $$Symbolic differentiation 🟩- This is what is done by engines like wolphram alpha, it computes the real derivative of the expression, but this is inefficient because in some cases there is repeated computation when the same expression compares in different parts. I don’t know how exactly is done by automatic differentiation, but probably you as a programmer need to write the function in some specific way.\nSupplementary notes With this notion of gradient flow, one can try to generalize this notion and create works like this (Du et al. 2023). Where the only thing needed is to have a semi-ring and then you can use the same algorithm for some interpretability analysis or similar.\nA simple exercise We will attempt to use backpropagation for the following function so that we can understand a little bit more about this algorithm:\n$$ f(x, y, z, v) = \\exp(y ^{-x} - \\log(z)) + (y - z)^{2} \\cdot \\log(v) $$$$ \\begin{array} \\\\ \\frac{ \\partial f }{ \\partial x } = \\exp(y^{-x} - \\log(z)) \\cdot e^{-x \\log y} (-\\log y) \\\\ \\frac{ \\partial f }{ \\partial y } = \\exp(y^{-x} - \\log(z)) \\cdot e^{-x \\log y} (-x / y) + 2(y - z) \\log(v) \\\\ \\frac{ \\partial f }{ \\partial z } = \\exp(y^{-x} - \\log(z)) / (-z) - 2(y - z) \\log(v) \\\\ \\frac{ \\partial f }{ \\partial v } = 0 + \\frac{(y - z)^{2}}{v} \\end{array} $$ We can observe that with the symbolic method a lot of computation is repeated in the $\\exp$ thing.\nThis is an example of the computational graph (forward in red and backward in blue, it has a small mistake when computing the backwards for the exponential function) Check the original excalidraw here.\nAn algorithm for k-th order gradients $$ \\nabla^{2}f = \\begin{bmatrix} \\nabla (e_{1}^{T} \\nabla f(x)) \\\\ \\vdots \\\\ \\nabla (e_{n}^{T} \\nabla f(x)) \\end{bmatrix} $$ This is a handy relation, that could be extended also to the $k-th$ order case. We need $\\mathcal{O(m)}$ operations to compute the gradient for a single component, we just unfold the computation graph (we take the last backward used for the gradient, as another forward component) and do the same for every component. This algorithm runs in $\\mathcal{O(n \\cdot m)}$. It’s easy to prove by induction that if we apply the same algorithm, in order to compute the $k$ order algorithm it will take $\\mathcal{O}(n^{k - 1}m)$ time.\nReferences [1] Huang “Advanced Dynamic Programming in Semiring and Hypergraph Frameworks” Coling 2008 Organizing Committee 2008\n[2] Du et al. “Generalizing Backpropagation for Gradient-Based Interpretability” arXiv preprint arXiv:2307.03056 2023\n",
  "wordCount" : "1500",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/backpropagation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Backpropagation
    </h1>
    <div class="post-meta">8 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul><ul>
                <li>
                    <a href="#a-brief-history-of-backpropagation" aria-label="A Brief History of Backpropagation">A Brief History of Backpropagation</a></li></ul>
                    
                <li>
                    <a href="#the-problem-setting" aria-label="The problem setting">The problem setting</a><ul>
                        
                <li>
                    <a href="#multivector-chain-rule" aria-label="Multivector chain rule">Multivector chain rule</a></li>
                <li>
                    <a href="#linear-layers-backprop" aria-label="Linear Layer&rsquo;s Backprop">Linear Layer&rsquo;s Backprop</a></li></ul>
                </li>
                <li>
                    <a href="#automatic-differentiation" aria-label="Automatic Differentiation">Automatic Differentiation</a><ul>
                        
                <li>
                    <a href="#hypergraph" aria-label="Hypergraph">Hypergraph</a></li>
                <li>
                    <a href="#computational-graphs-" aria-label="Computational graphs 🟨">Computational graphs 🟨</a></li>
                <li>
                    <a href="#the-forward-pass-" aria-label="The forward pass 🟩">The forward pass 🟩</a></li>
                <li>
                    <a href="#the-backward-pass---" aria-label="The backward pass 🟩&ndash;">The backward pass 🟩&ndash;</a></li>
                <li>
                    <a href="#bauer-paths---" aria-label="Bauer Paths 🟨&ndash;">Bauer Paths 🟨&ndash;</a></li></ul>
                </li>
                <li>
                    <a href="#types-of-differentiation" aria-label="Types of Differentiation">Types of Differentiation</a><ul>
                        
                <li>
                    <a href="#numerical-differentiation-" aria-label="Numerical differentiation 🟩">Numerical differentiation 🟩</a></li>
                <li>
                    <a href="#symbolic-differentiation--" aria-label="Symbolic differentiation 🟩-">Symbolic differentiation 🟩-</a></li></ul>
                </li>
                <li>
                    <a href="#supplementary-notes" aria-label="Supplementary notes">Supplementary notes</a><ul>
                        
                <li>
                    <a href="#a-simple-exercise" aria-label="A simple exercise">A simple exercise</a></li>
                <li>
                    <a href="#an-algorithm-for-k-th-order-gradients" aria-label="An algorithm for k-th order gradients">An algorithm for k-th order gradients</a></li></ul>
                </li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Backpropagation is perhaps the most important algorithm of the 21st century. It is used everywhere in machine learning and is also connected to computing marginal distributions. This is why all machine learning scientists and data scientists should understand this algorithm very well.
An important observation is that this algorithm is <strong>linear</strong>: the time complexity is the same as the forward pass. Derivatives are unexpectedly cheap to calculate. This took a lot of time to discover. See <a href="https://colah.github.io/posts/2015-08-Backprop/">colah&rsquo;s blog</a>.
<a href="https://youtu.be/VMj-3S1tku0?si=wRCObFw7woZTwU56">Karpathy</a> has a nice resource for this topic too!
<a href="https://www.youtube.com/watch?v=zUazLXZZA2U&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU">Stanford lecture</a> on backpropagation is another resource.</p>
<h4 id="a-brief-history-of-backpropagation">A Brief History of Backpropagation<a hidden class="anchor" aria-hidden="true" href="#a-brief-history-of-backpropagation">#</a></h4>
<p>Backpropagation builds on some backbone ideas in mathematics and computer science.</p>
<ul>
<li>
<p>Building blocks of backpropagation go back a long time</p>
<ul>
<li>The chain rule (Leibniz, 1676; L&rsquo;Hôpital, 1696)</li>
<li>Dynamic Programming (DP, Bellman, 1957)</li>
<li>Minimisation of errors through gradient descent (Cauchy 1847, Hadamard, 1908)</li>
</ul>
</li>
<li>
<p>in the parameter space of complex, nonlinear, differentiable, multi-stage, NN-related systems (Kelley 1960; Bryson, 1961; Bryson and Denham, 1961; Pontryagin et al., 1961, &hellip;)</p>
</li>
<li>
<p>Explicit, efficient error backpropagation (BP) in arbitrary, discrete, possibly sparsely connected, NN-like networks apparently was first described in 1970 by Finnish master student Seppo Linnainmaa</p>
</li>
<li>
<p>One of the first NN-specific applications of efficient BP was described by Werbos (1982)</p>
</li>
<li>
<p>Rumelhart, Hinton and William, 1986 significantly contributed to the popularization of BP for NNs as computers became faster.
We also list <a href="https://ee227c.github.io/notes/ee227c-lecture16.pdf">this</a> resource, it is a quite nice view on backpropagation as an optimization problem. But if you just study for the exams, probably this is not necessary.</p>
</li>
</ul>
<h3 id="the-problem-setting">The problem setting<a hidden class="anchor" aria-hidden="true" href="#the-problem-setting">#</a></h3>
$$
\min_{\theta} \sum_{(x, y) \in \mathcal{D}} \mathcal{L}(f(x ; \theta), y)
$$<p>
A common tool is <strong>gradient descent</strong> but we need to compute the gradients for this algorithm. The astounding fact is that the speed of computing the gradients is same as a forward pass.
In optimization classes computing the gradients is given for granted, while the algorithm itself is not trivial.</p>
<p>In the old times people would calculate the gradients by hand for every parameter.
We need <strong>automatic differentiation</strong> and we will explain here how does it work.</p>
<h4 id="multivector-chain-rule">Multivector chain rule<a hidden class="anchor" aria-hidden="true" href="#multivector-chain-rule">#</a></h4>
<p>Suppose you have these vectors $z = f(\mathbf{y}) \in \mathbb{R}, \mathbf{y} = g(\mathbf{x}) \quad \mathbf{x} \in \mathbb{R}^m, \mathbf{y} \in \mathbb{R}^n$</p>
$$
\frac{\partial z}{\partial \mathbf{x}} = \frac{\partial z}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = (\nabla_{\mathbf{y}} z)^T J_{\mathbf{x}}(\mathbf{y}) = \begin{bmatrix} \frac{\partial z}{\partial y_1}, \dots, \frac{\partial z}{\partial y_n} \end{bmatrix} \begin{bmatrix} \frac{\partial y_1}{\partial x_1} & \dots & \frac{\partial y_1}{\partial x_m} \\ \vdots & \ddots & \vdots \\ \frac{\partial y_n}{\partial x_1} & \dots & \frac{\partial y_n}{\partial x_m} \end{bmatrix}
$$<p>
We need to efficiently being able to compute Jacobians.</p>
$$
\frac{\partial z}{\partial x_i} = \frac{\partial z}{\partial y_1} \frac{\partial y_1}{\partial x_i} + \frac{\partial z}{\partial y_2} \frac{\partial y_2}{\partial x_i} + \dots + = \sum_{j=1}^{m} \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i}
$$<p>
Using the sum rule, but if you do it in graph form it helps to understand it better.</p>
<h4 id="linear-layers-backprop">Linear Layer&rsquo;s Backprop<a hidden class="anchor" aria-hidden="true" href="#linear-layers-backprop">#</a></h4>
$$ \frac{\partial z}{\partial \mathbf{x}} = \frac{\partial z }{\partial \mathbf{y} } \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \mathbf{J}_{\mathbf{y}}(z) \cdot \mathbf{J}_{\mathbf{x}}(\mathbf{y}) = (\nabla_{\mathbf{y}} z)^T \cdot \mathbf{J}_{\mathbf{x}}(\mathbf{y})$$<p>Usually, it is quite expensive to compute every single part in this manner, so you might need to use the tools developed with automatic differentiation explained in the next sections.</p>
<h3 id="automatic-differentiation">Automatic Differentiation<a hidden class="anchor" aria-hidden="true" href="#automatic-differentiation">#</a></h3>
<p>We will use a specific type of automatic differentiation, called <strong>reverse-mode</strong>. This is not all automatic because we need to know</p>
<ul>
<li>How to decompose the whole function in easy parts</li>
<li>Need to know the differential of the simple <em>primitive</em> functions that we use compute the forward pass.
We can summarize the whole thing with this theorem:</li>
</ul>
<blockquote>
<p><em>Reverse-mode</em> automatic differentiation can compute the gradient of $f$ in the same time complexity as computing $f$</p></blockquote>
<p>This will be a constructive theorem.
First refresh some basis of analysis, see for example <a href="/notes/limiti">Limiti</a>, <a href="/notes/derivate">Derivate</a>, <a href="/notes/hopital,-taylor,-peano">Hopital, Taylor, Peano</a>, <a href="/notes/calcolo-differenziale">Calcolo differenziale</a>, e Jacobians, and some notes about Computer Science i.e. <a href="/notes/notazione-asintotica">Notazione Asintotica</a>.</p>
<h4 id="hypergraph">Hypergraph<a hidden class="anchor" aria-hidden="true" href="#hypergraph">#</a></h4>
<p>TODO: see <a href="https://aclanthology.org/C08-5001">(Huang 2008)</a> for discussion on hypergraphs in NLP.</p>
<h4 id="computational-graphs-">Computational graphs 🟨<a hidden class="anchor" aria-hidden="true" href="#computational-graphs-">#</a></h4>
<p>A composite function is a function composed by a series of (nonlinear) functions.
Hypergraph is a graph that allows hyperedges, which is a edge that branches into more than one node, or two edges that merge into one, so it allows to have more than one source or more than one target, in normal graphs we only have single source and single targets for an edge.
In the graph, the squares are <strong>hyperedges</strong>, the order of the input nodes usually matters (for example for the division operator).</p>
<img src="/images/notes/Backpropagation-20240812112848923.webp" style="width: 100%" class="center" alt="Backpropagation-20240812112848923">
<p>For example the $+$ is a hyper-edge with two parents. We need hyper edges because we need a kind of ordering.</p>
$$
\frac{ \partial y }{ \partial x } = \sum_{p \in \mathcal{P(i, j)}} \prod_{(k, l) \in p} \frac{ \partial z_{l} }{ \partial z_{k} } 
$$<p>
Meaning we need to find all paths from $i$ to $j$ and then use the chain rule to compute the gradient contribution for this path. And this algorithm is exponential in time complexity.</p>
<h4 id="the-forward-pass-">The forward pass 🟩<a hidden class="anchor" aria-hidden="true" href="#the-forward-pass-">#</a></h4>
<p>We can write the forward pass of this algorithm in the following way:
<img src="/images/notes/Backpropagation-20240812120011527.webp" style="width: 100%" class="center" alt="Backpropagation-20240812120011527"></p>
<h4 id="the-backward-pass---">The backward pass 🟩&ndash;<a hidden class="anchor" aria-hidden="true" href="#the-backward-pass---">#</a></h4>
<p>The optimal accumulation problem is NP-hard. This problem has some similarities with the backward pass.</p>
<p>We just to the backward pass, this example is explanatory:
<img src="/images/notes/Backpropagation-20240812121212094.webp" style="width: 100%" class="center" alt="Backpropagation-20240812121212094">
You can see that the derivative of $g$ to intermediate point is stored when doing the backpropagation part, and this makes it easy to propagate back step by step.</p>
<p>We can write the algorithm mathematically
<img src="/images/notes/Backpropagation-20241021123919908.webp" style="width: 100%" class="center" alt="Backpropagation-20241021123919908"></p>
<h4 id="bauer-paths---">Bauer Paths 🟨&ndash;<a hidden class="anchor" aria-hidden="true" href="#bauer-paths---">#</a></h4>
<p><strong>Bauer&rsquo;s formula</strong> is a <em>chain rule</em> but for a graph!
Given a function $f: \mathbb{R}^{n} \to \mathbb{R}^{m}$ for input $x_{j}$ and output $y_{j}$ we have that</p>
$$
\frac{ \partial y_{i} }{ \partial x_{j} }  = \sum_{p \in \mathcal{P}(j, i)} \prod_{(k, l) \in p} \frac{ \partial z_{l} }{ \partial z_{k} } 
$$<p>Where  $\mathcal{P(j, i)}$ is the set of all paths from $j \to i$ and $(k, l)$ are the individual edges. Every path in this formula is called <strong>Bauer Path</strong>.</p>
<p>The following is an example:
<img src="/images/notes/Backpropagation-20240923124949223.webp" style="width: 100%" class="center" alt="Backpropagation-20240923124949223">.</p>
<p>We have <em>many of the paths</em> are <strong>re-used</strong>! This is also why dynamic programming is important at this stage.
Weird but interesting thing is that there is a link between factoring polynomials and dynamic programming, but I did not understand why.</p>
<h3 id="types-of-differentiation">Types of Differentiation<a hidden class="anchor" aria-hidden="true" href="#types-of-differentiation">#</a></h3>
<p>There are other types of automatic differentiation. In this section we will briefly present two of the most known aside auto-grad.</p>
<h4 id="numerical-differentiation-">Numerical differentiation 🟩<a hidden class="anchor" aria-hidden="true" href="#numerical-differentiation-">#</a></h4>
$$
\frac{ \partial f(x, y, z) }{ \partial z }  \approx \frac{f(x, y, z + h) - f(x, y, z)}{h}
$$<h4 id="symbolic-differentiation--">Symbolic differentiation 🟩-<a hidden class="anchor" aria-hidden="true" href="#symbolic-differentiation--">#</a></h4>
<p>This is what is done by engines like wolphram alpha, it computes the real derivative of the expression, but this is inefficient because in some cases there is repeated computation when the same expression compares in different parts.
I don&rsquo;t know how exactly is done by automatic differentiation, but probably you as a programmer need to write the function in some specific way.</p>
<h3 id="supplementary-notes">Supplementary notes<a hidden class="anchor" aria-hidden="true" href="#supplementary-notes">#</a></h3>
<p>With this notion of gradient flow, one can try to generalize this notion and create works like this <a href="http://arxiv.org/abs/2307.03056">(Du et al. 2023)</a>.
Where the only thing needed is to have a semi-ring and then you can use the same algorithm for some interpretability analysis or similar.</p>
<h4 id="a-simple-exercise">A simple exercise<a hidden class="anchor" aria-hidden="true" href="#a-simple-exercise">#</a></h4>
<p>We will attempt to use backpropagation for the following function so that we can understand a little bit more about this algorithm:</p>
$$
f(x, y, z, v) = \exp(y ^{-x} - \log(z)) + (y - z)^{2} \cdot \log(v)
$$$$
\begin{array}
 \\
\frac{ \partial f }{ \partial x }  = \exp(y^{-x} - \log(z)) \cdot e^{-x \log y} (-\log y) \\
\frac{ \partial f }{ \partial y } = \exp(y^{-x} - \log(z)) \cdot e^{-x \log y} (-x / y) + 2(y - z) \log(v) \\
\frac{ \partial f }{ \partial z } =    \exp(y^{-x} - \log(z)) / (-z) - 2(y - z) \log(v) \\
\frac{ \partial f }{ \partial v }  = 0 + \frac{(y - z)^{2}}{v}
\end{array}
$$<p>
We can observe that with the symbolic method a lot of computation is repeated in the $\exp$ thing.</p>
<p>This is an example of the computational graph (forward in red and backward in blue, it has a small mistake when computing the backwards for the exponential function)
<img src="/images/notes/Backpropagation-20240905191842592.webp" style="width: 100%" class="center" alt="Backpropagation-20240905191842592"></p>
<p>Check the original excalidraw <a href="https://excalidraw.com/#json=pT2sTl1liM19ESIIB_2Ey,N3HCG3kKnq12dhygrNFRGg">here</a>.</p>
<h4 id="an-algorithm-for-k-th-order-gradients">An algorithm for k-th order gradients<a hidden class="anchor" aria-hidden="true" href="#an-algorithm-for-k-th-order-gradients">#</a></h4>
$$
\nabla^{2}f = \begin{bmatrix}
\nabla (e_{1}^{T} \nabla f(x)) \\
\vdots \\
\nabla (e_{n}^{T} \nabla f(x))
\end{bmatrix}
$$<p>
This is a handy relation, that could be extended also to the $k-th$ order case.
We need $\mathcal{O(m)}$ operations to compute the gradient for a single component, we just unfold the computation graph (we take the last backward used for the gradient, as another forward component) and do the same for every component. This algorithm runs in $\mathcal{O(n \cdot m)}$.
It&rsquo;s easy to prove by induction that if we apply the same algorithm, in order to compute the $k$ order algorithm it will take $\mathcal{O}(n^{k - 1}m)$ time.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Huang <a href="https://aclanthology.org/C08-5001">“Advanced Dynamic Programming in Semiring and Hypergraph Frameworks”</a> Coling 2008 Organizing Committee  2008</p>
<p>[2] Du et al. <a href="http://arxiv.org/abs/2307.03056">“Generalizing Backpropagation for Gradient-Based Interpretability”</a> arXiv preprint arXiv:2307.03056 2023</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
      <li><a href="https://flecart.github.io/tags/natural-language-processing/">💬Natural-Language-Processing</a></li>
      <li><a href="https://flecart.github.io/tags/machine-perception/">Machine-Perception</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation on x"
            href="https://x.com/intent/tweet/?text=Backpropagation&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbackpropagation%2f&amp;hashtags=machinelearning%2c%f0%9f%92%acnatural-language-processing%2cmachine-perception">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbackpropagation%2f&amp;title=Backpropagation&amp;summary=Backpropagation&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fbackpropagation%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fbackpropagation%2f&title=Backpropagation">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fbackpropagation%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation on whatsapp"
            href="https://api.whatsapp.com/send?text=Backpropagation%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fbackpropagation%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation on telegram"
            href="https://telegram.me/share/url?text=Backpropagation&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbackpropagation%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Backpropagation&u=https%3a%2f%2fflecart.github.io%2fnotes%2fbackpropagation%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
