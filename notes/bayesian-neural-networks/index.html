<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bayesian neural networks | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="machinelearning, ➕probabilistic-artificial-intelligence">
<meta name="description" content="Robbins-Moro Algorithm the algorithm is very simple we do Until convergence: set some learning rates that satisfy the Robbins Moro Conditions, choose a $w_{0}$ then update in the following way: $$ w_{n&#43;1} = w_{n} - \alpha_{n} \Delta w_{n} $$ For example with $\alpha_{0} &gt; \alpha_{1} &gt; \dots &gt; \alpha_{n} \dots$, and $\alpha_{t} = \frac{1}{t}$ they satisfy the condition (in practice we use a constant $\alpha$, but we lose the convergence guarantee by Robbins Moro).">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/bayesian-neural-networks/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/bayesian-neural-networks/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Bayesian neural networks" />
<meta property="og:description" content="Robbins-Moro Algorithm the algorithm is very simple we do Until convergence: set some learning rates that satisfy the Robbins Moro Conditions, choose a $w_{0}$ then update in the following way: $$ w_{n&#43;1} = w_{n} - \alpha_{n} \Delta w_{n} $$ For example with $\alpha_{0} &gt; \alpha_{1} &gt; \dots &gt; \alpha_{n} \dots$, and $\alpha_{t} = \frac{1}{t}$ they satisfy the condition (in practice we use a constant $\alpha$, but we lose the convergence guarantee by Robbins Moro)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://flecart.github.io/notes/bayesian-neural-networks/" />
<meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="Bayesian neural networks"/>
<meta name="twitter:description" content="Robbins-Moro Algorithm the algorithm is very simple we do Until convergence: set some learning rates that satisfy the Robbins Moro Conditions, choose a $w_{0}$ then update in the following way: $$ w_{n&#43;1} = w_{n} - \alpha_{n} \Delta w_{n} $$ For example with $\alpha_{0} &gt; \alpha_{1} &gt; \dots &gt; \alpha_{n} \dots$, and $\alpha_{t} = \frac{1}{t}$ they satisfy the condition (in practice we use a constant $\alpha$, but we lose the convergence guarantee by Robbins Moro)."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bayesian neural networks",
      "item": "https://flecart.github.io/notes/bayesian-neural-networks/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bayesian neural networks",
  "name": "Bayesian neural networks",
  "description": "Robbins-Moro Algorithm the algorithm is very simple we do Until convergence: set some learning rates that satisfy the Robbins Moro Conditions, choose a $w_{0}$ then update in the following way: $$ w_{n+1} = w_{n} - \\alpha_{n} \\Delta w_{n} $$ For example with $\\alpha_{0} \u003e \\alpha_{1} \u003e \\dots \u003e \\alpha_{n} \\dots$, and $\\alpha_{t} = \\frac{1}{t}$ they satisfy the condition (in practice we use a constant $\\alpha$, but we lose the convergence guarantee by Robbins Moro).",
  "keywords": [
    "machinelearning", "➕probabilistic-artificial-intelligence"
  ],
  "articleBody": "Robbins-Moro Algorithm the algorithm is very simple we do Until convergence: set some learning rates that satisfy the Robbins Moro Conditions, choose a $w_{0}$ then update in the following way: $$ w_{n+1} = w_{n} - \\alpha_{n} \\Delta w_{n} $$ For example with $\\alpha_{0} \u003e \\alpha_{1} \u003e \\dots \u003e \\alpha_{n} \\dots$, and $\\alpha_{t} = \\frac{1}{t}$ they satisfy the condition (in practice we use a constant $\\alpha$, but we lose the convergence guarantee by Robbins Moro). More generally, the Robbins-Moro conditions re:\n$\\sum_{n} \\alpha_{n} = \\infty$ $\\sum_{n} \\alpha_{n}^{2} \u003c \\infty$ Then the algorithm is guaranteed to converge to the best answer. One nice thing about this, is that we don’t need gradients. But often we use gradient versions (stochastic gradient descent and similar), using autograd, see Backpropagation. But learning with gradients brings some drawbacks:\nWe have no quantification of the uncertainty. We are usually overconfident in our predictions (adversarial examples, and poor generalization to domain shifts). Bayesian Neural Networks The Idea The idea here is the same as the one applied to Bayesian neural networks: define a prior of the weights and likelihood and define a posterior over the weights. But the nature of the model makes it quite difficult, if not impossible to have a closed-form solution for the posterior.\nA simple model We want to predict the weights of a given model with some bounds on certainty about them. One thing is that we have always used linear models for this, but we can also use neural networks or more complex models. Usually, in some datasets also the variance changes with the input; this is called heteroskedastic noise. This justifies models in the following form:\n$$ p(y \\mid \\boldsymbol{x}, \\theta) = \\mathcal{N}(u; f_{1}(\\boldsymbol{x}, \\theta), \\exp(f_{2}(\\boldsymbol{x}, \\theta))) $$ where $f_{1}$ and $f_{2}$ are neural networks and $\\theta$ are the weights of the neural network. We use neural networks to predict mean and variance. We use the $\\exp$ function to ensure that the variance is always positive.\nFrom a practical point of view, this machinery is quite good at estimating variance in regions where data is present (same thing in heteroskedastic settings!), while it is overconfident in zones without data, often collapsing the variance.\nMap estimate for bayesian NN Assume a prior $p(\\theta) \\sim \\mathcal{N}(\\theta; 0, \\lambda^{-1})$ and the above parametrization of the likelihood function, let’s derive the posterior distribution, that is $p(\\theta \\mid y, \\boldsymbol{x})$.\nWe notice that while in the Bayesian Linear Regression the normalization constant is disregarded, here it is dependent on the $\\theta$, so we need to consider it. We can pay higher variance cost to minimize the error in the quadratic term.\n$$ \\hat{\\theta} = \\arg\\min_{\\theta} - \\underbrace{\\log p(\\theta)}_{\\lambda \\lVert \\theta \\rVert ^{2}_{2}} - \\sum_{i = 1}^{n} \\log p(y_{i} \\mid x_{i}, \\theta) $$ If we use the above model for Bayesian Modeling we derive the likelihood to be: $$ \\begin{align} \\log p(y_{i} \\mid x_{i}, \\theta) \u0026= \\log \\mathcal{N}(y_{i}; \\mu(x_{i}, \\theta), \\sigma^{2}(x_{i}, \\theta))) \\\\ \u0026 = -\\frac{1}{2} \\log 2\\pi \\sigma^{2}(x_{i}, \\theta) - \\frac{1}{2\\sigma^{2}(x_{i}, \\theta)} (y_{i} - \\mu(x_{i}, \\theta))^{2} \\\\ \u0026 = -\\frac{1}{2} \\log 2\\pi - \\frac{1}{2} \\log \\sigma^{2}(x_{i}, \\theta) - \\frac{1}{2\\sigma^{2}(x_{i}, \\theta)} (y_{i} - \\mu(x_{i}, \\theta))^{2} \\\\ \u0026= \\underbrace{-\\frac{1}{2} \\log 2\\pi}_{\\text{const}} -\\frac{1}{2}\\left[ \\log \\sigma^{2}(x_{i}, \\theta) + \\frac{ (y_{i} - \\mu(x_{i}, \\theta))^{2} }{\\sigma^{2}(x_{i}, \\theta)}\\right] \\end{align} $$ Here you observe that the likelihood is maximized by having high variance (plus penalty) or having correct estimates. Then adding the prior is just a regularizing term that corresponds to weight decay (a good exercise is to prove this part).\nThe Classification Case: TODO: minute 22 of part 2.\nPrediction with Bayesian Neural Networks We see that prediction is just averaging over the predictions of different networks sampled from the approximate distribution that we derived from the variational inference of the weights. Some formalization could help in clearing this out.\n$$ \\begin{align} p(y^{} \\mid x^{}, x_{1:n}, y_{1:n}) \u0026 = \\int p(y^{} \\mid x^{}, \\theta) p(\\theta \\mid x_{1:n}, y_{1:n}) d\\theta \\ \u0026 = \\mathbb{E}{\\theta \\sim p(\\cdot \\mid x{1:n}, y_{1:n})} [p(y^{} \\mid x^{}, \\theta)]) \\ \\text{Using Variational Inf.}\u0026 \\approx \\mathbb{E}{{\\theta \\sim q(\\theta)}} [p(y^{} \\mid x^{}, \\theta)] \\\n\\text{Monte Carlo }\u0026 \\approx \\frac{1}{S} \\sum_{s = 1}^{S} p(y^{} \\mid x^{}, \\theta_{s}) \\end{align} $$ The variational is usually the approximation that is introducing the most errors.\nApproximating with Gaussians 🟥++ Training Using the variational method, usually we parametrize with Gaussians. See Variational Inference. We usually use the ELBO to minimize the difference between the variational approximation and the true distribution.\n$$ \\log p(D) \\geq \\max_{q \\in \\mathcal{Q}} ELBO(q) = \\max_{q \\in \\mathcal{Q}} \\mathbb{E}_{q(\\theta)} [\\log p(D, \\theta) - \\log q(\\theta)] $$ Assuming a Gaussian prior, we can derive the ELBO as: $$ \\begin{align} ELBO(q) \u0026= \\mathbb{E}_{q(\\theta)} [\\log p(D, \\theta) - \\log q(\\theta)] \\\\ \u0026= \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, 1)} [\\log p(D, \\theta + \\sigma \\varepsilon)] + KL(q \\mid p) \\\\ \u0026 = \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, 1)} [\\log p(D, \\theta + \\sigma \\varepsilon)] + \\frac{1}{2}\\log \\lvert \\Sigma \\rvert + \\frac{D}{2} \\log 2\\pi e \\end{align} $$ This is known as Bayes by backprop (2015), and is parameterized with a diagonal covariance Gaussian.\nThis is used to learn the covariance matrix and mean of the variational family. After, we can do inference in the following manner:\nInference $$ \\begin{align} p(y^{*} \\mid x^{*}, x_{1:n}, y_{1:n})\u0026= \\int p(y^{*} \\mid x^{*}, \\theta) p(\\theta \\mid x_{1:n}, y_{1:n}) d\\theta \\\\ \u0026\\approx \\int p(y^{*} \\mid x^{*}, \\theta) q(\\theta) d\\theta \\\\ \u0026\\approx \\frac{1}{S} \\sum_{s = 1}^{S} p(y^{*} \\mid x^{*}, \\theta_{s}) \\\\ \u0026= \\frac{1}{S} \\sum_{s = 1}^{S} \\mathcal{N}(y^{*}; f(x^{*}, \\theta_{s}), \\exp(g(x^{*}, \\theta_{s}))) \\end{align} $$ Intuitively, variational inference in Bayesian neural networks can be interpreted as averaging the predictions of multiple neural networks drawn according to the variational posterior $q$\nUnpacking uncertainties We can un pack that uncertainty of the likelihood prediction using the classical aleatoric and epistemic uncertainties: $$ \\begin{align} \\text{Var}[y^{*} \\mid x^{*}, x_{1:n}, y_{1:n}] \u0026= \\underbrace{\\mathbb{E}_{q(\\theta)}[\\text{Var}[y^{*} \\mid x^{*}, \\theta]]}_{\\text{Aleatoric}} + \\underbrace{\\text{Var}_{q(\\theta)}[\\mathbb{E}[y^{*} \\mid x^{*}, \\theta]] }_{\\text{Epistemic}} \\\\ \u0026\\approx \\frac{1}{m} \\sum_{s = 1}^{m} \\exp(g(x^{*}, \\theta_{s})) + \\frac{1}{m - 1} \\sum_{s = 1}^{m} (\\mu(x^{*}, \\theta_{s}) - \\bar{\\mu})^{2} \\end{align} $$ Where $\\bar{\\mu} = \\frac{1}{m} \\sum_{s = 1}^{m} \\mu(x^{*}, \\theta_{s})$.\nWith Laplace Approximation We need to model the joint density as $$ \\log p(\\mathcal{D}, \\theta) \\approx \\log p (\\mathcal{D}, \\hat{\\theta}) + \\frac{1}{2} (\\theta - \\hat{\\theta})^{T} H(\\theta - \\hat{\\theta}) $$ If we marginalize over the parameters we get $$ p(\\mathcal{D}) = p(\\mathcal{D}, \\hat{\\theta}) (2\\pi)^{D/2} \\lvert -H_{\\theta} \\rvert ^{-1/2} $$ The problem is often the Hessian, which could be unstable, or difficult to approximate. For this reason usually we use low-rank matrices, diagonal matrices, or other approximations.\nDropout as Variational Inference Dropout and dropconnect can be seen as a special case of variational inference. It can been seen as if we are working with a probability distribution of different networks, which can be recalled as a special case of the one we have discussed above.\nThe Variational Family 🟨– With dropout, instead of working with a single network, we are working with many probabilistic networks. It can be interpreted as doing variational inference with the following variational family: $$ q(\\theta \\mid \\lambda) = \\prod_{j} q_{j}(\\theta_{j} \\mid \\lambda_{j}) $$ Where $q_{j}(\\theta \\mid \\lambda)) = p\\delta_{0}(\\theta_{j}) + (1 - p) \\delta_{\\lambda_{j}}(\\theta_{j})$ a mixture of Dirac deltas: with a certain probability $1- p$ we keep the weight, or set it to zero. Optimizing the network with dropout can be seen as optimizing the ELBO with respect of this variational family.\nInference with Dropout 🟩 One thing is using dropout during prediction to obtain uncertainty estimates. This is called Monte Carlo Dropout. The idea is very simple: keep the dropout during inference and sample from the network multiple times. The average of the predictions is the final prediction, and the variance is the uncertainty estimate.\nDeep Ensembles The Idea We train $m$ different models from a bootstrapped dataset and then just average their prediction. This is the main idea of doing Ensembles.\nWe can also extend the probabilistic case to classification. In this case we just add a noise in the softmax layer to have some variability.\nInference With this method we train different models on different data (maybe bootstrapped data, see Cross Validation and Model Selection). Then we just average the predictions of the different models. This is a very simple way to obtain uncertainty estimates, which allows to approximate the bayesian inference method.\n$$ \\begin{align} p(y^{*} \\mid x^{*}, x_{1:n}, y_{1:n}) \u0026= \\int p(y^{*} \\mid x^{*}, \\theta) p(\\theta \\mid x_{1:n}, y_{1:n}) d\\theta \\\\ \u0026= \\mathop{\\mathbb{E}}_{\\theta \\sim p(\\cdot \\mid x_{1:n}, y_{1:n})} [p(y^{*} \\mid x^{*}, \\theta)] \\\\ \u0026\\approx \\frac{1}{m} \\sum_{s = 1}^{m} p(y^{*} \\mid x^{*}, \\theta_{s}) \\end{align} $$ Markov Chain Monte Carlo The MCMC idea We would like here to sample from the posterior distribution of the weights using MCMC (somewhat similar to the SGLD method), and then use the samples to make predictions. This is a very general method, but it’s computationally expensive. These methods explained in Monte Carlo Methods are exactly the same here, you can use these to sample from the posterior and then use the same tricks in #Inference (remember the Ergodic theorem) and #Unpacking uncertainties.\nKey Challenges There are two main challenges with these methods:\nStoring the weights (usually to expensive). One way to circumvent this problem is to keep some snapshots or keeping some running averages (see next section). One idea is subsampling: we store some intermediate weights or predictions and use those for the final inference. Burn in period is too costly. Running averages Another idea is assuming a distribution over the weights (e.g. a Gaussian) and keeping the mean and variance of the weights. Then we can sample from this distribution. This is just the idea of SWA-Gaussians, presented (Maddox et al. 2019). This is a way not to store all the weights. $$ \\mu \\leftarrow \\frac{1}{T + 1} (T \\mu + \\theta) $$ And: $$ \\sigma^{2} \\leftarrow \\frac{1}{T + 1} (T \\sigma^{2} + \\theta^{T}\\theta) $$ Outlook: Othe rmethods Stein Variational Gradient Descent This is called (SVGD) by Liu and Wang 2016. We want to approximate the posterior distribution with a set of particles and minimize the KL divergence between the true posterior and the approximated one. We consider an ensemble of models $\\left\\{ \\theta_{i} \\right\\}^{n}_{i = 1}$ and we update them with the following update rule: $$ \\theta_{i} \\leftarrow \\theta_{i} + \\frac{\\epsilon}{n} \\sum_{j = 1}^{n} k(\\theta_{i}, \\theta_{j}) (\\nabla_{\\theta_{j}} \\log p(\\theta_{j} \\mid \\mathcal{D}) - \\nabla_{\\theta_{j}}k(\\theta_{j}, \\theta_{i})) $$ I have no idea why we use this update rule, but it should be minimizing a certain divergence between the two distributions. TODO: understand this part.\nCalibration What is Calibration? 🟩 Well calibrated models have a similar accuracy to the confidence of their predictions. This is a very important property for many applications, especially in medical fields. For example, if a model is well calibrated, if it is secure on a prediction, it can leave the burden from the doctors, who would just manually check the hard ones.\nExpected Calibration Error 🟩– A well calibrated model’s accuracy is the same as the probability, interpretable as a confidence score, of its prediction. Mathematically we say that a model is calibrated if $$ \\mathbb{E}_{p \\sim \\hat{P}} [\\lvert \\mathbb{P}(\\hat{Y} = Y \\mid \\hat{P} = p) - p \\rvert ] = 0 $$ With $\\hat{Y}$ the predictions of the model and $\\hat{P}$ its confidences and $Y$ is the true label. So we would like to have $$ \\mathbb{P}(\\hat{Y} = y \\mid \\hat{P} = p) = p $$ Usually it’s quite difficult to have the exact estimates, so we do some heuristics to calculate this value. We divide the model’s predictions into bins based on the confidence score. Let’s say we defide all the space in $[0, 1]$ into $m$ bins. Then we say that the empirical accuracy score in the bin is $$ \\text{acc}(b) = \\frac{1}{N_{b}} \\sum_{i \\in b} \\mathbb{I}(\\hat{Y}_{i} = Y_{i}) $$ where $N_{b}$ is the number of samples in the bin and $\\mathbb{I}$ is the indicator function. Then we can calculate the expected calibration error as And the confidence score is $$ \\text{conf}(b) = \\frac{1}{N_{b}} \\sum_{i \\in b} \\hat{P}_{i} $$ Then we can calculate the expected calibration error (ECE) as $$ \\text{ECE} = \\sum_{b = 1}^{m} \\frac{N_{b}}{N} \\lvert \\text{acc}(b) - \\text{conf}(b) \\rvert $$ Sometimes we are also interested in the Maximum Calibration Error defined as $$ \\text{MCE} = \\max_{b} \\lvert \\text{acc}(b) - \\text{conf}(b) \\rvert $$ Reliability Diagrams 🟩 We group the predictions of the model into bins based on their confidence. Then we plot the average confidence score against the accuracy of the model in that bin. If the model is well calibrated, this plot should be a diagonal line.\nTechniques for improving calibration Temperature Scaling There are many techniques to improve calibration. One of the most used is temperature scaling. This is just a scaling of the logits of the model.\nPlatt Scaling Another technique is Platt scaling. This is a logistic regression on the model’s output logits, interpretable as confidence scores.\nIsotonic Regression TODO:\nReferences [1] Maddox et al. “A Simple Baseline for Bayesian Uncertainty in Deep Learning” 2019\n",
  "wordCount" : "2142",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/bayesian-neural-networks/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Bayesian neural networks
    </h1>
    <div class="post-meta">11 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#robbins-moro-algorithm" aria-label="Robbins-Moro Algorithm">Robbins-Moro Algorithm</a></li></ul>
                    
                <li>
                    <a href="#bayesian-neural-networks" aria-label="Bayesian Neural Networks">Bayesian Neural Networks</a><ul>
                        
                <li>
                    <a href="#the-idea" aria-label="The Idea">The Idea</a><ul>
                        
                <li>
                    <a href="#a-simple-model" aria-label="A simple model">A simple model</a></li></ul>
                </li>
                <li>
                    <a href="#map-estimate-for-bayesian-nn" aria-label="Map estimate for bayesian NN">Map estimate for bayesian NN</a><ul>
                        
                <li>
                    <a href="#the-classification-case" aria-label="The Classification Case:">The Classification Case:</a></li></ul>
                </li>
                <li>
                    <a href="#prediction-with-bayesian-neural-networks" aria-label="Prediction with Bayesian Neural Networks">Prediction with Bayesian Neural Networks</a><ul>
                        
                <li>
                    <a href="#approximating-with-gaussians-" aria-label="Approximating with Gaussians 🟥&#43;&#43;">Approximating with Gaussians 🟥++</a><ul>
                        
                <li>
                    <a href="#training" aria-label="Training">Training</a></li>
                <li>
                    <a href="#inference" aria-label="Inference">Inference</a></li>
                <li>
                    <a href="#unpacking-uncertainties" aria-label="Unpacking uncertainties">Unpacking uncertainties</a></li></ul>
                </li>
                <li>
                    <a href="#with-laplace-approximation" aria-label="With Laplace Approximation">With Laplace Approximation</a></li></ul>
                </li>
                <li>
                    <a href="#dropout-as-variational-inference" aria-label="Dropout as Variational Inference">Dropout as Variational Inference</a><ul>
                        
                <li>
                    <a href="#the-variational-family---" aria-label="The Variational Family 🟨&ndash;">The Variational Family 🟨&ndash;</a></li>
                <li>
                    <a href="#inference-with-dropout-" aria-label="Inference with Dropout 🟩">Inference with Dropout 🟩</a></li></ul>
                </li>
                <li>
                    <a href="#deep-ensembles" aria-label="Deep Ensembles">Deep Ensembles</a><ul>
                        
                <li>
                    <a href="#the-idea-1" aria-label="The Idea">The Idea</a></li>
                <li>
                    <a href="#inference-1" aria-label="Inference">Inference</a></li></ul>
                </li>
                <li>
                    <a href="#markov-chain-monte-carlo" aria-label="Markov Chain Monte Carlo">Markov Chain Monte Carlo</a><ul>
                        
                <li>
                    <a href="#the-mcmc-idea" aria-label="The MCMC idea">The MCMC idea</a></li>
                <li>
                    <a href="#key-challenges" aria-label="Key Challenges">Key Challenges</a></li>
                <li>
                    <a href="#running-averages" aria-label="Running averages">Running averages</a></li></ul>
                </li>
                <li>
                    <a href="#outlook-othe-rmethods" aria-label="Outlook: Othe rmethods">Outlook: Othe rmethods</a><ul>
                        
                <li>
                    <a href="#stein-variational-gradient-descent" aria-label="Stein Variational Gradient Descent">Stein Variational Gradient Descent</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#calibration" aria-label="Calibration">Calibration</a><ul>
                        <ul>
                        
                <li>
                    <a href="#what-is-calibration-" aria-label="What is Calibration? 🟩">What is Calibration? 🟩</a></li>
                <li>
                    <a href="#expected-calibration-error---" aria-label="Expected Calibration Error 🟩&ndash;">Expected Calibration Error 🟩&ndash;</a></li>
                <li>
                    <a href="#reliability-diagrams-" aria-label="Reliability Diagrams 🟩">Reliability Diagrams 🟩</a></li></ul>
                    
                <li>
                    <a href="#techniques-for-improving-calibration" aria-label="Techniques for improving calibration">Techniques for improving calibration</a><ul>
                        
                <li>
                    <a href="#temperature-scaling" aria-label="Temperature Scaling">Temperature Scaling</a></li>
                <li>
                    <a href="#platt-scaling" aria-label="Platt Scaling">Platt Scaling</a></li>
                <li>
                    <a href="#isotonic-regression" aria-label="Isotonic Regression">Isotonic Regression</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="robbins-moro-algorithm">Robbins-Moro Algorithm<a hidden class="anchor" aria-hidden="true" href="#robbins-moro-algorithm">#</a></h3>
<p>the algorithm is very simple we do
Until convergence: set some learning rates that satisfy the Robbins Moro Conditions, choose a $w_{0}$ then update in the following way:
</p>
$$
w_{n+1} = w_{n} - \alpha_{n} \Delta w_{n}
$$
<p>For example with $\alpha_{0} > \alpha_{1} > \dots > \alpha_{n} \dots$, and $\alpha_{t} = \frac{1}{t}$ they satisfy the condition (in practice we use a constant $\alpha$, but we lose the convergence guarantee by Robbins Moro).
More generally, the Robbins-Moro conditions re:</p>
<ol>
<li>$\sum_{n} \alpha_{n} = \infty$</li>
<li>$\sum_{n} \alpha_{n}^{2} < \infty$
Then the algorithm is guaranteed to converge to the best answer.</li>
</ol>
<p>One nice thing about this, is that we <strong>don&rsquo;t need gradients</strong>.
But often we use gradient versions (stochastic gradient descent and similar), using autograd, see <a href="/notes/backpropagation/">Backpropagation</a>.
But learning with gradients brings some drawbacks:</p>
<ol>
<li>We have no quantification of the uncertainty.</li>
<li>We are usually overconfident in our predictions (adversarial examples, and poor generalization to domain shifts).</li>
</ol>
<h2 id="bayesian-neural-networks">Bayesian Neural Networks<a hidden class="anchor" aria-hidden="true" href="#bayesian-neural-networks">#</a></h2>
<h3 id="the-idea">The Idea<a hidden class="anchor" aria-hidden="true" href="#the-idea">#</a></h3>
<p>The idea here is the same as the one applied to <a href="/notes/bayesian-neural-networks/">Bayesian neural networks</a>: define a prior of the weights and likelihood and define a posterior over the weights. But the nature of the model makes it quite difficult, if not impossible to have a closed-form solution for the posterior.</p>
<h4 id="a-simple-model">A simple model<a hidden class="anchor" aria-hidden="true" href="#a-simple-model">#</a></h4>
<p>We want to predict the weights of a given model with some bounds on certainty about them. One thing is that we have always used linear models for this, but we can also use neural networks or more complex models.
Usually, in some datasets also the variance changes with the input; this is called heteroskedastic noise.
This justifies models in the following form:</p>
$$
p(y \mid \boldsymbol{x}, \theta) = \mathcal{N}(u; f_{1}(\boldsymbol{x}, \theta), \exp(f_{2}(\boldsymbol{x}, \theta)))
$$
<p>
where $f_{1}$ and $f_{2}$ are neural networks and $\theta$ are the weights of the neural network.
We use neural networks to predict mean and variance. We use the $\exp$ function to ensure that the variance is always positive.</p>
<p>From a practical point of view, this machinery is quite good at estimating variance in regions where data is present (same thing in heteroskedastic settings!), while it is overconfident in zones without data, often collapsing the variance.</p>
<h3 id="map-estimate-for-bayesian-nn">Map estimate for bayesian NN<a hidden class="anchor" aria-hidden="true" href="#map-estimate-for-bayesian-nn">#</a></h3>
<p>Assume a prior $p(\theta) \sim \mathcal{N}(\theta; 0, \lambda^{-1})$ and the above parametrization of the likelihood function, let&rsquo;s derive the posterior distribution, that is $p(\theta \mid y, \boldsymbol{x})$.</p>
<p>We notice that while in the <a href="/notes/bayesian-linear-regression/">Bayesian Linear Regression</a> the normalization constant is disregarded, here it is dependent on the $\theta$, so we need to consider it. We can pay higher variance cost to minimize the error in the quadratic term.</p>
$$
\hat{\theta} = \arg\min_{\theta} - \underbrace{\log p(\theta)}_{\lambda \lVert \theta \rVert ^{2}_{2}} - \sum_{i = 1}^{n} \log p(y_{i} \mid x_{i}, \theta)
$$
<p>If we use the above model for Bayesian Modeling we derive the likelihood to be:
</p>
$$
\begin{align}
\log p(y_{i} \mid x_{i}, \theta)  &= \log \mathcal{N}(y_{i}; \mu(x_{i}, \theta), \sigma^{2}(x_{i}, \theta)))  \\
& = -\frac{1}{2} \log 2\pi \sigma^{2}(x_{i}, \theta) - \frac{1}{2\sigma^{2}(x_{i}, \theta)} (y_{i} - \mu(x_{i}, \theta))^{2} \\
& = -\frac{1}{2} \log 2\pi - \frac{1}{2} \log \sigma^{2}(x_{i}, \theta) - \frac{1}{2\sigma^{2}(x_{i}, \theta)} (y_{i} - \mu(x_{i}, \theta))^{2} \\
&= \underbrace{-\frac{1}{2} \log 2\pi}_{\text{const}} -\frac{1}{2}\left[ \log \sigma^{2}(x_{i}, \theta) + \frac{ (y_{i} - \mu(x_{i}, \theta))^{2} }{\sigma^{2}(x_{i}, \theta)}\right]
\end{align}
$$
<p>
Here you observe that the likelihood is maximized by having high variance (plus penalty) or having correct estimates.
Then adding the prior is just a regularizing term that corresponds to weight decay (a good exercise is to prove this part).</p>
<h4 id="the-classification-case">The Classification Case:<a hidden class="anchor" aria-hidden="true" href="#the-classification-case">#</a></h4>
<p>TODO: minute 22 of part 2.</p>
<h3 id="prediction-with-bayesian-neural-networks">Prediction with Bayesian Neural Networks<a hidden class="anchor" aria-hidden="true" href="#prediction-with-bayesian-neural-networks">#</a></h3>
<p>We see that prediction is just averaging over the predictions of different networks sampled from the approximate distribution that we derived from the variational inference of the weights. Some formalization could help in clearing this out.</p>
<p>$$
\begin{align}
p(y^{<em>} \mid x^{</em>}, x_{1:n}, y_{1:n}) &amp; = \int p(y^{<em>} \mid x^{</em>}, \theta) p(\theta \mid x_{1:n}, y_{1:n}) d\theta \
&amp;  = \mathbb{E}<em>{\theta \sim p(\cdot \mid x</em>{1:n}, y_{1:n})} [p(y^{<em>} \mid x^{</em>}, \theta)]) \
\text{Using Variational Inf.}&amp; \approx \mathbb{E}<em>{</em>{\theta \sim q(\theta)}} [p(y^{<em>} \mid x^{</em>}, \theta)] \</p>
<p>\text{Monte Carlo }&amp;  \approx \frac{1}{S} \sum_{s = 1}^{S} p(y^{<em>} \mid x^{</em>}, \theta_{s})
\end{align}
$$
The variational is usually the approximation that is introducing the most errors.</p>
<h4 id="approximating-with-gaussians-">Approximating with Gaussians 🟥++<a hidden class="anchor" aria-hidden="true" href="#approximating-with-gaussians-">#</a></h4>
<h5 id="training">Training<a hidden class="anchor" aria-hidden="true" href="#training">#</a></h5>
<p>Using the variational method, usually we parametrize with Gaussians. See <a href="/notes/variational-inference/">Variational Inference</a>. We usually use the ELBO to minimize the difference between the variational approximation and the true distribution.</p>
$$
\log p(D) \geq \max_{q \in \mathcal{Q}} ELBO(q) = \max_{q \in \mathcal{Q}} \mathbb{E}_{q(\theta)} [\log p(D, \theta) - \log q(\theta)]
$$
<p>Assuming a Gaussian prior, we can derive the ELBO as:
</p>
$$
\begin{align}
ELBO(q) &= \mathbb{E}_{q(\theta)} [\log p(D, \theta) - \log q(\theta)]  \\
&= \mathbb{E}_{\varepsilon \sim \mathcal{N}(0, 1)} [\log p(D, \theta + \sigma \varepsilon)]  + KL(q \mid p) \\
& = \mathbb{E}_{\varepsilon \sim \mathcal{N}(0, 1)} [\log p(D, \theta + \sigma \varepsilon)] + \frac{1}{2}\log \lvert \Sigma \rvert  + \frac{D}{2} \log 2\pi e
\end{align}
$$
<p>This is known as Bayes by backprop (2015), and is parameterized with a diagonal covariance Gaussian.<br>
This is used to learn the covariance matrix and mean of the variational family. After, we can do inference in the following manner:</p>
<h5 id="inference">Inference<a hidden class="anchor" aria-hidden="true" href="#inference">#</a></h5>
$$
\begin{align}
p(y^{*} \mid x^{*}, x_{1:n}, y_{1:n})&= \int p(y^{*} \mid x^{*}, \theta) p(\theta \mid x_{1:n}, y_{1:n}) d\theta  \\
&\approx \int p(y^{*} \mid x^{*}, \theta) q(\theta) d\theta \\
&\approx \frac{1}{S} \sum_{s = 1}^{S} p(y^{*} \mid x^{*}, \theta_{s}) \\
&= \frac{1}{S} \sum_{s = 1}^{S} \mathcal{N}(y^{*}; f(x^{*}, \theta_{s}), \exp(g(x^{*}, \theta_{s})))
\end{align}
$$
<blockquote>
<p>Intuitively, variational inference in Bayesian neural networks can be
interpreted as averaging the predictions of multiple neural networks
drawn according to the variational posterior $q$</p>
</blockquote>
<h5 id="unpacking-uncertainties">Unpacking uncertainties<a hidden class="anchor" aria-hidden="true" href="#unpacking-uncertainties">#</a></h5>
<p>We can un pack that uncertainty of the likelihood prediction using the classical aleatoric and epistemic uncertainties:
</p>
$$
\begin{align}
\text{Var}[y^{*} \mid x^{*}, x_{1:n}, y_{1:n}] &= \underbrace{\mathbb{E}_{q(\theta)}[\text{Var}[y^{*} \mid x^{*}, \theta]]}_{\text{Aleatoric}} + \underbrace{\text{Var}_{q(\theta)}[\mathbb{E}[y^{*} \mid x^{*}, \theta]]
}_{\text{Epistemic}} \\
&\approx \frac{1}{m} \sum_{s = 1}^{m} \exp(g(x^{*}, \theta_{s})) + \frac{1}{m - 1} \sum_{s = 1}^{m} (\mu(x^{*}, \theta_{s}) - \bar{\mu})^{2}
\end{align}
$$
<p>
Where $\bar{\mu} = \frac{1}{m} \sum_{s = 1}^{m} \mu(x^{*}, \theta_{s})$.</p>
<h4 id="with-laplace-approximation">With Laplace Approximation<a hidden class="anchor" aria-hidden="true" href="#with-laplace-approximation">#</a></h4>
<p>We need to model the joint density as
</p>
$$
\log p(\mathcal{D}, \theta) \approx \log p (\mathcal{D}, \hat{\theta}) + \frac{1}{2} (\theta - \hat{\theta})^{T} H(\theta - \hat{\theta})
$$
<p>
If we marginalize over the parameters we get
</p>
$$
p(\mathcal{D}) = p(\mathcal{D}, \hat{\theta}) (2\pi)^{D/2} \lvert -H_{\theta} \rvert ^{-1/2}
$$
<p>The problem is often the Hessian, which could be unstable, or difficult to approximate. For this reason usually we use low-rank matrices, diagonal matrices, or other approximations.</p>
<h3 id="dropout-as-variational-inference">Dropout as Variational Inference<a hidden class="anchor" aria-hidden="true" href="#dropout-as-variational-inference">#</a></h3>
<p>Dropout and dropconnect can be seen as a <em>special case</em> of variational inference. It can been seen as if we are working with a probability distribution of different networks, which can be recalled as a special case of the one we have discussed above.</p>
<h4 id="the-variational-family---">The Variational Family 🟨&ndash;<a hidden class="anchor" aria-hidden="true" href="#the-variational-family---">#</a></h4>
<p>With dropout, instead of working with a single network, we are working with many probabilistic networks.
It can be interpreted as doing variational inference with the following variational family:
</p>
$$
q(\theta \mid \lambda) = \prod_{j} q_{j}(\theta_{j} \mid \lambda_{j})
$$
<p>
Where $q_{j}(\theta \mid \lambda)) = p\delta_{0}(\theta_{j}) + (1 - p) \delta_{\lambda_{j}}(\theta_{j})$ a mixture of Dirac deltas: with a certain probability $1- p$ we keep the weight, or set it to zero.
Optimizing the network with dropout can be seen as optimizing the ELBO with respect of this variational family.</p>
<h4 id="inference-with-dropout-">Inference with Dropout 🟩<a hidden class="anchor" aria-hidden="true" href="#inference-with-dropout-">#</a></h4>
<p>One thing is using dropout during prediction to obtain uncertainty estimates. This is called <strong>Monte Carlo Dropout</strong>. The idea is very simple: keep the dropout during inference and sample from the network multiple times. The average of the predictions is the final prediction, and the variance is the uncertainty estimate.</p>
<h3 id="deep-ensembles">Deep Ensembles<a hidden class="anchor" aria-hidden="true" href="#deep-ensembles">#</a></h3>
<h4 id="the-idea-1">The Idea<a hidden class="anchor" aria-hidden="true" href="#the-idea-1">#</a></h4>
<p>We train $m$ different models from a bootstrapped dataset and then just average their prediction. This is the main idea of doing Ensembles.</p>
<p>We can also extend the probabilistic case to classification. In this case we just add a noise in the softmax layer to have some variability.</p>
<h4 id="inference-1">Inference<a hidden class="anchor" aria-hidden="true" href="#inference-1">#</a></h4>
<p>With this method we train different models on different data (maybe <em>bootstrapped</em> data, see <a href="/notes/cross-validation-and-model-selection/">Cross Validation and Model Selection</a>).
Then we just average the predictions of the different models. This is a very simple way to obtain uncertainty estimates, which allows to approximate the bayesian inference method.</p>
$$
\begin{align}
p(y^{*} \mid x^{*}, x_{1:n}, y_{1:n}) &= \int p(y^{*} \mid x^{*}, \theta) p(\theta \mid x_{1:n}, y_{1:n}) d\theta  \\
&= \mathop{\mathbb{E}}_{\theta \sim p(\cdot \mid x_{1:n}, y_{1:n})} [p(y^{*} \mid x^{*}, \theta)]  \\
&\approx \frac{1}{m} \sum_{s = 1}^{m} p(y^{*} \mid x^{*}, \theta_{s})
\end{align}
$$
<h3 id="markov-chain-monte-carlo">Markov Chain Monte Carlo<a hidden class="anchor" aria-hidden="true" href="#markov-chain-monte-carlo">#</a></h3>
<h4 id="the-mcmc-idea">The MCMC idea<a hidden class="anchor" aria-hidden="true" href="#the-mcmc-idea">#</a></h4>
<p>We would like here to sample from the posterior distribution of the weights using MCMC (somewhat similar to the SGLD method), and then use the samples to make predictions. This is a very general method, but it&rsquo;s computationally expensive.
These methods explained in <a href="/notes/monte-carlo-methods/">Monte Carlo Methods</a> are exactly the same here, you can use these to sample from the posterior and then use the same tricks in <a href="/notes/bayesian-neural-networks/#inference">#Inference</a> (remember the Ergodic theorem) and <a href="/notes/bayesian-neural-networks/#unpacking-uncertainties">#Unpacking uncertainties</a>.</p>
<h4 id="key-challenges">Key Challenges<a hidden class="anchor" aria-hidden="true" href="#key-challenges">#</a></h4>
<p>There are two main challenges with these methods:</p>
<ol>
<li>Storing the weights (usually to expensive).
<ol>
<li>One way to circumvent this problem is to keep some <em>snapshots</em> or keeping some running averages (see next section).</li>
<li>One idea is <strong>subsampling</strong>: we store some intermediate weights or predictions and use those for the final inference.</li>
</ol>
</li>
<li>Burn in period is too costly.</li>
</ol>
<h4 id="running-averages">Running averages<a hidden class="anchor" aria-hidden="true" href="#running-averages">#</a></h4>
<p>Another idea is assuming a distribution over the weights (e.g. a Gaussian) and keeping the mean and variance of the weights. Then we can sample from this distribution.
This is just the idea of SWA-Gaussians, presented <a href="http://arxiv.org/abs/1902.02476">(Maddox et al. 2019)</a>.
This is a way not to store all the weights.
</p>
$$
\mu \leftarrow \frac{1}{T + 1} (T \mu + \theta)
$$
<p>
And:
</p>
$$
\sigma^{2} \leftarrow \frac{1}{T + 1} (T \sigma^{2} + \theta^{T}\theta)
$$
<h3 id="outlook-othe-rmethods">Outlook: Othe rmethods<a hidden class="anchor" aria-hidden="true" href="#outlook-othe-rmethods">#</a></h3>
<h4 id="stein-variational-gradient-descent">Stein Variational Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#stein-variational-gradient-descent">#</a></h4>
<p>This is called (SVGD) by Liu and Wang 2016.
We want to approximate the posterior distribution with a set of particles and minimize the KL divergence between the true posterior and the approximated one.
We consider an ensemble of models $\left\{ \theta_{i} \right\}^{n}_{i = 1}$ and we update them with the following update rule:
</p>
$$
\theta_{i} \leftarrow \theta_{i} + \frac{\epsilon}{n} \sum_{j = 1}^{n} k(\theta_{i}, \theta_{j}) (\nabla_{\theta_{j}} \log p(\theta_{j} \mid \mathcal{D}) - \nabla_{\theta_{j}}k(\theta_{j}, \theta_{i}))
$$
<p>
I have no idea why we use this update rule, but it should be minimizing a certain divergence between the two distributions. TODO: understand this part.</p>
<h2 id="calibration">Calibration<a hidden class="anchor" aria-hidden="true" href="#calibration">#</a></h2>
<h4 id="what-is-calibration-">What is Calibration? 🟩<a hidden class="anchor" aria-hidden="true" href="#what-is-calibration-">#</a></h4>
<p>Well calibrated models have a <em>similar accuracy to the confidence</em> of their predictions. This is a very important property for many applications, especially in medical fields.
For example, if a model is well calibrated, if it is secure on a prediction, it can leave the burden from the doctors, who would just manually check the hard ones.</p>
<h4 id="expected-calibration-error---">Expected Calibration Error 🟩&ndash;<a hidden class="anchor" aria-hidden="true" href="#expected-calibration-error---">#</a></h4>
<p>A well calibrated model&rsquo;s accuracy is the same as the probability, interpretable as a confidence score, of its prediction.
Mathematically we say that a model is calibrated if
</p>
$$
\mathbb{E}_{p \sim \hat{P}} [\lvert \mathbb{P}(\hat{Y} = Y \mid \hat{P} = p) - p \rvert ] = 0
$$
<p>
With $\hat{Y}$ the predictions of the model and $\hat{P}$ its confidences and $Y$ is the true label.
So we would like to have
</p>
$$
\mathbb{P}(\hat{Y} = y \mid \hat{P} = p) = p
$$
<p>Usually it&rsquo;s quite difficult to have the exact estimates, so we do some heuristics to calculate this value.
We divide the model&rsquo;s predictions into bins based on the confidence score. Let&rsquo;s say we defide all the space in $[0, 1]$ into $m$ bins. Then we say that the empirical accuracy score in the bin is
</p>
$$
\text{acc}(b) = \frac{1}{N_{b}} \sum_{i \in b} \mathbb{I}(\hat{Y}_{i} = Y_{i})
$$
<p>
where $N_{b}$ is the number of samples in the bin and $\mathbb{I}$ is the indicator function.
Then we can calculate the expected calibration error as
And the confidence score is
</p>
$$
\text{conf}(b) = \frac{1}{N_{b}} \sum_{i \in b} \hat{P}_{i}
$$
<p>
Then we can calculate the expected calibration error (<strong>ECE</strong>) as
</p>
$$
\text{ECE} = \sum_{b = 1}^{m} \frac{N_{b}}{N} \lvert \text{acc}(b) - \text{conf}(b) \rvert
$$
<p>Sometimes we are also interested in the <strong>Maximum Calibration Error</strong> defined as
</p>
$$
\text{MCE} = \max_{b} \lvert \text{acc}(b) - \text{conf}(b) \rvert
$$
<h4 id="reliability-diagrams-">Reliability Diagrams 🟩<a hidden class="anchor" aria-hidden="true" href="#reliability-diagrams-">#</a></h4>
<p>We group the predictions of the model into bins based on their confidence. Then we plot the average confidence score against the accuracy of the model in that bin. If the model is well calibrated, this plot should be a diagonal line.</p>
<p><img loading="lazy" src="/notes/bayesian-neural-networks-20241218213832047.webp" alt="Example from a paper"  />
</p>
<h3 id="techniques-for-improving-calibration">Techniques for improving calibration<a hidden class="anchor" aria-hidden="true" href="#techniques-for-improving-calibration">#</a></h3>
<h4 id="temperature-scaling">Temperature Scaling<a hidden class="anchor" aria-hidden="true" href="#temperature-scaling">#</a></h4>
<p>There are many techniques to improve calibration. One of the most used is <strong>temperature scaling</strong>. This is just a scaling of the logits of the model.</p>
<h4 id="platt-scaling">Platt Scaling<a hidden class="anchor" aria-hidden="true" href="#platt-scaling">#</a></h4>
<p>Another technique is <strong>Platt scaling</strong>. This is a logistic regression on the model&rsquo;s output logits, interpretable as confidence scores.</p>
<h4 id="isotonic-regression">Isotonic Regression<a hidden class="anchor" aria-hidden="true" href="#isotonic-regression">#</a></h4>
<p>TODO:</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Maddox et al. <a href="http://arxiv.org/abs/1902.02476">“A Simple Baseline for Bayesian Uncertainty in Deep Learning”</a>  2019</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/machinelearning/">Machinelearning</a></li>
      <li><a href="https://flecart.github.io/tags/probabilistic-artificial-intelligence/">➕Probabilistic-Artificial-Intelligence</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian neural networks on x"
            href="https://x.com/intent/tweet/?text=Bayesian%20neural%20networks&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-neural-networks%2f&amp;hashtags=machinelearning%2c%e2%9e%95probabilistic-artificial-intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian neural networks on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-neural-networks%2f&amp;title=Bayesian%20neural%20networks&amp;summary=Bayesian%20neural%20networks&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian neural networks on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-neural-networks%2f&title=Bayesian%20neural%20networks">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian neural networks on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian neural networks on whatsapp"
            href="https://api.whatsapp.com/send?text=Bayesian%20neural%20networks%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian neural networks on telegram"
            href="https://telegram.me/share/url?text=Bayesian%20neural%20networks&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-neural-networks%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian neural networks on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Bayesian%20neural%20networks&u=https%3a%2f%2fflecart.github.io%2fnotes%2fbayesian-neural-networks%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
