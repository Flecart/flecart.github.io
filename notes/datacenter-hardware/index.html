<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Datacenter Hardware | X. Angelo Huang&#39;s Blog</title>
<meta name="keywords" content="☁cloud-computing">
<meta name="description" content="We want to optimize the parts of the datacenter hardware such that the cost of operating the datacenter as a whole would be lower, we need to think about it as a whole.
Datacenter CPUs
Desktop CPU vs Cloud CPU

Isolation:

Desktop CPUs have low isolation, they are used by a single user.
Cloud CPUs have high isolation, they are shared among different users.


Workload and performance: usually high workloads and moving a lot of data around.

They have a spectrum of low and high end cores, so that if you have high parallelism you can use lower cores, while for resource intensive tasks, its better to have high end cores, especially for latency critical tasks.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="https://flecart.github.io/notes/datacenter-hardware/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://flecart.github.io/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://flecart.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://flecart.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://flecart.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://flecart.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://flecart.github.io/notes/datacenter-hardware/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
<script type="text/javascript" async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>






      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WW6NN2QGKF');
        }
      </script><meta property="og:url" content="https://flecart.github.io/notes/datacenter-hardware/">
  <meta property="og:site_name" content="X. Angelo Huang&#39;s Blog">
  <meta property="og:title" content="Datacenter Hardware">
  <meta property="og:description" content="We want to optimize the parts of the datacenter hardware such that the cost of operating the datacenter as a whole would be lower, we need to think about it as a whole.
Datacenter CPUs Desktop CPU vs Cloud CPU Isolation: Desktop CPUs have low isolation, they are used by a single user. Cloud CPUs have high isolation, they are shared among different users. Workload and performance: usually high workloads and moving a lot of data around. They have a spectrum of low and high end cores, so that if you have high parallelism you can use lower cores, while for resource intensive tasks, its better to have high end cores, especially for latency critical tasks.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:tag" content="☁Cloud-Computing">
      <meta property="og:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flecart.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="Datacenter Hardware">
<meta name="twitter:description" content="We want to optimize the parts of the datacenter hardware such that the cost of operating the datacenter as a whole would be lower, we need to think about it as a whole.
Datacenter CPUs
Desktop CPU vs Cloud CPU

Isolation:

Desktop CPUs have low isolation, they are used by a single user.
Cloud CPUs have high isolation, they are shared among different users.


Workload and performance: usually high workloads and moving a lot of data around.

They have a spectrum of low and high end cores, so that if you have high parallelism you can use lower cores, while for resource intensive tasks, its better to have high end cores, especially for latency critical tasks.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://flecart.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Datacenter Hardware",
      "item": "https://flecart.github.io/notes/datacenter-hardware/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Datacenter Hardware",
  "name": "Datacenter Hardware",
  "description": "We want to optimize the parts of the datacenter hardware such that the cost of operating the datacenter as a whole would be lower, we need to think about it as a whole.\nDatacenter CPUs Desktop CPU vs Cloud CPU Isolation: Desktop CPUs have low isolation, they are used by a single user. Cloud CPUs have high isolation, they are shared among different users. Workload and performance: usually high workloads and moving a lot of data around. They have a spectrum of low and high end cores, so that if you have high parallelism you can use lower cores, while for resource intensive tasks, its better to have high end cores, especially for latency critical tasks.\n",
  "keywords": [
    "☁cloud-computing"
  ],
  "articleBody": "We want to optimize the parts of the datacenter hardware such that the cost of operating the datacenter as a whole would be lower, we need to think about it as a whole.\nDatacenter CPUs Desktop CPU vs Cloud CPU Isolation: Desktop CPUs have low isolation, they are used by a single user. Cloud CPUs have high isolation, they are shared among different users. Workload and performance: usually high workloads and moving a lot of data around. They have a spectrum of low and high end cores, so that if you have high parallelism you can use lower cores, while for resource intensive tasks, its better to have high end cores, especially for latency critical tasks.\nDatacenters have developed own chips for custom software too. This is ok to specialize on their own workloads and get better on that market.\nAlso NVIDIA is designing a CPU, so that they can couple them together and co-optimize it (increases 30x memory movement, Grace Hopper architecture CPU linked with GPU). Alps cluster at ETH have 10k of those GPUs.\nThese are some examples of CPUs designed lately:\nAWS Graviton Arm CPU Microsoft Cobalt Arm CPU Google Axion Arm CPU Two socket datacenter server These are some of the design considerations that one need to take into account to design a chip more specific for cloud environments: - Type of core: “brawny” or “wimpy” - Meaning better more energy hungry but more efficient cores, or less energy hungry but less efficient cores, but in more quantity? - Number of cores - Number of sockets and NUMA topology - Cache hierarchy and size - Isolation mechanisms for multi-tenancy (for performance \u0026 security) - Integration with hardware accelerators, network devices, etc. - Power management. Usually these cores are brawny (performance) cores. Nowadays cores have dynamic voltage frequency scaling that allows frequency scaling per core.\nNUMA Technology NUMA stands for Non-Uniform Memory Access. It’s a memory design used in multi-socket and some multi-core systems where each CPU (or group of cores) has its own local memory and shared access to remote memory.\nIn simpler terms: Not all memory in the system is equally fast to access from every CPU core.\nThe above image on two core datacenter server is an example of a NUMA architecture, where each socket has its own memory and can access the other socket memory with a higher latency.\nPerformance with Cluster size and Communication The number of cores to allocate to a single application should depend on the type of application that is running, more communications means bigger overheads.\nThe basic takeaway is that you can optimize well if the program does not need to communicate a lot, and sometimes it is huge, the two socket architecture is considered the sweet spot for both performance and cost.\nAmdahl’s Law This law defines the speed up that you can achieve after you parallelize the code. Usually it is defined as:\n$$ \\text{Speedup} = \\frac{\\text{CPUTimeOLD}}{\\text{CPUTimeNEW}} = \\frac{1}{(1 - P) + \\frac{P}{N}} $$ Where $P$ is the parallelizable part of the code, and $N$ is the number of processors.\nGustafson’s Law Problem size scales with number of processors (constant work per processor). This means: as we add more processors in the system, the compute needed to solve the same problem is more, we can see this as the reverse side of the Amdahl’s law, since perhaps some compute is for communication. But it is more intended about solving larger problems at the same time.\nIntel Cache allocation technology Instead of every core/application having free rein over the full L3 cache, CAT lets you assign specific portions of that cache to specific workloads.\nThis is the main, idea, in this manner we don’t have many conflict evictions due to this part.\nEnables partitioning the ways of a highly-associative last-level caching into several subsets with smaller associativity. Cores assigned to one subset can only allocate cache lines in their subset on refills, but are allowed to hit in any part of the last-level cache (LLC). It is usually implemented using bitmasks on the associativity of the cache, see Cache Optimization. This enables higher priority applications running on some LLC to get more cache size for allocation during runtime. Real-time systems or latency-sensitive apps (like video streaming, trading, or telco) can’t tolerate unpredictable cache behavior.\nOther technologies like Dynamic Voltage Frequency Scaling (DVFS) enables core-independent scaling. And there is no bandwidth isolation.\nCPU Types and Power Management As we saw when studying the assignment related to Skylake Microprocessor, there are efficient and perfromance cores, in this context called wimpy and brawny cores. The former is more energy efficient (2x) but much slower 5x, while the latter is faster, yet because of Ahmdal’s law, it has much performance variability.\nThere are p-states and c-states:\np-states: power consumption when the system is executing code c-states: power consumption levels when the system is idle. Different cores have different consumption, we can control the performance by controlling frequency and power.\nTowards Hardware specialization CPU have low latency but also low memory throughput, and they are general purpose (instruction fetching, data fetching, execution storage etc.) GPU have high latency but high memory throughput, good for memory-bound computations. These GPUs have a high overhead in moving data, so when you have computations where you need to access data that is not sequential in memory, it is probably difficult to have a benefit from them. The case above is not true for ML workloads, where the main bottleneck was the memory for the matrix multiplications, see Cache Optimization. Google has been developing his own GPU, called TPU, they predicted high workload for inference, and wanted to be less dependent from hardware companies. The specialization trend is for everything (NIC, memory, storage, networking, more and more processors are seen to be developed in the next years), there are lots of money flowing to unique accelerators. Currently a server lasts about 6 years in a datacenter (average lifetime) (meeting performance requirements and hardware reliability). Other examples are NVIDIA GB300 NVL72, or Project Ceiba from Microsoft, which is a custom chip for AI workloads, they are trying to go more for scale up now instead of scale out.\nCompute Express Link CXL is an open standard cache-coherent interconnect for processors, memory expansion, and accelerators. It introduces a new level in the memory hierarchy.\nMaintains coherency between CPU memory and memory on attached devices. Enables resource sharing (or pooling) for higher performance, reduces software stack complexity, and lowers overall system cost. Why is it needed? Modern workloads have high memory capacity requirements. Spilling to SSD is too slow. Thus, need to make remote memory access fast! Accelerators are increasingly common and have their own memory need high-bandwidth access between CPU and devices, this is why a new protocol is needed. See Compute Express Link for other info about this section.\nFPGAs For example the TCP stack is using lots of cycles, you could program a chip just optimized for these kinds of operations, FPGAs are good with those specialized kinds of computations where neither CPU and GPUs are good at:\nNetwork operations Data parallelism with irregular accesses. Operations that are not float or int operations (filtering packets in network settings for example). Encryption operations Basically everything that could benefit offloading computation to other parts, when the CPU is still busy, but putting a GPU or TPU would be overkill. The tooling here in the chip design takes a lot of time. One example of a technology that uses FPGAs in a similar manner is AQUA, pushing computational resources for reduction and aggregation closer to the data (reducing network traffic), layer on top of S3 for Redshift.\nImage from course slides CCA 2025 ETHz | FPGA can be used to offload many common tasks from CPUs, (e.g. network encryption, serialization and similars), this makes the NIC quite faster, using custom smartNIC based on FPGA, see (Firestone et al. 2018)\nWe show that FPGAs are the best current platform for offloading our networking stack as ASICs do not provide sufficient programmability, and embedded CPU cores do not provide scalable performance, especially on single network flows.\nDatacenter Structure The whole datacenter is designed to minimize the ownership cost of the system. This section is anyways mostly just introductory.\nElectrical Systems To be done.\nCooling Systems Heating is a huge issue inside datacenter systems. We have many many systems up to handle a good cooling behaviour within the systems. some example of how datacenters are organized to favor better cooling\nAnalyzing Bottlenecks Profiling a warehouse scale computer These are mainly results from the paper (Kanev et al. 2015).\nwe show significant diversity in workload behavior with no single “silver-bullet” application to optimize for and with no major intra-application hotspots.\nOne main observation is that the workloads are diverse, and you cannot just optimize for a single kind of workload. They basically assert that there is not single bottleneck for performance of the Warehouse scale computer.\nTop 50 binaries cover only 60% of the total execution time, also in modern AI jobs, the statistics should be similar. The data-center tax There is a data-center tax associated with moving things around. and communication.\nImage from (Kanev et al. 2015)| The data-center tax is the cost of moving data around, this is the main bottleneck in datacenter performance.\nTypes of stalls We define three types of values:\nRetiring is considering useful work, the rest are sources of overhead. Front-end stall: captures all overheads associated with fetching \u0026 decoding instructions The instruction set needed is 100x the L1 instruction cache (and this is growing 20% a year), but it is impractical to grow the cache size so much. We can try to prefetch the part of the data that matters in the L1 cache, but not sure how is this possible. -\u003e What AsmDB does, when it recognizes common instruction sequences. Introduce custom instructions + 5% better instructions per cycle. Back-end stall: captures overheads due to data cache hierarchy and lack of instruction-level parallelism (see Skylake Microprocessor for more information about this parallelism). Most of the overhead are from back-end stalls.\nAsmDB From the paper (Ayers et al. 2019), collects data about block-level instruction usage into an assembly database, so that:\nYou know what instructions are used, so perhaps you can manually optimize for it. benchmarks for better compilers in i-cache design (software prefetch instructions). Data source for compiler driven improvements of the binaries. Gets 5% better instruction per cycle, saving millions of dollars. Imbalanced resource usages The resource usage in the datacenter are basically independent with each other, it is quite hard to predict a precise ration between CPU and memory that is needed to optimize the resources for a specific data center. By disaggregating resources, we are able to scale them independently.\nIdea of a solution: Dynamically allocate the ratio of resources an application needs by using compute \u0026 storage components connected over a network\nThere is something similar of storing Graph Databases in disaggregated manners in the disk, but in a wholly different field.\nThe Storage Hierarchy We need to expand the classical single server memory architecture (see Memoria) to datacenter level. Now we have rack memory and datacenter memory, and the idea that higher latency with higher memory possibility is still valid here.\nIntroduction to Memory Types Access Speeds Usually now the bottleneck is the network. One observation is that the remote latency of the disk is similar to the local disk latency (disk seek is the bottleneck, wire transfer is faster). One interesting observation is that rack memory and cluster memory seeks are faster than local disks and they also have higher bandwidth!\nConfiguration Component Capacity Latency Speed ONE SERVER DRAM 256GB 100ns 150GB/s ONE SERVER DISK 80TB 10ms 800MB/s ONE SERVER FLASH 4TB 100us 3GB/s LOCAL RACK (40 SERVERS) DRAM 10TB 20us 5GB/s LOCAL RACK (40 SERVERS) DISK 3.2PB 10ms 5GB/s LOCAL RACK (40 SERVERS) FLASH 160TB 120us 5GB/s CLUSTER (125 RACKS) DRAM 1.28PB 50us 1.2GB/s CLUSTER (125 RACKS) DISK 400PB 10ms 1.2GB/s CLUSTER (125 RACKS) FLASH 20PB 150us 1.2GB/s From Jeff Dean Tables:\nOperation Time L1 cache reference 1.5 ns L2 cache reference 5 ns Branch misprediction 6 ns Uncontended mutex lock/unlock 20 ns L3 cache reference 25 ns Main memory reference 100 ns Decompress 1 KB with Snappy [Sna] 500 ns “Far memory”/Fast NVM reference 1,000 ns (1 us) Compress 1 KB with Snappy [Sna] 2,000 ns (2 us) Read 1 MB sequentially from memory 12,000 ns (12 us) SSD Random Read 100,000 ns (100 us) Read 1 MB bytes sequentially from SSD 500,000 ns (500 us) Read 1 MB sequentially from 10Gbps network 1,000,000 ns (1 ms) Read 1 MB sequentially from disk 10,000,000 ns (10 ms) Disk seek 10,000,000 ns (10 ms) Send packet California-\u003eNetherlands-\u003eCalifornia 150,000,000 ns (150 ms) Types of Memories SRAM: Static RAM, used for cache. Fast and expensive (cost per bit). We need six transistors for a SRAM Amplifier to read quickly. No worries about refreshes. DRAM: Dynamic RAM, used for main memory. Slower and cheaper. Stores one bit with one transistor (a special one) Much higher density compared to SRAM Flash storage: Used for SSDs. Slower than DRAM but faster than traditional HDDs. Rough comparisons for DRAM : Flash : Disk\nCost per bit: 100 : 10 : 1 Access latency: 1 : 5,000 : 1,000,000 DRAM: Memory-Access Protocol As we studied in Circuiti Sequenziali when developing a small memory, we need a system to index into the memory and retrieve its content. This is done by a memory controller, which implements the memory-access protocol with its row decoders and similars.\nDRAM Operations DRAM has an access protocol that allows for some commands:\nACTIVATE (open a row) It opens the whole row and loads content into row buffer, after a nanosecond delay (called Row to Column Delay) Usually one row is 4k to 32k of data, DDR4 has 8K rows each with 8KB READ (read a column) It reads the specific Column, but needs to destroy that data after some amplification, the data needs to be restored. The number of banks is around 16 to 32. per chip, and you usually have 8 to 16 DRAM chips, so you can read about 4 megabyte just in one cycle if its always in the banks. WRITE WRITING trashes the row buffer temporarily and forces a restore cycle later. PRECHARGE (close row) Resets the bank to a neutral state so that another row can be read. REFRESH Every 64ms you need a refresh to maintain the memory, when a refresh is occurring the DRAM cannot do other things. Row policies And we also have row policies (open rows use energy), and you can also optimize for the row opening prediction. (open-row policy, and closed-row policy), or you can also use speculative row access policies.\nOpen Row:\nKeep the row open until it is needed for another, only then you can precharge This saves latency if you know that row will be used many times. Closed Row\nClose the row as soon as you are done with it, unless there is some request that is already going for that row. Helps to avoid conflict for changing rows. Speculative version Attempt to predict whether you need to close or open that row.\nFlash Storage This looks like a good introductory source.\nThis is for non-volatile memory, SSDs. They use specific transistors to control the charge on the gates and then understand if it is one or zero. Lower charge means 1. You can use different levels to directly encode 2 bits instead of one.\nCharge modulates the threshold voltage of the underlying transistor You can also store more than one bit in one transistor, this is the multi-level cell. Working Principles Floating-Gate Transistor: At the heart of a flash memory cell is a MOSFET (Metal-Oxide-Semiconductor Field-Effect Transistor) with the following structure:\nControl Gate (CG): This gate is used to control the flow of current through the transistor’s channel, similar to a regular MOSFET. Floating Gate (FG): This gate is electrically isolated by an insulating layer (typically silicon dioxide) surrounding it. The floating gate’s charge determines the threshold voltage of the transistor, which in turn dictates whether the cell represents a binary ‘0’ or ‘1’. Data Storage Mechanism:\nWriting (Programming): To store data, a voltage is applied to the control gate. This causes electrons to tunnel through the insulating layer and become trapped on the floating gate. The presence of these electrons increases the threshold voltage of the transistor. This state typically represents a binary ‘0’. Reading: When reading data, a voltage is applied to the control gate. The current flow through the transistor’s channel is then sensed. If the threshold voltage is low (no or few electrons on the floating gate), current flows, representing a binary ‘1’. If the threshold voltage is high (electrons on the floating gate), current does not flow, representing a binary ‘0’. Erasing: To erase data, a higher voltage of the opposite polarity is applied, typically to the source terminal. This forces the trapped electrons off the floating gate through quantum tunneling, returning the cell to its original, unprogrammed state (representing a binary ‘1’). In NAND flash, erasure typically happens in larger blocks of cells, while NOR flash allows for block or chip erasure Flash Storage Types SLC single-level cell Faster but less dense More reliable (100K -1M erase cycles) ~$1.5/GB Used in “enterprise” drives (i.e. Intel Extreme SSDs) Good for write intensive workflows. MLC: multi-level cell Slower but denser Less reliable (1K - 10K erase cycles, becomes a problem with write intensive workloads) $0.08/GB (two orders of magnitude cheaper!) Used in consumer drives (thumb drives, cheap SSDs, etc.) eMLC for enterprise use designed for lower error rates Internal Architecture of Flash Storage Flash storages have controllers, connected to blocks of data, these blocks (blocks contain pages) need to be erased at the same time, while pages are the minimum unit of read/write. Usually we have:\n512bytes or 8kb per page 64-256 pages per block About 1 to 16 chips, capacity of 1 to 16. you see that one block holds about 512MB to 2GB. Flash Operations Read the contents of a page 20-80us (reading operation is much much faster!) One problem is that each chip can process one operation per unit time. This creates sorts of interference (quite big interference!). Write (program) data to a page Only 1-0 transitions are allowed Writing within a block must be ordered by page 1-2ms Erase all bits in a block to 1 Pages must be erased before they can be written Update-in-place is not possible 3-7ms Because of physical constraints, just because of the physical constraints of these devices. There could be access interferences that need to be handled well. There is a big possibility in intereference because of this part.\nWrite amplification Write amplification is the ratio of physical pages written to Flash for every logical page written by the user application. This ratio is greater than 1 because the Flash Translation Layer performs additional writes to erase blocks without losing valid data.\nReliability of Flash Storage In this section we list some of the reliability issues that can happen with flash storage.\nWear out Flash cells are physically damaged programming and erasing them Writing Interferences Programming pages can corrupt the values of other pages in the block Read Interferences Reading data can corrupt the data in the block It takes many reads to see this effect Flash Translation Layer How can you handle some pages that just work out? We would like to have an interface that is closer to the original disk based access, this is what the FTL (flash translation layer) does.\nThe FTL keeps a datastructure for a map (address to physical page address) and information for each block (e.g. page counts) to know when to remove blocks. Writes are over a write pointer.\nRead memory: Tells you where to read for block and lines given a memory access. Write memory: Chooses where to write based on a write pointer, then you invalidate the old block. Count valid blocks Decrease available blocks count. If the a page has zero good pages, then it is just deleted with that hardware operation and then can be reused. When we want to erase, we need to move the good blocks to another part and only then erase it. Structures in the FTL You have two datastructures:\nMap: maps device addresses to actual physical addresses in the Flash Storage. Block Info Table: this keeps information about every blocks: valid pages, erase count (too see if the block is still in working condition (it has a flag for this)), and a sequence number. After a write, it there was something already there, you just invalidate the block (don’t erase it yet, since we would need to erase the entire block), and write to a different page in a different block. When you need to erase, you move every valid block, and set the block state to erased.\nDatacenter Networking Each server has a NIC (Network Interface Card) that connects to a switch, which connects to other switches and servers.\nDesiderata of Networking Low latency Security through virtualization, see Container Virtualization, and IO memory virtualization, see Virtual Machines. We need IO Memory Management Units to virtualize DMA accesses. IOMMU unit that intercepts DMA for virtualization, and translated it into physical addresses for IO parts. SR-IOV Single Root I/O Virtualization (SR-IOV) allows a specification for sharing a PCIe device to appear as multiple guests. This enables more efficient sharing of network resources among virtual machines (VMs) or containers.\nAllows a single device on a single physical port to appear as multiple, separate physical devices Hypervisor is not involved in data transfers à set up independent memory space, interrupts and DMA streams for guests Requires hardware support i.e., the I/O device must be SR-IOV capable We have explored this part in Virtual Machines. PF stands for private function, which is the privileged hardware function for accessing the network interface, while VF is the virtual functione exposed to the virtual machine, who then uses to access directly the hardware in a secure manner. | image from (Firestone et al. 2018)\nIO Memory Management Unit Communicating data availability Two main ways, interrupting and polling. We have presented these two methods in Note sull’architettura.\nPolling is not a bad approach within a datacenter setting (not much context switching for interrupts), that is still ok.\nSlow networking software Linux kernel is very slow in handling network part, it has been designed to be general, and offers sometimes too many generalizations that often hinder the latency aspect. The throughput for HW limits is much much higher. This is a strong motivator to write the best possible software for that specific hardware.\nFor example take the TCP/IP networking (Classical, see Livello di Rete) Sources of overhead in packet processing\nInterrupts \u0026 OS scheduling Kernel crossings (context switches) Buffer copies Packetizing \u0026 checking for errors Memory latency Poor locality of I/O data processing Memory bandwidth not an issue, why? Example 64 byte Ethernet packets @ 10Gbps This is 20M packets/sec Naively done: 20M interrupts/sec, which is a huge overhead! There are some ideas on how to optimize the slow networking software:\nMove everything to user space and optimize the software Offload processing to NIC hardware, as (Firestone et al. 2018) does. NIC optimizations Checksum Offload: NIC checks TCP and IP checksum with special hardware (CRC). Large Segment Offloading (LSO) (at transmit): NIC breaks up large TCP segments into smaller packets. Automatic Header Generation: Large Receive Offload (LRO): Combines several incoming packets into one much larger packet for processing. Header Splitting (at receive): NIC places header \u0026 payload in separate system buffers. Improves locality: (Why?) Allows more prefetching \u0026 zero copy. Interrupt Coalescing: so that you don’t need so many interrupts to handle single packets, but you can process in batches. It is also possible to use steering based optimizations. References [1] Kanev et al. “Profiling a Warehouse-Scale Computer” ACM 2015 [2] Ayers et al. “AsmDB: Understanding and Mitigating Front-End Stalls in Warehouse-Scale Computers” ACM 2019 [3] Firestone et al. “Azure Accelerated Networking: SmartNICs in the Public Cloud” 5th USENIX Symposium on Networked Systems Design and Implementation 2018 ",
  "wordCount" : "4042",
  "inLanguage": "en",
  "image": "https://flecart.github.io/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://flecart.github.io/notes/datacenter-hardware/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. Angelo Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://flecart.github.io/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://flecart.github.io/" accesskey="h" title="X. Angelo Huang&#39;s Blog (Alt + H)">X. Angelo Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://flecart.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://flecart.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://flecart.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://flecart.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Datacenter Hardware
    </h1>
    <div class="post-meta">19 min&nbsp;·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#datacenter-cpus" aria-label="Datacenter CPUs">Datacenter CPUs</a><ul>
                        
                <li>
                    <a href="#desktop-cpu-vs-cloud-cpu" aria-label="Desktop CPU vs Cloud CPU">Desktop CPU vs Cloud CPU</a></li>
                <li>
                    <a href="#two-socket-datacenter-server" aria-label="Two socket datacenter server">Two socket datacenter server</a></li>
                <li>
                    <a href="#numa-technology" aria-label="NUMA Technology">NUMA Technology</a></li>
                <li>
                    <a href="#performance-with-cluster-size-and-communication" aria-label="Performance with Cluster size and Communication">Performance with Cluster size and Communication</a></li>
                <li>
                    <a href="#amdahls-law" aria-label="Amdahl&rsquo;s Law">Amdahl&rsquo;s Law</a></li>
                <li>
                    <a href="#gustafsons-law" aria-label="Gustafson&rsquo;s Law">Gustafson&rsquo;s Law</a></li>
                <li>
                    <a href="#intel-cache-allocation-technology" aria-label="Intel Cache allocation technology">Intel Cache allocation technology</a></li>
                <li>
                    <a href="#cpu-types-and-power-management" aria-label="CPU Types and Power Management">CPU Types and Power Management</a></li>
                <li>
                    <a href="#towards-hardware-specialization" aria-label="Towards Hardware specialization">Towards Hardware specialization</a></li>
                <li>
                    <a href="#compute-express-link" aria-label="Compute Express Link">Compute Express Link</a></li>
                <li>
                    <a href="#fpgas" aria-label="FPGAs">FPGAs</a></li></ul>
                </li>
                <li>
                    <a href="#datacenter-structure" aria-label="Datacenter Structure">Datacenter Structure</a><ul>
                        
                <li>
                    <a href="#electrical-systems" aria-label="Electrical Systems">Electrical Systems</a></li>
                <li>
                    <a href="#cooling-systems" aria-label="Cooling Systems">Cooling Systems</a></li></ul>
                </li>
                <li>
                    <a href="#analyzing-bottlenecks" aria-label="Analyzing Bottlenecks">Analyzing Bottlenecks</a><ul>
                        
                <li>
                    <a href="#profiling-a-warehouse-scale-computer" aria-label="Profiling a warehouse scale computer">Profiling a warehouse scale computer</a></li>
                <li>
                    <a href="#the-data-center-tax" aria-label="The data-center tax">The data-center tax</a></li>
                <li>
                    <a href="#types-of-stalls" aria-label="Types of stalls">Types of stalls</a></li>
                <li>
                    <a href="#asmdb" aria-label="AsmDB">AsmDB</a></li>
                <li>
                    <a href="#imbalanced-resource-usages" aria-label="Imbalanced resource usages">Imbalanced resource usages</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#the-storage-hierarchy" aria-label="The Storage Hierarchy">The Storage Hierarchy</a><ul>
                        
                <li>
                    <a href="#introduction-to-memory-types" aria-label="Introduction to Memory Types">Introduction to Memory Types</a><ul>
                        
                <li>
                    <a href="#access-speeds" aria-label="Access Speeds">Access Speeds</a></li>
                <li>
                    <a href="#types-of-memories" aria-label="Types of Memories">Types of Memories</a></li></ul>
                </li>
                <li>
                    <a href="#dram-memory-access-protocol" aria-label="DRAM: Memory-Access Protocol">DRAM: Memory-Access Protocol</a><ul>
                        
                <li>
                    <a href="#dram-operations" aria-label="DRAM Operations">DRAM Operations</a></li>
                <li>
                    <a href="#row-policies" aria-label="Row policies">Row policies</a></li></ul>
                </li>
                <li>
                    <a href="#flash-storage" aria-label="Flash Storage">Flash Storage</a><ul>
                        
                <li>
                    <a href="#working-principles" aria-label="Working Principles">Working Principles</a></li>
                <li>
                    <a href="#flash-storage-types" aria-label="Flash Storage Types">Flash Storage Types</a></li>
                <li>
                    <a href="#internal-architecture-of-flash-storage" aria-label="Internal Architecture of Flash Storage">Internal Architecture of Flash Storage</a></li>
                <li>
                    <a href="#flash-operations" aria-label="Flash Operations">Flash Operations</a></li>
                <li>
                    <a href="#write-amplification" aria-label="Write amplification">Write amplification</a></li>
                <li>
                    <a href="#reliability-of-flash-storage" aria-label="Reliability of Flash Storage">Reliability of Flash Storage</a></li>
                <li>
                    <a href="#flash-translation-layer" aria-label="Flash Translation Layer">Flash Translation Layer</a></li>
                <li>
                    <a href="#structures-in-the-ftl" aria-label="Structures in the FTL">Structures in the FTL</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#datacenter-networking" aria-label="Datacenter Networking">Datacenter Networking</a><ul>
                        <ul>
                        
                <li>
                    <a href="#desiderata-of-networking" aria-label="Desiderata of Networking">Desiderata of Networking</a></li>
                <li>
                    <a href="#sr-iov" aria-label="SR-IOV">SR-IOV</a></li>
                <li>
                    <a href="#io-memory-management-unit" aria-label="IO Memory Management Unit">IO Memory Management Unit</a></li>
                <li>
                    <a href="#communicating-data-availability" aria-label="Communicating data availability">Communicating data availability</a></li>
                <li>
                    <a href="#slow-networking-software" aria-label="Slow networking software">Slow networking software</a></li>
                <li>
                    <a href="#nic-optimizations" aria-label="NIC optimizations">NIC optimizations</a></li></ul>
                    </ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>We want to optimize the parts of the datacenter hardware such that the cost of operating the datacenter as a <em>whole</em> would be lower, we need to think about it as a whole.</p>
<h3 id="datacenter-cpus">Datacenter CPUs<a hidden class="anchor" aria-hidden="true" href="#datacenter-cpus">#</a></h3>
<h4 id="desktop-cpu-vs-cloud-cpu">Desktop CPU vs Cloud CPU<a hidden class="anchor" aria-hidden="true" href="#desktop-cpu-vs-cloud-cpu">#</a></h4>
<ul>
<li><strong>Isolation</strong>:
<ul>
<li>Desktop CPUs have low isolation, they are used by a single user.</li>
<li>Cloud CPUs have high isolation, they are shared among <strong>different</strong> users.</li>
</ul>
</li>
<li><strong>Workload and performance</strong>: usually high workloads and moving a lot of data around.</li>
</ul>
<p>They have a spectrum of low and high end cores, so that if you have high parallelism you can use lower cores, while for resource intensive tasks, its better to have high end cores, especially for latency critical tasks.</p>
<p>Datacenters have developed own chips for custom software too. This is ok to specialize on their own workloads and get better on that market.</p>
<p>Also NVIDIA is designing a CPU, so that they can couple them together and co-optimize it (increases 30x memory movement, Grace Hopper architecture CPU linked with GPU). Alps cluster at ETH have 10k of those GPUs.</p>
<p>These are some examples of CPUs designed lately:</p>
<ul>
<li>AWS Graviton Arm CPU</li>
<li>Microsoft Cobalt Arm CPU</li>
<li>Google Axion Arm CPU</li>
</ul>
<h4 id="two-socket-datacenter-server">Two socket datacenter server<a hidden class="anchor" aria-hidden="true" href="#two-socket-datacenter-server">#</a></h4>
<img src="/images/notes/Datacenter Hardware-20250418173805446.webp" style="width: 100%" class="center" alt="Datacenter Hardware-20250418173805446">
These are some of the design considerations that one need to take into account to design a chip more specific for cloud environments:
- Type of core: “brawny” or “wimpy”
	- Meaning better more energy hungry but more efficient cores, or less energy hungry but less efficient cores, but in more quantity?
- Number of cores
- Number of sockets and NUMA topology
- Cache hierarchy and size
- Isolation mechanisms for multi-tenancy (for performance & security)
- Integration with hardware accelerators, network devices, etc.
- Power management.
<p>Usually these cores are brawny (performance) cores. Nowadays cores have <strong>dynamic voltage frequency scaling</strong> that allows frequency scaling per core.</p>
<h4 id="numa-technology">NUMA Technology<a hidden class="anchor" aria-hidden="true" href="#numa-technology">#</a></h4>
<p>NUMA stands for <strong>Non-Uniform Memory Access</strong>. It&rsquo;s a memory design used in multi-socket and some <strong>multi-core</strong> systems where each CPU (or group of cores) has its own <strong>local memory</strong> and <strong>shared access</strong> to remote memory.</p>
<blockquote>
<p>In simpler terms: Not all memory in the system is equally fast to access from every CPU core.</p></blockquote>
<p>The above image on two core datacenter server is an example of a NUMA architecture, where each socket has its own memory and can access the other socket memory with a higher latency.</p>
<h4 id="performance-with-cluster-size-and-communication">Performance with Cluster size and Communication<a hidden class="anchor" aria-hidden="true" href="#performance-with-cluster-size-and-communication">#</a></h4>
<p>The number of cores to allocate to a single application should depend on the type of application that is running, more communications means bigger overheads.</p>
<p>The basic takeaway is that you can optimize well if the program does not need to communicate a lot, and sometimes it is huge, the two socket architecture is considered the sweet spot for both performance and cost.</p>
<img src="/images/notes/Datacenter Hardware-20250422100057933.webp" style="width: 100%" class="center" alt="Datacenter Hardware-20250422100057933">
<h4 id="amdahls-law">Amdahl&rsquo;s Law<a hidden class="anchor" aria-hidden="true" href="#amdahls-law">#</a></h4>
<p>This law defines the <strong>speed up</strong> that you can achieve after you parallelize the code.
Usually it is defined as:</p>
$$
\text{Speedup} = \frac{\text{CPUTimeOLD}}{\text{CPUTimeNEW}} = \frac{1}{(1 - P) + \frac{P}{N}}
$$<p>
Where $P$ is the parallelizable part of the code, and $N$ is the number of processors.</p>
<h4 id="gustafsons-law">Gustafson&rsquo;s Law<a hidden class="anchor" aria-hidden="true" href="#gustafsons-law">#</a></h4>
<p>Problem size scales with number of processors (constant work per processor).
This means: as we add more processors in the system, the compute needed to solve the same problem is more, we can see this as the reverse side of the Amdahl&rsquo;s law, since perhaps some compute is for communication.
But it is more intended about solving larger problems at the same time.</p>
<h4 id="intel-cache-allocation-technology">Intel Cache allocation technology<a hidden class="anchor" aria-hidden="true" href="#intel-cache-allocation-technology">#</a></h4>
<blockquote>
<p>Instead of every core/application having free rein over the full L3 cache, CAT lets you <strong>assign specific portions</strong> of that cache to specific workloads.</p></blockquote>
<p>This is the main, idea, in this manner we don&rsquo;t have many conflict evictions due to this part.</p>
<ul>
<li>Enables partitioning the ways of a highly-associative last-level caching into several subsets with smaller associativity.</li>
<li>Cores assigned to one subset can only allocate cache lines in their subset on refills, but are allowed to hit in any part of the last-level cache (LLC).</li>
<li>It is usually implemented using bitmasks on the associativity of the cache, see <a href="/notes/cache-optimization">Cache Optimization</a>.
This enables higher priority applications running on some LLC to get more cache size for allocation during runtime.</li>
</ul>
<blockquote>
<p>Real-time systems or latency-sensitive apps (like video streaming, trading, or telco) can&rsquo;t tolerate unpredictable cache behavior.</p></blockquote>
<p>Other technologies like Dynamic Voltage Frequency Scaling (DVFS) enables core-independent scaling.
And there is <strong>no bandwidth isolation</strong>.</p>
<h4 id="cpu-types-and-power-management">CPU Types and Power Management<a hidden class="anchor" aria-hidden="true" href="#cpu-types-and-power-management">#</a></h4>
<p>As we saw when studying the assignment related to <a href="/notes/skylake-microprocessor">Skylake Microprocessor</a>, there are efficient and perfromance cores, in this context called wimpy and brawny cores. The former is more energy efficient (2x) but much slower 5x, while the latter is faster, yet because of Ahmdal&rsquo;s law, it has much performance variability.</p>
<p>There are p-states and c-states:</p>
<ul>
<li><strong>p-states</strong>: power consumption when the system is executing code</li>
<li><strong>c-states</strong>: power consumption levels when the system is idle.</li>
</ul>
<p>Different cores have different consumption, we can control the performance by controlling frequency and power.</p>
<h4 id="towards-hardware-specialization">Towards Hardware specialization<a hidden class="anchor" aria-hidden="true" href="#towards-hardware-specialization">#</a></h4>
<ul>
<li>CPU have low latency but also low memory throughput, and they are general purpose (instruction fetching, data fetching, execution storage etc.)</li>
<li>GPU have high latency but high memory throughput, good for memory-bound computations.
<ul>
<li>These GPUs have a high overhead in moving data, so when you have computations where you need to access data that is not sequential in memory, it is probably difficult to have a benefit from them.</li>
<li>The case above is not true for ML workloads, where the main bottleneck was the memory for the matrix multiplications, see <a href="/notes/cache-optimization">Cache Optimization</a>.</li>
<li>Google has been developing his own GPU, called TPU, they predicted high workload for inference, and wanted to be less dependent from hardware companies.</li>
</ul>
</li>
<li>The specialization trend is for everything (NIC, memory, storage, networking, more and more processors are seen to be developed in the next years), there are lots of money flowing to unique accelerators.
Currently a server lasts about 6 years in a datacenter (average lifetime) (meeting performance requirements and hardware reliability).</li>
</ul>
<p>Other examples are NVIDIA GB300 NVL72, or Project Ceiba from Microsoft, which is a custom chip for AI workloads, they are trying to go more for scale up now instead of scale out.</p>
<h4 id="compute-express-link">Compute Express Link<a hidden class="anchor" aria-hidden="true" href="#compute-express-link">#</a></h4>
<p>CXL is an open standard cache-coherent interconnect for processors, memory expansion, and accelerators. It introduces a new level in the memory hierarchy.</p>
<ul>
<li>Maintains coherency between CPU memory and memory on attached devices.</li>
<li>Enables resource sharing (or pooling) for higher performance, reduces software stack complexity, and lowers overall system cost.
Why is it needed?</li>
<li>Modern workloads have high memory capacity requirements. Spilling to SSD is too slow. Thus, need to make remote memory access fast!</li>
<li>Accelerators are increasingly common and have their own memory need high-bandwidth access between CPU and devices, this is why a new protocol is needed.</li>
</ul>
<p>See <a href="/notes/compute-express-link">Compute Express Link</a> for other info about this section.</p>
<h4 id="fpgas">FPGAs<a hidden class="anchor" aria-hidden="true" href="#fpgas">#</a></h4>
<p>For example the TCP stack is using lots of cycles, you could program a chip just optimized for these kinds of operations, FPGAs are good with those <em>specialized kinds of computations</em> where neither CPU and GPUs are good at:</p>
<ul>
<li>Network operations</li>
<li>Data parallelism with irregular accesses.</li>
<li>Operations that are not float or int operations (filtering packets in network settings for example).</li>
<li>Encryption operations
Basically everything that could benefit offloading computation to other parts, when the CPU is still busy, but putting a GPU or TPU would be overkill.</li>
</ul>
<p>The tooling here in the <strong>chip design takes a lot of time</strong>.
One example of a technology that uses FPGAs in a similar manner is <a href="https://aws.amazon.com/blogs/aws/new-aqua-advanced-query-accelerator-for-amazon-redshift/">AQUA</a>, pushing computational resources for reduction and aggregation closer to the data (reducing network traffic), layer on top of S3 for Redshift.</p>
<figure class="center">
<img src="/images/notes/Datacenter Hardware-20250528214546648.webp" style="width: 100%"   alt="Datacenter Hardware-20250528214546648" title="Datacenter Hardware-20250528214546648"/>
<figcaption><p style="text-align:center;">Image from course slides CCA 2025 ETHz | FPGA can be used to offload many common tasks from CPUs, (e.g. network encryption, serialization and similars), this makes the NIC quite faster, using custom smartNIC based on FPGA, see <a href="/notes/datacenter-hardware#firestoneAzureAcceleratedNetworking2018">(Firestone et al. 2018)</a></p></figcaption>
</figure>
<blockquote>
<p>We show that FPGAs are the best current platform for offloading our networking stack as ASICs do not provide sufficient programmability, and embedded CPU cores do not provide scalable performance, especially on single network flows.</p></blockquote>
<h3 id="datacenter-structure">Datacenter Structure<a hidden class="anchor" aria-hidden="true" href="#datacenter-structure">#</a></h3>
<p>The whole datacenter is designed to minimize the ownership cost of the system.
This section is anyways mostly just introductory.</p>
<h4 id="electrical-systems">Electrical Systems<a hidden class="anchor" aria-hidden="true" href="#electrical-systems">#</a></h4>
<p>To be done.</p>
<h4 id="cooling-systems">Cooling Systems<a hidden class="anchor" aria-hidden="true" href="#cooling-systems">#</a></h4>
<p>Heating is a huge issue inside datacenter systems. We have many many systems up to handle a good cooling behaviour within the systems.
<figure class="center">
<img src="/images/notes/Datacenter Hardware-20250924102954764.webp" style="width: 100%"   alt="Datacenter Hardware-20250924102954764" title="Datacenter Hardware-20250924102954764"/></p>
<figcaption><p style="text-align:center;"> some example of how datacenters are organized to favor better cooling</p></figcaption>
</figure>
<h3 id="analyzing-bottlenecks">Analyzing Bottlenecks<a hidden class="anchor" aria-hidden="true" href="#analyzing-bottlenecks">#</a></h3>
<h4 id="profiling-a-warehouse-scale-computer">Profiling a warehouse scale computer<a hidden class="anchor" aria-hidden="true" href="#profiling-a-warehouse-scale-computer">#</a></h4>
<p>These are mainly results from the paper <a href="https://dl.acm.org/doi/10.1145/2749469.2750392">(Kanev et al. 2015)</a>.</p>
<blockquote>
<p>we show significant diversity in workload behavior with no single “silver-bullet” application to optimize for and with no major intra-application hotspots.</p></blockquote>
<p>One main observation is that the workloads are <strong>diverse</strong>, and you cannot just optimize for a single kind of workload. They basically assert that there is not single bottleneck for performance of the Warehouse scale computer.</p>
<ul>
<li>Top 50 binaries cover only 60% of the total execution time, also in modern AI jobs, the statistics should be similar.</li>
</ul>
<h4 id="the-data-center-tax">The data-center tax<a hidden class="anchor" aria-hidden="true" href="#the-data-center-tax">#</a></h4>
<p>There is a <strong>data-center tax</strong> associated with moving things around. and communication.</p>
<figure class="center">
<img src="/images/notes/Datacenter Hardware-20250530215059419.webp" style="width: 100%"   alt="Datacenter Hardware-20250530215059419" title="Datacenter Hardware-20250530215059419"/>
<figcaption><p style="text-align:center;"> Image from <a href="https://dl.acm.org/doi/10.1145/2749469.2750392">(Kanev et al. 2015)</a>| The data-center tax is the cost of moving data around, this is the main bottleneck in datacenter performance.</p></figcaption>
</figure>
<h4 id="types-of-stalls">Types of stalls<a hidden class="anchor" aria-hidden="true" href="#types-of-stalls">#</a></h4>
<p>We define three types of values:</p>
<ul>
<li><strong>Retiring</strong> is considering useful work, the rest are sources of overhead.</li>
<li><strong>Front-end stall</strong>: captures all overheads associated with <em>fetching &amp; decoding instructions</em>
<ul>
<li>The instruction set needed is 100x the L1 instruction cache (and this is growing 20% a year), but it is impractical to grow the cache size so much.</li>
<li>We can try to prefetch the part of the data that matters in the L1 cache, but not sure how is this possible. -&gt; What AsmDB does, when it recognizes common instruction sequences.</li>
<li>Introduce custom instructions + 5% better instructions per cycle.</li>
</ul>
</li>
<li><strong>Back-end stall</strong>: captures overheads due to data cache hierarchy and lack of instruction-level  parallelism (see <a href="/notes/skylake-microprocessor">Skylake Microprocessor</a> for more information about this parallelism).</li>
</ul>
<p>Most of the overhead are from back-end stalls.</p>
<h4 id="asmdb">AsmDB<a hidden class="anchor" aria-hidden="true" href="#asmdb">#</a></h4>
<p>From the paper <a href="https://dl.acm.org/doi/10.1145/3307650.3322234">(Ayers et al. 2019)</a>, collects data about block-level instruction usage into an assembly database, so that:</p>
<ul>
<li>You know what instructions are used, so perhaps you can manually optimize for it.</li>
<li>benchmarks for better compilers in i-cache design (software prefetch instructions).</li>
<li>Data source for compiler driven improvements of the binaries.
Gets 5% better instruction per cycle, saving millions of dollars.</li>
</ul>
<h4 id="imbalanced-resource-usages">Imbalanced resource usages<a hidden class="anchor" aria-hidden="true" href="#imbalanced-resource-usages">#</a></h4>
<p>The resource usage in the datacenter are basically independent with each other, it is quite hard to predict a precise ration between CPU and memory that is needed to optimize the resources for a specific data center.
By disaggregating resources, we are able to <strong>scale them</strong> independently.</p>
<figure class="center">
<img src="/images/notes/Datacenter Hardware-20250419102135002.webp" style="width: 100%"   alt="Datacenter Hardware-20250419102135002" title="Datacenter Hardware-20250419102135002"/>
<figcaption><p style="text-align:center;">Idea of a solution: Dynamically allocate the ratio of resources an application needs by using compute & storage components connected over a network</p></figcaption>
</figure>
<p>There is something similar of storing <a href="/notes/graph-databases">Graph Databases</a> in disaggregated manners in the disk, but in a wholly different field.</p>
<h2 id="the-storage-hierarchy">The Storage Hierarchy<a hidden class="anchor" aria-hidden="true" href="#the-storage-hierarchy">#</a></h2>
<p>We need to expand the classical single server memory architecture (see <a href="/notes/memoria">Memoria</a>) to datacenter level. Now we have rack memory and datacenter memory, and the idea that higher latency with higher memory possibility is still valid here.</p>
<h3 id="introduction-to-memory-types">Introduction to Memory Types<a hidden class="anchor" aria-hidden="true" href="#introduction-to-memory-types">#</a></h3>
<h4 id="access-speeds">Access Speeds<a hidden class="anchor" aria-hidden="true" href="#access-speeds">#</a></h4>
<p>Usually now the bottleneck is the <strong>network</strong>. One observation is that the remote latency of the disk is similar to the local disk latency (disk seek is the bottleneck, wire transfer is faster).
One interesting observation is that rack memory and cluster memory seeks are <strong>faster than local disks</strong> and they also have higher bandwidth!</p>
<table>
  <thead>
      <tr>
          <th>Configuration</th>
          <th>Component</th>
          <th>Capacity</th>
          <th>Latency</th>
          <th>Speed</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>ONE SERVER</td>
          <td>DRAM</td>
          <td>256GB</td>
          <td>100ns</td>
          <td>150GB/s</td>
      </tr>
      <tr>
          <td>ONE SERVER</td>
          <td>DISK</td>
          <td>80TB</td>
          <td>10ms</td>
          <td>800MB/s</td>
      </tr>
      <tr>
          <td>ONE SERVER</td>
          <td>FLASH</td>
          <td>4TB</td>
          <td>100us</td>
          <td>3GB/s</td>
      </tr>
      <tr>
          <td>LOCAL RACK (40 SERVERS)</td>
          <td>DRAM</td>
          <td>10TB</td>
          <td>20us</td>
          <td>5GB/s</td>
      </tr>
      <tr>
          <td>LOCAL RACK (40 SERVERS)</td>
          <td>DISK</td>
          <td>3.2PB</td>
          <td>10ms</td>
          <td>5GB/s</td>
      </tr>
      <tr>
          <td>LOCAL RACK (40 SERVERS)</td>
          <td>FLASH</td>
          <td>160TB</td>
          <td>120us</td>
          <td>5GB/s</td>
      </tr>
      <tr>
          <td>CLUSTER (125 RACKS)</td>
          <td>DRAM</td>
          <td>1.28PB</td>
          <td>50us</td>
          <td>1.2GB/s</td>
      </tr>
      <tr>
          <td>CLUSTER (125 RACKS)</td>
          <td>DISK</td>
          <td>400PB</td>
          <td>10ms</td>
          <td>1.2GB/s</td>
      </tr>
      <tr>
          <td>CLUSTER (125 RACKS)</td>
          <td>FLASH</td>
          <td>20PB</td>
          <td>150us</td>
          <td>1.2GB/s</td>
      </tr>
  </tbody>
</table>
<p>From Jeff Dean Tables:</p>
<table>
  <thead>
      <tr>
          <th>Operation</th>
          <th>Time</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>L1 cache reference</td>
          <td>1.5 ns</td>
      </tr>
      <tr>
          <td>L2 cache reference</td>
          <td>5 ns</td>
      </tr>
      <tr>
          <td>Branch misprediction</td>
          <td>6 ns</td>
      </tr>
      <tr>
          <td>Uncontended mutex lock/unlock</td>
          <td>20 ns</td>
      </tr>
      <tr>
          <td>L3 cache reference</td>
          <td>25 ns</td>
      </tr>
      <tr>
          <td>Main memory reference</td>
          <td>100 ns</td>
      </tr>
      <tr>
          <td>Decompress 1 KB with Snappy [Sna]</td>
          <td>500 ns</td>
      </tr>
      <tr>
          <td>&ldquo;Far memory&rdquo;/Fast NVM reference</td>
          <td>1,000 ns (1 us)</td>
      </tr>
      <tr>
          <td>Compress 1 KB with Snappy [Sna]</td>
          <td>2,000 ns (2 us)</td>
      </tr>
      <tr>
          <td>Read 1 MB sequentially from memory</td>
          <td>12,000 ns (12 us)</td>
      </tr>
      <tr>
          <td>SSD Random Read</td>
          <td>100,000 ns (100 us)</td>
      </tr>
      <tr>
          <td>Read 1 MB bytes sequentially from SSD</td>
          <td>500,000 ns (500 us)</td>
      </tr>
      <tr>
          <td>Read 1 MB sequentially from 10Gbps network</td>
          <td>1,000,000 ns (1 ms)</td>
      </tr>
      <tr>
          <td>Read 1 MB sequentially from disk</td>
          <td>10,000,000 ns (10 ms)</td>
      </tr>
      <tr>
          <td>Disk seek</td>
          <td>10,000,000 ns (10 ms)</td>
      </tr>
      <tr>
          <td>Send packet California-&gt;Netherlands-&gt;California</td>
          <td>150,000,000 ns (150 ms)</td>
      </tr>
  </tbody>
</table>
<h4 id="types-of-memories">Types of Memories<a hidden class="anchor" aria-hidden="true" href="#types-of-memories">#</a></h4>
<ul>
<li><strong>SRAM</strong>: Static RAM, used for cache. Fast and expensive (cost per bit).
<ul>
<li>We need <em>six</em> transistors for a SRAM</li>
<li>Amplifier to read quickly.</li>
<li>No worries about refreshes.</li>
</ul>
</li>
<li><strong>DRAM</strong>: Dynamic RAM, used for main memory. Slower and cheaper.
<ul>
<li>Stores one bit with one transistor (a special one)</li>
<li>Much higher density compared to SRAM</li>
</ul>
</li>
<li><strong>Flash storage</strong>: Used for SSDs. Slower than DRAM but faster than traditional HDDs.</li>
</ul>
<p>Rough comparisons for DRAM : Flash : Disk</p>
<ul>
<li>Cost per bit: 100 : 10 : 1</li>
<li>Access latency: 1 : 5,000 : 1,000,000</li>
</ul>
<img src="/images/notes/Datacenter Hardware-20250419103049952.webp" style="width: 100%" class="center" alt="Datacenter Hardware-20250419103049952">
<h3 id="dram-memory-access-protocol">DRAM: Memory-Access Protocol<a hidden class="anchor" aria-hidden="true" href="#dram-memory-access-protocol">#</a></h3>
<p>As we studied in <a href="/notes/circuiti-sequenziali">Circuiti Sequenziali</a> when developing a small memory, we need a system to index into the memory and retrieve its content. This is done by a <strong>memory controller</strong>, which implements the memory-access protocol with its row decoders and similars.</p>
<h4 id="dram-operations">DRAM Operations<a hidden class="anchor" aria-hidden="true" href="#dram-operations">#</a></h4>
<p>DRAM has an access protocol that allows for some commands:</p>
<ul>
<li>ACTIVATE (open a row)
<ul>
<li>It opens the whole row and loads content into row buffer, after a nanosecond delay (called Row to Column Delay)</li>
<li>Usually one row is 4k to 32k of data, DDR4 has 8K rows each with 8KB</li>
</ul>
</li>
<li>READ (read a column)
<ul>
<li>It reads the specific Column, but needs to destroy that data after some amplification, the data needs to be restored.</li>
<li>The number of banks is around 16 to 32. per chip, and you usually have 8 to 16 DRAM chips, so you can read about 4 megabyte just in one cycle if its always in the banks.</li>
</ul>
</li>
<li>WRITE
<ul>
<li>WRITING <em>trashes</em> the row buffer temporarily and forces a restore cycle later.</li>
</ul>
</li>
<li>PRECHARGE (close row)
<ul>
<li>Resets the bank to a neutral state so that another row can be read.</li>
</ul>
</li>
<li>REFRESH
<ul>
<li>Every 64ms you need a refresh to maintain the memory, when a refresh is occurring the DRAM cannot do other things.</li>
</ul>
</li>
</ul>
<h4 id="row-policies">Row policies<a hidden class="anchor" aria-hidden="true" href="#row-policies">#</a></h4>
<p>And we also have row policies (open rows use energy), and you can also optimize for the row opening prediction. (<strong>open-row policy</strong>, and <strong>closed-row policy</strong>), or you can also use speculative row access policies.</p>
<p><strong>Open Row</strong>:</p>
<ul>
<li>Keep the row open until it is needed for another, only then you can precharge</li>
<li>This saves latency if you know that row will be used many times.</li>
</ul>
<p><strong>Closed Row</strong></p>
<ul>
<li>Close the row as soon as you are done with it, unless there is some request that is already going for that row.</li>
<li>Helps to avoid conflict for changing rows.</li>
</ul>
<p><strong>Speculative version</strong>
Attempt to predict whether you need to close or open that row.</p>
<h3 id="flash-storage">Flash Storage<a hidden class="anchor" aria-hidden="true" href="#flash-storage">#</a></h3>
<p><a href="https://flashdba.com/2015/01/09/understanding-flash-floating-gates-and-wear/">This</a> looks like a good introductory source.</p>
<p>This is for <strong>non-volatile</strong> memory, SSDs.
They use specific transistors to control the charge on the gates and then understand if it is one or zero. Lower charge means 1. You can use different levels to directly encode 2 bits instead of one.</p>
<ul>
<li>Charge modulates the threshold voltage of the underlying transistor</li>
<li>You can also store more than one bit in one transistor, this is the <strong>multi-level cell</strong>.</li>
</ul>
<img src="/images/notes/Datacenter Hardware-20250427120932558.webp" style="width: 100%" class="center" alt="Datacenter Hardware-20250427120932558">
<h4 id="working-principles">Working Principles<a hidden class="anchor" aria-hidden="true" href="#working-principles">#</a></h4>
<p><strong>Floating-Gate Transistor:</strong> At the heart of a flash memory cell is a MOSFET (Metal-Oxide-Semiconductor Field-Effect Transistor) with the following structure:</p>
<ul>
<li><strong>Control Gate (CG):</strong> This gate is used to control the flow of current through the transistor&rsquo;s channel, similar to a regular MOSFET.</li>
<li><strong>Floating Gate (FG):</strong> This gate is electrically isolated by an insulating layer (typically silicon dioxide) surrounding it. The floating gate&rsquo;s charge determines the threshold voltage of the transistor, which in turn dictates whether the cell represents a binary &lsquo;0&rsquo; or &lsquo;1&rsquo;.</li>
</ul>
<p><strong>Data Storage Mechanism:</strong></p>
<ul>
<li><strong>Writing (Programming):</strong> To store data, a voltage is applied to the control gate. This causes electrons to tunnel through the insulating layer and become trapped on the floating gate. The presence of these electrons increases the threshold voltage of the transistor. This state typically represents a binary &lsquo;0&rsquo;.  </li>
<li><strong>Reading:</strong> When reading data, a voltage is applied to the control gate. The current flow through the transistor&rsquo;s channel is then sensed. If the threshold voltage is low (no or few electrons on the floating gate), current flows, representing a binary &lsquo;1&rsquo;. If the threshold voltage is high (electrons on the floating gate), current does not flow, representing a binary &lsquo;0&rsquo;.  </li>
<li><strong>Erasing:</strong> To erase data, a higher voltage of the opposite polarity is applied, typically to the source terminal. This forces the trapped electrons off the floating gate through quantum tunneling, returning the cell to its original, unprogrammed state (representing a binary &lsquo;1&rsquo;). In NAND flash, erasure typically happens in larger blocks of cells, while NOR flash allows for block or chip erasure</li>
</ul>
<h4 id="flash-storage-types">Flash Storage Types<a hidden class="anchor" aria-hidden="true" href="#flash-storage-types">#</a></h4>
<ul>
<li>SLC <strong>single-level cell</strong>
<ul>
<li>Faster but less dense</li>
<li>More reliable (100K -1M erase cycles)</li>
<li>~$1.5/GB</li>
<li>Used in “enterprise” drives (i.e. Intel Extreme SSDs)</li>
<li>Good for write intensive workflows.</li>
</ul>
</li>
<li>MLC: <strong>multi-level cell</strong>
<ul>
<li>Slower but denser</li>
<li>Less reliable (1K - 10K erase cycles, becomes a problem with write intensive workloads)</li>
<li>$0.08/GB (two orders of magnitude cheaper!)</li>
<li>Used in consumer drives (thumb drives, cheap SSDs, etc.)</li>
<li>eMLC for enterprise use designed for lower error rates</li>
</ul>
</li>
</ul>
<h4 id="internal-architecture-of-flash-storage">Internal Architecture of Flash Storage<a hidden class="anchor" aria-hidden="true" href="#internal-architecture-of-flash-storage">#</a></h4>
<p>Flash storages have controllers, connected to blocks of data, these blocks (blocks contain pages) need to be erased at the same time, while pages are the minimum unit of read/write.
Usually we have:</p>
<ul>
<li>512bytes or 8kb per page</li>
<li>64-256 pages per block</li>
<li>About 1 to 16 chips, capacity of 1 to 16. you see that one block holds about 512MB to 2GB.</li>
</ul>
<img src="/images/notes/Datacenter Hardware-20250427121356755.webp" style="width: 100%" class="center" alt="Datacenter Hardware-20250427121356755">
<h4 id="flash-operations">Flash Operations<a hidden class="anchor" aria-hidden="true" href="#flash-operations">#</a></h4>
<ul>
<li>Read the contents of a page
<ul>
<li>20-80us (reading operation is much much faster!)</li>
<li>One problem is that each chip can process <strong>one operation</strong> per unit time.
<ul>
<li>This creates sorts of interference (quite big interference!).</li>
</ul>
</li>
</ul>
</li>
<li>Write (program) data to a page
<ul>
<li>Only 1-0 transitions are allowed</li>
<li>Writing within a block must be ordered by page</li>
<li>1-2ms</li>
</ul>
</li>
<li>Erase all bits in a block to 1
<ul>
<li>Pages must be erased before they can be written</li>
<li>Update-in-place is not possible</li>
<li>3-7ms</li>
</ul>
</li>
</ul>
<p>Because of physical constraints, just because of the physical constraints of these devices.
There could be access interferences that need to be handled well.
There is a big possibility in intereference because of this part.</p>
<h4 id="write-amplification">Write amplification<a hidden class="anchor" aria-hidden="true" href="#write-amplification">#</a></h4>
<p>Write amplification is the ratio of physical pages written to Flash for every logical page written
by the user application. This ratio is greater than 1 because the Flash Translation Layer performs
additional writes to erase blocks without losing valid data.</p>
<h4 id="reliability-of-flash-storage">Reliability of Flash Storage<a hidden class="anchor" aria-hidden="true" href="#reliability-of-flash-storage">#</a></h4>
<p>In this section we list some of the reliability issues that can happen with flash storage.</p>
<ul>
<li>Wear out
<ul>
<li>Flash cells are physically damaged programming and erasing them</li>
</ul>
</li>
<li>Writing Interferences
<ul>
<li>Programming pages can corrupt the values of other pages in the block</li>
</ul>
</li>
<li>Read Interferences
<ul>
<li>Reading data can corrupt the data in the block</li>
<li>It takes many reads to see this effect</li>
</ul>
</li>
</ul>
<h4 id="flash-translation-layer">Flash Translation Layer<a hidden class="anchor" aria-hidden="true" href="#flash-translation-layer">#</a></h4>
<p>How can you handle some pages that just work out? We would like to have an interface that is closer to the original disk based access, this is what the FTL (<strong>flash translation layer</strong>) does.</p>
<p>The FTL keeps a datastructure for a map (address to physical page address) and information for each block (e.g. page counts) to know when to remove blocks. Writes are over a write pointer.</p>
<ul>
<li><strong>Read memory</strong>: Tells you where to read for block and lines given a memory access.</li>
<li><strong>Write memory</strong>: Chooses where to write based on a write pointer, then you invalidate the old block.
<ul>
<li>Count valid blocks</li>
<li>Decrease available blocks count.</li>
<li>If the  a page has zero good pages, then it is just deleted with that hardware operation and then can be reused.</li>
<li>When we want to erase, we need to <em>move</em> the good blocks to another part and only then erase it.</li>
</ul>
</li>
</ul>
<h4 id="structures-in-the-ftl">Structures in the FTL<a hidden class="anchor" aria-hidden="true" href="#structures-in-the-ftl">#</a></h4>
<p>You have two datastructures:</p>
<ul>
<li><strong>Map</strong>: maps device addresses to actual physical addresses in the Flash Storage.</li>
<li><strong>Block Info Table</strong>: this keeps information about every blocks: valid pages, erase count (too see if the block is still in working condition (it has a flag for this)), and a sequence number.</li>
</ul>
<p>After a write, it there was something already there, you just invalidate the block (don&rsquo;t erase it yet, since we would need to erase the entire block), and write to a different page in a different block.
When you need to erase, you move every valid block, and set the block state to erased.</p>
<h2 id="datacenter-networking">Datacenter Networking<a hidden class="anchor" aria-hidden="true" href="#datacenter-networking">#</a></h2>
<p>Each server has a NIC (Network Interface Card) that connects to a switch, which connects to other switches and servers.</p>
<h4 id="desiderata-of-networking">Desiderata of Networking<a hidden class="anchor" aria-hidden="true" href="#desiderata-of-networking">#</a></h4>
<ul>
<li>Low latency</li>
<li>Security through virtualization, see <a href="/notes/container-virtualization">Container Virtualization</a>, and IO memory virtualization, see <a href="/notes/virtual-machines">Virtual Machines</a>.
<ul>
<li>We need IO Memory Management Units to virtualize DMA accesses.</li>
<li>IOMMU unit that intercepts DMA for virtualization, and translated it into physical addresses for IO parts.</li>
</ul>
</li>
</ul>
<img src="/images/notes/Datacenter Hardware-20250430194407671.webp" style="width: 100%" class="center" alt="Datacenter Hardware-20250430194407671">
<h4 id="sr-iov">SR-IOV<a hidden class="anchor" aria-hidden="true" href="#sr-iov">#</a></h4>
<p>Single Root I/O Virtualization (SR-IOV) allows a specification for sharing a <strong>PCIe device</strong> to appear as multiple guests. This enables more efficient sharing of network resources among virtual machines (VMs) or containers.</p>
<ul>
<li>Allows a single device on a single physical port to appear as multiple, separate physical devices</li>
<li>Hypervisor is not involved in data transfers à set up independent memory space, interrupts and DMA streams for guests</li>
<li>Requires <strong>hardware support</strong></li>
<li>i.e., the I/O device must be SR-IOV capable
We have explored this part in <a href="/notes/virtual-machines">Virtual Machines</a>.</li>
</ul>
<figure class="center">
<img src="/images/notes/Datacenter Hardware-20250528215201515.webp" style="width: 100%"   alt="Datacenter Hardware-20250528215201515" title="Datacenter Hardware-20250528215201515"/>
<figcaption><p style="text-align:center;">PF stands for private function, which is the privileged hardware function for accessing the network interface, while VF is the virtual functione exposed to the virtual machine, who then uses to access directly the hardware in a secure manner. | image from <a href="/notes/datacenter-hardware#firestoneAzureAcceleratedNetworking2018">(Firestone et al. 2018)</a></p></figcaption>
</figure>
<h4 id="io-memory-management-unit">IO Memory Management Unit<a hidden class="anchor" aria-hidden="true" href="#io-memory-management-unit">#</a></h4>
<h4 id="communicating-data-availability">Communicating data availability<a hidden class="anchor" aria-hidden="true" href="#communicating-data-availability">#</a></h4>
<p>Two main ways, interrupting and polling. We have presented these two methods in <a href="/notes/note-sull-architettura">Note sull&rsquo;architettura</a>.</p>
<p>Polling is not a bad approach within a datacenter setting (not much context switching for interrupts), that is still ok.</p>
<h4 id="slow-networking-software">Slow networking software<a hidden class="anchor" aria-hidden="true" href="#slow-networking-software">#</a></h4>
<p>Linux kernel is very slow in handling network part, it has been designed to be general, and offers sometimes too many generalizations that often hinder the latency aspect. The throughput for HW limits is much much higher. This is a strong motivator to write the best possible software for that specific hardware.</p>
<p>For example take the TCP/IP networking (Classical, see <a href="/notes/livello-di-rete">Livello di Rete</a>)
Sources of overhead in packet processing</p>
<ul>
<li>Interrupts &amp; OS scheduling</li>
<li>Kernel crossings (context switches)</li>
<li>Buffer copies</li>
<li>Packetizing &amp; checking for errors</li>
<li>Memory latency</li>
<li>Poor locality of I/O data processing</li>
<li>Memory bandwidth not an issue, why?
Example</li>
<li>64 byte Ethernet packets @ 10Gbps</li>
<li>This is 20M packets/sec</li>
<li>Naively done: 20M interrupts/sec, which is a huge overhead!</li>
</ul>
<p>There are some ideas on how to optimize the slow networking software:</p>
<ul>
<li>Move everything to user space and optimize the software</li>
<li>Offload processing to NIC hardware, as <a href="/notes/datacenter-hardware#firestoneAzureAcceleratedNetworking2018">(Firestone et al. 2018)</a> does.</li>
</ul>
<h4 id="nic-optimizations">NIC optimizations<a hidden class="anchor" aria-hidden="true" href="#nic-optimizations">#</a></h4>
<ul>
<li><strong>Checksum Offload:</strong>
<ul>
<li>NIC checks TCP and IP checksum with special hardware (CRC).</li>
</ul>
</li>
<li><strong>Large Segment Offloading (LSO) (at transmit):</strong>
<ul>
<li>NIC breaks up large TCP segments into smaller packets.</li>
</ul>
</li>
<li><strong>Automatic Header Generation:</strong></li>
<li><strong>Large Receive Offload (LRO):</strong>
<ul>
<li>Combines several incoming packets into one much larger packet for processing.</li>
</ul>
</li>
<li><strong>Header Splitting (at receive):</strong>
<ul>
<li>NIC places header &amp; payload in separate system buffers.</li>
<li><strong>Improves locality:</strong> (Why?)</li>
<li>Allows more prefetching &amp; zero copy.</li>
</ul>
</li>
<li><strong>Interrupt Coalescing</strong>: so that you don&rsquo;t need so many interrupts to handle single packets, but you can process in batches.
<ul>
<li>It is also possible to use steering based optimizations.</li>
</ul>
</li>
</ul>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p id=kanevProfilingWarehousescaleComputer2015>[1] Kanev et al. <a href="https://dl.acm.org/doi/10.1145/2749469.2750392">“Profiling a Warehouse-Scale Computer”</a> ACM  2015
 </p>
<p id=ayersAsmDBUnderstandingMitigating2019>[2] Ayers et al. <a href="https://dl.acm.org/doi/10.1145/3307650.3322234">“AsmDB: Understanding and Mitigating Front-End Stalls in Warehouse-Scale Computers”</a> ACM  2019
 </p>
<p id=firestoneAzureAcceleratedNetworking2018>[3] Firestone et al. “Azure Accelerated Networking: SmartNICs in the Public Cloud” 5th USENIX Symposium on Networked Systems Design and Implementation 2018
 </p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://flecart.github.io/tags/cloud-computing/">☁Cloud-Computing</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Datacenter Hardware on x"
            href="https://x.com/intent/tweet/?text=Datacenter%20Hardware&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fdatacenter-hardware%2f&amp;hashtags=%e2%98%81cloud-computing">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Datacenter Hardware on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fdatacenter-hardware%2f&amp;title=Datacenter%20Hardware&amp;summary=Datacenter%20Hardware&amp;source=https%3a%2f%2fflecart.github.io%2fnotes%2fdatacenter-hardware%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Datacenter Hardware on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fflecart.github.io%2fnotes%2fdatacenter-hardware%2f&title=Datacenter%20Hardware">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Datacenter Hardware on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fflecart.github.io%2fnotes%2fdatacenter-hardware%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Datacenter Hardware on whatsapp"
            href="https://api.whatsapp.com/send?text=Datacenter%20Hardware%20-%20https%3a%2f%2fflecart.github.io%2fnotes%2fdatacenter-hardware%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Datacenter Hardware on telegram"
            href="https://telegram.me/share/url?text=Datacenter%20Hardware&amp;url=https%3a%2f%2fflecart.github.io%2fnotes%2fdatacenter-hardware%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Datacenter Hardware on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Datacenter%20Hardware&u=https%3a%2f%2fflecart.github.io%2fnotes%2fdatacenter-hardware%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://flecart.github.io/">X. Angelo Huang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
