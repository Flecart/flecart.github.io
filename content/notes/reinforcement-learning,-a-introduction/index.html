<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Reinforcement Learning, a introduction | X. A. Huang&#39;s Blog</title>
<meta name="keywords" content="no-tags">
<meta name="description" content="Ripasso Prox: 40 Ripasso: May 20, 2023 Ultima modifica: April 10, 2023 2:32 PM Primo Abbozzo: January 26, 2023 10:00 PM Stato: ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ‘ðŸŒ‘ Studi Personali: No
Reinforcement Learning Introduzione Una delle idee migliori riguardanti questo campo del reinforcement learning Ã¨ il focus sul processo decisionale del singolo agente, condizionato al reward che lâ€™ambiente esterno gli dÃ  (feedback). Il setting classico di questo genere di problemi Ã¨ un caso speciale della caratterizzazione presente in lâ€™intelligenza.">
<meta name="author" content="Xuanqiang &#39;Angelo&#39; Huang">
<link rel="canonical" href="http://localhost:1313/content/notes/reinforcement-learning,-a-introduction/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/content/notes/reinforcement-learning,-a-introduction/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-WW6NN2QGKF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WW6NN2QGKF', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Reinforcement Learning, a introduction" />
<meta property="og:description" content="Ripasso Prox: 40 Ripasso: May 20, 2023 Ultima modifica: April 10, 2023 2:32 PM Primo Abbozzo: January 26, 2023 10:00 PM Stato: ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ‘ðŸŒ‘ Studi Personali: No
Reinforcement Learning Introduzione Una delle idee migliori riguardanti questo campo del reinforcement learning Ã¨ il focus sul processo decisionale del singolo agente, condizionato al reward che lâ€™ambiente esterno gli dÃ  (feedback). Il setting classico di questo genere di problemi Ã¨ un caso speciale della caratterizzazione presente in lâ€™intelligenza." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/content/notes/reinforcement-learning,-a-introduction/" />
<meta property="og:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta name="twitter:title" content="Reinforcement Learning, a introduction"/>
<meta name="twitter:description" content="Ripasso Prox: 40 Ripasso: May 20, 2023 Ultima modifica: April 10, 2023 2:32 PM Primo Abbozzo: January 26, 2023 10:00 PM Stato: ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ‘ðŸŒ‘ Studi Personali: No
Reinforcement Learning Introduzione Una delle idee migliori riguardanti questo campo del reinforcement learning Ã¨ il focus sul processo decisionale del singolo agente, condizionato al reward che lâ€™ambiente esterno gli dÃ  (feedback). Il setting classico di questo genere di problemi Ã¨ un caso speciale della caratterizzazione presente in lâ€™intelligenza."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "http://localhost:1313/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Reinforcement Learning, a introduction",
      "item": "http://localhost:1313/content/notes/reinforcement-learning,-a-introduction/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Reinforcement Learning, a introduction",
  "name": "Reinforcement Learning, a introduction",
  "description": "Ripasso Prox: 40 Ripasso: May 20, 2023 Ultima modifica: April 10, 2023 2:32 PM Primo Abbozzo: January 26, 2023 10:00 PM Stato: ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ‘ðŸŒ‘ Studi Personali: No\nReinforcement Learning Introduzione Una delle idee migliori riguardanti questo campo del reinforcement learning Ã¨ il focus sul processo decisionale del singolo agente, condizionato al reward che lâ€™ambiente esterno gli dÃ  (feedback). Il setting classico di questo genere di problemi Ã¨ un caso speciale della caratterizzazione presente in lâ€™intelligenza.",
  "keywords": [
    "no-tags"
  ],
  "articleBody": "Ripasso Prox: 40 Ripasso: May 20, 2023 Ultima modifica: April 10, 2023 2:32 PM Primo Abbozzo: January 26, 2023 10:00 PM Stato: ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ‘ðŸŒ‘ Studi Personali: No\nReinforcement Learning Introduzione Una delle idee migliori riguardanti questo campo del reinforcement learning Ã¨ il focus sul processo decisionale del singolo agente, condizionato al reward che lâ€™ambiente esterno gli dÃ  (feedback). Il setting classico di questo genere di problemi Ã¨ un caso speciale della caratterizzazione presente in lâ€™intelligenza.\nAbbiamo in questo caso un agente allâ€™interno del suo ambiente. Lâ€™agente Ã¨ in grado di interagire col suo ambiente attraverso alcune azioni ben definite, e lâ€™ambiente restituisce un feedback ad ogni azione. Lâ€™agente si regola di conseguenza, nel tentativo di massimizzare il reward che riceve.\nÃˆ da notare che questa impostazione Ã¨ molto diversa rispetto al machine learning classico, seppur si puÃ² comunque collocare al suo interno. Classifcamente nei modelli di machine learning supervised si cerca di minimizzare un errore con alcuni dataset etichettati, mentre qui non abbiamo nessuna etichetta, mentre nel unsupervised proviamo a trovare alcuni pattern nei dati, mentre qui non cerchiamo nessun pattern. Si potrebbe dire che questo sia un terzo paradigma di machine learning.\nNOTA: questi appunti riassumono concetti dai primi 4 capitoli del Sutton and Barto 2020\nUn problema classico: n-bandit Vedere N-Bandit Problem.\nSetting classico (Model Policy Reward) Quando andiamo a parlare di Reinforcement learning andiamo a considerare un setting classico di agente che interagisce con un ambiente attraverso delle azioni, e lâ€™ambiente che risponde attraverso i reward. Lâ€™agente osserva quindi lo stato (se Ã¨ full-observable vede lo stato esterno, altrimenti partially observable vede solamente parte delle informazioni dello stato dellâ€™ambiente) e insieme al reward percepito prova a eseguire delle altre azioni.\nSono particolarmente importanti quindi 3 parole chiave utili per descrivere una delle 3 frecce in immagine\nModel Il modello dellâ€™ambiente lo indichiamo anche come dinamica o sistema di transizione dellâ€™ambiente. nel modello sono definite tutte le distribuzioni di probabilitÃ  che portano uno stato a un altro: $P(sâ€™|s)$, questo possiamo dire, ossia partendo da uno stato s, quanto Ã¨ probabile finire in uno stato sâ€™ ??\nQuesto Ã¨ quello che ci dice il modello.\nPolicy La policy Ã¨ un indicatore delle azioni del singolo agente, ci dice quanto Ã¨ probabile che lâ€™agente esegua una certa azione, dato che sia sopra un certo stato s, lo indichiamo solitamente con $\\pi(a | s)$. Nel caso in cui Ã¨ una policy deterministica, nel senso che a uno stato corrisponde uno e un solo azione, potremmo scrivere qualcosa del tipo $\\pi (s) = a$\nReward Il reward descrive il feedback che lâ€™ambiente ritorna al giocatore una volta che una azione Ã¨ stata eseguita, spesso lo indichiamo in questi modi\n$$ r(s, a) \\ r(s, a, sâ€™) \\ r(s) $$\nA seconda di quanto vogliamo esprimere (quindi il reward atteso dopo aver fatto una azione da unc erto stato, il reward atteso dopo aver fatto una azione da un certo stato ed essere arrivati a un certo stao e cosÃ¬ via\nThe Value function Associamo ad ogni stato un reward $r$, si avrÃ  una history ossia una sequenza di $O, A, S$ osservazione dallâ€™ambiente, azione fatta e stato presente. Ad ogni stato avrÃ² un reward. quindi $$ v_{i}(S_{j}) = \\mathbf{E} [r_{i} + r_{i + 1} + \\dots | S_{j}] $$ Ossia se io al passo $i$ sono sullo stato $S_{j}$ la value function per quello stato Ã¨ il valore atteso dei rewards tutti successivi. Questo si puÃ² scrivere in maniera piÃ¹ compatta come $$ v_{i}(S_{j}) = \\mathbf{E} [r_{i} + v_{i+1}(S) | S_{j}] $$ Con $S$ uno stato su cui puoi essere al passo successivo.\nAll components are functions:\nPolicies: $\\pi: S \\rightarrow A$ (or to probabilities over A) Value functions: $v: S \\rightarrow R$ Models: $m: S \\rightarrow S$ and/or $r: S \\rightarrow R$ State update: $u: S \\times O \\rightarrow S$ Categorie di agenti Policy - Value categorization Value Based ha solamente value based, la sua policy Ã¨ basata sul suo valore (in modo greedy va a cercare quale sia lo stato con valore maggiore)\nPolicy based Il contrario, non ha value function, ma solamente la policy\nActor Critic Ha entrambi, ha sia policy (lâ€™attore) e il critico che cerca di aiutare\nModels Model free Se hanno policy o value, ma non hanno nessun modello sullâ€™ambiente in cui sono presenti\nModel based Hanno il modello dellâ€™ambiente, e non necessariamente hanno policy o value function.\nPrediction and control Prediction Ã¨ la capacitÃ  di sapere come sarÃ  il futuro Control Ã¨ la capacitÃ  di ottimizzare la propria value function. Solitamente sono molto legati fra di loro.\nMarkov chains Dovrebbe essere approfondito meglio in Markov Chains\nMarkov property ðŸŸ© Uno stato si puÃ² dire di godere della proprietÃ  di markov se, intuitivamente parlando, possiede giÃ  tutte le informazioni necessarie per predire lo stato successivo, ossia, supponiamo di avere la sequenza di stati $(S_n){n \\in \\N}$, allora si ha che $P(S_k | S{k-1}) = P(S_k|S_0S_1â€¦S_{k - 1})$, ossia lo stato attuale in Sk dipende solamente dallo stato precedente.\nNormalmente poche cose nel mondo reale si possono dire puramente Markoviane, perÃ² non si puÃ² negare che Ã¨ un modello molto buono di partenza come modello di decisione.\nma potremmo sempre rendere Markoviano creando una nuova variabile che ci rappresenta tutta la storia (Ã¨ qualcosa che non ho capito molto bene, ma credo si possa fare senza probbi).\nMarkov processes ðŸŸ¨- Possiamo andare a definire un processo markoviano come un insieme di stati e il modello di transizione probabilistico: $(S, P)$, una coppia di stati e tutto il modello di transizione. mi sembra di aver letto che un processo markoviano sia molto buono per studiare i moti browniani in fisica. Praticamente a random abbiamo che ogni punto si puÃ² muovere\nEsempio di processo markoviano\nMarkov Reward Processes (!!!) Quando andiamo a parlare di processo markoviano con reward indichiamo che associamo una funzione valore $V(s)$ che restituisce un certo valore a ogni stato. Di solito questo valore ci Ã¨ ignoto a noi agenti che seguiamo iil modello, quindi diventa un buon problema in questo setting provare a stimare il valore dello stato in seguito a numerose osservazioni. Solitamente non vogliamo considerare tutti i reward con lo stesso peso. Vorremmo avere anche a disposizione un parametro che ci indichi quanto siano importanti i reward subito di ora, e i reward nel futuro. Con questo indichiamo un discount factor $\\gamma$\nUn tale processo viene formalizzato tramite una quadrupla $S, P, R,\\gamma$, con s stati possibili, P il modello di transizoine e R la funzione che ritorna il reward per ogni stato.\nSolitamente viene definito state value function:\n$$ V(s) = \\mathbb{E}[R_t + \\gamma R_{t + 1} + \\gamma^2 R_{t + 2} + â€¦ | s = s _{t}] $$\nLa parte dentro il valore atteso Ã¨ solitamente indicata con $G_t$.\nMetodi di estimazione della funzione valore:\nAbbiamo abbastanza metodi per stimare il valore della funzione: metodi di sampling, metodi diretti (analitici) e metodi basati su programmazione dinamica.\nRiguardo i metodi di sampling questi sono i piÃ¹ dinamici, nel senso che permettono lâ€™applicazione a piÃ¹ problemi possibili, in generale hanno una precisione che va nellâ€™ordine dell $\\dfrac{1}{\\sqrt {n}}$ anche se non so su quali basi in particolare.\nI metodi diretti sono leggermente piÃ¹ lenti, perchÃ© si tratta di risolvere lâ€™inversa della matrice, cosa che va in $O(n^3)$. Il motivo di questo Ã¨ che possiamo sfruttare la proprietÃ  di V\nOssia possiamo dire che\n$$ V_k(s_t) = \\mathbb{E}[R_t + \\gamma G_{t + 1} | s = s {t}] = R(s) + \\gamma \\sum{sâ€™}P(sâ€™|s)V_{k -1}(sâ€™) $$\nQuesta osservazione permette di sviluppare un algoritmo iterativo per stivare il V fino a convergenza (sul perchÃ© converge sicuramente guardare altro, prolly idea degli operatori di bellman puÃ² essere utile)\nAlgoritmo iterativo per valutazione della MRP\nSoluzione analitica:\nUtilizzando la stessa proprietÃ  (solamente ora scritta in modo analitico, possiamo risolverlo come se fosse una matrice\nSoluzione analitica\nMarkov Decision Process Questo Ã¨ molto simile alla MRP, solo che ora introduciamo una policy, ossia una funzione che ci dica quanto Ã¨ probabile compiere una certa azione in un certo stato\n$(S, A, P, R, \\gamma)$, ossia ora abbiamo sia stato, sia azione possibile e la funzione di transizione deve contare entrambi: $P(\\cdot | s, a)$, mentre le reward sono ancora come prima.\nS lâ€™insieme di stati possibili A lâ€™insieme di azioni possibili P probabilitÃ  di raggiungereun certo stato, dato uno stato inizial e una azione R reward di uno stato gamma: decadimento del reward. Ãˆ da notare che se possediamo una policy, allora possiamo ridurci al caso di Markov Reward Process, infatti possiamo dire che\n$$ R^\\pi(s) = \\sum_{a \\in A}\\pi(a | s)R(s) \\\nP^\\pi(sâ€™|s) = \\sum_{a \\in A} \\pi(a | s) P(sâ€™|s, a) $$\nQuindi data una policy possiamo utilizzare gli argomenti fatti di sopra e riuscire a dare una valutazione di essa\nAlgoritmo iterativo DP per policy evaluation\nPolicy Search Cerchiamo ora la policy migliore possibile da applicare a un MDP, questo Ã¨ il problema del policy control ora che sappiamo come fare policy evalutation Ã¨ il momento giusto per introdurre soluzioni a questo problema.\nUna soluzione naÃ¯ve Ã¨ semplicemente enumerare tutte le policy e utilizzare lâ€™algoritmo di policy evaluation, poi andare a vedere quale sia la migliore. Questo Ã¨ molto dispendioso perchÃ© assumento che posso applicare tutto lâ€™insieme di azioni a tutti gli stati ho potenzialmente $|S|^{|A|}$ policy possibili,, che sono troppi , e troppo brutti.\nÃˆ bene raggiunti questo punto provare a dare alcune definizioni utili.\nDefinitions: state-action-value, optimal value policy Sia $\\pi$ una policy e $V^\\pi$ la evaluation di quella policy, allora possiamo andare a definire la state-action-value function in questo modo:\n$$ Q(s, a) = R(s, a) + \\gamma \\sum_{sâ€™} P(sâ€™|s, a)V^\\pi(sâ€™) $$\nossia ci dice piÃ¹ o meno il valore atteso dellâ€™azione a un certo stato!\nPossiamo anche definire la policy migliore:\n$$ \\pi^*(s) = argmax_{\\pi} V^\\pi(s) $$\nOssia Ã¨ la policy che rende massimo il valore in qualunque stato!\nPolicy iteration and his monotonicity Una volta creato una policy iteration, Ã¨ una cosa molto sensata andare a definire una nuova policy $\\pi_{k + 1}$ definita in questo modo:\n$$ \\forall s, \\pi_{k + 1}(s) = argmax_a Q(s, a) $$\nOssia andiamo proprio a crearci una nuova policy, cercando di rendere maggiore possibile il valore atteso a fare una certa azione a uno stato! Riusciremo a dimostrare che $\\forall s, V^{\\pi_{k + 1}}(s) \\geq V^{\\pi_{k}}(s)$\nDimostrazione dal sutton e barto\nlâ€™idea principalmente Ã¨ prendere sempre il massimo volta dopo volta, e dimostrarlo per induzione in praticaâ€¦ Anche non ho capito come formalizzare e non ho capito se posso trarne vantaggi didattici nella formalizzazione di questa merda\nValue Iteration Lâ€™idea di value iteration Ã¨ sostituirla subito, cioÃ¨ non stare a sviluppare fino in fondo la value evaluation, ma aggiornare la policy subito dopo.\nPseudocodice value iteration\nBellman operator Slide Si puÃ² dimostrare che questo operatore Ã¨ una contrazione, quindi value iteration converge qualunque sia il punto di partenza\nCon questo operatore, possiamo anche riscrivere in modo migliore la policy evaluation\nSlide\nProof of contraction of bellman operator\n",
  "wordCount" : "1833",
  "inLanguage": "en",
  "image": "http://localhost:1313/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xuanqiang 'Angelo' Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/content/notes/reinforcement-learning,-a-introduction/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "X. A. Huang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon-192x192.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="X. A. Huang&#39;s Blog (Alt + H)">X. A. Huang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Reinforcement Learning, a introduction
    </h1>
    <div class="post-meta">9 min&nbsp;Â·&nbsp;Xuanqiang &#39;Angelo&#39; Huang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#reinforcement-learning" aria-label="Reinforcement Learning">Reinforcement Learning</a><ul>
                        
                <li>
                    <a href="#introduzione" aria-label="Introduzione">Introduzione</a><ul>
                        
                <li>
                    <a href="#un-problema-classico-n-bandit" aria-label="Un problema classico: n-bandit">Un problema classico: n-bandit</a></li>
                <li>
                    <a href="#setting-classico-model-policy-reward" aria-label="Setting classico (Model Policy Reward)">Setting classico (Model Policy Reward)</a><ul>
                        
                <li>
                    <a href="#model" aria-label="Model">Model</a></li>
                <li>
                    <a href="#policy" aria-label="Policy">Policy</a></li>
                <li>
                    <a href="#reward" aria-label="Reward">Reward</a></li>
                <li>
                    <a href="#the-value-function" aria-label="The Value function">The Value function</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#categorie-di-agenti" aria-label="Categorie di agenti">Categorie di agenti</a><ul>
                        
                <li>
                    <a href="#policy---value-categorization" aria-label="Policy - Value categorization">Policy - Value categorization</a><ul>
                        
                <li>
                    <a href="#value-based" aria-label="Value Based">Value Based</a></li>
                <li>
                    <a href="#policy-based" aria-label="Policy based">Policy based</a></li>
                <li>
                    <a href="#actor-critic" aria-label="Actor Critic">Actor Critic</a></li></ul>
                </li>
                <li>
                    <a href="#models" aria-label="Models">Models</a><ul>
                        
                <li>
                    <a href="#model-free" aria-label="Model free">Model free</a></li>
                <li>
                    <a href="#model-based" aria-label="Model based">Model based</a></li></ul>
                </li>
                <li>
                    <a href="#prediction-and-control" aria-label="Prediction and control">Prediction and control</a></li></ul>
                </li>
                <li>
                    <a href="#markov-chains" aria-label="Markov chains">Markov chains</a><ul>
                        
                <li>
                    <a href="#markov-property-" aria-label="Markov property ðŸŸ©">Markov property ðŸŸ©</a></li>
                <li>
                    <a href="#markov-processes--" aria-label="Markov processes ðŸŸ¨-">Markov processes ðŸŸ¨-</a></li>
                <li>
                    <a href="#markov-reward-processes-" aria-label="Markov Reward Processes (!!!)">Markov Reward Processes (!!!)</a></li>
                <li>
                    <a href="#markov-decision-process" aria-label="Markov Decision Process">Markov Decision Process</a></li></ul>
                </li>
                <li>
                    <a href="#policy-search" aria-label="Policy Search">Policy Search</a><ul>
                        
                <li>
                    <a href="#definitions-state-action-value-optimal-value-policy" aria-label="Definitions: state-action-value, optimal value policy">Definitions: state-action-value, optimal value policy</a></li>
                <li>
                    <a href="#policy-iteration-and-his-monotonicity" aria-label="Policy iteration and his monotonicity">Policy iteration and his monotonicity</a></li>
                <li>
                    <a href="#value-iteration" aria-label="Value Iteration">Value Iteration</a></li>
                <li>
                    <a href="#bellman-operator" aria-label="Bellman operator">Bellman operator</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Ripasso Prox: 40
Ripasso: May 20, 2023
Ultima modifica: April 10, 2023 2:32 PM
Primo Abbozzo: January 26, 2023 10:00 PM
Stato: ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ‘ðŸŒ‘
Studi Personali: No</p>
<h1 id="reinforcement-learning">Reinforcement Learning<a hidden class="anchor" aria-hidden="true" href="#reinforcement-learning">#</a></h1>
<h2 id="introduzione">Introduzione<a hidden class="anchor" aria-hidden="true" href="#introduzione">#</a></h2>
<p>Una delle idee migliori riguardanti questo campo del reinforcement learning Ã¨ il focus sul processo decisionale del singolo agente, condizionato al reward che lâ€™ambiente esterno gli dÃ  (feedback). Il setting classico di questo genere di problemi Ã¨ un caso speciale della caratterizzazione presente in <a href="//notes/l%E2%80%99intelligenza">lâ€™intelligenza</a>.</p>
<p>Abbiamo in questo caso un agente allâ€™interno del suo ambiente. Lâ€™agente Ã¨ in grado di interagire col suo ambiente attraverso alcune azioni ben definite, e lâ€™ambiente restituisce un feedback ad ogni azione. Lâ€™agente si regola di conseguenza, nel tentativo di massimizzare il reward che riceve.</p>
<p>Ãˆ da notare che questa impostazione Ã¨ molto diversa rispetto al machine learning classico, seppur si puÃ² comunque collocare al suo interno. Classifcamente nei modelli di machine learning supervised si cerca di minimizzare un errore con alcuni dataset etichettati, mentre qui non abbiamo nessuna etichetta, mentre nel unsupervised proviamo a trovare alcuni pattern nei dati, mentre qui non cerchiamo nessun pattern. Si potrebbe dire che questo sia un terzo paradigma di machine learning.</p>
<p>NOTA: questi appunti riassumono concetti dai primi 4 capitoli del Sutton and Barto 2020</p>
<h3 id="un-problema-classico-n-bandit">Un problema classico: n-bandit<a hidden class="anchor" aria-hidden="true" href="#un-problema-classico-n-bandit">#</a></h3>
<p>Vedere <a href="//notes/n-bandit-problem">N-Bandit Problem</a>.</p>
<h3 id="setting-classico-model-policy-reward">Setting classico (Model Policy Reward)<a hidden class="anchor" aria-hidden="true" href="#setting-classico-model-policy-reward">#</a></h3>
<p>Quando andiamo a parlare di Reinforcement learning andiamo a considerare un setting classico di agente che interagisce con un ambiente attraverso delle azioni, e lâ€™ambiente che risponde attraverso i reward. Lâ€™agente osserva quindi lo stato (se Ã¨ full-observable vede lo stato esterno, altrimenti partially observable vede solamente parte delle informazioni dello stato dellâ€™ambiente) e insieme al reward percepito prova a eseguire delle altre azioni.</p>
<img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled">
<p>Sono particolarmente importanti quindi 3 parole chiave utili per descrivere una delle 3 frecce in immagine</p>
<h4 id="model">Model<a hidden class="anchor" aria-hidden="true" href="#model">#</a></h4>
<p>Il modello dell&rsquo;ambiente lo indichiamo anche come dinamica o sistema di transizione dellâ€™ambiente. nel modello sono definite tutte le distribuzioni di probabilitÃ  che portano uno stato a un altro: $P(s&rsquo;|s)$, questo possiamo dire, ossia partendo da uno stato s, quanto Ã¨ probabile finire in uno stato sâ€™ ??</p>
<p>Questo Ã¨ quello che ci dice il modello.</p>
<h4 id="policy">Policy<a hidden class="anchor" aria-hidden="true" href="#policy">#</a></h4>
<p>La policy Ã¨ un indicatore delle azioni del singolo agente, ci dice quanto Ã¨ probabile che lâ€™agente esegua una certa azione, dato che sia sopra un certo stato s, lo indichiamo solitamente con $\pi(a | s)$. Nel caso in cui Ã¨ una policy deterministica, nel senso che a uno stato corrisponde uno e un solo azione, potremmo scrivere qualcosa del tipo $\pi (s) = a$</p>
<h4 id="reward">Reward<a hidden class="anchor" aria-hidden="true" href="#reward">#</a></h4>
<p>Il reward descrive il feedback che lâ€™ambiente ritorna al giocatore una volta che una azione Ã¨ stata eseguita, spesso lo indichiamo in questi modi</p>
<p>$$
r(s, a) \ r(s, a, s&rsquo;) \ r(s)
$$</p>
<p>A seconda di quanto vogliamo esprimere (quindi il reward atteso dopo aver fatto una azione da unc erto stato, il reward atteso dopo aver fatto una azione da un certo stato ed essere arrivati a un certo stao e cosÃ¬ via</p>
<h4 id="the-value-function">The Value function<a hidden class="anchor" aria-hidden="true" href="#the-value-function">#</a></h4>
<p>Associamo ad ogni stato un reward $r$, si avrÃ  una <strong>history</strong> ossia una sequenza di $O, A, S$ osservazione dall&rsquo;ambiente, azione fatta e stato presente. Ad ogni stato avrÃ² un reward.
quindi
$$
v_{i}(S_{j}) = \mathbf{E} [r_{i} + r_{i + 1} + \dots | S_{j}]
$$
Ossia se io al passo $i$ sono sullo stato $S_{j}$ la value function per quello stato Ã¨ il valore atteso dei rewards tutti successivi.
Questo si puÃ² scrivere in maniera piÃ¹ compatta come
$$
v_{i}(S_{j}) = \mathbf{E} [r_{i} + v_{i+1}(S) | S_{j}]
$$
Con $S$ uno stato su cui puoi essere al passo successivo.</p>
<p>All components are functions:</p>
<ul>
<li>Policies: $\pi: S \rightarrow A$ (or to probabilities over A)</li>
<li>Value functions: $v: S \rightarrow R$</li>
<li>Models: $m: S \rightarrow S$ and/or $r: S \rightarrow R$</li>
<li>State update: $u: S \times O \rightarrow S$</li>
</ul>
<h2 id="categorie-di-agenti">Categorie di agenti<a hidden class="anchor" aria-hidden="true" href="#categorie-di-agenti">#</a></h2>
<h3 id="policy---value-categorization">Policy - Value categorization<a hidden class="anchor" aria-hidden="true" href="#policy---value-categorization">#</a></h3>
<h4 id="value-based">Value Based<a hidden class="anchor" aria-hidden="true" href="#value-based">#</a></h4>
<p>ha solamente value based, la sua policy Ã¨ basata sul suo valore (in modo greedy va a cercare quale sia lo stato con valore maggiore)</p>
<h4 id="policy-based">Policy based<a hidden class="anchor" aria-hidden="true" href="#policy-based">#</a></h4>
<p>Il contrario, non ha value function, ma solamente la policy</p>
<h4 id="actor-critic">Actor Critic<a hidden class="anchor" aria-hidden="true" href="#actor-critic">#</a></h4>
<p>Ha entrambi, ha sia policy (l&rsquo;attore) e il critico che cerca di aiutare</p>
<h3 id="models">Models<a hidden class="anchor" aria-hidden="true" href="#models">#</a></h3>
<h4 id="model-free">Model free<a hidden class="anchor" aria-hidden="true" href="#model-free">#</a></h4>
<p>Se hanno policy o value, ma non hanno <strong>nessun modello sull&rsquo;ambiente</strong> in cui sono presenti</p>
<h4 id="model-based">Model based<a hidden class="anchor" aria-hidden="true" href="#model-based">#</a></h4>
<p>Hanno il modello dell&rsquo;ambiente, e non necessariamente hanno policy o value function.</p>
<h3 id="prediction-and-control">Prediction and control<a hidden class="anchor" aria-hidden="true" href="#prediction-and-control">#</a></h3>
<p><strong>Prediction</strong> Ã¨ la capacitÃ  di sapere come sarÃ  il futuro
<strong>Control</strong> Ã¨ la capacitÃ  di ottimizzare la propria value function.
Solitamente sono molto legati fra di loro.</p>
<h2 id="markov-chains">Markov chains<a hidden class="anchor" aria-hidden="true" href="#markov-chains">#</a></h2>
<p>Dovrebbe essere approfondito meglio in <a href="//notes/markov-chains">Markov Chains</a></p>
<h3 id="markov-property-">Markov property ðŸŸ©<a hidden class="anchor" aria-hidden="true" href="#markov-property-">#</a></h3>
<p>Uno stato si puÃ² dire di godere della proprietÃ  di markov se, intuitivamente parlando, possiede giÃ  tutte le informazioni necessarie per predire lo stato successivo, ossia, supponiamo di avere la sequenza di stati $(S_n)<em>{n \in \N}$, allora si ha che $P(S_k | S</em>{k-1}) = P(S_k|S_0S_1&hellip;S_{k - 1})$, ossia lo stato attuale in Sk dipende solamente dallo stato precedente.</p>
<p>Normalmente poche cose nel mondo reale si possono dire puramente Markoviane, perÃ² non si puÃ² negare che Ã¨ un modello molto buono di partenza come modello di decisione.</p>
<p>ma potremmo sempre rendere Markoviano creando una nuova variabile che ci rappresenta tutta la storia (Ã¨ qualcosa che non ho capito molto bene, ma credo si possa fare senza probbi).</p>
<h3 id="markov-processes--">Markov processes ðŸŸ¨-<a hidden class="anchor" aria-hidden="true" href="#markov-processes--">#</a></h3>
<p>Possiamo andare a definire un processo markoviano come un insieme di stati e il modello di transizione probabilistico: $(S, P)$, una coppia di stati e tutto il modello di transizione. mi sembra di aver letto che un processo markoviano sia molto buono per studiare i moti browniani in fisica. Praticamente a random abbiamo che ogni punto si puÃ² muovere</p>
<ul>
<li>
<p>Esempio di processo markoviano</p>
  <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 1.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 1">
</li>
</ul>
<h3 id="markov-reward-processes-">Markov Reward Processes (!!!)<a hidden class="anchor" aria-hidden="true" href="#markov-reward-processes-">#</a></h3>
<p>Quando andiamo a parlare di processo markoviano con reward indichiamo che associamo una funzione valore $V(s)$ che restituisce un certo valore a ogni stato. Di solito questo valore ci Ã¨ ignoto a noi agenti che seguiamo iil modello, quindi diventa un buon problema in questo setting provare a stimare il valore dello stato in seguito a numerose osservazioni. Solitamente non vogliamo considerare tutti i reward con lo stesso peso. Vorremmo avere anche a disposizione un parametro che ci indichi quanto siano importanti i reward subito di ora, e i reward nel futuro. Con questo indichiamo un <strong>discount factor</strong> $\gamma$</p>
<p>Un tale processo viene formalizzato tramite una quadrupla   $S, P, R,\gamma$, con s stati possibili, P il modello di transizoine e R la funzione che ritorna il reward per ogni stato.</p>
<p>Solitamente viene definito <strong>state value function:</strong></p>
<p>$$
V(s) = \mathbb{E}[R_t + \gamma R_{t + 1} + \gamma^2 R_{t + 2} + &hellip; |  s = s _{t}]
$$</p>
<p>La parte dentro il valore atteso Ã¨ solitamente indicata con $G_t$.</p>
<p><strong>Metodi di estimazione della funzione valore:</strong></p>
<p>Abbiamo abbastanza metodi per stimare il valore della funzione: metodi di sampling, metodi diretti (analitici) e metodi basati su programmazione dinamica.</p>
<p>Riguardo i metodi di sampling questi sono i piÃ¹ dinamici, nel senso che permettono lâ€™applicazione a piÃ¹ problemi possibili, in generale hanno una precisione che va nellâ€™ordine dell $\dfrac{1}{\sqrt {n}}$ anche se non so su quali basi in particolare.</p>
<p>I metodi diretti sono leggermente piÃ¹ lenti, perchÃ© si tratta di risolvere lâ€™inversa della matrice, cosa che va in $O(n^3)$. Il motivo di questo Ã¨ che possiamo <strong>sfruttare la proprietÃ  di V</strong></p>
<p>Ossia possiamo dire che</p>
<p>$$
V_k(s_t) = \mathbb{E}[R_t + \gamma G_{t + 1}  |  s = s <em>{t}] = R(s)  + \gamma \sum</em>{s&rsquo;}P(s&rsquo;|s)V_{k -1}(s&rsquo;)
$$</p>
<p>Questa osservazione permette di sviluppare un algoritmo iterativo per stivare il V fino a convergenza (sul perchÃ© converge sicuramente guardare altro, prolly idea degli operatori di bellman puÃ² essere utile)</p>
<ul>
<li>
<p>Algoritmo iterativo per valutazione della MRP</p>
  <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 2.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 2">
</li>
</ul>
<p><strong>Soluzione analitica:</strong></p>
<p>Utilizzando la stessa proprietÃ  (solamente ora scritta in modo analitico, possiamo risolverlo come se fosse una matrice</p>
<ul>
<li>
<p>Soluzione analitica</p>
  <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 3.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 3">
</li>
</ul>
<h3 id="markov-decision-process">Markov Decision Process<a hidden class="anchor" aria-hidden="true" href="#markov-decision-process">#</a></h3>
<p>Questo Ã¨ molto simile alla MRP, solo che ora introduciamo una policy, ossia una funzione che ci dica quanto Ã¨ probabile compiere una certa azione in un certo stato</p>
<p>$(S, A, P, R, \gamma)$, ossia ora abbiamo sia stato, sia azione possibile e la funzione di transizione deve contare entrambi: $P(\cdot | s, a)$, mentre le reward sono ancora come prima.</p>
<ul>
<li>S lâ€™insieme di stati possibili</li>
<li>A lâ€™insieme di azioni possibili</li>
<li>P probabilitÃ  di raggiungereun certo stato, dato uno stato inizial e una azione</li>
<li>R reward di uno stato</li>
<li>gamma: decadimento del reward.</li>
</ul>
<p>Ãˆ da notare che se possediamo una policy, allora possiamo ridurci al caso di Markov Reward Process, infatti possiamo dire che</p>
<p>$$
R^\pi(s) = \sum_{a \in A}\pi(a | s)R(s) \</p>
<p>P^\pi(s&rsquo;|s) = \sum_{a \in A}  \pi(a | s) P(s&rsquo;|s, a)
$$</p>
<p>Quindi data una policy possiamo utilizzare gli argomenti fatti di sopra e <strong>riuscire a dare una valutazione di essa</strong></p>
<ul>
<li>
<p>Algoritmo iterativo DP per policy evaluation</p>
  <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 4.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 4">
</li>
</ul>
<h2 id="policy-search">Policy Search<a hidden class="anchor" aria-hidden="true" href="#policy-search">#</a></h2>
<p>Cerchiamo ora la policy migliore possibile da applicare a un MDP, questo Ã¨ il problema del <strong>policy control</strong> ora che sappiamo come fare <strong>policy evalutation</strong> Ã¨ il momento giusto per introdurre soluzioni a questo problema.</p>
<p>Una soluzione <strong>naÃ¯ve</strong> Ã¨ semplicemente enumerare tutte le policy e utilizzare lâ€™algoritmo di policy evaluation, poi andare a vedere quale sia la migliore. Questo Ã¨ molto dispendioso perchÃ© assumento che posso applicare tutto lâ€™insieme di azioni a tutti gli stati ho potenzialmente $|S|^{|A|}$ policy possibili,, che sono troppi , e troppo brutti.</p>
<p>Ãˆ bene raggiunti questo punto provare a dare alcune definizioni utili.</p>
<h3 id="definitions-state-action-value-optimal-value-policy">Definitions: state-action-value, optimal value policy<a hidden class="anchor" aria-hidden="true" href="#definitions-state-action-value-optimal-value-policy">#</a></h3>
<p>Sia $\pi$ una policy e $V^\pi$ la evaluation di quella policy, allora possiamo andare a definire la <strong>state-action-value</strong> function in questo modo:</p>
<p>$$
Q(s, a) = R(s, a) + \gamma \sum_{s&rsquo;} P(s&rsquo;|s, a)V^\pi(s&rsquo;)
$$</p>
<p>ossia ci dice piÃ¹ o meno il valore atteso dellâ€™azione a un certo stato!</p>
<p>Possiamo anche definire la policy migliore:</p>
<p>$$
\pi^*(s) = argmax_{\pi} V^\pi(s)
$$</p>
<p>Ossia Ã¨ la policy che rende massimo il valore in qualunque stato!</p>
<h3 id="policy-iteration-and-his-monotonicity">Policy iteration and his monotonicity<a hidden class="anchor" aria-hidden="true" href="#policy-iteration-and-his-monotonicity">#</a></h3>
<p>Una volta creato una policy iteration, Ã¨ una cosa molto sensata andare a definire una nuova policy $\pi_{k + 1}$ definita in questo modo:</p>
<p>$$
\forall s, \pi_{k + 1}(s) = argmax_a Q(s, a)
$$</p>
<p>Ossia andiamo proprio a crearci una nuova policy, cercando di rendere maggiore possibile il valore atteso a fare una certa azione a uno stato! Riusciremo a dimostrare che $\forall s, V^{\pi_{k + 1}}(s) \geq V^{\pi_{k}}(s)$</p>
<ul>
<li>
<p>Dimostrazione dal sutton e barto</p>
  <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 5.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 5">
<p>lâ€™idea principalmente Ã¨ prendere sempre il massimo volta dopo volta, e dimostrarlo per induzione in praticaâ€¦ Anche non ho capito come formalizzare e non ho capito se posso trarne vantaggi didattici nella formalizzazione di questa merda</p>
</li>
</ul>
<h3 id="value-iteration">Value Iteration<a hidden class="anchor" aria-hidden="true" href="#value-iteration">#</a></h3>
<p>Lâ€™idea di value iteration Ã¨ sostituirla subito, cioÃ¨ non stare a sviluppare fino in fondo la value evaluation, ma aggiornare la policy subito dopo.</p>
<ul>
<li>
<p>Pseudocodice value iteration</p>
  <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 6.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 6">
</li>
</ul>
<h3 id="bellman-operator">Bellman operator<a hidden class="anchor" aria-hidden="true" href="#bellman-operator">#</a></h3>
<ul>
<li>Slide</li>
</ul>
<img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 7.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 7">
<p>Si puÃ² dimostrare che questo operatore Ã¨ una contrazione, quindi value iteration converge qualunque sia il punto di partenza</p>
<p>Con questo operatore, possiamo anche riscrivere in modo migliore la <strong>policy evaluation</strong></p>
<ul>
<li>
<p>Slide</p>
  <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 8.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 8">
</li>
<li>
<p>Proof of contraction of bellman operator</p>
  <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 9.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 9"></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/no-tags/">No-Tags</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on x"
            href="https://x.com/intent/tweet/?text=Reinforcement%20Learning%2c%20a%20introduction&amp;url=http%3a%2f%2flocalhost%3a1313%2fcontent%2fnotes%2freinforcement-learning%2c-a-introduction%2f&amp;hashtags=no-tags">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fcontent%2fnotes%2freinforcement-learning%2c-a-introduction%2f&amp;title=Reinforcement%20Learning%2c%20a%20introduction&amp;summary=Reinforcement%20Learning%2c%20a%20introduction&amp;source=http%3a%2f%2flocalhost%3a1313%2fcontent%2fnotes%2freinforcement-learning%2c-a-introduction%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fcontent%2fnotes%2freinforcement-learning%2c-a-introduction%2f&title=Reinforcement%20Learning%2c%20a%20introduction">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fcontent%2fnotes%2freinforcement-learning%2c-a-introduction%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on whatsapp"
            href="https://api.whatsapp.com/send?text=Reinforcement%20Learning%2c%20a%20introduction%20-%20http%3a%2f%2flocalhost%3a1313%2fcontent%2fnotes%2freinforcement-learning%2c-a-introduction%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on telegram"
            href="https://telegram.me/share/url?text=Reinforcement%20Learning%2c%20a%20introduction&amp;url=http%3a%2f%2flocalhost%3a1313%2fcontent%2fnotes%2freinforcement-learning%2c-a-introduction%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning, a introduction on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Reinforcement%20Learning%2c%20a%20introduction&u=http%3a%2f%2flocalhost%3a1313%2fcontent%2fnotes%2freinforcement-learning%2c-a-introduction%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://localhost:1313/">X. A. Huang&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
